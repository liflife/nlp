Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

Speech and Language Processing

An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition

Third Edition draft

Daniel Jurafsky

Stanford University

James H. Martin

University of Colorado at Boulder

Copyright c(cid:13)2018

Draft of September 23, 2018. Comments and typos welcome!

Summary of Contents

1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2 Regular Expressions, Text Normalization, Edit Distance . . . . . . . . . 10
3 N-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4 Naive Bayes and Sentiment Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . 63
5 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
6 Vector Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7 Neural Networks and Neural Language Models . . . . . . . . . . . . . . . . . 131
8 Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
9 Sequence Processing with Recurrent Networks . . . . . . . . . . . . . . . . . . 177
10 Formal Grammars of English . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
11 Syntactic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
12 Statistical Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
13 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
14 The Representation of Sentence Meaning . . . . . . . . . . . . . . . . . . . . . . . 295
15 Computational Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
16 Semantic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
17 Information Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
18 Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
19 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 378
20 Coreference Resolution and Entity Linking . . . . . . . . . . . . . . . . . . . . . 399
21 Discourse Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
22 Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
23 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
24 Dialog Systems and Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
25 Advanced Dialog Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
26 Speech Recognition and Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461

A Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
B Spelling Correction and the Noisy Channel . . . . . . . . . . . . . . . . . . . . . 480
C WordNet: Word Relations, Senses, and Disambiguation . . . . . . . . . 493
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
Subject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551

Appendix

463

2

Contents

1 Introduction

2 Regular Expressions, Text Normalization, Edit Distance

2.1
Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Words
.
.
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
2.3
Corpora
.
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
2.4
Text Normalization . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Minimum Edit Distance . . . . . . . . . . . . . . . . . . . . . . .
2.6
Summary .
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
Exercises
.
.
.
.
. . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . .

3 N-gram Language Models

3.1
N-Grams .
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.2
Evaluating Language Models . . . . . . . . . . . . . . . . . . . .
3.3
Generalization and Zeros
. . . . . . . . . . . . . . . . . . . . . .
3.4
Smoothing .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.5
Kneser-Ney Smoothing . . . . . . . . . . . . . . . . . . . . . . .
3.6
The Web and Stupid Backoff
. . . . . . . . . . . . . . . . . . . .
3.7
Advanced: Perplexity’s Relation to Entropy . . . . . . . . . . . .
3.8
Summary .
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
Exercises
.
.
.
.
. . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . .

4 Naive Bayes and Sentiment Classiﬁcation

4.1
Naive Bayes Classiﬁers . . . . . . . . . . . . . . . . . . . . . . .
4.2
Training the Naive Bayes Classiﬁer . . . . . . . . . . . . . . . . .
4.3 Worked example . .
. . . . . . . . . . . . . . . . . . . . . . . . .
4.4
Optimizing for Sentiment Analysis . . . . . . . . . . . . . . . . .
4.5
Naive Bayes for other text classiﬁcation tasks
. . . . . . . . . . .
4.6
Naive Bayes as a Language Model
. . . . . . . . . . . . . . . . .
4.7
Evaluation: Precision, Recall, F-measure . . . . . . . . . . . . . .
4.8
Test sets and Cross-validation . . . . . . . . . . . . . . . . . . . .
4.9
Statistical Signiﬁcance Testing . . . . . . . . . . . . . . . . . . .
4.10 Advanced: Feature Selection . . . . . . . . . . . . . . . . . . . .
4.11 Summary .
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
Exercises
.
.
.
.
. . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . .

5 Logistic Regression

5.1
Classiﬁcation: the sigmoid . . . . . . . . . . . . . . . . . . . . .
5.2
Learning in Logistic Regression . . . . . . . . . . . . . . . . . . .
5.3
The cross-entropy loss function . . . . . . . . . . . . . . . . . . .
5.4
Gradient Descent
.
. . . . . . . . . . . . . . . . . . . . . . . . .
5.5
Regularization . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Multinomial logistic regression . . . . . . . . . . . . . . . . . . .
5.7
Interpreting models
. . . . . . . . . . . . . . . . . . . . . . . . .
5.8
Advanced: Deriving the Gradient Equation . . . . . . . . . . . . .
5.9
Summary .
.
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .

3

9

10

11
19
21
22
30
34
34
35

37

38
43
45
49
53
55
56
60
60
61

63

65
67
68
70
71
72
73
76
77
79
79
80
81

82

83
87
88
89
93
95
97
98
99

4 CONT EN T S

Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . .
99
Exercises
.
.
.
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 100

6 Vector Semantics

101

6.1
Lexical Semantics . .
. . . . . . . . . . . . . . . . . . . . . . . . 102
6.2
Vector Semantics
. .
. . . . . . . . . . . . . . . . . . . . . . . . 106
6.3 Words and Vectors . . .
. . . . . . . . . . . . . . . . . . . . . . . 108
6.4
Cosine for measuring similarity . . . . . . . . . . . . . . . . . . . 111
6.5
TF-IDF: Weighing terms in the vector
. . . . . . . . . . . . . . . 112
6.6
Applications of the tf-idf vector model
. . . . . . . . . . . . . . . 115
6.7
Optional: Pointwise Mutual Information (PMI) . . . . . . . . . . . 116
6.8 Word2vec .
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . 118
6.9
Visualizing Embeddings . . . . . . . . . . . . . . . . . . . . . . . 123
6.10 Semantic properties of embeddings . . . . . . . . . . . . . . . . . 123
6.11 Bias and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . 125
6.12 Evaluating Vector Models . . . . . . . . . . . . . . . . . . . . . . 126
6.13 Summary .
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . 127
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 127
Exercises
.
.
.
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 130

7 Neural Networks and Neural Language Models

131

7.1
Units .
.
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 132
7.2
The XOR problem . .
. . . . . . . . . . . . . . . . . . . . . . . . 134
7.3
Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . 137
7.4
Training Neural Nets
. . . . . . . . . . . . . . . . . . . . . . . . 140
7.5
Neural Language Models
. . . . . . . . . . . . . . . . . . . . . . 145
7.6
Summary .
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . 149
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 149

8 Part-of-Speech Tagging

8.1
(Mostly) English Word Classes . . . . . . . . . . . . . . . . . . . 151
8.2
The Penn Treebank Part-of-Speech Tagset
. . . . . . . . . . . . . 154
8.3
Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . 156
8.4
HMM Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . 157
8.5 Maximum Entropy Markov Models . . . . . . . . . . . . . . . . . 167
8.6
Bidirectionality . .
. . . . . . . . . . . . . . . . . . . . . . . . . 171
8.7
Part-of-Speech Tagging for Other Languages . . . . . . . . . . . . 172
8.8
Summary .
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . 173
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 174
Exercises
.
.
.
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 175

151

177

194

9 Sequence Processing with Recurrent Networks

9.1
Simple Recurrent Networks . . . . . . . . . . . . . . . . . . . . . 177
9.2
Applications of RNNs . . . . . . . . . . . . . . . . . . . . . . . . 184
9.3
Deep Networks: Stacked and Bidirectional RNNs
. . . . . . . . . 186
9.4 Managing Context in RNNs: LSTMs and GRUs . . . . . . . . . . 188
9.5 Words, Characters and Byte-Pairs . . . . . . . . . . . . . . . . . . 192
9.6
Summary .
.
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . 193

10 Formal Grammars of English

10.1 Constituency .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 194
10.2 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 195

12 Statistical Parsing

12.1 Probabilistic Context-Free Grammars . . . . . . . . . . . . . . . . 238
12.2 Probabilistic CKY Parsing of PCFGs . . . . . . . . . . . . . . . . 242
12.3 Ways to Learn PCFG Rule Probabilities
. . . . . . . . . . . . . . 243
12.4 Problems with PCFGs . . . . . . . . . . . . . . . . . . . . . . . . 245
12.5
Improving PCFGs by Splitting Non-Terminals . . . . . . . . . . . 248
12.6 Probabilistic Lexicalized CFGs . . . . . . . . . . . . . . . . . . . 250
12.7 Probabilistic CCG Parsing . . . . . . . . . . . . . . . . . . . . . . 255
12.8 Evaluating Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . 263
12.9 Human Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
12.10 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 267
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 268

223

237

270

CONT EN T S

5

10.3 Some Grammar Rules for English . . . . . . . . . . . . . . . . . . 200
10.4 Treebanks
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . 207
10.5 Grammar Equivalence and Normal Form . . . . . . . . . . . . . . 213
10.6 Lexicalized Grammars . . . . . . . . . . . . . . . . . . . . . . . . 214
10.7 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 220
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 221

11 Syntactic Parsing

11.1 Ambiguity . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
11.2 CKY Parsing: A Dynamic Programming Approach . . . . . . . . 225
11.3 Partial Parsing . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 231
11.4 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 235
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 236

13 Dependency Parsing

13.1 Dependency Relations . . . . . . . . . . . . . . . . . . . . . . . . 271
13.2 Dependency Formalisms . . . . . . . . . . . . . . . . . . . . . . . 273
13.3 Dependency Treebanks
. . . . . . . . . . . . . . . . . . . . . . . 274
13.4 Transition-Based Dependency Parsing . . . . . . . . . . . . . . . 275
13.5 Graph-Based Dependency Parsing . . . . . . . . . . . . . . . . . 286
13.6 Evaluation . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
13.7 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 292
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 294

14 The Representation of Sentence Meaning

295

14.1 Computational Desiderata for Representations . . . . . . . . . . . 297
14.2 Model-Theoretic Semantics . . . . . . . . . . . . . . . . . . . . . 301
14.3 First-Order Logic . .
. . . . . . . . . . . . . . . . . . . . . . . . 304
14.4 Event and State Representations . . . . . . . . . . . . . . . . . . . 311
14.5 Description Logics . . . . . . . . . . . . . . . . . . . . . . . . . . 316
14.6 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 322
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 323

15 Computational Semantics

325

6 CONT EN T S

16 Semantic Parsing

326

17 Information Extraction

17.1 Named Entity Recognition . . . . . . . . . . . . . . . . . . . . . 328
17.2 Relation Extraction . .
. . . . . . . . . . . . . . . . . . . . . . . 334
17.3 Extracting Times . .
. . . . . . . . . . . . . . . . . . . . . . . . . 344
17.4 Extracting Events and their Times . . . . . . . . . . . . . . . . . . 348
17.5 Template Filling . .
. . . . . . . . . . . . . . . . . . . . . . . . . 351
17.6 Summary .
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 353
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 354
Exercises
.
.
.
.
.
.
. .
. . .
. . . . . . . . . . . . . . . . . . . . . . . 355

327

356

18 Semantic Role Labeling

18.1 Semantic Roles
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . 357
18.2 Diathesis Alternations . . . . . . . . . . . . . . . . . . . . . . . . 358
18.3 Semantic Roles: Problems with Thematic Roles . . . . . . . . . . 359
18.4 The Proposition Bank . . . . . . . . . . . . . . . . . . . . . . . . 360
18.5 FrameNet
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 362
18.6 Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . 364
18.7 Selectional Restrictions . . . . . . . . . . . . . . . . . . . . . . . 368
18.8 Primitive Decomposition of Predicates . . . . . . . . . . . . . . . 372
18.9 Summary .
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 374
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 374
Exercises
.
.
.
.
.
.
. .
. . .
. . . . . . . . . . . . . . . . . . . . . . . 377

19 Lexicons for Sentiment, Affect, and Connotation

378

19.1 Deﬁning Emotion .
. . . . . . . . . . . . . . . . . . . . . . . . . 379
19.2 Available Sentiment and Affect Lexicons . . . . . . . . . . . . . . 381
19.3 Creating affect lexicons by human labeling . . . . . . . . . . . . . 382
19.4 Semi-supervised induction of affect lexicons . . . . . . . . . . . . 384
19.5 Supervised learning of word sentiment
. . . . . . . . . . . . . . . 387
19.6 Using Lexicons for Sentiment Recognition . . . . . . . . . . . . . 391
19.7 Other tasks: Personality . . . . . . . . . . . . . . . . . . . . . . . 392
19.8 Affect Recognition .
. . . . . . . . . . . . . . . . . . . . . . . . . 393
19.9 Connotation Frames . . . . . . . . . . . . . . . . . . . . . . . . . 395
19.10 Summary .
.
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 396
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 397

20 Coreference Resolution and Entity Linking

21 Discourse Coherence

22 Machine Translation

399

400

401

23 Question Answering

402

23.1
IR-based Factoid Question Answering . . . . . . . . . . . . . . . 403
23.2 Knowledge-based Question Answering . . . . . . . . . . . . . . . 411
23.3 Using multiple information sources: IBM’s Watson . . . . . . . . 415
23.4 Evaluation of Factoid Answers
. . . . . . . . . . . . . . . . . . . 418
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 420
Exercises
.
.
.
.
.
.
. .
. . .
. . . . . . . . . . . . . . . . . . . . . . . 421

24 Dialog Systems and Chatbots

422

CONT EN T S

7

24.1 Chatbots .
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 425
24.2 Frame Based Dialog Agents . . . . . . . . . . . . . . . . . . . . . 430
24.3 VoiceXML . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
24.4 Evaluating Dialog Systems
. . . . . . . . . . . . . . . . . . . . . 441
24.5 Dialog System Design . . . . . . . . . . . . . . . . . . . . . . . . 442
24.6 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 444
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 445

25 Advanced Dialog Systems

446

25.1 Dialog Acts
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
25.2 Dialog State: Interpreting Dialog Acts
. . . . . . . . . . . . . . . 452
25.3 Dialog Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
25.4 A simple policy based on local context
. . . . . . . . . . . . . . . 456
25.5 Natural language generation in the dialog-state model
. . . . . . . 456
25.6 Deep Reinforcement Learning for Dialog . . . . . . . . . . . . . . 459
25.7 Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 459

26 Speech Recognition and Synthesis

Appendix

461

463

A Hidden Markov Models

A.1 Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
A.2
The Hidden Markov Model
. . . . . . . . . . . . . . . . . . . . . 465
A.3
Likelihood Computation: The Forward Algorithm . . . . . . . . . 467
A.4 Decoding: The Viterbi Algorithm . . . . . . . . . . . . . . . . . . 471
A.5 HMM Training: The Forward-Backward Algorithm . . . . . . . . 473
A.6
Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 479

464

480

B Spelling Correction and the Noisy Channel

B.1
The Noisy Channel Model . . . . . . . . . . . . . . . . . . . . . . 481
B.2
Real-word spelling errors . . . . . . . . . . . . . . . . . . . . . . 486
B.3 Noisy Channel Model: The State of the Art . . . . . . . . . . . . . 488
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 491
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 492

C WordNet: Word Relations, Senses, and Disambiguation

493

C.1 Word Senses .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
C.2 WordNet: A Database of Lexical Relations . . . . . . . . . . . . . 497
C.3 Word Similarity: Thesaurus Methods . . . . . . . . . . . . . . . . 498
C.4 Word Sense Disambiguation: Overview . . . . . . . . . . . . . . . 504
C.5
Supervised Word Sense Disambiguation . . . . . . . . . . . . . . 505
C.6 WSD: Dictionary and Thesaurus Methods
. . . . . . . . . . . . . 507
C.7
Semi-Supervised WSD: Bootstrapping . . . . . . . . . . . . . . . 510
C.8 Unsupervised Word Sense Induction . . . . . . . . . . . . . . . . 511
C.9
Summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 513
Exercises
.
.
.
.
.
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . 514

8 CONT EN T S

Bibliography

Author Index

Subject Index

517

543

551

CHAPTER

1 Introduction

Placeholder

9

10 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

CHAPTER

2 Regular Expressions, Text
Normalization, Edit Distance

User:
You are like my father in some ways.
ELIZA: WHAT RESEMBLANCE DO YOU SEE
User:
You are not very aggressive but I think you don’t want me to
notice that.
ELIZA: WHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE
User:
You don’t argue with me.
ELIZA: WHY DO YOU THINK I DON’T ARGUE WITH YOU
User:
You are afraid of me.
ELIZA: DOES IT PLEASE YOU TO BELIEVE I AM AFRAID OF YOU

Weizenbaum (1966)
The dialogue above is from ELIZA, an early natural language processing sys-
tem that could carry on a limited conversation with a user by imitating the responses
of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple
program that uses pattern matching to recognize phrases like “You are X” and trans-
late them into suitable outputs like “What makes you think I am X?”. This simple
technique succeeds in this domain because ELIZA doesn’t actually need to know
anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one
of the few dialogue genres where listeners can act as if they know nothing of the
world. Eliza’s mimicry of human conversation was remarkably successful: many
people who interacted with ELIZA came to believe that it really understood them
and their problems, many continued to believe in ELIZA’s abilities even after the
program’s operation was explained to them (Weizenbaum, 1976), and even today
such chatbots are a fun diversion.
Of course modern conversational agents are much more than a diversion; they
can answer questions, book ﬂights, or ﬁnd restaurants, functions for which they rely
on a much more sophisticated understanding of the user’s intent, as we will see in
Chapter 24. Nonetheless, the simple pattern-based methods that powered ELIZA
and other chatbots play a crucial role in natural language processing.
We’ll begin with the most important tool for describing text patterns: the regular
expression. Regular expressions can be used to specify strings we might want to
extract from a document, from transforming “You are X” in Eliza above, to deﬁning
strings like $199 or $24.99 for extracting tables of prices from a document.
We’ll then turn to a set of tasks collectively called text normalization, in which
regular expressions play an important part. Normalizing text means converting it
to a more convenient, standard form. For example, most of what we are going to
do with language relies on ﬁrst separating out or tokenizing words from running
text, the task of tokenization. English words are often separated from each other
by whitespace, but whitespace is not always sufﬁcient. New York and rock ’n’ roll
are sometimes treated as large words despite the fact that they contain spaces, while
sometimes we’ll need to separate I’m into the two words I and am. For processing
tweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.
Some languages, like Chinese, don’t have spaces between words, so word tokeniza-
tion becomes more difﬁcult.

ELIZA

chatbots

text
normalization

tokenization

2 .1

• R EGU LAR EX PR E S S ION S

11

lemmatization

stemming

sentence
segmentation

Another part of text normalization is lemmatization, the task of determining
that two words have the same root, despite their surface differences. For example,
the words sang, sung, and sings are forms of the verb sing. The word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
Lemmatization is essential for processing morphologically complex languages like
Arabic. Stemming refers to a simpler version of lemmatization in which we mainly
just strip sufﬁxes from the end of the word. Text normalization also includes sen-
tence segmentation: breaking up a text into individual sentences, using cues like
periods or exclamation points.
Finally, we’ll need to compare words and other strings. We’ll introduce a metric
called edit distance that measures how similar two strings are based on the number
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. Edit distance is an algorithm with applications throughout language process-
ing, from spelling correction to speech recognition to coreference resolution.

2.1 Regular Expressions

regular
expression

corpus

SIR ANDREW: Her C’s, her U’s and her T’s: why that?

Shakespeare, Twelfth Night
One of the unsung successes in standardization in computer science has been the
regular expression (RE), a language for specifying text search strings. This prac-
tical language is used in every computer language, word processor, and text pro-
cessing tools like the Unix tools grep or Emacs. Formally, a regular expression is
an algebraic notation for characterizing a set of strings. They are particularly use-
ful for searching in texts, when we have a pattern to search for and a corpus of
texts to search through. A regular expression search function will search through the
corpus, returning all texts that match the pattern. The corpus can be a single docu-
ment or a collection. For example, the Unix command-line tool grep takes a regular
expression and returns every line of the input document that matches the expression.
A search can be designed to return every match on a line, if there are more than
one, or just the ﬁrst match. In the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the ﬁrst
match. We’ll show regular expressions delimited by slashes but note that slashes are
not part of the regular expressions.
Regular expressions come in many variants. We’ll be describing extended regu-
lar expressions; different regular expression parsers may only recognize subsets of
these, or treat some expressions slightly differently. Using an online regular expres-
sion tester is a handy way to test out your expressions and explore these variations.

2.1.1 Basic Regular Expression Patterns

The simplest kind of regular expression is a sequence of simple characters. To search
for woodchuck, we type /woodchuck/. The expression /Buttercup/ matches any
string containing the substring Buttercup; grep with that expression would return the
line I’m called little Buttercup. The search string can consist of a single character
(like /!/) or a sequence of characters (like /urgl/).
Regular expressions are case sensitive; lower case /s/ is distinct from upper
case /S/ (/s/ matches a lower case s but not an upper case S). This means that
the pattern /woodchucks/ will not match the string Woodchucks. We can solve this

12 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

RE

/woodchucks/
/a/
/!/

Example Patterns Matched

“interesting links to woodchucks and lemurs”
“Mary Ann stopped by Mona’s”
“You’ve left the burglar behind again!” said Nori

Figure 2.1 Some simple regex searches.

problem with the use of the square braces [ and ]. The string of characters inside the
braces speciﬁes a disjunction of characters to match. For example, Fig. 2.2 shows
that the pattern /[wW]/ matches patterns containing either w or W.

RE

Match

/[wW]oodchuck/ Woodchuck or woodchuck
‘a’, ‘b’, or ‘c’
any digit

/[abc]/
/[1234567890]/

Example Patterns

“Woodchuck”
“In uomini, in soldati”
“plenty of 7 to 5”

Figure 2.2 The use of the brackets [] to specify a disjunction of characters.

The regular expression /[1234567890]/ speciﬁed any single digit. While such
classes of characters as digits or letters are important building blocks in expressions,
they can get awkward (e.g., it’s inconvenient to specify

/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/

range

to mean “any capital letter”). In cases where there is a well-deﬁned sequence asso-
ciated with a set of characters, the brackets can be used with the dash (-) to specify
any one character in a range. The pattern /[2-5]/ speciﬁes any one of the charac-
ters 2, 3, 4, or 5. The pattern /[b-g]/ speciﬁes one of the characters b, c, d, e, f, or
g. Some other examples are shown in Fig. 2.3.

RE

/[A-Z]/
/[a-z]/
/[0-9]/

Match

an upper case letter
a lower case letter
a single digit

Example Patterns Matched

“we should call it ‘Drenched Blossoms’ ”
“my beans were impatient to be hoed!”
“Chapter 1: Down the Rabbit Hole”

Figure 2.3 The use of the brackets [] plus the dash - to specify a range.

The square braces can also be used to specify what a single character cannot be,
by use of the caret ˆ. If the caret ˆ is the ﬁrst symbol after the open square brace [,
the resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single
character (including special characters) except a. This is only true when the caret
is the ﬁrst symbol after the open square brace. If it occurs anywhere else, it usually
stands for a caret; Fig. 2.4 shows some examples.

RE

/[ˆA-Z]/
/[ˆSs]/
/[ˆ\.]/
/[eˆ]/
/aˆb/

Match (single characters)

not an upper case letter
neither ‘S’ nor ‘s’
not a period
either ‘e’ or ‘ˆ’
the pattern ‘aˆb’

Example Patterns Matched

“Oyfn pripetchik”
“I have no exquisite reason for’t”
“our resident Djinn”
“look up ˆ now”
“look up aˆ b now”

Figure 2.4 The caret ˆ for negation or just to mean ˆ. See below re: the backslash for escaping the period.

How can we talk about optional elements, like an optional s in woodchuck and
woodchucks? We can’t use the square brackets, because while they allow us to say
“s or S”, they don’t allow us to say “s or nothing”. For this we use the question mark
/?/, which means “the preceding character or nothing”, as shown in Fig. 2.5.

2 .1

• R EGU LAR EX PR E S S ION S

13

RE

/woodchucks?/
/colou?r/

Match

woodchuck or woodchucks
color or colour

Example Patterns Matched

“woodchuck”
“colour”

Figure 2.5 The question mark ? marks optionality of the previous expression.

We can think of the question mark as meaning “zero or one instances of the
previous character”. That is, it’s a way of specifying how many of something that
we want, something that is very important in regular expressions. For example,
consider the language of certain sheep, which consists of strings that look like the
following:

baa!
baaa!
baaaa!
baaaaa!
. . .

Kleene *

Kleene +

Anchors

This language consists of strings with a b, followed by at least two a’s, followed
by an exclamation point. The set of operators that allows us to say things like “some
number of as” are based on the asterisk or *, commonly called the Kleene * (gen-
erally pronounced “cleany star”). The Kleene star means “zero or more occurrences
of the immediately previous character or regular expression”. So /a*/ means “any
string of zero or more as”. This will match a or aaaaaa, but it will also match Off
Minor since the string Off Minor has zero a’s. So the regular expression for matching
one or more a is /aa*/, meaning one a followed by zero or more as. More complex
patterns can also be repeated. So /[ab]*/ means “zero or more a’s or b’s” (not
“zero or more right square braces”). This will match strings like aaaa or ababab or
bbbb.
For specifying multiple digits (useful for ﬁnding prices) we can extend /[0-9]/,
the regular expression for a single digit. An integer (a string of digits) is thus

/[0-9][0-9]*/. (Why isn’t it just /[0-9]*/?)

Sometimes it’s annoying to have to write the regular expression for digits twice,
so there is a shorter way to specify “at least one” of some character. This is the
Kleene +, which means “one or more occurrences of the immediately preceding
character or regular expression”. Thus, the expression /[0-9]+/ is the normal way
to specify “a sequence of digits”. There are thus two ways to specify the sheep

language: /baaa*!/ or /baa+!/.

One very important special character is the period (/./), a wildcard expression
that matches any single character (except a carriage return), as shown in Fig. 2.6.

RE

/beg.n/

Match

any character between beg and n

Example Matches

begin, beg’n, begun

Figure 2.6 The use of the period . to specify any character.

The wildcard is often used together with the Kleene star to mean “any string of
characters”. For example, suppose we want to ﬁnd any line in which a particular
word, for example, aardvark, appears twice. We can specify this with the regular

expression /aardvark.*aardvark/.

Anchors are special characters that anchor regular expressions to particular places
in a string. The most common anchors are the caret ˆ and the dollar sign $. The caret
ˆ matches the start of a line. The pattern /ˆThe/ matches the word The only at the

14 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to in-
dicate a negation inside of square brackets, and just to mean a caret. (What are the
contexts that allow grep or Python to know which function a given caret is supposed
to have?) The dollar sign $ matches the end of a line. So the pattern (cid:32)$ is a useful
pattern for matching a space at the end of a line, and /ˆThe dog\.$/ matches a
line that contains only the phrase The dog. (We have to use the backslash here since
we want the . to mean “period” and not the wildcard.)
There are also two other anchors: \b matches a word boundary, and \B matches
a non-boundary. Thus, /\bthe\b/ matches the word the but not the word other.
More technically, a “word” for the purposes of a regular expression is deﬁned as any
sequence of digits, underscores, or letters; this is based on the deﬁnition of “words”
in programming languages. For example, /\b99\b/ will match the string 99 in
There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in
There are 299 bottles of beer on the wall (since 99 follows a number). But it will
match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore,
or letter).

2.1.2 Disjunction, Grouping, and Precedence

Suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. In such a case, we might want to search for either the string cat or
the string dog. Since we can’t use the square brackets to search for “cat or dog” (why
can’t we say /[catdog]/?), we need a new operator, the disjunction operator, also
called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or
the string dog.
Sometimes we need to use this disjunction operator in the midst of a larger se-
quence. For example, suppose I want to search for information about pet ﬁsh for
my cousin David. How can I specify both guppy and guppies? We cannot simply
say /guppy|ies/, because that would match only the strings guppy and ies. This
is because sequences like guppy take precedence over the disjunction operator |.
To make the disjunction operator apply only to a speciﬁc pattern, we need to use the
parenthesis operators ( and ). Enclosing a pattern in parentheses makes it act like
a single character for the purposes of neighboring operators like the pipe | and the
Kleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-
tion only to apply to the sufﬁxes y and ies.
The parenthesis operator ( is also useful when we are using counters like the
Kleene*. Unlike the | operator, the Kleene* operator applies by default only to
a single character, not to a whole sequence. Suppose we want to match repeated
instances of a string. Perhaps we have a line that has column labels of the form
Column 1 Column 2 Column 3. The expression /Column(cid:32)[0-9]+(cid:32)*/ will not
match any number of columns; instead, it will match a single column followed by
any number of spaces! The star here applies only to the space (cid:32) that precedes it,
not to the whole sequence. With the parentheses, we could write the expression
/(Column(cid:32)[0-9]+(cid:32)*)*/ to match the word Column, followed by a number and
optional spaces, the whole pattern repeated any number of times.
This idea that one operator may take precedence over another, requiring us to
sometimes use parentheses to specify what we mean, is formalized by the operator
precedence hierarchy for regular expressions. The following table gives the order
of RE operator precedence, from highest precedence to lowest precedence.

disjunction

Precedence

operator
precedence

2 .1

• R EGU LAR EX PR E S S ION S
()
* + ? {}
the ˆmy end$

Parenthesis
Counters
Sequences and anchors
Disjunction

|

15

greedy
non-greedy
*?
+?

Thus,
because
counters have
a higher precedence
than sequences,
/the*/ matches theeeee but not thethe. Because sequences have a higher prece-
dence than disjunction, /the|any/ matches the or any but not theny.
Patterns can be ambiguous in another way. Consider the expression /[a-z]*/
when matching against the text once upon a time. Since /[a-z]*/ matches zero or
more letters, this expression could match nothing, or just the ﬁrst letter o, on, onc,
or once. In these cases regular expressions always match the largest string they can;
we say that patterns are greedy, expanding to cover as much of a string as they can.
There are, however, ways to enforce non-greedy matching, using another mean-
ing of the ? qualiﬁer. The operator *? is a Kleene star that matches as little text as
possible. The operator +? is a Kleene plus that matches as little text as possible.

2.1.3 A Simple Example

Suppose we wanted to write a RE to ﬁnd cases of the English article the. A simple
(but incorrect) pattern might be:

/the/

One problem is that this pattern will miss the word when it begins a sentence
and hence is capitalized (i.e., The). This might lead us to the following pattern:

/[tT]he/

But we will still incorrectly return texts with the embedded in other words (e.g.,
other or theology). So we need to specify that we want instances with a word bound-
ary on both sides:

/\b[tT]he\b/

Suppose we wanted to do this without the use of /\b/. We might want this since
/\b/ won’t treat underscores and numbers as word boundaries; but we might want
to ﬁnd the in some context where it might also have underlines or numbers nearby
(the or the25). We need to specify that we want instances in which there are no
alphabetic letters on either side of the the:

/[ˆa-zA-Z][tT]he[ˆa-zA-Z]/

But there is still one more problem with this pattern: it won’t ﬁnd the word the
when it begins a line. This is because the regular expression [ˆa-zA-Z], which
we used to avoid embedded instances of the, implies that there must be some single
(although non-alphabetic) character before the the. We can avoid this by specify-
ing that before the the we require either the beginning-of-line or a non-alphabetic
character, and the same at the end of the line:

/(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/

false positives
false negatives

The process we just went through was based on ﬁxing two kinds of errors: false
positives, strings that we incorrectly matched like other or there, and false nega-
tives, strings that we incorrectly missed, like The. Addressing these two kinds of

16 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

errors comes up again and again in implementing speech and language processing
systems. Reducing the overall error rate for an application thus involves two antag-
onistic efforts:
• Increasing precision (minimizing false positives)
• Increasing recall (minimizing false negatives)

2.1.4 A More Complex Example

Let’s try out a more signiﬁcant example of the power of REs. Suppose we want to
build an application to help a user buy a computer on the Web. The user might want
“any machine with at least 6 GHz and 500 GB of disk space for less than $1000”.
To do this kind of retrieval, we ﬁrst need to be able to look for expressions like 6
GHz or 500 GB or Mac or $999.99. In the rest of this section we’ll work out some
simple regular expressions for this task.
First, let’s complete our regular expression for prices. Here’s a regular expres-
sion for a dollar sign followed by a string of digits:

/$[0-9]+/

Note that the $ character has a different function here than the end-of-line function
we discussed earlier. Most regular expression parsers are smart enough to realize
that $ here doesn’t mean end-of-line. (As a thought experiment, think about how
regex parsers might ﬁgure out the function of $ from the context.)
Now we just need to deal with fractions of dollars. We’ll add a decimal point
and two digits afterwards:

/$[0-9]+\.[0-9][0-9]/

This pattern only allows $199.99 but not $199. We need to make the cents
optional and to make sure we’re at a word boundary:

/(ˆ|\W)$[0-9]+(\.[0-9][0-9])?\b/

One last catch! This pattern allows prices like $199999.99 which would be far
too expensive! We need to limit the dollar

/(ˆ|\W)$[0-9]{0,3}(\.[0-9][0-9])?\b/

How about speciﬁcations for > 6GHz processor speed? Here’s a pattern for that:

/\b[6-9]+(cid:32)*(GHz|[Gg]igahertz)\b/

Note that we use /(cid:32)*/ to mean “zero or more spaces” since there might always
be extra spaces lying around. For disk space, we’ll need to allow for optional frac-
tions again (5.5 GB); note the use of ? for making the ﬁnal s optional:

/\b[0-9]+(\.[0-9]+)?(cid:32)*(GB|[Gg]igabytes?)\b/

Modifying this regular expression so that it only matches more than 500 GB is
left as an exercise for the reader.

2.1.5 More Operators

Figure 2.7 shows some aliases for common ranges, which can be used mainly to
save typing. Besides the Kleene * and Kleene + we can also use explicit numbers as

2 .1

• R EGU LAR EX PR E S S ION S

17

counters, by enclosing them in curly brackets. The regular expression /{3}/ means
“exactly 3 occurrences of the previous character or expression”. So /a\.{24}z/
will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots
followed by a z).

RE

\d
\D
\w
\W
\s
\S

Expansion

[0-9]
[ˆ0-9]
[a-zA-Z0-9_]
[ˆ\w]
[(cid:32)\r\t\n\f]
[ˆ\s]

Match

any digit
any non-digit
any alphanumeric/underscore
a non-alphanumeric
whitespace (space, tab)
Non-whitespace

First Matches

Party(cid:32)of(cid:32)5
Blue(cid:32)moon
Daiyu
!!!!

in(cid:32)Concord

Figure 2.7 Aliases for common sets of characters.

A range of numbers can also be speciﬁed. So /{n,m}/ speciﬁes from n to m
occurrences of the previous char or expression, and /{n,}/ means at least n occur-
rences of the previous expression. REs for counting are summarized in Fig. 2.8.

RE

*
+
?
{n}
{n,m}
{n,}
{,m}

Match

zero or more occurrences of the previous char or expression
one or more occurrences of the previous char or expression
exactly zero or one occurrence of the previous char or expression
n occurrences of the previous char or expression
from n to m occurrences of the previous char or expression
at least n occurrences of the previous char or expression
up to m occurrences of the previous char or expression

Figure 2.8 Regular expression operators for counting.

Finally, certain special characters are referred to by special notation based on the
backslash (

18 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

capture group

register

non-capturing
group

integers in a text, for example, changing the 35 boxes to the <35> boxes. We’d
like a way to refer to the integer we’ve found so that we can easily add the brackets.
To do this, we put parentheses ( and ) around the ﬁrst pattern and use the number
operator \1 in the second pattern to refer back. Here’s how it looks:

s/([0-9]+)/<\1>/

The parenthesis and number operators can also specify that a certain string or
expression must occur twice in the text. For example, suppose we are looking for
the pattern “the Xer they were, the Xer they will be”, where we want to constrain
the two X’s to be the same string. We do this by surrounding the ﬁrst X with the
parenthesis operator, and replacing the second X with the number operator \1, as
follows:

/the (.*)er they were, the \1er they will be/

Here the \1 will be replaced by whatever string matched the ﬁrst item in paren-
theses. So this will match the bigger they were, the bigger they will be but not the
bigger they were, the faster they will be.
This use of parentheses to store a pattern in memory is called a capture group.
Every time a capture group is used (i.e., parentheses surround a pattern), the re-
sulting match is stored in a numbered register. If you match two different sets of
parentheses, \2 means whatever matched the second capture group. Thus

/the (.*)er they (.*), the \1er we \2/

will match the faster they ran, the faster we ran but not the faster they ran, the faster
we ate. Similarly, the third capture group is stored in \3, the fourth is \4, and so on.
Parentheses thus have a double function in regular expressions; they are used to
group terms for specifying the order in which operators should apply, and they are
used to capture something in a register. Occasionally we might want to use parenthe-
ses for grouping, but don’t want to capture the resulting pattern in a register. In that
case we use a non-capturing group, which is speciﬁed by putting the commands
?: after the open paren, in the form (?: pattern ).

/(?:some|a few) (people|cats) like some \1/

will match some cats like some cats but not some cats like some a few.
Substitutions and capture groups are very useful in implementing simple chat-
bots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian
psychologist by carrying on conversations like the following:

User1 : Men are all alike.
ELIZA1 : IN WHAT WAY
User2 :
They’re always bugging us about something or other.
ELIZA2 : CAN YOU THINK OF A SPECIFIC EXAMPLE
User3 : Well, my boyfriend made me come here.
ELIZA3 : YOUR BOYFRIEND MADE YOU COME HERE
User4 :
He says I’m depressed much of the time.
ELIZA4 : I AM SORRY TO HEAR YOU ARE DEPRESSED

ELIZA works by having a series or cascade of regular expression substitutions
each of which matches and changes some part of the input lines. Input lines are
ﬁrst uppercased. The ﬁrst substitutions then change all instances of MY to YOUR,
and I’M to YOU ARE, and so on. The next set of substitutions matches and replaces
other patterns in the input. Here are some examples:

• WORD S
s/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \1/
s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \1/
s/.* all .*/IN WHAT WAY/
s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/

2 .2

19

Since multiple substitutions can apply to a given input, substitutions are assigned
a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we
return to the details of the ELIZA architecture in Chapter 24.

2.1.7 Lookahead assertions

Finally, there will be times when we need to predict the future: look ahead in the
text to see if some pattern matches, but not advance the match cursor, so that we can
then deal with the pattern if it occurs.
These lookahead assertions make use of the (? syntax that we saw in the previ-
ous section for non-capture groups. The operator (?= pattern) is true if pattern
occurs, but is zero-width, i.e.
the match pointer doesn’t advance. The operator
(?! pattern) only returns true if a pattern does not match, but again is zero-width
and doesn’t advance the cursor. Negative lookahead is commonly used when we
are parsing some complex pattern but want to rule out a special case. For example
suppose we want to match, at the beginning of a line, any single word that doesn’t
start with “Volcano”. We can use negative lookahead to do this:

/ˆ(?!Volcano)[A-Za-z]+/

lookahead

zero-width

2.2 Words

corpus
corpora

utterance

Before we talk about processing words, we need to decide what counts as a word.
Let’s start by looking at one particular corpus (plural corpora), a computer-readable
collection of text or speech. For example the Brown corpus is a million-word col-
lection of samples from 500 written English texts from different genres (newspa-
per, ﬁction, non-ﬁction, academic, etc.), assembled at Brown University in 1963–64
(Ku ˇcera and Francis, 1967). How many words are in the following Brown sentence?
He stepped out into the hall, was delighted to encounter a water brother.
This sentence has 13 words if we don’t count punctuation marks as words, 15
if we count punctuation. Whether we treat period (“.”), comma (“,”), and so on as
words depends on the task. Punctuation is critical for ﬁnding boundaries of things
(commas, periods, colons) and for identifying some aspects of meaning (question
marks, exclamation marks, quotation marks). For some tasks, like part-of-speech
tagging or parsing or speech synthesis, we sometimes treat punctuation marks as if
they were separate words.
The Switchboard corpus of American English telephone conversations between
strangers was collected in the early 1990s; it contains 2430 conversations averaging
6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey
et al., 1992). Such corpora of spoken language don’t have punctuation but do intro-
duce other complications with regard to deﬁning words. Let’s look at one utterance
from Switchboard; an utterance is the spoken correlate of a sentence:
I do uh main- mainly business data processing

20 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

disﬂuency
fragment
ﬁlled pause

lemma

wordform

word type

word token

This utterance has two kinds of disﬂuencies. The broken-off word main- is
called a fragment. Words like uh and um are called ﬁllers or ﬁlled pauses. Should
we consider these to be words? Again, it depends on the application. If we are
building a speech transcription system, we might want to eventually strip out the
disﬂuencies.
But we also sometimes keep disﬂuencies around. Disﬂuencies like uh or um
are actually helpful in speech recognition in predicting the upcoming word, because
they may signal that the speaker is restarting the clause or idea, and so for speech
recognition they are treated as regular words. Because people use different disﬂu-
encies they can also be a cue to speaker identiﬁcation. In fact Clark and Fox Tree
(2002) showed that uh and um have different meanings. What do you think they are?
Are capitalized tokens like They and uncapitalized tokens like they the same
word? These are lumped together in some tasks (speech recognition), while for part-
of-speech or named-entity tagging, capitalization is a useful feature and is retained.
How about inﬂected forms like cats versus cat? These two words have the same
lemma cat but are different wordforms. A lemma is a set of lexical forms having
the same stem, the same major part-of-speech, and the same word sense. The word-
form is the full inﬂected or derived form of the word. For morphologically complex
languages like Arabic, we often need to deal with lemmatization. For many tasks in
English, however, wordforms are sufﬁcient.
How many words are there in English? To answer this question we need to
distinguish two ways of talking about words. Types are the number of distinct words
in a corpus; if the set of words in the vocabulary is V , the number of types is the
vocabulary size |V |. Tokens are the total number N of running words. If we ignore
punctuation, the following Brown sentence has 16 tokens and 14 types:

They picnicked by the pool, then lay back on the grass and looked at the stars.

When we speak about the number of words in the language, we are generally
referring to word types.

Corpus

Shakespeare
Brown corpus
Switchboard telephone conversations
COCA
Google N-grams

Tokens = N Types = |V |

884 thousand 31 thousand
1 million 38 thousand
2.4 million 20 thousand
440 million
2 million
1 trillion
13 million

Figure 2.10 Rough numbers of types and tokens for some English language corpora. The
largest, the Google N-grams corpus, contains 13 million types, but this count only includes
types appearing 40 or more times, so the true number would be much larger.

Fig. 2.10 shows the rough numbers of types and tokens computed from some
popular English corpora. The larger the corpora we look at, the more word types
we ﬁnd, and in fact this relationship between the number of types |V | and number
of tokens N is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978)
after its discoverers (in linguistics and information retrieval respectively). It is shown
in Eq. 2.1, where k and β are positive constants, and 0 < β < 1.

Herdan’s Law
Heaps’ Law

|V | = kN β
The value of β depends on the corpus size and the genre, but at least for the
large corpora in Fig. 2.10, β ranges from .67 to .75. Roughly then we can say that

(2.1)

2 .3

• COR PORA

21

the vocabulary size for a text goes up signiﬁcantly faster than the square root of its
length in words.
Another measure of the number of words in the language is the number of lem-
mas instead of wordform types. Dictionaries can help in giving lemma counts; dic-
tionary entries or boldface forms are a very rough upper bound on the number of
lemmas (since some lemmas have multiple boldface forms). The 1989 edition of the
Oxford English Dictionary had 615,000 entries.

2.3 Corpora

AAVE

SAE

code switching

Words don’t appear out of nowhere. Any particular piece of text that we study
is produced by one or more speciﬁc speakers or writers, in a speciﬁc dialect of a
speciﬁc language, at a speciﬁc time, in a speciﬁc place, for a speciﬁc function.
Perhaps the most important dimension of variation is the language. NLP algo-
rithms are most useful when they apply across many languages. The world has 7097
languages at the time of this writing, according to the online Ethnologue catalog
(Simons and Fennig, 2018). Most NLP tools tend to be developed for the ofﬁcial
languages of large industrialized nations (Chinese, English, Spanish, Arabic, etc.),
but we don’t want to limit tools to just these few languages. Furthermore, most lan-
guages also have multiple varieties, such as dialects spoken in different regions or
by different social groups. Thus, for example, if we’re processing text in African
American Vernacular English (AAVE), a dialect spoken by millions of people in the
United States, it’s important to make use of NLP tools that function with that dialect.
Twitter posts written in AAVE make use of constructions like iont (I don’t in Stan-
dard American English (SAE)), or talmbout corresponding to SAE talking about,
both examples that inﬂuence word segmentation (Blodgett et al. 2016, Jones 2015).
It’s also quite common for speakers or writers to use multiple languages in a
single communicative act, a phenomenon called code switching. Code switch-
ing is enormously common across the world; here are examples showing Spanish
and (transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens
et al. 2017):
(2.2) Por primera vez veo a @username actually being hateful! it was beautiful:)
[For the ﬁrst time I get to see @username actually being hateful! it was
beautiful:]
(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe
[“he was and will remain a friend ... don’t worry ... but have faith”]
Another dimension of variation is the genre. The text that our algorithms must
process might come from newswire, ﬁction or non-ﬁction books, scientiﬁc articles,
Wikipedia, or religious texts.
It might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. It might come from work situations
like doctors’ notes, legal text, or parliamentary or congressional proceedings.
Text also reﬂects the demographic characteristics of the writer (or speaker): their
age, gender, race, socio-economic class can all inﬂuence the linguistic properties of
the text we are processing.
And ﬁnally, time matters too. Language changes over time, and for some lan-
guages we have good corpora of texts from different historical periods.
Because language is so situated, when developing computational models for lan-

22 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

guage processing, it’s important to consider who produced the language, in what
context, for what purpose, and make sure that the models are ﬁt to the data.

2.4 Text Normalization

Before almost any natural language processing of a text, the text has to be normal-
ized. At least three tasks are commonly applied as part of any normalization process:
1. Segmenting/tokenizing words from running text
2. Normalizing word formats
3. Segmenting sentences in running text.
In the next sections we walk through each of these tasks.

2.4.1 Unix tools for crude tokenization and normalization

Let’s begin with an easy, if somewhat naive version of word tokenization and nor-
malization (and frequency computation) that can be accomplished for English solely
in a single UNIX command-line, inspired by Church (1994). We’ll make use of some
Unix commands: tr, used to systematically change particular characters in the in-
put; sort, which sorts input lines in alphabetical order; and uniq, which collapses
and counts adjacent identical lines.
For example let’s begin with the ‘complete words’ of Shakespeare in one textﬁle,
sh.txt. We can use tr to tokenize the words by changing every sequence of non-
alphabetic characters to a newline (’A-Za-z’ means alphabetic, the -c option com-
plements to non-alphabet, and the -s option squeezes all sequences into a single
character):

tr -sc ’A-Za-z’ ’\n’ < sh.txt

The output of this command will be:

THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
We
...

Now that there is one word per line, we can sort the lines, and pass them to uniq
-c which will collapse and count them:

tr -sc ’A-Za-z’ ’\n’ < sh.txt | sort | uniq -c

with the following output:

1945 A
72 AARON
19 ABBESS
25 Aaron

2 .4

• T EX T NORMAL I ZAT ION

23

6 Abate
1 Abates
5 Abbess
6 Abbey
3 Abbot
...

Alternatively, we can collapse all the upper case to lower case:

tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c

whose output is

14725 a
97 aaron
1 abaissiez
10 abandon
2 abandoned
2 abase
1 abash
14 abate
3 abated
3 abatement
...

Now we can sort again to ﬁnd the frequent words. The -n option to sort means
to sort numerically rather than alphabetically, and the -r option means to sort in
reverse order (highest-to-lowest):

tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r

The results show that the most frequent words in Shakespeare, as in any other
corpus, are the short function words like articles, pronouns, prepositions:

27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
12489 my
11318 that
11112 in
...

Unix tools of this sort can be very handy in building quick word count statistics
for any corpus.

2.4.2 Word Tokenization and Normalization

The simple UNIX tools above were ﬁne for getting rough word statistics but more
sophisticated algorithms are generally necessary for tokenization, the task of seg-
menting running text into words, and normalization, the task of putting words/to-
kens in a standard format.
While the Unix command sequence just removed all the numbers and punctu-
ation, for most NLP applications we’ll need to keep these in our tokenization. We

tokenization
normalization

24 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

often want to break off punctuation as a separate token; commas are a useful piece of
information for parsers, periods help indicate sentence boundaries. But we’ll often
want to keep the punctuation that occurs word internally, in examples like m.p.h,,
Ph.D., AT&T, cap’n. Special characters and numbers will need to be kept in prices
($45.55) and dates (01/02/06); we don’t want to segment that price into separate to-
kens of “45” and “55”. And there are URLs (http://www.stanford.edu), Twitter
hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).
Number expressions introduce other complications as well; while commas nor-
mally appear at word boundaries, commas are used inside numbers in English, every
three digits: 555,500.50. Languages, and hence tokenization requirements, differ
on this; many continental European languages like Spanish, French, and German, by
contrast, use a comma to mark the decimal point, and spaces (or sometimes periods)
where English puts commas, for example, 555 500,50.
A tokenizer can also be used to expand clitic contractions that are marked by
apostrophes, for example, converting what’re to the two tokens what are, and
we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only
occur when it is attached to another word. Some such contractions occur in other
alphabetic languages, including articles and pronouns in French (j’ai, l’homme).
Depending on the application, tokenization algorithms may also tokenize mul-
tiword expressions like New York or rock ’n’ roll as a single token, which re-
quires a multiword expression dictionary of some sort. Tokenization is thus inti-
mately tied up with named entity detection, the task of detecting names, dates, and
organizations (Chapter 17).
One commonly used tokenization standard is known as the Penn Treebank to-
kenization standard, used for the parsed corpora (treebanks) released by the Lin-
guistic Data Consortium (LDC), the source of many useful datasets. This standard
separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words to-
gether, and separates out all punctuation:

Input:
Output:

“The San Francisco-based restaurant,” they said, “doesn’t charge $10”.
“
The
San
Francisco-based
restaurant
,
”
they
said
,
“
does
n’t
charge
$
10

”

.

Tokens can also be normalized, in which a single normalized form is chosen for
words with multiple forms like USA and US or uh-huh and uhhuh. This standard-
ization may be valuable, despite the spelling information that is lost in the normal-
ization process. For information retrieval, we might want a query for US to match a
document that has USA; for information extraction we might want to extract coherent
information that is consistent across differently-spelled instances.
Case folding is another kind of normalization. For tasks like speech recognition
and information retrieval, everything is mapped to lower case. For sentiment anal-
ysis and other text classiﬁcation tasks, information extraction, and machine transla-
tion, by contrast, case is quite helpful and case folding is generally not done (losing
the difference, for example, between US the country and us the pronoun can out-
weigh the advantage in generality that case folding provides).
In practice, since tokenization needs to be run before any other language process-
ing, it is important for it to be very fast. The standard method for tokenization/nor-
malization is therefore to use deterministic algorithms based on regular expressions
compiled into very efﬁcient ﬁnite state automata. Carefully designed deterministic
algorithms can deal with the ambiguities that arise, such as the fact that the apos-
trophe needs to be tokenized differently when used as a genitive marker (as in the

clitic

Penn Treebank
tokenization

case folding

2 .4

• T EX T NORMAL I ZAT ION

25

book’s cover), a quotative as in ‘The other class’, she said, or in clitics like they’re.

2.4.3 Word Segmentation in Chinese: the MaxMatch algorithm

Some languages, including written Chinese, Japanese, and Thai, do not use spaces to
mark potential word-boundaries, and so require alternative segmentation methods.
In Chinese, for example, words are composed of characters known as hanzi. Each
character generally represents a single morpheme and is pronounceable as a single
syllable. Words are about 2.4 characters long on average. A simple algorithm that
does remarkably well for segmenting Chinese, and often used as a baseline com-
parison for more advanced methods, is a version of greedy search called maximum
matching or sometimes MaxMatch. The algorithm requires a dictionary (wordlist)
of the language.
The maximum matching algorithm starts by pointing at the beginning of a string.
It chooses the longest word in the dictionary that matches the input at the current
position. The pointer is then advanced to the end of that word in the string.
If
no word matches, the pointer is instead advanced one character (creating a one-
character word). The algorithm is then iteratively applied again starting from the
new pointer position. Fig. 2.11 shows a version of the algorithm.

hanzi

maximum
matching

function MAXMATCH(sentence, dictionary) returns word sequence W

if sentence is empty
return empty list
for i ← length(sentence) downto 1
ﬁrstword = ﬁrst i chars of sentence
remainder = rest of sentence
if InDictionary(ﬁrstword, dictionary)
return list(ﬁrstword, MaxMatch(remainder,dictionary) )

# no word was found, so make a one-character word
ﬁrstword = ﬁrst char of sentence
remainder = rest of sentence
return list(ﬁrstword, MaxMatch(remainder,dictionary) )
Figure 2.11 The MaxMatch algorithm for word segmentation.

MaxMatch works very well on Chinese; the following example shows an appli-
cation to a simple Chinese sentence using a simple Chinese lexicon available from
the Linguistic Data Consortium:

Input: 他特别喜欢北京烤鸭
喜欢 北京烤鸭

Output: 他 特别

“He especially likes Peking duck”

He especially likes Peking duck
MaxMatch doesn’t work as well on English. To make the intuition clear, we’ll
create an example by removing the spaces from the beginning of Turing’s famous
quote “We can only see a short distance ahead”, producing “wecanonlyseeashortdis-
tanceahead”. The MaxMatch results are shown below.

Input:

wecanonlyseeashortdistanceahead
Output: we canon l y see ash ort distance ahead

On English the algorithm incorrectly chose canon instead of stopping at can,
which left the algorithm confused and having to create single-character words l and

26 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

word error rate

morpheme
stem
afﬁx

y and use the very rare word ort.
The algorithm works better in Chinese than English, because Chinese has much
shorter words than English. We can quantify how well a segmenter works using a
metric called word error rate. We compare our output segmentation with a perfect
hand-segmented (‘gold’) sentence, seeing how many words differ. The word error
rate is then the normalized minimum edit distance in words between our output and
the gold: the number of word insertions, deletions, and substitutions divided by the
length of the gold sentence in words; we’ll see in Section 2.5 how to compute edit
distance. Even in Chinese, however, MaxMatch has problems, for example dealing
with unknown words (words not in the dictionary) or genres that differ a lot from
the assumptions made by the dictionary builder.
The most accurate Chinese segmentation algorithms generally use statistical se-
quence models trained via supervised machine learning on hand-segmented training
sets; we’ll introduce sequence models in Chapter 8.

2.4.4 Collapsing words: Lemmatization and Stemming

For many natural language processing situations we want two different forms of
a word to behave similarly. For example in web search, someone may type the
string woodchucks but a useful system might want to also return pages that mention
woodchuck with no s. This is especially common in morphologically complex lan-
guages like Russian, where for example the word Moscow has different endings in
the phrases Moscow, of Moscow, from Moscow, and so on.
Lemmatization is the task of determining that two words have the same root,
despite their surface differences. The words am, are, and is have the shared lemma
be; the words dinner and dinners both have the lemma dinner.
Lemmatizing each of these forms to the same lemma will let us ﬁnd all mentions
of words like Moscow. The the lemmatized form of a sentence like He is reading
detective stories would thus be He be read detective story.
How is lemmatization done? The most sophisticated methods for lemmatization
involve complete morphological parsing of the word. Morphology is the study of
the way words are built up from smaller meaning-bearing units called morphemes.
Two broad classes of morphemes can be distinguished: stems—the central mor-
pheme of the word, supplying the main meaning— and afﬁxes—adding “additional”
meanings of various kinds. So, for example, the word fox consists of one morpheme
(the morpheme fox) and the word cats consists of two: the morpheme cat and the
morpheme -s. A morphological parser takes a word like cats and parses it into the
two morphemes cat and s, or a Spanish word like amaren (‘if in the future they
would love’) into the morphemes amar ‘to love’, 3PL, and future subjunctive.

The Porter Stemmer

stemming
Porter stemmer

Lemmatization algorithms can be complex. For this reason we sometimes make use
of a simpler but cruder method, which mainly consists of chopping off word-ﬁnal
afﬁxes. This naive version of morphological analysis is called stemming. One of
the most widely used stemming algorithms is the Porter (1980). The Porter stemmer
applied to the following paragraph:

This was not the map we found in Billy Bones’s chest, but
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.

cascade

unknown
words

byte-pair
encoding
BPE

2 .4

• T EX T NORMAL I ZAT ION

27

produces the following stemmed output:

Thi wa not the map we found in Billi Bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note

The algorithm is based on series of rewrite rules run in series, as a cascade, in
which the output of each pass is fed as input to the next pass; here is a sampling of
the rules:
ATIONAL → ATE (e.g., relational → relate)
ING → 
if stem contains vowel (e.g., motoring → motor)
SSES → SS (e.g., grasses → grass)
Detailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.)
can be found on Martin Porter’s homepage; see also the original paper (Porter, 1980).
Simple stemmers can be useful in cases where we need to collapse across differ-
ent variants of the same lemma. Nonetheless, they do tend to commit errors of both
over- and under-generalizing, as shown in the table below (Krovetz, 1993):

Errors of Commission

organization organ
doing
doe
numerical
numerous
policy
police

Errors of Omission

European Europe
analysis
analyzes
noise
noisy
sparse
sparsity

2.4.5 Byte-Pair Encoding

Stemming or lemmatizing has another side-beneﬁt. By treating two similar words
identically, these normalization methods help deal with the problem of unknown
words, words that a system has not seen before.
Unknown words are particularly relevant for machine learning systems. As we
will see in the next chapter, machine learning systems often learn some facts about
words in one corpus (a training corpus) and then use these facts to make decisions
about a separate test corpus and its words. Thus if our training corpus contains, say
the words low, and lowest, but not lower, but then the word lower appears in our
test corpus, our system will not know what to do with it. Stemming or lemmatizing
everything to low can solve the problem, but has the disadvantage that sometimes
we don’t want words to be completely collapsed. For some purposes (for example
part-of-speech tagging) the words low and lower need to remain distinct.
A solution to this problem is to use a different kind of tokenization in which
most tokens are words, but some tokens are frequent word parts like -er, so that an
unseen word can be represented by combining the parts.
The simplest such algorithm is byte-pair encoding, or BPE (Sennrich et al.,
2016). Byte-pair encoding is based on a method for text compression (Gage, 1994),
but here we use it for tokenization instead. The intuition of the algorithm is to
iteratively merge frequent pairs of characters,
The algorithm begins with the set of symbols equal to the set of characters. Each
word is represented as a sequence of characters plus a special end-of-word symbol
·. At each step of the algorithm, we count the number of symbol pairs, ﬁnd the
most frequent pair (‘A’, ‘B’), and replace it with the new merged symbol (‘AB’). We
continue to count and merge, creating new longer and longer character strings, until

28 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

we’ve done k merges; k is a parameter of the algorithm. The resulting symbol set
will consist of the original set of characters plus k new symbols.
The algorithm is run inside words (we don’t merge across word boundaries).
For this reason, the algorithm can take as input a dictionary of words together with
counts. For example, consider the following tiny input dictionary:

word

frequency

5

l o w ·
l o w e s t · 2
n e w e r ·
w i d e r ·
n e w ·

6
3
2
We ﬁrst count all pairs of symbols: the most frequent is the pair r · because it
occurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 occur-
rences. We then merge these symbols, treating r· as one symbol, and count again:

word

frequency

5

l o w ·
l o w e s t · 2
n e w e r·
w i d e r·
n e w ·

6
3
2
Now the most frequent pair is e r·, which we merge:

word

frequency

5

l o w ·
l o w e s t · 2
n e w er·
w i d er·
n e w ·

6
3
2
Our system has learned that there should be a token for word-ﬁnal er, repre-
sented as er·. If we continue, the next merges are

(’e’, ’w’)
(’n’, ’ew’)
(’l’, ’o’)
(’lo’, ’w’)
(’new’, ’er·’)
(’low’, ’·’)

r·, er·, ew, new, lo, low, newer·, low·}

The current set of symbols is thus {·, d, e, i, l, n, o, r, s, t, w,
When we need to tokenize a test sentence, we just run the merges we have
learned, greedily, in the order we learned them, on the test data.
(Thus the fre-
quencies in the test data don’t play a role, just the frequencies in the training data).
So ﬁrst we segment each test sentence word into characters. Then we apply the ﬁrst
rule: replace every instance of r · in the test corpus with r·, and then the second
rule: replace every instance of e r· in the test corpus with er·, and so on. By the
end, if the test corpus contained the word n e w e r ·, it would be tokenized as a
full word. But a new (unknown) word like l o w e r · would be merged into the
two tokens low er·.
Of course in real algorithms BPE is run with many thousands of merges on a
very large input dictionary. The result is that most words will be represented as

2 .4

• T EX T NORMAL I ZAT ION

29

full symbols, and only the very rare words (and unknown words) will have to be
represented by their parts.
The full BPE learning algorithm is given in Fig. 2.12.

i m p o r t

r e ,

c o l l e c t i o n s

d e f g e t s t a t s ( v o c a b ) :
p a i r s = c o l l e c t i o n s . d e f a u l t d i c t ( i n t )
f o r w o r d ,
f r e q i n v o c a b . i t e m s ( ) :
s y m b o l s = w o r d . s p l i t ( )
f o r
i
i n r a n g e ( l e n ( s y m b o l s ) − 1 ) :
p a i r s [ s y m b o l s [ i ] , s y m b o l s [ i + 1 ] ] += f r e q
r e t u r n p a i r s

d e f m e r g e v o c a b ( p a i r , v i n ) :
v o u t = {}
p = r e . c o m p i l e ( r ’ ( ? < ! \ S ) ’ + b i g r a m + r ’ ( ? ! \ S ) ’ )
b i g r a m = r e . e s c a p e ( ’
’ . j o i n ( p a i r ) )
f o r w o r d i n v i n :
w o u t = p . s u b ( ’ ’ . j o i n ( p a i r ) , w o r d )
v o u t [ w o u t ] = v i n [ w o r d ]
r e t u r n v o u t
v o c a b = { ’ l o w </w> ’
: 5 ,
’ l o w e s
t </w> ’
: 2 ,
’ n e w </w> ’ : 2 }
’ n e w e r </w> ’ : 6 ,
’w i d e r </w> ’ : 3 ,
n u m m e r g e s = 8

f o r

i
i n r a n g e ( n u m m e r g e s ) :
p a i r s = g e t s t a t s ( v o c a b )
b e s t = max ( p a i r s , k e y = p a i r s . g e t )
v o c a b = m e r g e v o c a b ( b e s t , v o c a b )
p r i n t ( b e s t )

Figure 2.12 Python code for BPE learning algorithm from Sennrich et al. (2016).

Sentence
segmentation

2.4.6 Sentence Segmentation

Sentence segmentation is another important step in text processing. The most use-
ful cues for segmenting a text into sentences are punctuation, like periods, question
marks, exclamation points. Question marks and exclamation points are relatively
unambiguous markers of sentence boundaries. Periods, on the other hand, are more
ambiguous. The period character “.” is ambiguous between a sentence boundary
marker and a marker of abbreviations like Mr. or Inc. The previous sentence that
you just read showed an even more complex case of this ambiguity, in which the ﬁnal
period of Inc. marked both an abbreviation and the sentence boundary marker. For
this reason, sentence tokenization and word tokenization may be addressed jointly.
In general, sentence tokenization methods work by building a binary classiﬁer
(based on a sequence of rules or on machine learning) that decides if a period is part
of the word or is a sentence-boundary marker. In making this decision, it helps to
know if the period is attached to a commonly used abbreviation; thus, an abbrevia-
tion dictionary is useful.
State-of-the-art methods for sentence tokenization are based on machine learning
and are introduced in later chapters.

30 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

2.5 Minimum Edit Distance

Much of natural language processing is concerned with measuring how similar two
strings are. For example in spelling correction, the user typed some erroneous
string—let’s say graffe–and we want to know what the user meant. The user prob-
ably intended a word that is similar to graffe. Among candidate similar words,
the word giraffe, which differs by only one letter from graffe, seems intuitively
to be more similar than, say grail or graf, which differ in more letters. Another
example comes from coreference, the task of deciding whether two strings such as
the following refer to the same entity:

Stanford President John Hennessy
Stanford University President John Hennessy

Again, the fact that these two strings are very similar (differing by only one word)
seems like useful evidence for deciding that they might be coreferent.
Edit distance gives us a way to quantify both of these intuitions about string sim-
ilarity. More formally, the minimum edit distance between two strings is deﬁned
as the minimum number of editing operations (operations like insertion, deletion,
substitution) needed to transform one string into another.
The gap between intention and execution, for example, is 5 (delete an i, substi-
tute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see
this by looking at the most important visualization for string distances, an alignment
between the two strings, shown in Fig. 2.13. Given two sequences, an alignment is
a correspondence between substrings of the two sequences. Thus, we say I aligns
with the empty string, N with E, and so on. Beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the
top string into the bottom string: d for deletion, s for substitution, i for insertion.

minimum edit
distance

alignment

I N T E * N T I O N

| | | | | | | | | |

* E X E C U T I O N

d s s

i s

Figure 2.13 Representing the minimum edit distance between two strings as an alignment.
The ﬁnal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.

We can also assign a particular cost or weight to each of these operations. The
Levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume
that the substitution of a letter for itself, for example, t for t, has zero cost. The Lev-
enshtein distance between intention and execution is 5. Levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (This is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). Using this version, the Levenshtein distance between
intention and execution is 8.

2 .5

• M IN IMUM ED I T D I STANC E

31

2.5.1 The Minimum Edit Distance Algorithm

How do we ﬁnd the minimum edit distance? We can think of this as a search task, in
which we are searching for the shortest path—a sequence of edits—from one string
to another.

dynamic
programming

Figure 2.14 Finding the edit distance viewed as a search problem

The space of all possible edits is enormous, so we can’t search naively. However,
lots of distinct edit paths will end up in the same state (string), so rather than recom-
puting all those paths, we could just remember the shortest path to a state each time
we saw it. We can do this by using dynamic programming. Dynamic programming
is the name for a class of algorithms, ﬁrst introduced by Bellman (1957), that apply
a table-driven method to solve problems by combining solutions to sub-problems.
Some of the most commonly used algorithms in natural language processing make
use of dynamic programming, such as the Viterbi algorithm (Chapter 8) and the
CKY algorithm for parsing (Chapter 11).
The intuition of a dynamic programming problem is that a large problem can
be solved by properly combining the solutions to various sub-problems. Consider
the shortest path of transformed words that represents the minimum edit distance
between the strings intention and execution shown in Fig. 2.15.

Figure 2.15 Path from intention to execution.

Imagine some string (perhaps it is exention) that is in this optimal path (whatever
it is). The intuition of dynamic programming is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intention to exention. Why? If there were a shorter path from intention to exention,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn’t be optimal, thus leading to a contradiction.
The minimum edit distance algorithm was named by Wagner and Fischer (1974)
but independently discovered by many people (see the Historical Notes section of
Chapter 8).
Let’s ﬁrst deﬁne the minimum edit distance between two strings. Given two
strings, the source string X of length n, and target string Y of length m, we’ll deﬁne
D(i, j) as the edit distance between X [1..i] and Y [1.. j], i.e., the ﬁrst i characters of X
and the ﬁrst j characters of Y . The edit distance between X and Y is thus D(n, m).

minimum edit
distance

32 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

if source[i] (cid:54)= t arget [ j]
if source[i] = t arget [ j]

We’ll use dynamic programming to compute D(n, m) bottom up, combining so-
lutions to subproblems. In the base case, with a source substring of length i but an
empty target string, going from i characters to 0 requires i deletes. With a target
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. Having computed D(i, j) for small i, j we then compute larger
D(i, j) based on previously computed smaller values. The value of D(i, j) is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:
D[i, j] = min 
D[i − 1, j] + del-cost(source[i])
D[i, j − 1] + ins-cost(target[ j])
D[i − 1, j − 1] + sub-cost(source[i], target[ j])
If we assume the version of Levenshtein distance in which the insertions and
deletions each have a cost of 1 (ins-cost(·) = del-cost(·) = 1), and substitutions have
a cost of 2 (except substitution of identical letters have zero cost), the computation
for D(i, j) becomes:
D[i, j] = min 
D[i − 1, j] + 1
D[i, j − 1] + 1
D[i − 1, j − 1] + (cid:26) 2;
0;
The algorithm is summarized in Fig. 2.16; Fig. 2.17 shows the results of applying
the algorithm to the distance between intention and execution with the version of
Levenshtein in Eq. 2.4.
Knowing the minimum edit distance is useful for algorithms like ﬁnding poten-
tial spelling error corrections. But the edit distance algorithm is important in another
way; with a small change, it can also provide the minimum cost alignment between
two strings. Aligning two strings is useful throughout speech and language process-
ing. In speech recognition, minimum edit distance alignment is used to compute
the word error rate (Chapter 26). Alignment plays a role in machine translation, in
which sentences in a parallel corpus (a corpus with a text in two languages) need to
be matched to each other.
To extend the edit distance algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the edit distance matrix. Figure 2.18
shows this path with the boldfaced cell. Each boldfaced cell represents an alignment
of a pair of letters in the two strings. If two boldfaced cells occur in the same row,
there will be an insertion in going from the source to the target; two boldfaced cells
in the same column indicate a deletion.
Figure 2.18 also shows the intuition of how to compute this alignment path. The
computation proceeds in two steps. In the ﬁrst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. The backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
We’ve shown a schematic of these backpointers in Fig. 2.18. Some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. In the second step, we perform a backtrace. In a backtrace, we start
from the last cell (at the ﬁnal row and column), and follow the pointers back through
the dynamic programming matrix. Each complete path between the ﬁnal cell and the
initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the

(2.4)

backtrace

2 .5

• M IN IMUM ED I T D I STANC E

33

function M IN -ED I T-D I STANC E(source, target) returns min-distance
n ← L ENG TH(source)
m ← L ENG TH(target)
Create a distance matrix distance[n+1,m+1]

# Initialization: the zeroth row and column is the distance from the empty string
D[0,0] = 0

for each row i from 1 to n do

D[i,0] ← D[i-1,0] + del-cost(source[i])
for each column j from 1 to m do
D[0,j] ← D[0, j-1] + ins-cost(target[j])
# Recurrence relation:

for each row i from 1 to n do

for each column j from 1 to m do
D[i, j] ← M IN( D[i−1, j] + del-cost(source[i]),
D[i−1, j−1] + sub-cost(source[i], target[j]),
D[i, j−1] + ins-cost(target[j]))

# Termination

return D[n,m]

Figure 2.16 The minimum edit distance algorithm, an example of the class of dynamic
programming algorithms. The various costs can either be ﬁxed (e.g., ∀x, ins-cost(x) = 1)
or can be speciﬁc to the letter (to model the fact that some letters are more likely to be in-
serted than others). We assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x, x) = 0).

Src\Tar

#
i
n
t
e
n
t
i
o
n

#

0
1
2
3
4
5
6
7
8
9

e

1
2
3
4
3
4
5
6
7
8

x

2
3
4
5
4
5
6
7
8
9

e

3
4
5
6
5
6
7
8
9
10

c

4
5
6
7
6
7
8
9
10
11

u

5
6
7
8
7
8
9
10
11
12

t

6
7
8
7
8
9
8
9
10
11

i

7
6
7
8
9
10
9
8
9
10

o

8
7
8
9
10
11
10
9
8
9

n

9
8
7
8
9
10
11
10
9
8

Figure 2.17 Computation of minimum edit distance between intention and execution with
the algorithm of Fig. 2.16, using Levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.

minimum edit distance algorithm to store the pointers and compute the backtrace to
output an alignment.
While we worked our example with simple Levenshtein distance, the algorithm
in Fig. 2.16 allows arbitrary weights on the operations. For spelling correction, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. The Viterbi algorithm is a probabilistic extension of
minimum edit distance. Instead of computing the “minimum edit distance” between
two strings, Viterbi computes the “maximum probability alignment” of one string
with another. We’ll discuss this more in Chapter 8.

34 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

#

0

e
x
e
c
u
t
i
o
n
#
← 1
← 2
← 3
← 4
← 5
← 6
← 7
← 8 ← 9
i
↑ 1 (cid:45)←↑ 2 (cid:45)←↑ 3 (cid:45)←↑ 4 (cid:45)←↑ 5 (cid:45)←↑ 6 (cid:45)←↑ 7
(cid:45) 6
← 7 ← 8
n ↑ 2 (cid:45)←↑ 3 (cid:45)←↑ 4 (cid:45)←↑ 5 (cid:45)←↑ 6 (cid:45)←↑ 7 (cid:45)←↑ 8
↑ 7 (cid:45)←↑ 8 (cid:45) 7
t
↑ 3 (cid:45)←↑ 4 (cid:45)←↑ 5 (cid:45)←↑ 6 (cid:45)←↑ 7 (cid:45)←↑ 8
(cid:45) 7
←↑ 8 (cid:45)←↑ 9
↑ 8
e
↑ 4
(cid:45) 3
← 4 (cid:45)← 5
← 6
← 7 ←↑ 8 (cid:45)←↑ 9 (cid:45)←↑ 10
↑ 9
n ↑ 5
↑ 4 (cid:45)←↑ 5 (cid:45)←↑ 6 (cid:45)←↑ 7 (cid:45)←↑ 8 (cid:45)←↑ 9 (cid:45)←↑ 10 (cid:45)←↑ 11 (cid:45)↑ 10
t
↑ 6
↑ 5 (cid:45)←↑ 6 (cid:45)←↑ 7 (cid:45)←↑ 8 (cid:45)←↑ 9
(cid:45) 8
← 9
i
↑ 7
↑ 6 (cid:45)←↑ 7 (cid:45)←↑ 8 (cid:45)←↑ 9 (cid:45)←↑ 10
↑ 9
(cid:45) 8
o ↑ 8
↑ 7 (cid:45)←↑ 8 (cid:45)←↑ 9 (cid:45)←↑ 10 (cid:45)←↑ 11
↑ 9
n ↑ 9
↑ 8 (cid:45)←↑ 9 (cid:45)←↑ 10 (cid:45)←↑ 11 (cid:45)←↑ 12

← 10 ←↑ 11
← 9 ← 10

(cid:45) 8 ← 9
↑ 9 (cid:45) 8

↑ 10
↑ 11

Figure 2.18 When entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. After the table is full we compute an alignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
following the arrows back. The sequence of bold cells represents one possible minimum cost
alignment between the two strings. Diagram design after Gusﬁeld (1997).

↑ 10

2.6 Summary

This chapter introduced a fundamental tool in language processing, the regular ex-
pression, and showed how to perform basic text normalization tasks including

word segmentation and normalization, sentence segmentation, and stemming.

We also introduce the important minimum edit distance algorithm for comparing
strings. Here’s a summary of the main points we covered about these ideas:
• The regular expression language is a powerful tool for pattern-matching.
• Basic operations in regular expressions include concatenation of symbols,
disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors
(ˆ, $) and precedence operators ((,)).

• Word tokenization and normalization are generally done by cascades of

simple regular expressions substitutions or ﬁnite automata.
• The Porter algorithm is a simple and efﬁcient way to do stemming, stripping
off afﬁxes. It does not have high accuracy but may be useful for some tasks.
• The minimum edit distance between two strings is the minimum number of
operations it takes to edit one into the other. Minimum edit distance can be
computed by dynamic programming, which also results in an alignment of
the two strings.

Bibliographical and Historical Notes

Kleene (1951) and (1956) ﬁrst deﬁned regular expressions and the ﬁnite automaton,
based on the McCulloch-Pitts neuron. Ken Thompson was one of the ﬁrst to build
regular expressions compilers into editors for text searching (Thompson, 1968). His
editor ed included a command “g/regular expression/p”, or Global Regular Expres-
sion Print, which later became the Unix grep utility.
Text normalization algorithms has been applied since the beginning of the ﬁeld.
One of the earliest widely-used stemmers was Lovins (1968). Stemming was also
applied early to the digital humanities, by Packard (1973), who built an afﬁx-stripping
morphological parser for Ancient Greek. Currently a wide variety of code for tok-

EX ERC I SE S

35

enization and normalization is available, such as the Stanford Tokenizer (http://

nlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for

Twitter (O’Connor et al., 2010), or for sentiment (http://sentiment.christopherpotts.
net/tokenizing.html). See Palmer (2012) for a survey of text preprocessing.
While the max-match algorithm we describe is commonly used as a segmentation
baseline in languages like Chinese, higher accuracy algorithms like the Stanford
CRF segmenter, are based on sequence models; see Tseng et al. (2005a) and Chang
et al. (2008). NLTK is an essential tool that offers both useful Python libraries
(http://www.nltk.org) and textbook descriptions (Bird et al., 2009) of many al-
gorithms including text normalization and corpus interfaces.
For more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps
(1978), Egghe (2007) and Baayen (2001); Yasseri et al. (2012) discuss the relation-
ship with other measures of linguistic complexity. For more on edit distance, see the
excellent Gusﬁeld (1997). Our example measuring the edit distance from ‘intention’
to ‘execution’ was adapted from Kruskal (1983). There are various publicly avail-
able packages to compute edit distance, including Unix diff and the NIST sclite
program (NIST, 2005).
In his autobiography Bellman (1984) explains how he originally came up with
the term dynamic programming:
“...The 1950s were not good years for mathematical research. [the]
Secretary of Defense ...had a pathological fear and hatred of the word,
research...
I decided therefore to use the word, “programming”.
I
wanted to get across the idea that this was dynamic, this was multi-
stage... I thought, let’s ...
take a word that has an absolutely precise
meaning, namely dynamic... it’s impossible to use the word, dynamic,
in a pejorative sense. Try thinking of some combination that will pos-
sibly give it a pejorative meaning.
It’s impossible. Thus, I thought
dynamic programming was a good name. It was something not even a
Congressman could object to.”

Exercises

2.1 Write regular expressions for the following languages.
1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
3. the set of all strings from the alphabet a, b such that each a is immedi-
ately preceded by and immediately followed by a b;
2.2 Write regular expressions for the following languages. By “word”, we mean
an alphabetic string separated from other words by whitespace, any relevant
punctuation, line breaks, and so forth.
1. the set of all strings with two consecutive repeated words (e.g., “Hum-
bert Humbert” and “the the” but not “the bug” or “the big bug”);
2. all strings that start at the beginning of the line with an integer and that
end at the end of the line with a word;
3. all strings that have both the word grotto and the word raven in them
(but not, e.g., words like grottos that merely contain the word grotto);

36 CHA P TER 2

• R EGU LAR EX PR E S S ION S , T EX T NORMAL I ZAT ION , ED I T D I STANC E

2.3

2.5

4. write a pattern that places the ﬁrst word of an English sentence in a
register. Deal with punctuation.
Implement an ELIZA-like program, using substitutions such as those described
on page 18. You might want to choose a different domain than a Rogerian psy-
chologist, although keep in mind that you would need a domain in which your
program can legitimately engage in a lot of simple repetition.
2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution
cost 1) of “leda” to “deal”. Show your work (using the edit distance grid).
Figure out whether drive is closer to brief or to divers and what the edit dis-
tance is to each. You may use any version of distance that you like.
2.6 Now implement a minimum edit distance algorithm and use your hand-computed
results to check your code.
2.7 Augment the minimum edit distance algorithm to output an alignment; you
will need to store pointers and add a stage to compute the backtrace.
Implement the MaxMatch algorithm.
To test how well your MaxMatch algorithm works, create a test set by remov-
ing spaces from a set of sentences. Implement the Word Error Rate metric (the
number of word insertions + deletions + substitutions, divided by the length
in words of the correct string) and compute the WER for your test set.

2.8
2.9

CHAPTER

3 N-gram Language Models

“You are uniformly charming!” cried he, with a smile of associating and now
and then I bowed and they perceived a chaise and four to wish for.

Random sentence generated from a Jane Austen trigram model

Being able to predict the future is not always a good thing. Cassandra of Troy
had the gift of foreseeing but was cursed by Apollo that no one would believe her
predictions. Her warnings of the destruction of Troy were ignored and—well, let’s
just say that things didn’t turn out great for her.
In this chapter we take up the somewhat less fraught topic of predicting words.
What word, for example, is likely to follow

Please turn your homework ...

Hopefully, most of you concluded that a very likely word is in, or possibly over,
but probably not refrigerator or the. In the following sections we will formalize
this intuition by introducing models that assign a probability to each possible next
word. The same models will also serve to assign a probability to an entire sentence.
Such a model, for example, could predict that the following sequence has a much
higher probability of appearing in a text:

all of a sudden I notice three guys standing on the sidewalk

than does this same set of words in a different order:

on guys all I of notice sidewalk three a sudden standing the

Why would you want to predict upcoming words, or assign probabilities to sen-
tences? Probabilities are essential in any task in which we have to identify words

in noisy, ambiguous input, like speech recognition or handwriting recognition. In

the movie Take the Money and Run, Woody Allen tries to rob a bank with a sloppily
written hold-up note that the teller incorrectly reads as “I have a gub”. As Rus-
sell and Norvig (2002) point out, a language processing system could avoid making
this mistake by using the knowledge that the sequence “I have a gun” is far more
probable than the non-word “I have a gub” or even “I have a gull”.
In spelling correction, we need to ﬁnd and correct spelling errors like Their
are two midterms in this class, in which There was mistyped as Their. A sentence
starting with the phrase There are will be much more probable than one starting with
Their are, allowing a spellchecker to both detect and correct these errors.
Assigning probabilities to sequences of words is also essential in machine trans-
lation. Suppose we are translating a Chinese source sentence:

他 向 记者

介绍了 主要 内容

He to reporters introduced main content

38 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

As part of the process we might have built the following set of potential rough
English translations:
he introduced reporters to the main contents of the statement
he briefed to reporters the main contents of the statement

he briefed reporters on the main contents of the statement

A probabilistic model of word sequences could suggest that briefed reporters on
is a more probable English phrase than briefed to reporters (which has an awkward
to after briefed) or introduced reporters to (which uses a verb that is less ﬂuent
English in this context), allowing us to correctly select the boldfaced sentence above.
Probabilities are also important for augmentative communication (Newell et al.,
1998) systems. People like the late physicist Stephen Hawking who are unable to
physically talk or sign can instead use simple movements to select words from a
menu to be spoken by the system. Word prediction can be used to suggest likely
words for the menu.
Models that assign probabilities to sequences of words are called language mod-
els or LMs. In this chapter we introduce the simplest model that assigns probabilities
to sentences and sequences of words, the n-gram. An n-gram is a sequence of N
words: a 2-gram (or bigram) is a two-word sequence of words like “please turn”,
“turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word se-
quence of words like “please turn your”, or “turn your homework”. We’ll see how
to use n-gram models to estimate the probability of the last word of an n-gram given
the previous words, and also to assign probabilities to entire sequences. In a bit of
terminological ambiguity, we usually drop the word “model”, and thus the term n-
gram is used to mean either the word sequence itself or the predictive model that
assigns it a probability.

language model
LM
n-gram

3.1 N-Grams

Let’s begin with the task of computing P(w|h), the probability of a word w given
some history h. Suppose the history h is “its water is so transparent that” and we
want to know the probability that the next word is the:
P(the|its water is so transparent that).
One way to estimate this probability is from relative frequency counts: take a
very large corpus, count the number of times we see its water is so transparent that,
and count the number of times this is followed by the. This would be answering the
question “Out of the times we saw the history h, how many times was it followed by
the word w”, as follows:
P(the|its water is so transparent that) =
C(its water is so transparent that the)
C(its water is so transparent that)

(3.1)

(3.2)

With a large enough corpus, such as the web, we can compute these counts and
estimate the probability from Eq. 3.2. You should pause now, go to the web, and
compute this estimate for yourself.
While this method of estimating probabilities directly from counts works ﬁne in
many cases, it turns out that even the web isn’t big enough to give us good estimates

39

3 .1

• N -GRAM S
in most cases. This is because language is creative; new sentences are created all the
time, and we won’t always be able to count entire sentences. Even simple extensions
of the example sentence may have counts of zero on the web (such as “Walden
Pond’s water is so transparent that the”).
Similarly, if we wanted to know the joint probability of an entire sequence of
words like its water is so transparent, we could do it by asking “out of all possible
sequences of ﬁve words, how many of them are its water is so transparent?” We
would have to get the count of its water is so transparent and divide by the sum of
the counts of all possible ﬁve word sequences. That seems rather a lot to estimate!
For this reason, we’ll need to introduce cleverer ways of estimating the proba-
bility of a word w given a history h, or the probability of an entire word sequence W .
Let’s start with a little formalizing of notation. To represent the probability of a par-
ticular random variable Xi taking on the value “the”, or P(Xi = “the”), we will use
the simpliﬁcation P(the). We’ll represent a sequence of N words either as w1 . . . wn
or wn
1 (so the expression wn−1
1 means the string w1 , w2 , ..., wn−1 ). For the joint prob-
ability of each word in a sequence having a particular value P(X = w1 , Y = w2 , Z =

w3 , ...,W = wn ) we’ll use P(w1 , w2 , ..., wn ).

Now how can we compute probabilities of entire sequences like P(w1 , w2 , ..., wn )?
One thing we can do is decompose this probability using the chain rule of proba-

bility:

P(X1 ...Xn ) = P(X1 )P(X2 |X1 )P(X3 |X 2
1 ) . . . P(Xn |X n−1
=
)

1

P(Xk |X k−1

1

n(cid:89)k=1

Applying the chain rule to words, we get

P(wn

1 ) = P(w1 )P(w2 |w1 )P(w3 |w2
1 ) . . . P(wn |wn−1
=
)

1

)

n(cid:89)k=1

P(wk |wk−1

1

)

(3.3)

(3.4)

1

The chain rule shows the link between computing the joint probability of a se-
quence and computing the conditional probability of a word given previous words.
Equation 3.4 suggests that we could estimate the joint probability of an entire se-
quence of words by multiplying together a number of conditional probabilities. But
using the chain rule doesn’t really seem to help us! We don’t know any way to
compute the exact probability of a word given a long sequence of preceding words,
P(wn |wn−1
). As we said above, we can’t just estimate by counting the number of
times every word occurs following every long string, because language is creative
and any particular context might have never occurred before!
The intuition of the n-gram model is that instead of computing the probability of
a word given its entire history, we can approximate the history by just the last few
words.
The bigram model, for example, approximates the probability of a word given
all the previous words P(wn |wn−1
) by using only the conditional probability of the
preceding word P(wn |wn−1 ). In other words, instead of computing the probability
P(the|Walden Pond’s water is so transparent that)
(3.5)

1

bigram

40 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

we approximate it with the probability
P(the|that)

(3.6)

Markov

n-gram

maximum
likelihood
estimation

normalize

1

(3.7)

When we use a bigram model to predict the conditional probability of the next
word, we are thus making the following approximation:
P(wn |wn−1
) ≈ P(wn |wn−1 )
The assumption that the probability of a word depends only on the previous word
is called a Markov assumption. Markov models are the class of probabilistic models
that assume we can predict the probability of some future unit without looking too
far into the past. We can generalize the bigram (which looks one word into the past)
to the trigram (which looks two words into the past) and thus to the n-gram (which
looks n − 1 words into the past).
Thus, the general equation for this n-gram approximation to the conditional
probability of the next word in a sequence is
P(wn |wn−1
) ≈ P(wn |wn−1
(3.8)
Given the bigram assumption for the probability of an individual word, we can
compute the probability of a complete word sequence by substituting Eq. 3.7 into
Eq. 3.4:

n−N+1 )

1

P(wn

n(cid:89)k=1

(3.9)

1 ) ≈

P(wk |wk−1 )
How do we estimate these bigram or n-gram probabilities? An intuitive way to
estimate probabilities is called maximum likelihood estimation or MLE. We get
the MLE estimate for the parameters of an n-gram model by getting counts from a
corpus, and normalizing the counts so that they lie between 0 and 1.1
For example, to compute a particular bigram probability of a word y given a
previous word x, we’ll compute the count of the bigram C(xy) and normalize by the
sum of all the bigrams that share the same ﬁrst word x:
(cid:80)w C(wn−1w)
We can simplify this equation, since the sum of all bigram counts that start with
a given word wn−1 must be equal to the unigram count for that word wn−1 (the reader
should take a moment to be convinced of this):

P(wn |wn−1 ) =

C(wn−1wn )

(3.10)

P(wn |wn−1 ) =

C(wn−1wn )
C(wn−1 )

(3.11)

Let’s work through an example using a mini-corpus of three sentences. We’ll
ﬁrst need to augment each sentence with a special symbol <s> at the beginning
of the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a
special end-symbol. </s>2

1 For probabilistic models, normalizing means dividing by some total count so that the resulting prob-
abilities fall legally between 0 and 1.
2 We need the end-symbol to make the bigram grammar a true probability distribution. Without an
end-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model
would deﬁne an inﬁnite set of probability distributions, with one distribution per sentence length. See
Exercise 3.5.

relative
frequency

3 .1

• N -GRAM S

41

<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>

Here are the calculations for some of the bigram probabilities from this corpus

P(I|<s>) = 2
P(</s>|Sam) = 1

3 = .67
2 = 0.5

P(Sam|<s>) = 1
P(Sam|am) = 1

3 = .33
2 = .5

P(am|I) = 2
P(do|I) = 1

3 = .67
3 = .33

For the general case of MLE n-gram parameter estimation:

P(wn |wn−1

n−N+1 ) =

C(wn−1
C(wn−1

n−N+1wn )
n−N+1 )

(3.12)

Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the
observed frequency of a particular sequence by the observed frequency of a preﬁx.
This ratio is called a relative frequency. We said above that this use of relative
frequencies as a way to estimate probabilities is an example of maximum likelihood
estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood
of the training set T given the model M (i.e., P(T |M )). For example, suppose the
word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.
What is the probability that a random word selected from some other text of, say,
a million words will be the word Chinese? The MLE of its probability is
or .0004. Now .0004 is not the best possible estimate of the probability of Chinese
occurring in all situations; it might turn out that in some other corpus or context
Chinese is a very unlikely word. But it is the probability that makes it most likely
that Chinese will occur 400 times in a million-word corpus. We present ways to
modify the MLE estimates slightly to get better probability estimates in Section 3.4.
Let’s move on to some examples from a slightly larger corpus than our 14-word
example above. We’ll use data from the now-defunct Berkeley Restaurant Project,
a dialogue system from the last century that answered questions about a database
of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-
normalized sample user queries (a sample of 9332 sentences is on the website):

400
1000000

can you tell me about any good cantonese restaurants close by
mid priced thai food is what i’m looking for
tell me about chez panisse
can you give me a listing of the kinds of food that are available
i’m looking for a good place to eat breakfast
when is caffe venezia open during the day

Figure 3.1 shows the bigram counts from a piece of a bigram grammar from the
Berkeley Restaurant Project. Note that the majority of the values are zero. In fact,
we have chosen the sample words to cohere with each other; a matrix selected from
a random set of seven words would be even more sparse.
Figure 3.2 shows the bigram probabilities after normalization (dividing each cell
in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of
unigram probabilities):

i

want to

eat chinese food lunch spend

2533 927

2417 746 158

1093 341

278

Here are a few other useful probabilities:

42 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

i
want
to
eat
chinese
food
lunch
spend

i

5
2
2
0
1
15
2
1

want

827
0
0
0
0
0
0
0

to

0
608
4
2
0
15
0
1

eat

9
1
686
0
0
0
0
0

chinese

0
6
2
16
0
1
0
0

food

0
6
0
2
82
4
1
0

lunch

0
5
6
42
1
0
0
0

spend

2
1
211
0
0
0
0
0

Figure 3.1 Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-
rant Project corpus of 9332 sentences. Zero counts are in gray.

i

want

to

eat

chinese

food

lunch

spend

i
want
to
eat
chinese 0.0063
food
lunch
spend

0.002
0.33
0.0022
0
0.00083 0
0
0
0
0
0
0

0.014
0.0059
0.0036

0
0.0036 0
0
0
0.00079
0.66
0.0011 0.0065
0.0065 0.0054 0.0011
0.0017 0.28
0.00083 0
0.0025 0.087
0.0027 0
0.021
0.0027 0.056
0
0
0
0
0.52
0.0063 0
0.014
0
0.00092 0.0037 0
0
0
0
0
0.0029 0
0
0.0036 0
0
0
0
0

Figure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus
of 9332 sentences. Zero probabilities are in gray.

P(i|<s>) = 0.25

P(food|english) = 0.5

P(english|want) = 0.0011
P(</s>|food) = 0.68

Now we can compute the probability of sentences like I want English food or
I want Chinese food by simply multiplying the appropriate bigram probabilities to-
gether, as follows:

P(<s> i want english food </s>)
= P(i|<s>)P(want|i)P(english|want)
P(food|english)P(</s>|food)

= .25 × .33 × .0011 × 0.5 × 0.68
= .000031

We leave it as Exercise 3.2 to compute the probability of i want chinese food.
What kinds of linguistic phenomena are captured in these bigram statistics?
Some of the bigram probabilities above encode some facts that we think of as strictly
syntactic in nature, like the fact that what comes after eat is usually a noun or an
adjective, or that what comes after to is usually a verb. Others might be a fact about
the personal assistant task, like the high probability of sentences beginning with
the words I. And some might even be cultural rather than linguistic, like the higher
probability that people are looking for Chinese versus English food.

trigram
4-gram
5-gram

Some practical issues: Although for pedagogical purposes we have only described
bigram models, in practice it’s more common to use trigram models, which con-
dition on the previous two words rather than the previous word, or 4-gram or even
5-gram models, when there is sufﬁcient training data. Note that for these larger n-
grams, we’ll need to assume extra context for the contexts to the left and right of the

log
probabilities

3 .2

• EVA LUAT ING LANGUAG E MODE L S

43

sentence end. For example, to compute trigram probabilities at the very beginning of
the sentence, we can use two pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s>).
We always represent and compute language model probabilities in log format
as log probabilities. Since probabilities are (by deﬁnition) less than or equal to
1, the more probabilities we multiply together, the smaller the product becomes.
Multiplying enough n-grams together would result in numerical underﬂow. By using
log probabilities instead of raw probabilities, we get numbers that are not as small.
Adding in log space is equivalent to multiplying in linear space, so we combine log
probabilities by adding them. The result of doing all computation and storage in log
space is that we only need to convert back into probabilities if we need to report
them at the end; then we can just take the exp of the logprob:

p1 × p2 × p3 × p4 = exp(log p1 + log p2 + log p3 + log p4 )

(3.13)

3.2 Evaluating Language Models

extrinsic
evaluation

intrinsic
evaluation

training set

test set
held out

The best way to evaluate the performance of a language model is to embed it in
an application and measure how much the application improves. Such end-to-end
evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to
know if a particular improvement in a component is really going to help the task
at hand. Thus, for speech recognition, we can compare the performance of two
language models by running the speech recognizer twice, once with each language
model, and seeing which gives the more accurate transcription.
Unfortunately, running big NLP systems end-to-end is often very expensive. In-
stead, it would be nice to have a metric that can be used to quickly evaluate potential
improvements in a language model. An intrinsic evaluation metric is one that mea-
sures the quality of a model independent of any application.
For an intrinsic evaluation of a language model we need a test set. As with many
of the statistical models in our ﬁeld, the probabilities of an n-gram model come from
the corpus it is trained on, the training set or training corpus. We can then measure
the quality of an n-gram model by its performance on some unseen data called the
test set or test corpus. We will also sometimes call test sets and other datasets that
are not in our training sets held out corpora because we hold them out from the
training data.
So if we are given a corpus of text and want to compare two different n-gram
models, we divide the data into training and test sets, train the parameters of both
models on the training set, and then compare how well the two trained models ﬁt the
test set.
But what does it mean to “ﬁt the test set”? The answer is simple: whichever
model assigns a higher probability to the test set—meaning it more accurately
predicts the test set—is a better model. Given two probabilistic models, the better
model is the one that has a tighter ﬁt to the test data or that better predicts the details
of the test data, and hence will assign a higher probability to the test data.
Since our evaluation metric is based on test set probability, it’s important not to
let the test sentences into the training set. Suppose we are trying to compute the
probability of a particular “test” sentence. If our test sentence is part of the training
corpus, we will mistakenly assign it an artiﬁcially high probability when it occurs
in the test set. We call this situation training on the test set. Training on the test
set introduces a bias that makes the probabilities all look too high, and causes huge

44 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

development
test

inaccuracies in perplexity, the probability-based metric we introduce below.
Sometimes we use a particular test set so often that we implicitly tune to its
characteristics. We then need a fresh test set that is truly unseen. In such cases, we
call the initial test set the development test set or, devset. How do we divide our
data into training, development, and test sets? We want our test set to be as large
as possible, since a small test set may be accidentally unrepresentative, but we also
want as much training data as possible. At the minimum, we would want to pick
the smallest test set that gives us enough statistical power to measure a statistically
signiﬁcant difference between two potential models. In practice, we often just divide
our data into 80% training, 10% development, and 10% test. Given a large corpus
that we want to divide into training and test, test data can either be taken from some
continuous sequence of text inside the corpus, or we can remove smaller “stripes”
of text from randomly selected parts of our corpus and combine them into a test set.

3.2.1 Perplexity

perplexity

In practice we don’t use raw probability as our metric for evaluating language mod-
els, but a variant called perplexity. The perplexity (sometimes called PP for short)
of a language model on a test set is the inverse probability of the test set, normalized
by the number of words. For a test set W = w1w2 . . . wN ,:

PP(W ) = P(w1w2 . . . wN )− 1
= N(cid:115)

P(w1w2 . . . wN )

1

N

(3.14)

(3.15)

(3.16)

1

N(cid:89)i=1

P(wi |w1 . . . wi−1 )

We can use the chain rule to expand the probability of W :
PP(W ) = N(cid:118)(cid:117)(cid:117)(cid:116)
Thus, if we are computing the perplexity of W with a bigram language model,
we get:
PP(W ) = N(cid:118)(cid:117)(cid:117)(cid:116)
1
P(wi |wi−1 )
Note that because of the inverse in Eq. 3.15, the higher the conditional probabil-
ity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is
equivalent to maximizing the test set probability according to the language model.
What we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire se-
quence of words in some test set. Since this sequence will cross many sentence
boundaries, we need to include the begin- and end-sentence markers <s> and </s>
in the probability computation. We also need to include the end-of-sentence marker
</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-
kens N .
There is another way to think about perplexity: as the weighted average branch-
ing factor of a language. The branching factor of a language is the number of possi-
ble next words that can follow any word. Consider the task of recognizing the digits

N(cid:89)i=1

3 .3

• G ENERAL I ZAT ION AND Z ERO S

45

in English (zero, one, two,..., nine), given that each of the 10 digits occurs with equal
probability P = 1
10 . The perplexity of this mini-language is in fact 10. To see that,
imagine a string of digits of length N . By Eq. 3.15, the perplexity will be

PP(W ) = P(w1w2 . . . wN )− 1
)− 1

N

N

N

= (

1
10
1
10
= 10

=

−1

(3.17)

But suppose that the number zero is really frequent and occurs 10 times more
often than other numbers. Now we should expect the perplexity to be lower since
most of the time the next number will be zero. Thus, although the branching factor
is still 10, the perplexity or weighted branching factor is smaller. We leave this
calculation as an exercise to the reader.
We see in Section 3.7 that perplexity is also closely related to the information-
theoretic notion of entropy.
Finally, let’s look at an example of how perplexity can be used to compare dif-
ferent n-gram models. We trained unigram, bigram, and trigram grammars on 38
million words (including start-of-sentence tokens) from the Wall Street Journal, us-
ing a 19,979 word vocabulary. We then computed the perplexity of each of these
models on a test set of 1.5 million words with Eq. 3.16. The table below shows the
perplexity of a 1.5 million word WSJ test set according to each of these grammars.

Unigram Bigram Trigram
Perplexity 962

170
109
As we see above, the more information the n-gram gives us about the word
sequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related
inversely to the likelihood of the test sequence according to the model).
Note that in computing perplexities, the n-gram model P must be constructed
without any knowledge of the test set or any prior knowledge of the vocabulary of
the test set. Any kind of knowledge of the test set can cause the perplexity to be
artiﬁcially low. The perplexity of two language models is only comparable if they
use identical vocabularies.
An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-
provement in the performance of a language processing task like speech recognition
or machine translation. Nonetheless, because perplexity often correlates with such
improvements, it is commonly used as a quick check on an algorithm. But a model’s
improvement in perplexity should always be conﬁrmed by an end-to-end evaluation
of a real task before concluding the evaluation of the model.

3.3 Generalization and Zeros

The n-gram model, like many statistical models, is dependent on the training corpus.
One implication of this is that the probabilities often encode speciﬁc facts about a
given training corpus. Another implication is that n-grams do a better and better job
of modeling the training corpus as we increase the value of N .

46 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

We can visualize both of these facts by borrowing the technique of Shannon
(1951) and Miller and Selfridge (1950) of generating random sentences from dif-
ferent n-gram models.
It’s simplest to visualize how this works for the unigram
case. Imagine all the words of the English language covering the probability space
between 0 and 1, each word covering an interval proportional to its frequency. We
choose a random value between 0 and 1 and print the word whose interval includes
this chosen value. We continue choosing random numbers and generating words
until we randomly generate the sentence-ﬁnal token </s>. We can use the same
technique to generate bigrams by ﬁrst generating a random bigram that starts with
<s> (according to its bigram probability). Let’s say the second word of that bigram
is w. We next chose a random bigram starting with w (again, drawn according to its
bigram probability), and so on.
To give an intuition for the increasing power of higher-order n-grams, Fig. 3.3
shows random sentences generated from unigram, bigram, trigram, and 4-gram
models trained on Shakespeare’s works.

1 –To him swallowed confess hear both. Which. Of save on trail for are ay device and
rote life have
gram
–Hill he late speaks; or! a more to leg less ﬁrst you enter
2 –Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live
king. Follow.
gram
–What means, sir. I confess she? then all sorts, he is trim, captain.
3 –Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,
’tis done.
gram
–This shall forbid it should be branded, if renown made it empty.
4 –King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A
great banquet serv’d in;
gram
–It cannot be but so.

Figure 3.3 Eight sentences randomly generated from four n-grams computed from Shakespeare’s works. All
characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected
for capitalization to improve readability.

The longer the context on which we train the model, the more coherent the sen-
tences. In the unigram sentences, there is no coherent relation between words or any
sentence-ﬁnal punctuation. The bigram sentences have some local word-to-word
coherence (especially if we consider that punctuation counts as a word). The tri-
gram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a
careful investigation of the 4-gram sentences shows that they look a little too much
like Shakespeare. The words It cannot be but so are directly from King John. This is
because, not to put the knock on Shakespeare, his oeuvre is not very large as corpora
go (N = 884, 647,V = 29, 066), and our n-gram probability matrices are ridiculously
sparse. There are V 2 = 844, 000, 000 possible bigrams alone, and the number of pos-
sible 4-grams is V 4 = 7 × 1017 . Thus, once the generator has chosen the ﬁrst 4-gram
(It cannot be but), there are only ﬁve possible continuations (that, I, he, thou, and
so); indeed, for many 4-grams, there is only one continuation.
To get an idea of the dependence of a grammar on its training set, let’s look at an
n-gram grammar trained on a completely different corpus: the Wall Street Journal
(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so
we might expect some overlap between our n-grams for the two genres. Fig. 3.4

3 .3

• G ENERAL I ZAT ION AND Z ERO S

47

shows sentences generated by unigram, bigram, and trigram grammars trained on
40 million words from WSJ.
1 Months the my and issue of year foreign new exchange’s september
gram
were recession exchange new endorsed a acquire to six executives
2 Last December through the way to preserve the Hudson corporation N.
B. E. C. Taylor would seem to complete the major central planners one
gram
point ﬁve percent of U. S. E. has already old M. X. corporation of living
on information such as more frequently ﬁshing to keep her
3 They also point to ninety nine point six billion dollars from two hundred
four oh six three percent of the rates of interest stores as Mexico and
gram
Brazil on market conditions

Figure 3.4 Three sentences randomly generated from three n-gram models computed from
40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-
tion as words. Output was then hand-corrected for capitalization to improve readability.

Compare these examples to the pseudo-Shakespeare in Fig. 3.3. While they both
model “English-like sentences”, there is clearly no overlap in generated sentences,
and little overlap even in small phrases. Statistical models are likely to be pretty use-
less as predictors if the training sets and the test sets are as different as Shakespeare
and WSJ.
How should we deal with this problem when we build n-gram models? One step
is to be sure to use a training corpus that has a similar genre to whatever task we are
trying to accomplish. To build a language model for translating legal documents,
we need a training corpus of legal documents. To build a language model for a
question-answering system, we need a training corpus of questions.
It is equally important to get training data in the appropriate dialect, especially
when processing social media posts or spoken transcripts. Thus tweets in AAVE
(African American Vernacular English) often use words like ﬁnna—an auxiliary
verb that markes immediate future tense —that don’t occur in other dialects, or
spellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):
(3.18) Bored af den my phone ﬁnna die!!!
while tweets from varieties like Nigerian English have markedly different vocabu-
lary and n-gram patterns from American English (Jurgens et al., 2017):
(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u
tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter
Matching genres and dialects is still not sufﬁcient. Our models may still be
subject to the problem of sparsity. For any n-gram that occurred a sufﬁcient number
of times, we might have a good estimate of its probability. But because any corpus is
limited, some perfectly acceptable English word sequences are bound to be missing
from it. That is, we’ll have many cases of putative “zero probability n-grams” that
should really have some non-zero probability. Consider the words that follow the
bigram denied the in the WSJ Treebank3 corpus, together with their counts:
denied the allegations: 5
denied the speculation: 2
denied the rumors:
1
denied the report:
1
But suppose our test set has phrases like:

48 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

zeros

denied the offer
denied the loan
Our model will incorrectly estimate that the P(offer|denied the) is 0!
These zeros— things that don’t ever occur in the training set but do occur in
the test set—are a problem for two reasons. First, their presence means we are
underestimating the probability of all sorts of words that might occur, which will
hurt the performance of any application we want to run on this data.
Second, if the probability of any word in the test set is 0, the entire probability
of the test set is 0. By deﬁnition, perplexity is based on the inverse probability of the
test set. Thus if some words have zero probability, we can’t compute perplexity at
all, since we can’t divide by 0!

closed
vocabulary

OOV
open
vocabulary

3.3.1 Unknown Words

The previous section discussed the problem of words whose bigram probability is
zero. But what about words we simply have never seen before?
Sometimes we have a language task in which this can’t happen because we know
all the words that can occur. In such a closed vocabulary system the test set can
only contain words from this lexicon, and there will be no unknown words. This is
a reasonable assumption in some domains, such as speech recognition or machine
translation, where we have a pronunciation dictionary or a phrase table that are ﬁxed
in advance, and so the language model can only use the words in that dictionary or
phrase table.
In other cases we have to deal with words we haven’t seen before, which we’ll
call unknown words, or out of vocabulary (OOV) words. The percentage of OOV
words that appear in the test set is called the OOV rate. An open vocabulary system
is one in which we model these potential unknown words in the test set by adding a
pseudo-word called <UNK>.
There are two common ways to train the probabilities of the unknown word
model <UNK>. The ﬁrst one is to turn the problem back into a closed vocabulary one
by choosing a ﬁxed vocabulary in advance:
1. Choose a vocabulary (word list) that is ﬁxed in advance.
2. Convert in the training set any word that is not in this set (any OOV word) to
the unknown word token <UNK> in a text normalization step.
3. Estimate the probabilities for <UNK> from its counts just like any other regular
word in the training set.
The second alternative, in situations where we don’t have a prior vocabulary in ad-
vance, is to create such a vocabulary implicitly, replacing words in the training data
by <UNK> based on their frequency. For example we can replace by <UNK> all words
that occur fewer than n times in the training set, where n is some small number, or
equivalently select a vocabulary size V in advance (say 50,000) and choose the top
V words by frequency and replace the rest by UNK. In either case we then proceed
to train the language model as before, treating <UNK> like a regular word.
The exact choice of <UNK> model does have an effect on metrics like perplexity.
A language model can achieve low perplexity by choosing a small vocabulary and
assigning the unknown word a high probability. For this reason, perplexities should
only be compared across language models with the same vocabularies (Buck et al.,
2014).

3.4 Smoothing

3 .4

• SMOOTH ING

49

smoothing
discounting

Laplace
smoothing

What do we do with words that are in our vocabulary (they are not unknown words)
but appear in a test set in an unseen context (for example they appear after a word
they never appeared after in training)? To keep a language model from assigning
zero probability to these unseen events, we’ll have to shave off a bit of probability
mass from some more frequent events and give it to the events we’ve never seen.
This modiﬁcation is called smoothing or discounting. In this section and the fol-
lowing ones we’ll introduce a variety of ways to do smoothing: add-1 smoothing,

add-k smoothing, stupid backoff, and Kneser-Ney smoothing.

3.4.1 Laplace Smoothing

The simplest way to do smoothing is to add one to all the bigram counts, before
we normalize them into probabilities. All the counts that used to be zero will now
have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called
Laplace smoothing. Laplace smoothing does not perform well enough to be used
in modern n-gram models, but it usefully introduces many of the concepts that we
see in other smoothing algorithms, gives a useful baseline, and is also a practical
smoothing algorithm for other tasks like text classiﬁcation (Chapter 4).
Let’s start with the application of Laplace smoothing to unigram probabilities.
Recall that the unsmoothed maximum likelihood estimate of the unigram probability
of the word wi is its count ci normalized by the total number of word tokens N :

P(wi ) =

ci
N
Laplace smoothing merely adds one to each count (hence its alternate name add-
one smoothing). Since there are V words in the vocabulary and each one was incre-
mented, we also need to adjust the denominator to take into account the extra V
observations. (What happens to our P values if we don’t increase the denominator?)

add-one

PLaplace (wi ) =

ci + 1
N + V

(3.20)

Instead of changing both the numerator and denominator, it is convenient to
describe how a smoothing algorithm affects the numerator, by deﬁning an adjusted
count c∗ . This adjusted count is easier to compare directly with the MLE counts and
can be turned into a probability like an MLE count by normalizing by N . To deﬁne
this count, since we are only changing the numerator in addition to adding 1 we’ll
also need to multiply by a normalization factor N

N+V :

c∗i = (ci + 1)

N
N + V
We can now turn c∗i into a probability P∗i by normalizing by N .
A related way to view smoothing is as discounting (lowering) some non-zero
counts in order to get the probability mass that will be assigned to the zero counts.
Thus, instead of referring to the discounted counts c∗ , we might describe a smooth-
ing algorithm in terms of a relative discount dc , the ratio of the discounted counts to
the original counts:

(3.21)

discounting

discount

50 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

c∗

dc =

c
Now that we have the intuition for the unigram case, let’s smooth our Berkeley
Restaurant Project bigrams. Figure 3.5 shows the add-one smoothed counts for the
bigrams in Fig. 3.1.

i
want
to
eat
chinese
food
lunch
spend

i

6
3
3
1
2
16
3
2

want

828
1
1
1
1
1
1
1

to

1
609
5
3
1
16
1
2

eat

10
2
687
1
1
1
1
1

chinese

food

lunch

spend

1
7
3
17
1
2
1
1

1
7
1
3
83
5
2
1

1
6
7
43
2
1
1
1

3
2
212
1
1
1
1
1

Figure 3.5 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in
the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.

Figure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.
Recall that normal bigram probabilities are computed by normalizing each row of
counts by the unigram count:

P(wn |wn−1 ) =

C(wn−1wn )
C(wn−1 )

For add-one smoothed bigram counts, we need to augment the unigram count by
the number of total word types in the vocabulary V :

(3.22)

(3.23)

P∗Laplace (wn |wn−1 ) =

C(wn−1wn ) + 1
(cid:80)w (C(wn−1w) + 1)

=

C(wn−1wn ) + 1
C(wn−1 ) + V

Thus, each of the unigram counts given in the previous section will need to be
augmented by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.6.

i
want
to
eat
chinese
food
lunch
spend

i

0.0015
0.0013
0.00078
0.00046
0.0012
0.0063
0.0017
0.0012

want

0.21
0.00042
0.00026
0.00046
0.00062
0.00039
0.00056
0.00058

to

0.00025
0.26
0.0013
0.0014
0.00062
0.0063
0.00056
0.0012

eat

0.0025
0.00084
0.18
0.00046
0.00062
0.00039
0.00056
0.00058

chinese

0.00025
0.0029
0.00078
0.0078
0.00062
0.00079
0.00056
0.00058

food

0.00025
0.0029
0.00026
0.0014
0.052
0.002
0.0011
0.00058

lunch

0.00025
0.0025
0.0018
0.02
0.0012
0.00039
0.00056
0.00058

spend

0.00075
0.00084
0.055
0.00046
0.00062
0.00039
0.00056
0.00058

Figure 3.6 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP
corpus of 9332 sentences. Previously-zero probabilities are in gray.

It is often convenient to reconstruct the count matrix so we can see how much a
smoothing algorithm has changed the original counts. These adjusted counts can be
computed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.

c∗ (wn−1wn ) =

[C(wn−1wn ) + 1] × C(wn−1 )
C(wn−1 ) + V

(3.24)

3 .4

• SMOOTH ING

51

i
want
to
eat
chinese
food
lunch
spend

i

3.8
1.2
1.9
0.34
0.2
6.9
0.57
0.32

want

527
0.39
0.63
0.34
0.098
0.43
0.19
0.16

to

0.64
238
3.1
1
0.098
6.9
0.19
0.32

eat

6.4
0.78
430
0.34
0.098
0.43
0.19
0.16

chinese

0.64
2.7
1.9
5.8
0.098
0.86
0.19
0.16

food

0.64
2.7
0.63
1
8.2
2.2
0.38
0.16

lunch

spend

0.64
2.3
4.4
15
0.2
0.43
0.19
0.16

1.9
0.78
133
0.34
0.098
0.43
0.19
0.16

Figure 3.7 Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus
of 9332 sentences. Previously-zero counts are in gray.

Note that add-one smoothing has made a very big change to the counts. C(want to)
changed from 608 to 238! We can see this in probability space as well: P(to|want)
decreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at
the discount d (the ratio between new and old counts) shows us how strikingly the
counts for each preﬁx word have been reduced; the discount for the bigram want to
is .39, while the discount for Chinese food is .10, a factor of 10!
The sharp change in counts and probabilities occurs because too much probabil-
ity mass is moved to all the zeros.

3.4.2 Add-k smoothing

One alternative to add-one smoothing is to move a bit less of the probability mass
from the seen to the unseen events. Instead of adding 1 to each count, we add a frac-
tional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.

add-k

C(wn−1wn ) + k

(3.25)

P∗Add-k (wn |wn−1 ) =

C(wn−1 ) + kV
Add-k smoothing requires that we have a method for choosing k; this can be
done, for example, by optimizing on a devset. Although add-k is useful for some
tasks (including text classiﬁcation), it turns out that it still doesn’t work well for
language modeling, generating counts with poor variances and often inappropriate
discounts (Gale and Church, 1994).

3.4.3 Backoff and Interpolation

The discounting we have been discussing so far can help solve the problem of zero
frequency n-grams. But there is an additional source of knowledge we can draw on.
If we are trying to compute P(wn |wn−2wn−1 ) but we have no examples of a particular
trigram wn−2wn−1wn , we can instead estimate its probability by using the bigram
probability P(wn |wn−1 ). Similarly, if we don’t have counts to compute P(wn |wn−1 ),
we can look to the unigram P(wn ).
In other words, sometimes using less context is a good thing, helping to general-
ize more for contexts that the model hasn’t learned much about. There are two ways
to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is
sufﬁcient, otherwise we use the bigram, otherwise the unigram. In other words, we
only “back off ” to a lower-order n-gram if we have zero evidence for a higher-order
n-gram. By contrast, in interpolation, we always mix the probability estimates from
all the n-gram estimators, weighing and combining the trigram, bigram, and unigram
counts.

backoff

interpolation

52 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

In simple linear interpolation, we combine different order n-grams by linearly in-
terpolating all the models. Thus, we estimate the trigram probability P(wn |wn−2wn−1 )
by mixing together the unigram, bigram, and trigram probabilities, each weighted
by a λ :

ˆP(wn |wn−2wn−1 ) = λ1P(wn |wn−2wn−1 )

+λ2P(wn |wn−1 )
+λ3P(wn )

such that the λ s sum to 1:

λi = 1

(cid:88)i

(3.26)

(3.27)

In a slightly more sophisticated version of linear interpolation, each λ weight is
computed by conditioning on the context. This way, if we have particularly accurate
counts for a particular bigram, we assume that the counts of the trigrams based on
this bigram will be more trustworthy, so we can make the λ s for those trigrams
higher and thus give that trigram more weight in the interpolation. Equation 3.28
shows the equation for interpolation with context-conditioned weights:

ˆP(wn |wn−2wn−1 ) = λ1 (wn−1
n−2 )P(wn |wn−2wn−1 )
+λ2 (wn−1
n−2 )P(wn |wn−1 )
+ λ3 (wn−1
n−2 )P(wn )

(3.28)

How are these λ values set? Both the simple interpolation and conditional inter-
polation λ s are learned from a held-out corpus. A held-out corpus is an additional
training corpus that we use to set hyperparameters like these λ values, by choosing
the λ values that maximize the likelihood of the held-out corpus. That is, we ﬁx
the n-gram probabilities and then search for the λ values that—when plugged into
Eq. 3.26—give us the highest probability of the held-out set. There are various ways
to ﬁnd this optimal set of λ s. One way is to use the EM algorithm, an iterative
learning algorithm that converges on locally optimal λ s (Jelinek and Mercer, 1980).
In a backoff n-gram model, if the n-gram we need has zero counts, we approxi-
mate it by backing off to the (N-1)-gram. We continue backing off until we reach a
history that has some counts.
In order for a backoff model to give a correct probability distribution, we have
to discount the higher-order n-grams to save some probability mass for the lower
order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t
discounted and we just used the undiscounted MLE probability, then as soon as we
replaced an n-gram which has zero probability with a lower-order n-gram, we would
be adding probability mass, and the total probability assigned to all possible strings
by the language model would be greater than 1! In addition to this explicit discount
factor, we’ll need a function α to distribute this probability mass to the lower order
n-grams.
This kind of backoff with discounting is also called Katz backoff. In Katz back-
off we rely on a discounted probability P∗ if we’ve seen this n-gram before (i.e., if
we have non-zero counts). Otherwise, we recursively back off to the Katz probabil-
ity for the shorter-history (N-1)-gram. The probability for a backoff n-gram PBO is

held-out

discount

Katz backoff

thus computed as follows:

3 .5

• KN E S ER -N EY SMOOTH ING

53

PBO (wn |wn−1

n−N+1 ) = 

P∗ (wn |wn−1
α (wn−1

n−N+1 ),

n−N+1 )PBO (wn |wn−1

n−N+2 ),

n−N+1 ) > 0

if C(wn
otherwise.

Good-Turing

(3.29)
Katz backoff is often combined with a smoothing method called Good-Turing.
The combined Good-Turing backoff algorithm involves quite detailed computation
for estimating the Good-Turing smoothing and the P∗ and α values.

3.5 Kneser-Ney Smoothing

Kneser-Ney

One of the most commonly used and best performing n-gram smoothing methods
is the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good-
man 1998).
Kneser-Ney has its roots in a method called absolute discounting. Recall that
discounting of the counts for frequent n-grams is necessary to save some probability
mass for the smoothing algorithm to distribute to the unseen n-grams.
To see this, we can use a clever idea from Church and Gale (1991). Consider
an n-gram that has count 4. We need to discount this count by some amount. But
how much should we discount it? Church and Gale’s clever idea was to look at a
held-out corpus and just see what the count is for all those bigrams that had count
4 in the training set. They computed a bigram grammar from 22 million words of
AP newswire and then checked the counts of each of these bigrams in another 22
million words. On average, a bigram that occurred 4 times in the ﬁrst 22 million
words occurred 3.23 times in the next 22 million words. The following table from
Church and Gale (1991) shows these counts for bigrams with c from 0 to 9:

Bigram count in Bigram count in
training set heldout set

0 0.0000270
1 0.448
2 1.25
3 2.24
4 3.23
5 4.21
6 5.23
7 6.21
8 7.21
9 8.26

Figure 3.8 For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the
counts of these bigrams in a held-out corpus also of 22 million words.

Absolute
discounting

The astute reader may have noticed that except for the held-out counts for 0
and 1, all the other bigram counts in the held-out set could be estimated pretty well
by just subtracting 0.75 from the count in the training set! Absolute discounting
formalizes this intuition by subtracting a ﬁxed (absolute) discount d from each count.
The intuition is that since we have good estimates already for the very high counts, a
small discount d won’t affect them much. It will mainly modify the smaller counts,

54 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

for which we don’t necessarily trust the estimate anyway, and Fig. 3.8 suggests that
in practice this discount is actually a good one for bigrams with counts 2 through 9.
The equation for interpolated absolute discounting applied to bigrams:

(3.30)

+ λ (wi−1 )P(wi )

PAbsoluteDiscounting (wi |wi−1 ) =

C(wi−1wi ) − d
(cid:80)v C(wi−1 v)
The ﬁrst term is the discounted bigram, and the second term is the unigram with
an interpolation weight λ . We could just set all the d values to .75, or we could keep
a separate discount value of 0.5 for the bigrams with counts of 1.
Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-
ing with a more sophisticated way to handle the lower-order unigram distribution.
Consider the job of predicting the next word in this sentence, assuming we are inter-
polating a bigram and a unigram model.
I can’t see without my reading
.
The word glasses seems much more likely to follow here than, say, the word
Kong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is
more common, since Hong Kong is a very frequent word. A standard unigram model
will assign Kong a higher probability than glasses. We would like to capture the
intuition that although Kong is frequent, it is mainly only frequent in the phrase Hong
Kong, that is, after the word Hong. The word glasses has a much wider distribution.
In other words, instead of P(w), which answers the question “How likely is
w?”, we’d like to create a unigram model that we might call PCONTINUATION , which
answers the question “How likely is w to appear as a novel continuation?”. How can
we estimate this probability of seeing the word w as a novel continuation, in a new
unseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION
on the number of different contexts word w has appeared in, that is, the number of
bigram types it completes. Every bigram type was a novel continuation the ﬁrst time
it was seen. We hypothesize that words that have appeared in more contexts in the
past are more likely to appear in some new context as well. The number of times a
word w appears as a novel continuation can be expressed as:

PCONTINUATION (w) ∝ |{v : C(vw) > 0}|

(3.31)

To turn this count into a probability, we normalize by the total number of word
bigram types. In summary:

PCONTINUATION (w) =

|{v : C(vw) > 0}|
|{(u(cid:48) , w(cid:48) ) : C(u(cid:48)w(cid:48) ) > 0}|
An alternative metaphor for an equivalent formulation is to use the number of
word types seen to precede w (Eq. 3.31 repeated):

(3.32)

PCONTINUATION (w) ∝ |{v : C(vw) > 0}|

(3.33)

normalized by the number of words preceding all words, as follows:
(cid:80)w(cid:48) |{v : C(vw(cid:48) ) > 0}|
A frequent word (Kong) occurring in only one context (Hong) will have a low
continuation probability.

PCONTINUATION (w) = |{v : C(vw) > 0}|

(3.34)

Interpolated
Kneser-Ney

The ﬁnal equation for Interpolated Kneser-Ney smoothing for bigrams is then:

3 .6

• TH E W EB AND S TU P ID BACKO FF

55

PKN (wi |wi−1 ) =

max(C(wi−1wi ) − d , 0)

C(wi−1 )

+ λ (wi−1 )PCONTINUATION (wi )

(3.35)

The λ is a normalizing constant that is used to distribute the probability mass
we’ve discounted.:

d(cid:80)

λ (wi−1 ) =

d
(cid:80)v C(wi−1 v) |{w : C(wi−1w) > 0}|
The ﬁrst term
v C(wi−1 v) is the normalized discount. The second term |{w : C(wi−1w) > 0}|
is the number of word types that can follow wi−1 or, equivalently, the number of
word types that we discounted; in other words, the number of times we applied the
normalized discount.
The general recursive formulation is as follows:

(3.36)

max(cKN (w i

+ λ (wi−1

i−n+1 v)

i−n+1 ) =

PKN (wi |wi−1

i−n+1 )PKN (wi |wi−1

i−n+1 ) − d , 0)
(cid:80)v cKN (w i−1

i−n+2 ) (3.37)
where the deﬁnition of the count cKN depends on whether we are counting the
highest-order n-gram being interpolated (for example trigram if we are interpolating
trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram
if we are interpolating trigram, bigram, and unigram):
cKN (·) = (cid:26) count(·)
for the highest order
continuationcount(·)
for lower orders
The continuation count is the number of unique single word contexts for ·.
At the termination of the recursion, unigrams are interpolated with the uniform
distribution, where the parameter  is the empty string:
max(cKN (w) − d , 0)

PKN (w) =

(3.38)

(3.39)

+ λ ()

1
V

(cid:80)w(cid:48) cKN (w(cid:48) )

modiﬁed
Kneser-Ney

If we want to include an unknown word <UNK>, it’s just included as a regular vo-
cabulary entry with count zero, and hence its probability will be a lambda-weighted
uniform distribution λ ()
V .
The best-performing version of Kneser-Ney smoothing is called modiﬁed Kneser-
Ney smoothing, and is due to Chen and Goodman (1998). Rather than use a single
ﬁxed discount d , modiﬁed Kneser-Ney uses three different discounts d1 , d2 , and
d3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and
Goodman (1998, p. 19) or Heaﬁeld et al. (2013) for the details.

3.6 The Web and Stupid Backoff

By using text from the web, it is possible to build extremely large language mod-
els. In 2006 Google released a very large set of n-gram counts, including n-grams
(1-grams through 5-grams) from all the ﬁve-word sequences that appear at least
40 times from 1,024,908,267,229 words of running text on the web; this includes

56 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

1,176,470,663 ﬁve-word sequences using over 13 million unique words types (Franz
and Brants, 2006). Some examples:

4-gram

Count

serve as the incoming
92
serve as the incubator
99
serve as the independent
794
serve as the index
223
serve as the indication
72
serve as the indicator
120
serve as the indicators
45
serve as the indispensable
111
serve as the indispensible
40
serve as the individual
234
Efﬁciency considerations are important when building language models that use
such large sets of n-grams. Rather than store each word as a string, it is generally
represented in memory as a 64-bit hash number, with the words themselves stored
on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte
ﬂoats), and n-grams are stored in reverse tries.
N-grams can also be shrunk by pruning, for example only storing n-grams with
counts greater than some threshold (such as the count threshold of 40 used for the
Google n-gram release) or using entropy to prune less-important n-grams (Stolcke,
1998). Another option is to build approximate language models using techniques
like Bloom ﬁlters (Talbot and Osborne 2007, Church et al. 2007). Finally, efﬁ-
cient language model toolkits like KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013) use
sorted arrays, efﬁciently combine probabilities and backoffs in a single value, and
use merge sorts to efﬁciently build the probability tables in a minimal number of
passes through a large corpus.
Although with these toolkits it is possible to build web-scale language models
using full Kneser-Ney smoothing, Brants et al. (2007) show that with very large lan-
guage models a much simpler algorithm may be sufﬁcient. The algorithm is called
stupid backoff. Stupid backoff gives up the idea of trying to make the language
model a true probability distribution. There is no discounting of the higher-order
probabilities. If a higher-order n-gram has a zero count, we simply backoff to a
lower order n-gram, weighed by a ﬁxed (context-independent) weight. This algo-
rithm does not produce a probability distribution, so we’ll follow Brants et al. (2007)
in referring to it as S:

Bloom ﬁlters

stupid backoff

S(wi |wi−1

i−k+1 ) = 

count(wi
if count(wi
count(wi−1
λ S(wi |wi−1
i−k+2 ) otherwise
The backoff terminates in the unigram, which has probability S(w) = count (w)
et al. (2007) ﬁnd that a value of 0.4 worked well for λ .

i−k+1 ) > 0

i−k+1 )
i−k+1 )

(3.40)

. Brants

N

3.7 Advanced: Perplexity’s Relation to Entropy

We introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on
a test set. A better n-gram model is one that assigns a higher probability to the

Entropy

3 .7

• ADVANC ED : P ER P LEX I TY ’ S R ELAT ION TO EN TRO PY

57

p(x) log2 p(x)

test data, and perplexity is a normalized version of the probability of the test set.
The perplexity measure actually arises from the information-theoretic concept of
cross-entropy, which explains otherwise mysterious properties of perplexity (why
the inverse probability, for example?) and its relationship to entropy. Entropy is a
measure of information. Given a random variable X ranging over whatever we are
predicting (words, letters, parts of speech, the set of which we’ll call χ ) and with a
particular probability function, call it p(x), the entropy of the random variable X is:
H (X ) = −(cid:88)x∈χ
The log can, in principle, be computed in any base. If we use log base 2, the
resulting value of entropy will be measured in bits.
One intuitive way to think about entropy is as a lower bound on the number of
bits it would take to encode a certain decision or piece of information in the optimal
coding scheme.
Consider an example from the standard information theory textbook Cover and
Thomas (1991). Imagine that we want to place a bet on a horse race but it is too
far to go all the way to Yonkers Racetrack, so we’d like to send a short message to
the bookie to tell him which of the eight horses to bet on. One way to encode this
message is just to use the binary representation of the horse’s number as the code;
thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded
as 000. If we spend the whole day betting and each horse is coded with 3 bits, on
average we would be sending 3 bits per race.
Can we do better? Suppose that the spread is the actual distribution of the bets
placed and that we represent it as the prior probability of each horse as follows:

(3.41)

Horse 1 1
Horse 5 1
Horse 2 1
Horse 6 1
Horse 3 1
Horse 7 1
Horse 4 1
16 Horse 8 1
The entropy of the random variable X that ranges over horses gives us a lower
bound on the number of bits and is

64
64
64
64

2
4
8

H (X ) = −

p(i) log p(i)

i=8(cid:88)i=1
2 log 1

= − 1

2 − 1

4 log 1

4 − 1

8 log 1

8 − 1

16 log 1

16 −4( 1

64 log 1

64 )

= 2 bits
A code that averages 2 bits per race can be built with short encodings for more
probable horses, and longer encodings for less probable horses. For example, we
could encode the most likely horse with the code 0, and the remaining horses as 10,

(3.42)

then 110, 1110, 111100, 111101, 111110, and 111111.

What if the horses are equally likely? We saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3.
Is the entropy the same? In this case each horse would have a
probability of 1
8 . The entropy of the choice of horses is then

H (X ) = −

1
8

i=8(cid:88)i=1

log

1
8

= − log

1
8

= 3 bits

(3.43)

58 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

Until now we have been computing the entropy of a single variable. But most of
what we will use entropy for involves sequences. For a grammar, for example, we
will be computing the entropy of some sequence of words W = {w0 , w1 , w2 , . . . , wn }.
One way to do this is to have a variable that ranges over sequences of words. For
example we can compute the entropy of a random variable that ranges over all ﬁnite
sequences of words of length n in some language L as follows:

H (w1 , w2 , . . . , wn ) = − (cid:88)W n

1 ∈L

p(W n
1 ) log p(W n

1 )

(3.44)

entropy rate

Stationary

cross-entropy

1
n

H (W n

We could deﬁne the entropy rate (we could also think of this as the per-word
entropy) as the entropy of this sequence divided by the number of words:
1
n (cid:88)W n
But to measure the true entropy of a language, we need to consider sequences of
inﬁnite length. If we think of a language as a stochastic process L that produces a
sequence of words, and allow W to represent the sequence of words w1 , . . . , wn , then
L’s entropy rate H (L) is deﬁned as

p(W n
1 ) log p(W n

1 ) = −

(3.45)

1 ∈L

1 )

H (L) = lim

H (w1 , w2 , . . . , wn )

n→∞

1
n
= − lim

n (cid:88)W ∈L
1
The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and
Thomas 1991) states that if the language is regular in certain ways (to be exact, if it
is both stationary and ergodic),

p(w1 , . . . , wn ) log p(w1 , . . . , wn )

n→∞

(3.46)

(3.47)

n→∞ −

H (L) = lim

log p(w1w2 . . . wn )

1
n
That is, we can take a single sequence that is long enough instead of summing
over all possible sequences. The intuition of the Shannon-McMillan-Breiman the-
orem is that a long-enough sequence of words will contain in it many other shorter
sequences and that each of these shorter sequences will reoccur in the longer se-
quence according to their probabilities.
A stochastic process is said to be stationary if the probabilities it assigns to a
sequence are invariant with respect to shifts in the time index. In other words, the
probability distribution for words at time t is the same as the probability distribution
at time t + 1. Markov models, and hence n-grams, are stationary. For example, in
a bigram, Pi is dependent only on Pi−1 . So if we shift our time index by x, Pi+x is
still dependent on Pi+x−1 . But natural language is not stationary, since as we show
in Chapter 10, the probability of upcoming words can be dependent on events that
were arbitrarily distant and time dependent. Thus, our statistical models only give
an approximation to the correct distributions and entropies of natural language.
To summarize, by making some incorrect but convenient simplifying assump-
tions, we can compute the entropy of some stochastic process by taking a very long
sample of the output and computing its average log probability.
Now we are ready to introduce cross-entropy. The cross-entropy is useful when
we don’t know the actual probability distribution p that generated some data.
It

3 .7

• ADVANC ED : P ER P LEX I TY ’ S R ELAT ION TO EN TRO PY

59

H ( p, m) = lim

allows us to use some m, which is a model of p (i.e., an approximation to p). The
cross-entropy of m on p is deﬁned by
n (cid:88)W ∈L
1
That is, we draw sequences according to the probability distribution p, but sum
the log of their probabilities according to m.
Again, following the Shannon-McMillan-Breiman theorem, for a stationary er-
godic process:

p(w1 , . . . , wn ) log m(w1 , . . . , wn )

n→∞ −

(3.48)

n→∞ −

H ( p, m) = lim

log m(w1w2 . . . wn )

1
n
This means that, as for entropy, we can estimate the cross-entropy of a model
m on some distribution p by taking a single sequence that is long enough instead of
summing over all possible sequences.
What makes the cross-entropy useful is that the cross-entropy H ( p, m) is an up-
per bound on the entropy H ( p). For any model m:

(3.49)

(3.50)

H ( p) ≤ H ( p, m)
This means that we can use some simpliﬁed model m to help estimate the true en-
tropy of a sequence of symbols drawn according to probability p. The more accurate
m is, the closer the cross-entropy H ( p, m) will be to the true entropy H ( p). Thus,
the difference between H ( p, m) and H ( p) is a measure of how accurate a model is.
Between two models m1 and m2 , the more accurate model will be the one with the
lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so
a model cannot err by underestimating the true entropy.)
We are ﬁnally ready to see the relation between perplexity and cross-entropy as
we saw it in Eq. 3.49. Cross-entropy is deﬁned in the limit, as the length of the
observed word sequence goes to inﬁnity. We will need an approximation to cross-
entropy, relying on a (sufﬁciently long) sequence of ﬁxed length. This approxima-
tion to the cross-entropy of a model M = P(wi |wi−N+1 ...wi−1 ) on a sequence of
words W is

1
H (W ) = −
N
The perplexity of a model P on a sequence of words W is now formally deﬁned as
the exp of this cross-entropy:

log P(w1w2 . . . wN )

(3.51)

perplexity

Perplexity(W ) = 2H (W )

N

= P(w1w2 . . . wN )− 1
= N(cid:115)

P(w1w2 . . . wN )

1

= N(cid:118)(cid:117)(cid:117)(cid:116)

N(cid:89)i=1

1

P(wi |w1 . . . wi−1 )

(3.52)

60 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

3.8 Summary

This chapter introduced language modeling and the n-gram, one of the most widely
used tools in language processing.
• Language models offer a way to assign a probability to a sentence or other
sequence of words, and to predict a word from preceding words.
• n-grams are Markov models that estimate words from a ﬁxed window of pre-
vious words. n-gram probabilities can be estimated by counting in a corpus

and normalizing (the maximum likelihood estimate).

cally using perplexity.

• n-gram language models are evaluated extrinsically in some task, or intrinsi-
• The perplexity of a test set according to a language model is the geometric
mean of the inverse test set probability computed by the model.
• Smoothing algorithms provide a more sophisticated way to estimate the prob-
ability of n-grams. Commonly used smoothing algorithms for n-grams rely on
lower-order n-gram counts through backoff or interpolation.
• Both backoff and interpolation require discounting to create a probability dis-
tribution.
• Kneser-Ney smoothing makes use of the probability of a word being a novel
continuation. The interpolated Kneser-Ney smoothing algorithm mixes a
discounted probability with a lower-order continuation probability.

Bibliographical and Historical Notes

The underlying mathematics of the n-gram was ﬁrst proposed by Markov (1913),
who used what are now called Markov chains (bigrams and trigrams) to predict
whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con-
sonant. Markov classiﬁed 20,000 letters as V or C and computed the bigram and
trigram probability that a given letter would be a vowel given the previous one or
two letters. Shannon (1948) applied n-grams to compute approximations to English
word sequences. Based on Shannon’s work, Markov models were commonly used in
engineering, linguistic, and psychological work on modeling word sequences by the
1950s. In a series of extremely inﬂuential papers starting with Chomsky (1956) and
including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued
that “ﬁnite-state Markov processes”, while a possibly useful engineering heuristic,
were incapable of being a complete cognitive model of human grammatical knowl-
edge. These arguments led many linguists and computational linguists to ignore
work in statistical modeling for decades.
The resurgence of n-gram models came from Jelinek and colleagues at the IBM
Thomas J. Watson Research Center, who were inﬂuenced by Shannon, and Baker
at CMU, who was inﬂuenced by the work of Baum and colleagues. Independently
these two labs successfully used n-grams in their speech recognition systems (Baker 1990,
Jelinek 1976, Baker 1975, Bahl et al. 1983, Jelinek 1990). A trigram model was used
in the IBM TANGORA speech recognition system in the 1970s, but the idea was not
written up until later.
Add-one smoothing derives from Laplace’s 1812 law of succession and was ﬁrst
applied as an engineering solution to the zero-frequency problem by Jeffreys (1948)

EX ERC I SE S

61

based on an earlier Add-K suggestion by Johnson (1932). Problems with the add-
one algorithm are summarized in Gale and Church (1994).
A wide variety of different language modeling and smoothing techniques were
proposed in the 80s and 90s, including Good-Turing discounting—ﬁrst applied to
the n-gram smoothing at IBM by Katz (N ´adas 1984, Church and Gale 1991)—
Witten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-
gram models that used information about word classes.
Starting in the late 1990s, Chen and Goodman produced a highly inﬂuential
series of papers with a comparison of different language models (Chen and Good-
man 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).
They performed a number of carefully controlled experiments comparing differ-
ent discounting algorithms, cache models, class-based models, and other language
model parameters. They showed the advantages of Modiﬁed Interpolated Kneser-
Ney, which has since become the standard baseline for language modeling, espe-
cially because they showed that caches and class-based models provided only minor
additional improvement. These papers are recommended for any reader with further
interest in language modeling.
Two commonly used toolkits for building language models are SRILM (Stolcke,
2002) and KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013). Both are publicly available.
SRILM offers a wider range of options and types of discounting, while KenLM is
optimized for speed and memory size, making it possible to build web-scale lan-
guage models.
The highest accuracy language models at the time of this writing make use of
neural nets. The problem with standard language models is that the number of pa-
rameters increases exponentially as the n-gram order increases, and n-grams have no
way to generalize from training to test set. Neural networks instead project words
into a continuous space in which words with similar contexts have similar represen-
tations. Both feedforward nets Bengio et al. 2006, Schwenk 2007 and recurrent
nets (Mikolov, 2012) are used.
Other important classes of language models are maximum entropy language
models (Rosenfeld, 1996), based on logistic regression classiﬁers that use lots of
features to help predict upcoming words. These classiﬁers can use the standard
features presented in this chapter (i.e., the previous words) but also lots of other
useful predictors, as can other kinds of discriminative language models (Roark et al.,
2007). We’ll introduce logistic regression language modeling when we introduce
classiﬁcation in Chapter 4.
Another important technique is language model adaptation, where we want to
combine data from multiple domains (for example we might have less in-domain
training data but more general data that we then need to adapt) (Bulyko et al. 2003,
Bacchiani et al. 2004, Bellegarda 2004, Bacchiani et al. 2006, Hsu 2007, Liu et al. 2013).

class-based
n-gram

neural nets

maximum
entropy

adaptation

Exercises

3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11).
Now write out all the non-zero trigram probabilities for the I am Sam corpus
on page 41.
3.2 Calculate the probability of the sentence i want chinese food. Give two
probabilities, one using Fig. 3.2, and another using the add-1 smoothed table
in Fig. 3.6.

62 CHA P TER 3

• N -GRAM LANGUAGE MODE L S

3.3 Which of the two probabilities you computed in the previous exercise is higher,
unsmoothed or smoothed? Explain why.
3.4 We are given the following corpus, modiﬁed from the one in the chapter:

<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>

Using a bigram language model with add-one smoothing, what is P(Sam |
am)? Include <s> and </s> in your counts just like any other token.
Suppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram
grammar on the following training corpus without using the end-symbol </s>:

3.5

<s> a b
<s> b b
<s> b a
<s> a a

3.6

Demonstrate that your bigram model does not assign a single probability dis-
tribution across all sentence lengths by showing that the sum of the probability
of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the
sum of the probability of all possible 3 word sentences over the alphabet {a,b}
is also 1.0.
Suppose we train a trigram language model with add-one smoothing on a
given corpus. The corpus contains V word types. Express a formula for esti-
mating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),
in terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to
denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and
so on for bigrams and unigrams.
3.7 We are given the following corpus, modiﬁed from the one in the chapter:

<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>

1

If we use linear interpolation smoothing between a maximum-likelihood bi-
gram model and a maximum-likelihood unigram model with λ1 = 1
2 and λ2 =
2 , what is P(Sam|am)? Include <s> and </s>\verb in your counts just like
any other token.
3.8 Write a program to compute unsmoothed unigrams and bigrams.
3.9 Run your n-gram program on two different small corpora of your choice (you
might use email text or newsgroups). Now compare the statistics of the two
corpora. What are the differences in the most common unigrams between the
two? How about interesting differences in bigrams?
3.10 Add an option to your program to generate random sentences.
3.11 Add an option to your program to compute the perplexity of a test set.

CHAPTER

4 Naive Bayes and Sentiment
Classiﬁcation

Classiﬁcation lies at the heart of both human and machine intelligence. Deciding
what letter, word, or image has been presented to our senses, recognizing faces
or voices, sorting mail, assigning grades to homeworks; these are all examples of
assigning a category to an input. The potential challenges of this task are highlighted
by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:
(a) those that belong to the Emperor, (b) embalmed ones, (c) those that
are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray
dogs, (h) those that are included in this classiﬁcation, (i) those that
tremble as if they were mad, (j) innumerable ones, (k) those drawn with
a very ﬁne camel’s hair brush, (l) others, (m) those that have just broken
a ﬂower vase, (n) those that resemble ﬂies from a distance.
Many language processing tasks involve classiﬁcation, although luckily our classes
are much easier to deﬁne than those of Borges. In this chapter we introduce the naive
Bayes algorithm and apply it to text categorization, the task of assigning a label or
category to an entire text or document.
We focus on one common text categorization task, sentiment analysis, the ex-
traction of sentiment, the positive or negative orientation that a writer expresses
toward some object. A review of a movie, book, or product on the web expresses the
author’s sentiment toward the product, while an editorial or political text expresses
sentiment toward a candidate or political action. Extracting consumer or public sen-
timent is thus relevant for ﬁelds from marketing to politics.
The simplest version of sentiment analysis is a binary classiﬁcation task, and the
words of the review provide excellent cues. Consider, for example, the following
phrases extracted from positive and negative reviews of movies and restaurants,.
Words like great, richly, awesome, and pathetic, and awful and ridiculously are very
informative cues:
+ ...zany characters and richly applied satire, and some great plot twists
− It was pathetic. The worst part about it was the boxing scenes...
+ ...awesome caramel sauce and sweet toasty almonds. I love this place!
− ...awful pizza and ridiculously overpriced...
Spam detection is another important commercial application, the binary clas-
siﬁcation task of assigning an email to one of the two classes spam or not-spam.
Many lexical and other features can be used to perform this classiﬁcation. For ex-
ample you might quite reasonably be suspicious of an email containing phrases like
“online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.
Another thing we might want to know about a text is the language it’s written
in. Texts on social media, for example, can be in any number of languages and we’ll
need to apply different processing. The task of language id is thus the ﬁrst step
in most language processing pipelines. Related tasks like determining a text’s au-
thor, (authorship attribution), or author characteristics like gender, age, and native

text
categorization

sentiment
analysis

spam detection

language id

authorship
attribution

64 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

supervised
machine
learning

language are text classiﬁcation tasks that are also relevant to the digital humanities,
social sciences, and forensic linguistics.
Finally, one of the oldest tasks in text classiﬁcation is assigning a library sub-
ject category or topic label to a text. Deciding whether a research paper concerns
epidemiology or instead, perhaps, embryology, is an important component of infor-
mation retrieval. Various sets of subject categories exist, such as the MeSH (Medical
Subject Headings) thesaurus. In fact, as we will see, subject category classiﬁcation
is the task for which the naive Bayes algorithm was invented in 1961.
Classiﬁcation is essential for tasks below the level of the document as well.
We’ve already seen period disambiguation (deciding if a period is the end of a sen-
tence or part of a word), and word tokenization (deciding if a character should be
a word boundary). Even language modeling can be viewed as classiﬁcation: each
word can be thought of as a class, and so predicting the next word is classifying the
context-so-far into a class for each next word. A part-of-speech tagger (Chapter 8)
classiﬁes each occurrence of a word in a sentence as, e.g., a noun or a verb.
The goal of classiﬁcation is to take a single observation, extract some useful
features, and thereby classify the observation into one of a set of discrete classes.
One method for classifying text is to use hand-written rules. There are many areas
of language processing where hand-written rule-based classiﬁers constitute a state-
of-the-art system, or at least part of it.
Rules can be fragile, however, as situations or data change over time, and for
some tasks humans aren’t necessarily good at coming up with the rules. Most cases
of classiﬁcation in language processing are instead done via supervised machine
learning, and this will be the subject of the remainder of this chapter. In supervised
learning, we have a data set of input observations, each associated with some correct
output (a ‘supervision signal’). The goal of the algorithm is to learn how to map
from a new observation to a correct output.
Formally, the task of supervised classiﬁcation is to take an input x and a ﬁxed
set of output classes Y = y1 , y2 , ..., yM and return a predicted class y ∈ Y . For text
classiﬁcation, we’ll sometimes talk about c (for “class”) instead of y as our output
variable, and d (for “document”) instead of x as our input variable. In the supervised
situation we have a training set of N documents that have each been hand-labeled
with a class: (d1 , c1 ), ...., (dN , cN ). Our goal is to learn a classiﬁer that is capable of
mapping from a new document d to its correct class c ∈ C. A probabilistic classiﬁer
additionally will tell us the probability of the observation being in the class. This
full distribution over the classes can be useful information for downstream decisions;
avoiding making discrete decisions early on can be useful when combining systems.
Many kinds of machine learning algorithms are used to build classiﬁers. This
chapter introduces naive Bayes; the following one introduces logistic regression.
These exemplify two ways of doing classiﬁcation. Generative classiﬁers like naive
Bayes build a model of how a class could generate some input data. Given an ob-
servation, they return the class most likely to have generated the observation. Dis-
criminative classiﬁers like logistic regression instead learn what features from the
input are most useful to discriminate between the different possible classes. While
discriminative systems are often more accurate and hence more commonly used,
generative classiﬁers still have a role.

4.1 Naive Bayes Classiﬁers

4 .1

• NA IV E BAY E S C LA S S I FIER S

65

naive Bayes
classiﬁer

bag-of-words

In this section we introduce the multinomial naive Bayes classiﬁer, so called be-
cause it is a Bayesian classiﬁer that makes a simplifying (naive) assumption about
how the features interact.
The intuition of the classiﬁer is shown in Fig. 4.1. We represent a text document
as if it were a bag-of-words, that is, an unordered set of words with their position
ignored, keeping only their frequency in the document. In the example in the ﬁgure,
instead of representing the word order in all the phrases like “I love this movie” and
“I would recommend it”, we simply note that the word I occurred 5 times in the
entire excerpt, the word it 6 times, the words love, recommend, and movie once, and
so on.

Figure 4.1

Intuition of the multinomial naive Bayes classiﬁer applied to a movie review. The position of the
words is ignored (the bag of words assumption) and we make use of the frequency of each word.

Naive Bayes is a probabilistic classiﬁer, meaning that for a document d , out of
all classes c ∈ C the classiﬁer returns the class ˆc which has the maximum posterior
probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our
estimate of the correct class”.

ˆ

ˆc = argmax

c∈C

P(c|d )

(4.1)

Bayesian
inference

This idea of Bayesian inference has been known since the work of Bayes (1763),
and was ﬁrst applied to text classiﬁcation by Mosteller and Wallace (1964). The in-
tuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. 4.1 into other
probabilities that have some useful properties. Bayes’ rule is presented in Eq. 4.2;
it gives us a way to break down any conditional probability P(x|y) into three other

66 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

probabilities:

P(y|x)P(x)
P(x|y) =
P(y)
We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:
P(d |c)P(c)
P(d )

P(c|d ) = argmax

ˆc = argmax

c∈C

c∈C

(4.2)

(4.3)

We can conveniently simplify Eq. 4.3 by dropping the denominator P(d ). This
is possible because we will be computing P(d |c)P(c)
for each possible class. But P(d )
doesn’t change for each class; we are always asking about the most likely class for
the same document d , which must have the same probability P(d ). Thus, we can
choose the class that maximizes this simpler formula:

P(d )

c∈C

c∈C

(4.4)

ˆc = argmax

P(c|d ) = argmax
P(d |c)P(c)
We thus compute the most probable class ˆc given some document d by choosing
the class which has the highest product of two probabilities: the prior probability
of the class P(c) and the likelihood of the document P(d |c):
likelihood
prior
P(d |c)
Without loss of generalization, we can represent a document d as a set of features

(cid:122) (cid:125)(cid:124) (cid:123)

(cid:122)(cid:125)(cid:124)(cid:123)P(c)

ˆc = argmax

(4.5)

c∈C

f1 , f2 , ..., fn :

likelihood

ˆc = argmax

P( f1 , f2 , ...., fn |c)

c∈C

(cid:122)

(cid:125)(cid:124)

prior

(cid:122)(cid:125)(cid:124)(cid:123)P(c)

(cid:123)

(4.6)

Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the probability of every possible combination of
features (for example, every possible set of words and positions) would require huge
numbers of parameters and impossibly large training sets. Naive Bayes classiﬁers
therefore make two simplifying assumptions.
The ﬁrst is the bag of words assumption discussed intuitively above: we assume
position doesn’t matter, and that the word “love” has the same effect on classiﬁcation
whether it occurs as the 1st, 20th, or last word in the document. Thus we assume
that the features f1 , f2 , ..., fn only encode word identity and not position.
The second is commonly called the naive Bayes assumption: this is the condi-
tional independence assumption that the probabilities P( f i |c) are independent given
the class c and hence can be ‘naively’ multiplied as follows:

P( f1 , f2 , ...., fn |c) = P( f1 |c) · P( f2 |c) · ... · P( fn |c)

(4.7)

The ﬁnal equation for the class chosen by a naive Bayes classiﬁer is thus:
P(c) (cid:89)f ∈F
To apply the naive Bayes classiﬁer to text, we need to consider word positions,
by simply walking an index through every word position in the document:

cNB = argmax

P( f |c)

(4.8)

c∈C

prior
probability
likelihood

naive Bayes
assumption

4 .2

• TRA IN ING THE NA IVE BAYE S C LA S S I FIER

67

positions ← all word positions in test document
cNB = argmax
P(wi |c)

P(c) (cid:89)i∈ posit ions

c∈C

(4.9)

Naive Bayes calculations, like calculations for language modeling, are done in
log space, to avoid underﬂow and increase speed. Thus Eq. 4.9 is generally instead
expressed as

cNB = argmax

c∈C

log P(c) + (cid:88)i∈ posit ions

log P(wi |c)

(4.10)

By considering features in log space Eq. 4.10 computes the predicted class as
a linear function of input features. Classiﬁers that use a linear combination of
the inputs to make a classiﬁcation decision —like naive Bayes and also logistic
regression— are called linear classiﬁers.

linear
classiﬁers

4.2 Training the Naive Bayes Classiﬁer

How can we learn the probabilities P(c) and P( f i |c)? Let’s ﬁrst consider the max-
imum likelihood estimate. We’ll simply use the frequencies in the data. For the
document prior P(c) we ask what percentage of the documents in our training set
are in each class c. Let Nc be the number of documents in our training data with
class c and Nd oc be the total number of documents. Then:

ˆP(c) =

Nc

Nd oc

(4.11)

To learn the probability P( f i |c), we’ll assume a feature is just the existence of a
word in the document’s bag of words, and so we’ll want P(wi |c), which we compute
as the fraction of times the word wi appears among all words in all documents of
topic c. We ﬁrst concatenate all documents with category c into one big “category
c” text. Then we use the frequency of wi in this concatenated document to give a
maximum likelihood estimate of the probability:

(4.12)

count (wi , c)
ˆP(wi |c) =
(cid:80)w∈V count (w, c)
Here the vocabulary V consists of the union of all the word types in all classes,
not just the words in one class c.
There is a problem, however, with maximum likelihood training. Imagine we
are trying to estimate the likelihood of the word “fantastic” given class positive, but
suppose there are no training documents that both contain the word “fantastic” and
are classiﬁed as positive. Perhaps the word “fantastic” happens to occur (sarcasti-
cally?) in the class negative. In such a case the probability for this feature will be
zero:

68 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

= 0

count (“fantastic”, positive)
ˆP(“fantastic”|positive) =
(cid:80)w∈V count (w, positive)
But since naive Bayes naively multiplies all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the probability of the
class to be zero, no matter the other evidence!
The simplest solution is the add-one (Laplace) smoothing introduced in Chap-
ter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing
algorithms in language modeling, it is commonly used in naive Bayes text catego-
rization:

(4.13)

unknown word

stop words

=

(4.14)

count (wi , c) + 1
count (wi , c) + 1
ˆP(wi |c) =
(cid:80)w∈V (count (w, c) + 1)
(cid:0)(cid:80)w∈V count (w, c)(cid:1) + |V |
Note once again that it is crucial that the vocabulary V consists of the union of
all the word types in all classes, not just the words in one class c (try to convince
yourself why this must be true; see the exercise at the end of the chapter).
What do we do about words that occur in our test data but are not in our vocab-
ulary at all because they did not occur in any training document in any class? The
solution for such unknown words is to ignore them—remove them from the test
document and not include any probability for them at all.
Finally, some systems choose to completely ignore another class of words: stop
words, very frequent words like the and a. This can be done by sorting the vocabu-
lary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries
as stop words, or alternatively by using one of the many pre-deﬁned stop word list
available online. Then every instance of these stop words are simply removed from
both training and test documents as if they had never occurred. In most text classi-
ﬁcation applications, however, using a stop word list doesn’t improve performance,
and so it is more common to make use of the entire vocabulary and not use a stop
word list.
Fig. 4.2 shows the ﬁnal algorithm.

4.3 Worked example

Let’s walk through an example of training and testing naive Bayes with add-one
smoothing. We’ll use a sentiment analysis domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simpliﬁed from actual movie reviews.

Cat

Documents

Training -
just plain boring
-
entirely predictable and lacks energy
-
no surprises and very few laughs
+
very powerful
+
the most fun ﬁlm of the summer
Test
?
predictable with no fun
The prior P(c) for the two classes is computed via Eq. 4.11 as Nc

:

Nd oc

4 .3

• WORKED EXAM P LE

69

for each class c ∈ C

function TRA IN NA IV E BAYE S(D, C) returns log P(c) and log P(w|c)
# Calculate P(c) terms
Nd oc = number of documents in D
Nc = number of documents from D in class c
Nc
logprior[c] ← log
V ← vocabulary of D
bigdoc[c] ← append(d) for d ∈ D with class c
for each word w in V
# Calculate P(w|c) terms
count(w,c) ← # of occurrences of w in bigdoc[c]
count (w, c) + 1
loglikelihood[w,c] ← log
return logprior, loglikelihood, V

w(cid:48) in V (count (w(cid:48) , c) + 1)

Nd oc

(cid:80)

function T E ST NA IVE BAY E S(testdoc, logprior, loglikelihood, C, V) returns best c

for each class c ∈ C

sum[c] ← logprior[c]
for each position i in testdoc
word ← testdoc[i]
if word ∈ V
sum[c] ← sum[c]+ loglikelihood[word,c]
return argmaxc sum[c]

Figure 4.2 The naive Bayes algorithm, using add-1 smoothing. To use add-α smoothing
instead, change the +1 to +α for loglikelihood counts in training.

P(+) =

P(−) =

3
2
5
5
The word with doesn’t occur in the training set, so we drop it completely (as
mentioned above, we don’t use unknown word models for naive Bayes). The like-
lihoods from the training set for the remaining three words “predictable”, “no”, and
“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder
of the words in the training set is left as Exercise 4.?? (TBD)).

P(“predictable”|−) =
P(“no”|−) =
P(“fun”|−) =

1 + 1
14 + 20
1 + 1
14 + 20
0 + 1
14 + 20

P(“predictable”|+) =
P(“no”|+) =
0 + 1
9 + 20
1 + 1
P(“fun”|+) =
9 + 20

0 + 1
9 + 20

For the test sentence S = “predictable with no fun”, after removing the word
‘with’, the chosen class, via Eq. 4.9, is therefore computed as follows:

P(−)P(S|−) =

P(+)P(S|+) =

3

5 ×

2

5 ×

2 × 2 × 1
343 = 6.1 × 10−5
1 × 1 × 2
293 = 3.2 × 10−5

70 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

The model thus predicts the class negative for the test sentence.

4.4 Optimizing for Sentiment Analysis

binary NB

While standard naive Bayes text classiﬁcation can work well for sentiment analysis,
some small changes are generally employed that improve performance.
First, for sentiment classiﬁcation and a number of other text classiﬁcation tasks,
whether a word occurs or not seems to matter more than its frequency. Thus it
often improves performance to clip the word counts in each document at 1 (see
the end of the chapter for pointers to these results). This variant is called binary
multinomial naive Bayes or binary NB. The variant uses the same Eq. 4.10 except
that for each document we remove all duplicate words before concatenating them
into the single big document. Fig. 4.3 shows an example in which a set of four
documents (shortened and text-normalized for this example) are remapped to binary,
with the modiﬁed counts shown in the table on the right. The example is worked
without add-1 smoothing to make the differences clearer. Note that the results counts
need not be 1; the word great has a count of 2 even for Binary NB, because it appears
in multiple documents.

NB
Binary
Counts Counts

+ − + −

Four original documents:

− it was pathetic the worst part was the
boxing scenes
− no plot twists or great scenes
+ and satire and great plot twists
+ great scenes great ﬁlm

After per-document binarization:

− it was pathetic the worst part boxing
scenes
− no plot twists or great scenes
+ and satire great plot twists
+ great scenes ﬁlm

and
2
boxing
0
ﬁlm
1
great
3
it
0
no
0
or
0
part
0
pathetic 0
plot
1
satire
1
scenes
1
the
0
twists
1
was
0
worst
0

0
1
0
1
1
1
1
1
1
1
0
2
2
1
2
1

1
0
1
2
0
0
0
0
0
1
1
1
0
1
0
0

0
1
0
1
1
1
1
1
1
1
0
2
1
1
1
1

Figure 4.3 An example of binarization for the binary naive Bayes algorithm.

A second important addition commonly made when doing text classiﬁcation for
sentiment is to deal with negation. Consider the difference between I really like this
movie (positive) and I didn’t like this movie (negative). The negation expressed by
didn’t completely alters the inferences we draw from the predicate like. Similarly,
negation can modify a negative word to produce a positive review (don’t dismiss this
ﬁlm, doesn’t let us get bored).
A very simple baseline that is commonly used in sentiment to deal with negation
is during text normalization to prepend the preﬁx NOT to every word after a token
of logical negation (n’t, not, no, never) until the next punctuation mark. Thus the
phrase

didn’t like this movie , but I

sentiment
lexicons

General
Inquirer
LIWC

4 .5

• NA IVE BAYE S FOR OTHER T EX T C LA S S I FICAT ION TA SK S

71

becomes

didn’t NOT_like NOT_this NOT_movie , but I

Newly formed ‘words’ like NOT like, NOT recommend will thus occur more of-
ten in negative document and act as cues for negative sentiment, while words like
NOT bored, NOT dismiss will acquire positive associations. We will return in Chap-
ter 15 to the use of parsing to deal more accurately with the scope relationship be-
tween these negation words and the predicates they modify, but this simple baseline
works quite well in practice.
Finally, in some situations we might have insufﬁcient labeled training data to
train accurate naive Bayes classiﬁers using all words in the training set to estimate
positive and negative sentiment. In such cases we can instead derive the positive
and negative word features from sentiment lexicons, lists of words that are pre-
annotated with positive or negative sentiment. Four popular lexicons are the General
Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon
of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).
For example the MPQA subjectivity lexicon has 6885 words, 2718 positive and
4912 negative, each marked for whether it is strongly or weakly biased. (Chapter 19
will discuss how these lexicons can be learned automatically.) Some samples of
positive and negative words from the MPQA lexicon include:
+ : admirable, beautiful, conﬁdent, dazzling, ecstatic, favor, glee, great
− : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate
A common way to use lexicons in a naive Bayes classiﬁer is to add a feature
that is counted whenever a word from that lexicon occurs. Thus we might add a
feature called ‘this word occurs in the positive lexicon’, and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. Similarly, we might add as a second feature ‘this word occurs in the
negative lexicon’ of words in the negative lexicon. If we have lots of training data,
and if the test data matches the training data, using just two features won’t work as
well as using all the words. But when training data is sparse or not representative of
the test set, using dense lexicon features instead of sparse individual-word features
may generalize better.

4.5 Naive Bayes for other text classiﬁcation tasks

spam detection

In the previous section we pointed out that naive Bayes doesn’t require that our
classiﬁer use all the words in the training data as features. In fact features in naive
Bayes can express any property of the input text we want.
Consider the task of spam detection, deciding if a particular piece of email is
an example of spam (unsolicited bulk email) — and one of the ﬁrst applications of
naive Bayes to text classiﬁcation (Sahami et al., 1998).
A common solution here, rather than using all the words as individual features, is
to predeﬁne likely sets of words or phrases as features, combined these with features
that are not purely linguistic. For example the open-source SpamAssassin tool1
predeﬁnes features like the phrase “one hundred percent guaranteed”, or the feature
mentions millions of dollars, which is a regular expression that matches suspiciously
large sums of money. But it also includes features like HTML has a low ratio of

1 https://spamassassin.apache.org

72 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

language ID

text to image area, that isn’t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
took to arrive. More sample SpamAssassin features:

• Email subject line is all capital letters
• Contains phrases of urgency like “urgent reply”
• Email subject line contains “online pharmaceutical”
• HTML has unbalanced ”head” tags
• Claims you can be removed from the list
For other tasks, like language ID—determining what language a given piece of
text is written in—the most effective naive Bayes features are not words at all, but
byte n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie z’, ‘thei’).
Because spaces count as a byte, byte n-grams can model statistics about the begin-
ning or ending of words. 2 A widely used naive Bayes system, langid.py (Lui
and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature
selection to winnow down to the most informative 7000 ﬁnal features.
Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia
text in 68 different languages were used in (Lui and Baldwin, 2011)), or newswire.
To make sure that this multilingual text correctly reﬂects different regions, dialects,
and socio-economic classes, systems also add Twitter text in many languages geo-
tagged to many regions (important for getting world English dialects from countries
with large Anglophone populations like Nigeria or India), Bible and Quran transla-
tions, slang websites like Urban Dictionary, corpora of African American Vernacular
English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).

4.6 Naive Bayes as a Language Model

As we saw in the previous section, naive Bayes classiﬁers can use any sort of fea-
ture: dictionaries, URLs, email addresses, network features, phrases, and so on. But
if, as in the previous section, we use only individual word features, and we use all
of the words in the text (not a subset), then naive Bayes has an important similar-
ity to language modeling. Speciﬁcally, a naive Bayes model can be viewed as a
set of class-speciﬁc unigram language models, in which the model for each class
instantiates a unigram language model.
Since the likelihood features from the naive Bayes model assign a probability to
each word P(word |c), the model also assigns a probability to each sentence:

P(s|c) = (cid:89)i∈ posit ions

P(wi |c)

(4.15)

Thus consider a naive Bayes model with the classes positive (+) and negative (-)
and the following model parameters:

2

It’s also possible to use codepoints, which are multi-byte Unicode representations of characters in
character sets, but simply using bytes seems to work better.

4 .7

• EVALUAT ION : PR EC I S ION , R ECA LL , F -M EA SURE

73

w

P(w|+) P(w|-)

I
0.1
love 0.1
this 0.01
fun 0.05
ﬁlm 0.1
...
...

0.2
0.001
0.01
0.005
0.1
...

Each of the two columns above instantiates a language model that can assign a
probability to the sentence “I love this fun ﬁlm”:

P(”I love this fun ﬁlm”|+) = 0.1 × 0.1 × 0.01 × 0.05 × 0.1 = 0.0000005
P(”I love this fun ﬁlm”|−) = 0.2 × 0.001 × 0.01 × 0.005 × 0.1 = .0000000010
As it happens, the positive model assigns a higher probability to the sentence:
P(s| pos) > P(s|neg). Note that this is just the likelihood part of the naive Bayes
model; once we multiply in the prior a full naive Bayes model might well make a
different classiﬁcation decision.

4.7 Evaluation: Precision, Recall, F-measure

gold labels

contingency
table

To introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some
simple binary detection tasks. For example, in spam detection, our goal is to label
every text as being in the spam category (“positive”) or not in the spam category
(“negative”). For each item (email document) we therefore need to know whether
our system called it spam or not. We also need to know whether the email is actually
spam or not, i.e. the human-deﬁned labels for each document that we are trying to
match. We will refer to these human labels as the gold labels.
Or imagine you’re the CEO of the Delicious Pie Company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning Delicious Pie. Here the positive class is tweets about
Delicious Pie and the negative class is all other tweets.
In both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. To evaluate any system for detecting things, we start
by building a contingency table like the one shown in Fig. 4.4. Each cell labels a
set of possible outcomes. In the spam detection case, for example, true positives are
documents that are indeed spam (indicated by human-created gold labels) and our
system said they were spam. False negatives are documents that are indeed spam
but our system labeled as non-spam.
To the bottom right of the table is the equation for accuracy, which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. Although accuracy might seem a natural
metric, we generally don’t use it. That’s because accuracy doesn’t work well when
the classes are unbalanced (as indeed they are with spam, which is a large majority
of email, or with tweets, which are mainly not about pie).
To make this more explicit, imagine that we looked at a million tweets, and
let’s say that only 100 of them are discussing their love (or hatred) for our pie,
while the other 999,900 are tweets about something completely unrelated. Imagine a

74 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

Figure 4.4 Contingency table

simple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should
be happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would
be completely useless, since it wouldn’t ﬁnd a single one of the customer comments
we are looking for. In other words, accuracy is not a good metric when the goal is
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.
That’s why instead of accuracy we generally turn to two other metrics: precision
and recall. Precision measures the percentage of the items that the system detected
(i.e., the system labeled as positive) that are in fact positive (i.e., are positive accord-
ing to the human gold labels). Precision is deﬁned as

precision

Precision =

true positives
true positives + false positives

recall

Recall measures the percentage of items actually present in the input that were
correctly identiﬁed by the system. Recall is deﬁned as

Recall =

true positives
true positives + false negatives

Precision and recall will help solve the problem with the useless “nothing is
pie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). You should convince yourself that the precision at ﬁnding relevant
tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize
true positives: ﬁnding the things that we are supposed to be looking for.
There are many ways to deﬁne a single metric that incorporates aspects of both
precision and recall. The simplest of these combinations is the F-measure (van
Rijsbergen, 1975) , deﬁned as:

F-measure

Fβ =

(β 2 + 1)PR
β 2P + R
The β parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β > 1 favor recall, while
values of β < 1 favor precision. When β = 1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called Fβ =1 or just F1 :

F1

4 .7

• EVALUAT ION : PR EC I S ION , R ECA LL , F -M EA SURE

75

2PR
P + R
F-measure comes from a weighted harmonic mean of precision and recall. The
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:

(4.16)

F1 =

HarmonicMean(a1 , a2 , a3 , a4 , ..., an ) =

n

1
a1

+ 1

a2

+ 1

a3

+ ... + 1

an

(4.17)

and hence F-measure is

1

α 1

P + (1 − α ) 1

R

F =

or (cid:18)with β 2 =
Harmonic mean is used because it is a conservative metric; the harmonic mean of
two values is closer to the minimum of the two values than the arithmetic mean is.
Thus it weighs the lower of the two numbers more heavily.

1 − α
α (cid:19) F =

(β 2 + 1)PR
β 2P + R

(4.18)

any-of

one-of
multinomial
classiﬁcation

macroaveraging

microaveraging

4.7.1 More than two classes

Up to now we have been assuming text classiﬁcation tasks with only two classes.
But lots of classiﬁcation tasks in language processing have more than two classes.
For sentiment analysis we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, semantic role labeling, emotion detection, and so on.
There are two kinds of multi-class classiﬁcation tasks. In any-of or multi-label
classiﬁcation, each document or item can be assigned more than one label. We can
solve any-of classiﬁcation by building separate binary classiﬁers for each class c,
trained on positive examples labeled c and negative examples not labeled c. Given
a test document or item d , then each classiﬁer makes their decision independently,
and we may assign multiple labels to d .
More common in language processing is one-of or multinomial classiﬁcation,
in which the classes are mutually exclusive and each document or item appears in
exactly one class. Here we again build a separate binary classiﬁer trained on positive
examples from c and negative examples from all other classes. Now given a test
document or item d , we run all the classiﬁers and choose the label from the classiﬁer
with the highest score. Consider the sample confusion matrix for a hypothetical 3-
way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5.
The matrix shows, for example, that the system mistakenly labeled 1 spam doc-
ument as urgent, and we have shown how to compute a distinct precision and recall
value for each class. In order to derive a single metric that tells us how well the
system is doing, we can combine these values in two ways. In macroaveraging, we
compute the performance for each class, and then average over classes. In microav-
eraging, we collect the decisions for all classes into a single contingency table, and
then compute precision and recall from that table. Fig. 4.6 shows the contingency
table for each class separately, and shows the computation of microaveraged and
macroaveraged precision.
As the ﬁgure shows, a microaverage is dominated by the more frequent class (in
this case spam), since the counts are pooled. The macroaverage better reﬂects the
statistics of the smaller classes, and so is more appropriate when performance on all
the classes is equally important.

76 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

Figure 4.5 Confusion matrix for a three-class categorization task, showing for each pair of
classes (c1 , c2 ), how many documents from c1 were (in)correctly assigned to c2

Figure 4.6 Separate contingency tables for the 3 classes from the previous ﬁgure, showing the pooled contin-
gency table and the microaveraged and macroaveraged precision.

4.8 Test sets and Cross-validation

development
test set
devset

cross-validation

10-fold
cross-validation

The training and testing procedure for text classiﬁcation follows what we saw with
language modeling (Section 3.2): we use the training set to train the model, then use
the development test set (also called a devset) to perhaps tune some parameters,
and in general decide what the best model is. Once we come up with what we think
is the best model, we run it on the (hitherto unseen) test set to report its performance.
While the use of a devset avoids overﬁtting the test set, having a ﬁxed training
set, devset, and test set creates another problem: in order to save lots of data for
training, the test set (or devset) might not be large enough to be representative. It
would be better if we could somehow use all our data both for training and test. We
do this by cross-validation: we randomly choose a training and test set division of
our data, train our classiﬁer, and then compute the error rate on the test set. Then
we repeat with a different randomly selected training set and test set. We do this
sampling process 10 times and average these 10 runs to get an average error rate.

This is called 10-fold cross-validation.

The only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can’t examine any of the data
to suggest possible features and in general see what’s going on. But looking at the
corpus is often important for designing the system. For this reason, it is common

4 .9

• S TAT I ST ICAL S IGN I FICANCE T E S T ING

77

to create a ﬁxed training set and test set, then do 10-fold cross-validation inside
the training set, but compute error rate the normal way in the test set, as shown in
Fig. 4.7.

Figure 4.7

10-fold cross-validation

4.9 Statistical Signiﬁcance Testing

null hypothesis

In building systems we are constantly comparing the performance of systems. Often
we have added some new bells and whistles to our algorithm and want to compare
the new version of the system to the unaugmented version. Or we want to compare
our algorithm to a previously published one to know which is better.
We might imagine that to compare the performance of two classiﬁers A and B
all we have to do is look at A and B’s score on the same test set—for example we
might choose to compare macro-averaged F1— and see whether it’s A or B that has
the higher score. But just looking at this one difference isn’t good enough, because
A might have a better performance than B on a particular test set just by chance.
Let’s say we have a test set x of n observations x = x1 , x2 , .., xn on which A’s
performance is better than B by δ (x). How can we know if A is really better than B?
To do so we’d need to reject the null hypothesis that A isn’t really better than B and
this difference δ (x) occurred purely by chance. If the null hypothesis was correct,
we would expect that if we had many test sets of size n and we measured A and B’s
performance on all of them, that on average A might accidentally still be better than
B by this amount δ (x) just by chance.
More formally, if we had a random variable X ranging over test sets, the null
hypothesis H0 expects P(δ (X ) > δ (x)|H0 ), the probability that we’ll see similarly
big differences just by chance, to be high.
If we had all these test sets we could just measure all the δ (x(cid:48) ) for all the x(cid:48) . If we
found that those deltas didn’t seem to be bigger than δ (x), that is, that p-value(x) was
sufﬁciently small, less than the standard thresholds of 0.05 or 0.01, then we might
reject the null hypothesis and agree that δ (x) was a sufﬁciently surprising difference
and A is really a better algorithm than B. Following Berg-Kirkpatrick et al. (2012)
we’ll refer to P(δ (X ) > δ (x)|H0 ) as p-value(x).
In language processing we don’t generally use traditional statistical approaches
like paired t-tests to compare system outputs because most metrics are not normally

78 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

bootstrap test
approximate
randomization

bootstrapping

distributed, violating the assumptions of the tests. The standard approach to comput-
ing p-value(x) in natural language processing is to use non-parametric tests like the
bootstrap test (Efron and Tibshirani, 1993)— which we will describe below—or a
similar test, approximate randomization (Noreen, 1989). The advantage of these
tests is that they can apply to any metric; from precision, recall, or F1 to the BLEU
metric used in machine translation.
The word bootstrapping refers to repeatedly drawing large numbers of smaller
samples with replacement (called bootstrap samples) from an original larger sam-
ple. The intuition of the bootstrap test is that we can create many virtual test sets
from an observed test set by repeatedly sampling from it. The method only makes
the assumption that the sample is representative of the population.
Consider a tiny text classiﬁcation example with a test set x of 10 documents. The
ﬁrst row of Fig. 4.8 shows the results of two classiﬁers (A and B) on this test set,
with each document labeled by one of the four possibilities: (A and B both right,
both wrong, A right and B wrong, A wrong and B right); a slash through a letter
(B) means that that classiﬁer got the answer wrong. On the ﬁrst document both A
and B get the correct class (AB), while on the second document A got it right but B
got it wrong (AB). If we assume for simplicity that our metric is accuracy, A has an
accuracy of .70 and B of .50, so δ (x) is .20. To create each virtual test set of size
N = 10, we repeatedly (10 times) select a cell from row x with replacement. Fig. 4.8
shows a few examples.

1
2
3
4
5
6
7
8
9
10 A% B% δ ()
x
AB AB AB   AB AB   AB AB AB   AB AB .70 .50 .20
x∗(1) AB AB AB   AB   AB AB   AB AB   AB AB .60 .60 .00
x∗(2) AB AB   AB   AB   AB AB   AB AB AB AB .60 .70 -.10
...

x∗(b)
Figure 4.8 The bootstrap: Examples of b pseudo test sets being created from an initial true
test set x. Each pseudo test set is created by sampling n = 10 times with replacement; thus an
individual sample is a single cell, a document with its gold label and the correct or incorrect
performance of classiﬁers A and B.

Now that we have a sampling distribution, we can do statistics on how how often
A has an accidental advantage. There are various ways to compute this advantage;
here we follow the version laid out in Berg-Kirkpatrick et al. (2012). We might
think that we should just ask, for each bootstrap sample x∗(i) , whether A beats B
by more than δ (x). But there’s a problem: we didn’t draw these samples from a
distribution with 0 mean. The x∗(i) were sampled from x, and so the expected value
of δ (x∗(i) ) lies very close to δ (x). That is, about half the time A will be better than
B, so we expect A to beat B by δ (x). Instead, we want to know how often A beats
these expectations by more than δ (x). To correct for the expected success, we need
to zero-center, subtracting δ (x) from each pseudo test set. Thus we’ll be comparing
for each x∗(i) whether δ (x∗(i) ) > 2δ (x). The full algorithm for the bootstrap is shown
in Fig. 4.9. It is given a test set x, a number of samples b, and counts the percentage
of the b bootstrap test sets in which d elt a(x∗(i) ) > 2δ (x). This percentage then
acts as a one-sided empirical p-value (more sophisticated ways to get p-values from
conﬁdence intervals also exist).

4 .10

• ADVANC ED : F EATUR E S E LEC T ION

79

function BOOT STRA P(test set x, num of samples b) returns p-value(x)
Calculate δ (x) # how much better does algorithm A do than B on x

for i = 1 to b do
for j = 1 to n do

# Draw a bootstrap sample x∗(i) of size n
Select a member of x at random and add it to x∗(i)
Calculate δ (x∗(i) ) # how much better does algorithm A do than B on x∗(i)
s ← s + 1 if δ (x∗(i) ) > 2δ (x)
p-value(x) ≈ s
# on what % of the b samples did algorithm A beat expectations?
return p-value(x)

for each x∗(i)

b

Figure 4.9 A version of the bootstrap algorithm after Berg-Kirkpatrick et al. (2012).

4.10 Advanced: Feature Selection

Feature
selection

information
gain

The regularization technique introduced in the previous section is feature selection
is a method of removing features that are unlikely to generalize well. The basis
of feature selection is to assign some metric of goodness to each feature, rank the
features, and keep the best ones. The number of features to keep is a meta-parameter
that can be optimized on a dev set.
Features are generally ranked by how informative they are about the classiﬁca-
tion decision. A very common metric is information gain. Information gain tells
us how many bits of information the presence of the word gives us for guessing the
class, and can be computed as follows (where ci is the ith class and ¯w means that a
document does not contain the word w):

G(w) = −

C(cid:88)i=1

P(ci ) log P(ci )

+P(w)

+P( ¯w)

4.11 Summary

C(cid:88)i=1
C(cid:88)i=1

P(ci |w) log P(ci |w)

P(ci | ¯w) log P(ci | ¯w)

(4.19)

This chapter introduced the naive Bayes model for classiﬁcation and applied it to

the text categorization task of sentiment analysis.

• Many language processing tasks can be viewed as tasks of classiﬁcation.
learn to model the class given the observation.
• Text categorization, in which an entire text is assigned a class from a ﬁnite set,
includes such tasks as sentiment analysis, spam detection, language identi-
ﬁcation, and authorship attribution.
• Sentiment analysis classiﬁes a text as reﬂecting the positive or negative orien-
tation (sentiment) that a writer expresses toward some object.

80 CHA P TER 4

• NA IVE BAYE S AND S ENT IM EN T C LA S S I FICAT ION

• Naive Bayes is a generative model that make the bag of words assumption
(position doesn’t matter) and the conditional independence assumption (words
are conditionally independent of each other given the class)
• Naive Bayes with binarized features seems to work better for many text clas-
siﬁcation tasks.
• Feature selection can be used to automatically remove features that aren’t
helpful.
• Classiﬁers are evaluated based on precision and recall.
• Classiﬁers are trained using distinct training, dev, and test sets, including the
use of cross-validation in the training set.

Bibliographical and Historical Notes

Multinomial naive Bayes text classiﬁcation was proposed by Maron (1961) at the
RAND Corporation for the task of assigning subject categories to journal abstracts.
His model introduced most of the features of the modern form presented here, ap-
proximating the classiﬁcation task with one-of categorization, and implementing
add-δ smoothing and information-based feature selection.
The conditional independence assumptions of naive Bayes and the idea of Bayes-
ian analysis of text seem to have been arisen multiple times. The same year as
Maron’s paper, Minsky (1961) proposed a naive Bayes classiﬁer for vision and other
artiﬁcial intelligence problems, and Bayesian techniques were also applied to the
text classiﬁcation task of authorship attribution by Mosteller and Wallace (1963). It
had long been known that Alexander Hamilton, John Jay, and James Madison wrote
the anonymously-published Federalist papers. in 1787–1788 to persuade New York
to ratify the United States Constitution. Yet although some of the 85 essays were
clearly attributable to one author or another, the authorship of 12 were in dispute
between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian
probabilistic model of the writing of Hamilton and another model on the writings
of Madison, then computed the maximum-likelihood author for each of the disputed
essays. Naive Bayes was ﬁrst applied to spam detection in Heckerman et al. (1998).
Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show
that using boolean attributes with multinomial naive Bayes works better than full
counts. Binary multinomial naive Bayes is sometimes confused with another variant
of naive Bayes that also use a binary representation of whether a term occurs in
a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead
estimates P(w|c) as the fraction of documents that contain a term, and includes a
probability for whether a term is not in a document. McCallum and Nigam (1998)
and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive
Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text
tasks.
There are a variety of sources covering the many kinds of text classiﬁcation
tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).
Stamatatos (2009) surveys authorship attribute algorithms. On language identiﬁca-
tion see Jauhiainen et al. (2018); Jaech et al. (2016) is an important early neural
system. The task of newswire indexing was often used as a test case for text classi-
ﬁcation algorithms, based on the Reuters-21578 collection of newswire articles.
See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classiﬁcation;
classiﬁcation in general is covered in machine learning textbooks (Hastie et al. 2001,

EX ERC I SE S

81

Witten and Frank 2005, Bishop 2006, Murphy 2012).
Non-parametric methods for computing statistical signiﬁcance were used ﬁrst in
NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech
recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the
bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work
has focused on issues including multiple test sets and multiple metrics (Søgaard
et al. 2014, Dror et al. 2017).
Metrics besides information gain for feature selection include χ 2 , pointwise mu-
tual information, and GINI index; see Yang and Pedersen (1997) for a comparison
and Guyon and Elisseeff (2003) for a broad introduction survey of feature selection.

Exercises

comedy

4.1 Assume the following likelihoods for each word being part of a positive or
negative movie review, and equal prior probabilities for each class.
pos neg
I
0.09 0.16
always 0.07 0.06
like
0.29 0.06
foreign 0.04 0.15
ﬁlms
0.08 0.11
What class will Naive bayes assign to the sentence “I always like foreign
ﬁlms.”?
4.2 Given the following short movie reviews, each labeled with a genre, either
comedy or action:
1. fun, couple, love, love
2. fast, furious, shoot action
3. couple, ﬂy, fast, fun, fun comedy
4. furious, shoot, shoot, fun action
5. ﬂy, fast, shoot, love action
and a new document D:
fast, couple, shoot, ﬂy
compute the most likely class for D. Assume a naive Bayes classiﬁer and use
add-1 smoothing for the likelihoods.
Train two models, multinominal naive Bayes and binarized naive Bayes, both
with add-1 smoothing, on the following document counts for key sentiment
words, with positive or negative class assigned as noted.
doc “good” “poor” “great” (class)
d1. 3
0
3
pos
d2. 0
1
2
pos
d3. 1
3
0
neg
d4. 1
5
2
neg
d5. 0
2
0
neg
Use both naive Bayes models to assign a class (pos or neg) to this sentence:
A good, good plot and great characters, but poor acting.
Do the two models agree or disagree?

4.3

82 CHA P TER 5

• LOG I S T IC R EGR E S S ION

CHAPTER

5 Logistic Regression

logistic
regression

”And how do you know that these ﬁne begonias are not of equal importance?”

Hercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles

Detective stories are as littered with clues as texts are with words. Yet for the
poor reader it can be challenging to know how to weigh the author’s clues in order
to make the crucial classiﬁcation task: deciding whodunnit.
In this chapter we introduce an algorithm that is admirably suited for discovering
the link between features or cues and some particular outcome: logistic regression.
Indeed, logistic regression is one of the most important analytic tool in the social and
natural sciences. In natural language processing, logistic regression is the baseline
supervised machine learning algorithm for classiﬁcation, and also has a very close
relationship with neural networks. As we will see in Chapter 7, a neural network can
be viewed as a series of logistic regression classiﬁers stacked on top of each other.
Thus the classiﬁcation and machine learning techniques introduced here will play
an important role throughout the book.
Logistic regression can be used to classify an observation into one of two classes
(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.
Because the mathematics for the two-class case is simpler, we’ll describe this special
case of logistic regression ﬁrst in the next few sections, and then brieﬂy summarize
the use of multinomial logistic regression for more than two classes in Section 5.6.
We’ll introduce the mathematics of logistic regression in the next few sections.
But let’s begin with some high-level issues.

Generative and Discriminative Classiﬁers: The most important difference be-

tween naive Bayes and logistic regression is that logistic regression is a discrimina-
tive classiﬁer while naive Bayes is a generative classiﬁer.
These are two very different frameworks for how
to build a machine learning model. Consider a visual
metaphor:
imagine we’re trying to distinguish dog
images from cat images. A generative model would
have the goal of understanding what dogs look like
and what cats look like. You might literally ask such
a model to ‘generate’, i.e. draw, a dog. Given a test
image, the system then asks whether it’s the cat model or the dog model that better
ﬁts (is less surprised by) the image, and chooses that as its label.
A discriminative model, by contrast, is only try-
ing to learn to distinguish the classes (perhaps with-
out learning much about them). So maybe all the
dogs in the training data are wearing collars and the
cats aren’t. If that one feature neatly separates the
classes, the model is satisﬁed.
If you ask such a
model what it knows about cats all it can say is that
they don’t wear collars.

5 .1

• C LA S S I FICAT ION : THE S IGMO ID

83

More formally, recall that the naive Bayes assigns a class c to a document d not
by directly computing P(c|d ) but by computing a likelihood and a prior
likelihood
prior
P(d |c)
A generative model like naive Bayes makes use of this likelihood term, which
expresses how to generate the features of a document if we knew it was of class c.
By contrast a discriminative model in this text categorization scenario attempts
to directly compute P(c|d ). Perhaps it will learn to assign high weight to document
features that directly improve its ability to discriminate between possible classes,
even if it couldn’t generate an example of one of the classes.

(cid:122) (cid:125)(cid:124) (cid:123)

(cid:122)(cid:125)(cid:124)(cid:123)P(c)

ˆc = argmax

c∈C

(5.1)

generative
model

discriminative
model

Components of a probabilistic machine learning classiﬁer: Like naive Bayes,

logistic regression is a probabilistic classiﬁer that makes use of supervised machine
learning. Machine learning classiﬁers require a training corpus of M observations
input/output pairs (x(i) , y(i) ). (We’ll use superscripts in parentheses to refer to indi-
vidual instances in the training set—for sentiment classiﬁcation each instance might
be an individual document to be classiﬁed). A machine learning system for classiﬁ-
cation then has four components:
1. A feature representation of the input. For each input observation x(i) , this
will be a vector of features [x1 , x2 , ..., xn ]. We will generally refer to feature
i for input x( j) as x( j)
, sometimes simpliﬁed as xi , but we will also see the
notation f i , f i (x), or, for multiclass classiﬁcation, f i (c, x).
2. A classiﬁcation function that computes ˆy, the estimated class, via p(y|x). In
the next section we will introduce the sigmoid and softmax tools for classiﬁ-
cation.
3. An objective function for learning, usually involving minimizing error on
training examples. We will introduce the cross-entropy loss function
4. An algorithm for optimizing the objective function. We introduce the stochas-

i

tic gradient descent algorithm.

Logistic regression has two phases:
training: we train the system (speciﬁcally the weights w and b) using stochastic
gradient descent and the cross-entropy loss.
test: Given a test example x we compute p(y|x) and return the higher probability
label y = 1 or y = 0.

5.1 Classiﬁcation: the sigmoid

The goal of binary logistic regression is to train a classiﬁer that can make a binary
decision about the class of a new input observation. Here we introduce the sigmoid
classiﬁer that will help us make this decision.
Consider a single input observation x, which we will represent by a vector of
features [x1 , x2 , ..., xn ] (we’ll show sample features in the next subsection). The clas-
siﬁer output y can be 1 (meaning the observation is a member of the class) or 0
(the observation is not a member of the class). We want to know the probability
P(y = 1|x) that this observation is a member of the class. So perhaps the decision

84 CHA P TER 5

• LOG I S T IC R EGR E S S ION

is “positive sentiment” versus “negative sentiment”, the features represent counts
of words in a document, and P(y = 1|x) is the probability that the document has
positive sentiment, while and P(y = 0|x) is the probability that the document has
negative sentiment.
Logistic regression solves this task by learning, from a training set, a vector of
weights and a bias term. Each weight wi is a real number, and is associated with one
of the input features xi . The weight wi represents how important that input feature is
to the classiﬁcation decision, and can be positive (meaning the feature is associated
with the class) or negative (meaning the feature is not associated with the class).
Thus we might expect in a sentiment task the word awesome to have a high positive
weight, and abysmal to have a very negative weight. The bias term, also called the
intercept, is another real number that’s added to the weighted inputs.
To make a decision on a test instance— after we’ve learned the weights in
training— the classiﬁer ﬁrst multiplies each xi by its weight wi , sums up the weighted
features, and adds the bias term b. The resulting single number z expresses the
weighted sum of the evidence for the class.
wi xi(cid:33) + b
In the rest of the book we’ll represent such sums using the dot product notation from
linear algebra. The dot product of two vectors a and b, written as a · b is the sum of
the products of the corresponding elements of each vector. Thus the following is an
equivalent formation to Eq. 5.2:

z = (cid:32) n(cid:88)i=1

(5.2)

bias term
intercept

dot product

z = w · x + b
But note that nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie
between 0 and 1. In fact, since weights are real-valued, the output might even be
negative; z ranges from −∞ to ∞.

(5.3)

Figure 5.1 The sigmoid function y = 1
1+e−z takes a real value and maps it to the range [0, 1].
Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash
outlier values toward 0 or 1.

sigmoid

logistic
function

To create a probability, we’ll pass z through the sigmoid function, σ (z). The
sigmoid function (named because it looks like an s) is also called the logistic func-
tion, and gives logistic regression its name. The sigmoid has the following equation,
shown graphically in Fig. 5.1:

y = σ (z) =

1
1 + e−z

(5.4)

5 .1

• C LA S S I FICAT ION : THE S IGMO ID

85

The sigmoid has a number of advantages; it take a real-valued number and maps
it into the range [0, 1], which is just what we want for a probability. Because it is
nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1. And it’s differentiable, which as we’ll see in Section 5.8 will
be handy for learning.
We’re almost there. If we apply the sigmoid to the sum of the weighted features,
we get a number between 0 and 1. To make it a probability, we just need to make
sure that the two cases, p(y = 1) and p(y = 0), sum to 1. We can do this as follows:

P(y = 1) = σ (w · x + b)
1

=

1 + e−(w·x+b)

P(y = 0) = 1 − σ (w · x + b)
1
= 1 −

1 + e−(w·x+b)
e−(w·x+b)
1 + e−(w·x+b)

=

(5.5)

decision
boundary

boundary:

Now we have an algorithm that given an instance x computes the probability
P(y = 1|x). How do we make a decision? For a test instance x, we say yes if the
probability P(y = 1|x) is more than .5, and no otherwise. We call .5 the decision
ˆy = (cid:26) 1 if P(y = 1|x) > 0.5
0 otherwise

5.1.1 Example: sentiment classiﬁcation

Let’s have an example. Suppose we are doing binary sentiment classiﬁcation on
movie review text, and we would like to know whether to assign the sentiment class
+ or − to a review document doc. We’ll represent each input observation by the
following 6 features x1 ...x6 of the input; Fig. 5.2 shows the features in a sample mini
test document.

Var Deﬁnition
x1
count(positive lexicon) ∈ doc)
x2
count(negative lexicon) ∈ doc)
(cid:26) 1 if “no” ∈ doc
x3
0 otherwise
x4
count(1st and 2nd pronouns ∈ doc)
(cid:26) 1 if “!” ∈ doc
x5
0 otherwise
x6
log(word count of doc)

Value in Fig. 5.2
3
2

1
3

0
ln(64) = 4.15

Let’s assume for the moment that we’ve already learned a real-valued weight
for each of these features, and that the 6 weights corresponding to the 6 features
are [2.5, −5.0, −1.2, 0.5, 2.0, 0.7], while b = 0.1. (We’ll discuss in the next section
how the weights are learned.) The weight w1 , for example indicates how important

86 CHA P TER 5

• LOG I S T IC R EGR E S S ION

Figure 5.2 A sample mini test document showing the extracted features in the vector x.

a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to
a positive sentiment decision, while w2 tells us the importance of negative lexicon
words. Note that w1 = 2.5 is positive, while w2 = −5.0, meaning that negative words
are negatively associated with a positive sentiment decision, and are about twice as
important as positive words.
Given these 6 features and the input review x, P(+|x) and P(−|x) can be com-
puted using Eq. 5.5:

p(+|x) = P(Y = 1|x) = σ (w · x + b)
= σ ([2.5, −5.0, −1.2, 0.5, 2.0, 0.7] · [3, 2, 1, 3, 0, 4.15] + 0.1)
= σ (1.805)
= 0.86
p(−|x) = P(Y = 0|x) = 1 − σ (w · x + b)
= 0.14

Logistic regression is commonly applied to all sorts of NLP tasks, and any prop-
erty of the input can be a feature. Consider the task of period disambiguation:
deciding if a period is the end of a sentence or part of a word, by classifying each
period into one of two classes EOS (end-of-sentence) and not-EOS. We might use
features like x1 below expressing that the current word is lower case and the class
is EOS (perhaps with a positive weight), or that the current word is in our abbrevia-
tions dictionary (“Prof.”) and the class is EOS (perhaps with a negative weight). A
feature can also express a quite complex combination of properties. For example a
period following a upper cased word is a likely to be an EOS, but if the word itself is
St. and the previous word is capitalized, then the period is likely part of a shortening
of the word street.
x1 = (cid:26) 1 if “Case(wi ) = Lower”
0 otherwise
x2 = (cid:26) 1 if “wi ∈ AcronymDict”
0 otherwise
x3 = (cid:26) 1 if “wi = St. & Case(wi−1 ) = Cap”
0 otherwise
Designing features: Features are generally designed by examining the training
set with an eye to linguistic intuitions and the linguistic literature on the domain. A
careful error analysis on the training or dev set. of an early version of a system often
provides insights into features.

feature
interactions

feature
templates

5 .2

• L EARN ING IN LOG I S T IC R EGR E S S ION

87

For some tasks it is especially helpful to build complex features that are combi-
nations of more primitive features. We saw such a feature for period disambiguation
above, where a period on the word St. was less likely to be the end of sentence if
the previous word was capitalized. For logistic regression and naive Bayes these
combination features or feature interactions have to be designed by hand.
For many tasks (especially when feature values can reference speciﬁc words)
we’ll need large numbers of features. Often these are created automatically via fea-
ture templates, abstract speciﬁcations of features. For example a bigram template
for period disambiguation might create a feature for every pair of words that occurs
before a period in the training set. Thus the feature space is sparse, since we only
have to create a feature if that n-gram exists in that position in the training set. The
feature is generally created as a hash from the string descriptions. A user description
of a feature as, “bigram(American breakfast)” is hashed into a unique integer i that
becomes the feature number f i .
In order to avoid the extensive human effort of feature design, recent research in
NLP has focused on representation learning: ways to learn features automatically
in an unsupervised way from the input. We’ll introduce methods for representation
learning in Chapter 6 and Chapter 7.

Choosing a classiﬁer Logistic regression has a number of advantages over naive
Bayes. Naive Bayes has overly strong conditional independence assumptions. Con-
sider two features which are strongly correlated; in fact, imagine that we just add the
same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were sep-
arate, multiplying them both in, overestimating the evidence. By contrast, logistic
regression is much more robust to correlated features; if two features f1 and f2 are
perfectly correlated, regression will simply assign part of the weight to w1 and part
to w2 . Thus when there are many correlated features, logistic regression will assign
a more accurate probability than naive Bayes. So logistic regression generally works
better on larger documents or datasets and is a common default.
Despite the less accurate probabilities, naive Bayes still often makes the correct
classiﬁcation decision. Furthermore, naive Bayes works extremely well (even bet-
ter than logistic regression) on very small datasets (Ng and Jordan, 2002) or short
documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to imple-
ment and very fast to train (there’s no optimization step). So it’s still a reasonable
approach to use in some situations.

5.2 Learning in Logistic Regression

How are the parameters of the model, the weights w and bias b, learned?
Logistic regression is an instance of supervised classiﬁcation in which we know
the correct label y (either 0 or 1) for each observation x. What the system produces,
via Eq. 5.5 is ˆy, the system’s estimate of the true y. We want to learn parameters
(meaning w and b) that make ˆy for each training observation as close as possible to
the true y .
This requires 2 components that we foreshadowed in the introduction to the
chapter. The ﬁrst is a metric for how close the current label ( ˆy) is to the true gold
label y. Rather than measure similarity, we usually talk about the opposite of this:
the distance between the system output and the gold output, and we call this distance
the loss function or the cost function. In the next section we’ll introduce the loss

loss

88 CHA P TER 5

• LOG I S T IC R EGR E S S ION

function that is commonly used for logistic regression and also for neural networks,

the cross-entropy loss.

The second thing we need is an optimization algorithm for iteratively updating
the weights so as to minimize this loss function. The standard algorithm for this is

gradient descent; we’ll introduce the stochastic gradient descent algorithm in the

following section.

5.3 The cross-entropy loss function

We need a loss function that expresses, for an observation x, how close the classiﬁer
output ( ˆy = σ (w · x + b)) is to the correct output (y, which is 0 or 1). We’ll call this:
L( ˆy, y) = How much ˆy differs from the true y

(5.6)

You could imagine using a simple loss function that just takes the mean squared
error between ˆy and y.

cross entropy
loss

1
LMSE ( ˆy, y) =
( ˆy − y)2
2
It turns out that this MSE loss, which is very useful for some algorithms like
linear regression, becomes harder to optimize (technically, non-convex), when it’s
applied to probabilistic classiﬁcation.
Instead, we use a loss function that prefers the correct class labels of the training

(5.7)

example to be more likely. This is called conditional maximum likelihood estima-
tion: we choose the parameters w, b that maximize the log probability of the true

y labels in the training data given the observations x. The resulting loss function
is the negative log likelihood loss, generally called the cross entropy loss.
Let’s derive this loss function, applied to a single observation x. We’d like to
learn weights that maximize the probability of the correct label p(y|x). Since there
are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can
express the probability p(y|x) that our classiﬁer produces for one observation as
the following (keeping in mind that if y=1, Eq. 5.8 simpliﬁes to ˆy; if y=0, Eq. 5.8
simpliﬁes to 1 − ˆy):

p(y|x) = ˆy y (1 − ˆy)1−y
Now we take the log of both sides. This will turn out to be handy mathematically,
and doesn’t hurt us; whatever values maximize a probability will also maximize the
log of the probability:
log p(y|x) = log (cid:2) ˆy y (1 − ˆy)1−y (cid:3)
= y log ˆy + (1 − y) log(1 − ˆy)
Eq. 5.9 describes a log likelihood that should be maximized. In order to turn this
into loss function (something that we need to minimize), we’ll just ﬂip the sign on
Eq. 5.9. The result is the cross-entropy loss LCE :
LCE ( ˆy, y) = − log p(y|x) = − [y log ˆy + (1 − y) log(1 − ˆy)]
Finally, we can plug in the deﬁnition of ˆy = σ (w · x) + b:
LCE (w, b) = − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]

(5.10)

(5.11)

(5.8)

(5.9)

5 .4

• GRAD I ENT D E SC EN T

89

Why does minimizing this negative log probability do what we want? A perfect
classiﬁer would assign probability 1 to the correct outcome (y=1 or y=0) and prob-
ability 0 to the incorrect outcome. That means the higher ˆy (the closer it is to 1), the
better the classiﬁer; the lower ˆy is (the closer it is to 0), the worse the classiﬁer. The
negative log of this probability is a convenient loss metric since it goes from 0 (neg-
ative log of 1, no loss) to inﬁnity (negative log of 0, inﬁnite loss). This loss function
also insures that as probability of the correct answer is maximized, the probability
of the incorrect answer is minimized; since the two sum to one, any increase in the
probability of the correct answer is coming at the expense of the incorrect answer.
It’s called the cross-entropy loss, because Eq. 5.9 is also the formula for the cross-
entropy between the true probability distribution y and our estimated distribution
ˆy.

Let’s now extend Eq. 5.10 from one example to the whole training set: we’ll con-
tinue to use the notation that x(i) and y(i) mean the ith training features and training
label, respectively. We make the assumption that the training examples are indepen-
dent:

(5.12)

(5.13)

(5.14)

log p(training labels) = log

p(y(i) |x(i) )

m(cid:89)i=1
m(cid:88)i=1
m(cid:88)i=1

=

log p(y(i) |x(i) )

= −

LCE ( ˆy(i) , y(i) )

We’ll deﬁne the cost function for the whole dataset as the average loss for each
example:

Cost (w, b) =

1
m

m(cid:88)i=1
LCE ( ˆy(i) , y(i) )
m(cid:88)i=1

1
m

= −

y(i) log σ (w · x(i) + b) + (1 − y(i) ) log (cid:16)1 − σ (w · x(i) + b)(cid:17)
Now we know what we want to minimize; in the next section, we’ll see how to
ﬁnd the minimum.

(5.15)

5.4 Gradient Descent

Our goal with gradient descent is to ﬁnd the optimal weights: minimize the loss
function we’ve deﬁned for the model. In Eq. 5.16 below, we’ll explicitly represent
the fact that the loss function L is parameterized by the weights, which we’ll refer to
in machine learning in general as θ (in the case of logistic regression θ = w, b):

ˆθ = argmin

θ

1
m

m(cid:88)i=1

LCE (y(i) , x(i) ; θ )

(5.16)

90 CHA P TER 5

• LOG I S T IC R EGR E S S ION

convex

How shall we ﬁnd the minimum of this (or any) loss function? Gradient descent
is a method that ﬁnds a minimum of a function by ﬁguring out in which direction
(in the space of the parameters θ ) the function’s slope is rising the most steeply,
and moving in the opposite direction. The intuition is that if you are hiking in a
canyon and trying to descend most quickly down to the river at the bottom, you might
look around yourself 360 degrees, ﬁnd the direction where the ground is sloping the
steepest, and walk downhill in that direction.
For logistic regression, this loss function is conveniently convex. A convex func-
tion has just one minimum; there are no local minima to get stuck in, so gradient
descent starting from any point is guaranteed to ﬁnd the minimum.
Although the algorithm (and the concept of gradient) are designed for direction
vectors, let’s ﬁrst consider a visualization of the the case where the parameter of our
system, is just a single scalar w, shown in Fig. 5.3.
Given a random initialization of w at some value w1 , and assuming the loss
function L happened to have the shape in Fig. 5.3, we need the algorithm to tell us
whether at the next iteration, we should move left (making w2 smaller than w1 ) or
right (making w2 bigger than w1 ) to reach the minimum.

Figure 5.3 The ﬁrst step in iteratively ﬁnding the minimum of this loss function, by moving
w in the reverse direction from the slope of the function. Since the slope is negative, we need
to move w in a positive direction, to the right. Here superscripts are used for learning steps,
so w1 means the initial value of w (which is 0), w2 at the second step, and so on.

gradient

learning rate

d

The gradient descent algorithm answers this question by ﬁnding the gradient
of the loss function at the current point and moving in the opposite direction. The
gradient of a function of many variables is a vector pointing in the direction the
greatest increase in a function. The gradient is a multi-variable generalization of the
slope, so for a function of one variable like the one in Fig. 5.3, we can informally
think of the gradient as the slope. The dotted line in Fig. 5.3 shows the slope of this
hypothetical loss function at point w = w1 . You can see that the slope of this dotted
line is negative. Thus to ﬁnd the minimum, gradient descent tells us to go in the
opposite direction: moving w in a positive direction.
The magnitude of the amount to move in gradient descent is the value of the slope
dw f (x; w) weighted by a learning rate η . A higher (faster) learning rate means that
we should move w more on each step. The change we make in our parameter is the
learning rate times the gradient (or the slope, in our single-variable example):
wt+1 = wt − η d
dw
Now let’s extend the intuition from a function of one scalar variable w to many

f (x; w)

(5.17)

5 .4

• GRAD I ENT D E SC EN T

91

variables, because we don’t just want to move left or right, we want to know where
in the N-dimensional space (of the N parameters that make up θ ) we should move.
The gradient is just such a vector; it expresses the directional components of the
sharpest slope along each of those N dimensions. If we’re just imagining two weight
dimension (say for one weight w and one bias b), the gradient might be a vector with
two orthogonal components, each of which tells us how much the ground slopes in
the w dimension and in the b dimension. Fig. 5.4 shows a visualization:

Figure 5.4 Visualization of the gradient vector in two dimensions w and b.

In an actual logistic regression, the parameter vector w is much longer than 1 or
2, since the input feature vector x can be quite long, and we need a weight wi for
each xi For each dimension/variable wi in w (plus the bias b), the gradient will have
a component that tells us the slope with respect to that variable. Essentially we’re
asking: “How much would a small change in that variable wi inﬂuence the total loss
function L?”
In each dimension wi , we express the slope as a partial derivative ∂
of the loss
function. The gradient is then deﬁned as a vector of these partials. We’ll represent ˆy
as f (x; θ ) to make the dependence on θ more obvious:

∂ wi

L( f (x; θ ), y)
L( f (x; θ ), y)
...
L( f (x; θ ), y)
The ﬁnal equation for updating θ based on the gradient is thus

∇θ L( f (x; θ ), y)) =

∂ w1
∂ w2

∂ wn

∂



∂

∂



θt+1 = θt − η ∇L( f (x; θ ), y)

5.4.1 The Gradient for Logistic Regression

(5.18)

(5.19)

In order to update θ , we need a deﬁnition for the gradient ∇L( f (x; θ ), y). Recall that
for logistic regression, the cross-entropy loss function is:
LCE (w, b) = − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]
It turns out that the derivative of this function for one observation vector x is
Eq. 5.21 (the interested reader can see Section 5.8 for the derivation of this equation):
= [σ (w · x + b) − y]x j

∂ LCE (w, b)

(5.20)

(5.21)

∂ w j

92 CHA P TER 5

• LOG I S T IC R EGR E S S ION

Note in Eq. 5.21 that the gradient with respect to a single weight w j represents a
very intuitive value: the difference between the true y and our estimated ˆy = σ (w ·
x + b) for that observation, multiplied by the corresponding input value x j .
The loss for a batch of data or an entire dataset is just the average loss over the
m examples:

Cost (w, b) = −

1
m

m(cid:88)i=1

y(i) log σ (w · x(i) + b) + (1 − y(i) ) log (cid:16)1 − σ (w · x(i) + b)(cid:17)

(5.22)

And the gradient for multiple data points is the sum of the individual gradients::

∂ Cost (w, b)
∂ w j

=

m(cid:88)i=1 (cid:104)σ (w · x(i) + b) − y(i) (cid:105) x(i)

j

(5.23)

5.4.2 The Stochastic Gradient Descent Algorithm

Stochastic gradient descent is an online algorithm that minimizes the loss function
by computing its gradient after each training example, and nudging θ in the right
direction (the opposite direction of the gradient). Fig. 5.5 shows the algorithm.

function S TOCHA ST IC GRAD I ENT D E SC EN T(L(), f (), x, y) returns θ
# where: L is the loss function
#
f is a function parameterized by θ
#
x is the set of training inputs x(1) , x(2) , ..., x(n)
#
y is the set of training outputs (labels) y(1) , y(2) , ..., y(n)

θ ← 0
repeat T times

For each training tuple (x(i) , y(i) ) (in random order)
Compute ˆy (i) = f (x(i) ; θ )
# What is our estimated output ˆy?
Compute the loss L( ˆy (i) , y(i) ) # How far off is ˆy(i) ) from the true output y(i) ?
# How should we move θ to maximize loss ?
# go the other way instead

g ← ∇θ L( f (x(i) ; θ ), y(i) )

θ ← θ − η g

return θ

minibatch

Figure 5.5 The stochastic gradient descent algorithm

Stochastic gradient descent is called stochastic because it chooses a single ran-
dom example at a time, moving the weights so as to improve performance on that
single example. That can result in very choppy movements, so it’s also common to
do minibatch gradient descent, which computes the gradient over batches of train-
ing instances rather than a single instance.
The learning rate η is a parameter that must be adjusted. If it’s too high, the
learner will take steps that are too large, overshooting the minimum of the loss func-
tion. If it’s too low, the learner will take steps that are too small, and take too long to
get to the minimum. It is most common to begin the learning rate at a higher value,
and then slowly decrease it, so that it is a function of the iteration k of training; you
will sometimes see the notation ηk to mean the value of the learning rate at iteration
k.

5 .5

• R EGU LAR I ZAT ION

93

5.4.3 Working through an example

Let’s walk though a single step of the gradient descent algorithm. We’ll use a sim-
pliﬁed version of the example in Fig. 5.2 as it sees a single observation x, whose
correct value is y = 1 (this is a positive review), and with only two features:

x1 = 3
x2 = 2

(count of positive lexicon words)
(count of negative lexicon words)

Let’s assume the initial weights and bias in θ 0 are all set to 0, and the initial learning
rate η is 0.1:

w1 = w2 = b = 0

η = 0.1

The single update step requires that we compute the gradient, multiplied by the
learning rate

θ t+1 = θ t − η ∇θ L( f (x(i) ; θ ), y(i) )

In our mini example there are three parameters, so the gradient vector has 3 dimen-
sions, for w1 , w2 , and b. We can compute the ﬁrst gradient as follows:

∇w,b = 

 = 

 = 

−0.5x1
−0.5x2

−0.5  = 

−1.5
−1.0

−0.5 

 = 

∂ LCE (w,b)
∂ LCE (w,b)
∂ LCE (w,b)

∂ w1
∂ w2

∂ b

(σ (w · x + b) − y)x1
(σ (0) − 1)x1
(σ (w · x + b) − y)x2
(σ (0) − 1)x2
σ (w · x + b) − y
σ (0) − 1
Now that we have a gradient, we compute the new parameter vector θ 2 by mov-
ing θ 1 in the opposite direction from the gradient:
.05 
.15
.1
So after one step of gradient descent, the weights have shifted to be: w1 = .15,
w2 = .1, and b = .05.
Note that this observation x happened to be a positive example. We would expect
that after seeing more negative examples with high counts of negative words, that
the weight w2 would shift to have a negative value.

−0.5  = 

b  − η 

θ 2 = 

−1.5
−1.0

w1
w2

5.5 Regularization

Numquam ponenda est pluralitas sine necessitate
‘Plurality should never be proposed unless needed’

William of Occam

94 CHA P TER 5

• LOG I S T IC R EGR E S S ION

There is a problem with learning weights that make the model perfectly match
the training data. If a feature is perfectly predictive of the outcome because it hap-
pens to only occur in one class, it will be assigned a very high weight. The weights
for features will attempt to perfectly ﬁt details of the training set, in fact too per-
fectly, modeling noisy factors that just accidentally correlate with the class. This
problem is called overﬁtting. A good model should be able to generalize well from
the training data to the unseen test set, but a model that overﬁts will have poor gen-
eralization.
To avoid overﬁtting, a regularization term is added to the objective function in
Eq. 5.16, resulting in the following objective:

overﬁtting
generalize

regularization

ˆw = argmax

w

m(cid:88)1=1

log P(y(i) |x(i) ) − α R(w)

(5.24)

The new component, R(w) is called a regularization term, and is used to penalize
large weights. Thus a setting of the weights that matches the training data perfectly,
but uses many weights with high values to do so, will be penalized more than a
setting that matches the data a little less well, but does so using smaller weights.
There are two common regularization terms R(w). L2 regularization is a quad-
ratic function of the weight values, named because it uses the (square of the) L2
norm of the weight values. The L2 norm, ||W ||2 , is the same as the Euclidean

distance:

L2
regularization

R(W ) = ||W ||2

2 =

w2

j

N(cid:88)j=1

(5.25)

ˆw = argmax

The L2 regularized objective function becomes:
log P(y(i) |x(i) )(cid:35) − α
L1 regularization is a linear function of the weight values, named after the L1
norm ||W ||1 , the sum of the absolute values of the weights, or Manhattan distance
(the Manhattan distance is the distance you’d have to walk between two points in a
city with a street grid like New York):

(cid:34) m(cid:88)1=i

n(cid:88)j=1

(5.26)

w2

w

j

L1
regularization

R(W ) = ||W ||1 =

N(cid:88)i=1

|wi |

(5.27)

The L1 regularized objective function becomes:
log P(y(i) |x(i) )(cid:35) − α
These kinds of regularization come from statistics, where L1 regularization is
called the ‘lasso’ or lasso regression (Tibshirani, 1996) and L2 regression is called

(cid:34) m(cid:88)1=i

ˆw = argmax

n(cid:88)j=1

|w j |

(5.28)

w

5 .6

• MU LT INOM IA L LOG I ST IC R EGR E S S ION

95

ridge regression, and both are commonly used in language processing. L2 regu-
larization is easier to optimize because of its simple derivative (the derivative of w2
is just 2w), while L1 regularization is more complex (the derivative of |w| is non-
continuous at zero). But where L2 prefers weight vectors with many small weights,
L1 prefers sparse solutions with some larger weights but many more weights set to
zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer
features.
Both L1 and L2 regularization have Bayesian interpretations as constraints on
the prior of how weights should look. L1 regularization can be viewed as a Laplace
prior on the weights. L2 regularization corresponds to assuming that weights are
distributed according to a gaussian distribution with mean µ = 0.
In a gaussian
or normal distribution, the further away a value is from the mean, the lower its
probability (scaled by the variance σ ). By using a gaussian prior on the weights, we
are saying that weights prefer to have the value 0. A gaussian for a weight w j is
exp (cid:32)−
If we multiply each weight by a gaussian prior on the weight, we are thus maxi-
mizing the following constraint:

(cid:113)2π σ 2

(w j − µ j )2

(cid:33)

(5.29)

2σ 2

1

j

j

ˆw = argmax

exp (cid:32)−
which in log space, with µ = 0, and assuming 2σ 2 = 1, corresponds to

(cid:113)2π σ 2

P(y(i) |x(i) ) ×

(w j − µ j )2

n(cid:89)j=1

M(cid:89)i=1

2σ 2

1

w

j

j

ˆw = argmax

w

m(cid:88)i=1

log P(y(i) |x(i) ) − α

w2

j

n(cid:88)j=1

which is in the same form as Eq. 5.26.

5.6 Multinomial logistic regression

(cid:33)

(5.30)

(5.31)

multinominal
logistic
regression

softmax

Sometimes we need more than two classes. Perhaps we might want to do 3-way
sentiment classiﬁcation (positive, negative, or neutral). Or we could be classifying
the part of speech of a word (choosing from 10, 30, or even 50 different parts of
speech), or assigning semantic labels like the named entities or semantic relations
we will introduce in Chapter 17.

In such cases we use multinominal logistic regression, also called softmax re-

gression (or, historically, the maxent classiﬁer). In multinominal logistic regression
the target y is a variable that ranges over more than two classes; we want to know
the probability of y being in each potential class c ∈ C, p(y = c|x).
The multinominal logistic classiﬁer uses a generalization of the sigmoid, called
the softmax function, to compute the probability p(y = c|x). The softmax function
takes a vector z = [z1 , z2 , ..., zk ] of k arbitrary values and maps them to a probability
distribution, with each value in the range (0,1], and all the values summing to 1.
Like the sigmoid, it is an exponential function;

96 CHA P TER 5

• LOG I S T IC R EGR E S S ION

For a vector z of dimensionality k, the softmax is deﬁned as:

softmax(zi ) =

1 ≤ i ≤ k

(5.32)

ezi
j=1 ez j

(cid:80)k

(cid:80)k

The softmax of an input vector z = [z1 , z2 , ..., zk ] is thus a vector itself:
softmax(z) = (cid:34)
The denominator (cid:80)k
i=1 ezi is used to normalize all the values into probabilities.
Thus for example given a vector:
z = [0.6, 1.1, −1.5, 1.2, 3.2, −1.1]

i=1 ezi (cid:35)
ezk

ez2
i=1 ezi

ez1
i=1 ezi

,

(cid:80)k

, ...,

(cid:80)k

(5.33)

the result softmax(z) is

[0.055, 0.090, 0.0067, 0.10, 0.74, 0.010]

Again like the sigmoid, the input to the softmax will be the dot product between
a weight vector w and an input vector x (plus a bias). But now we’ll need separate
weight vectors (and bias) for each of the K classes.

p(y = c|x) =

ewc · x + bc
ew j · x + b j

k(cid:88)j=1

(5.34)

Like the sigmoid, the softmax has the property of squashing values toward 0 or
1. thus if one of the inputs is larger than the others, will tend to push its probability
toward 1, and suppress the probabilities of the smaller inputs.

5.6.1 Features in Multinomial Logistic Regression

For multiclass classiﬁcation the input features need to be a function of both the
observation x and the candidate output class c. Thus instead of the notation xi , f i
or f i (x), when we’re discussing features we will use the notation f i (c, x), meaning
feature i for a particular class c for a given observation x.
In binary classiﬁcation, a positive weight on a feature pointed toward y=1 and
a negative weight toward y=0... but in multiclass a feature could be evidence for or
against an individual class.
Let’s look at some sample features for a few NLP tasks to help understand this
perhaps unintuitive use of features that are functions of both the observation x and
the class c,
Suppose we are doing text classiﬁcation, and instead of binary classiﬁcation our
task is to assign one of the 3 classes +, −, or 0 (neutral) to a document. Now a
feature related to exclamation marks might have a negative weight for 0 documents,
and a positive weight for + or − documents:

5 .7

•

IN TER PR ET ING MODE L S

97

Var
Deﬁnition
f1 (0, x) (cid:26) 1 if “!” ∈ doc
0 otherwise
f1 (+, x) (cid:26) 1 if “!” ∈ doc
0 otherwise
f1 (0, x) (cid:26) 1 if “!” ∈ doc
0 otherwise

Wt
−4.5
2.6

1.3

5.6.2 Learning in Multinomial Logistic Regression

Multinomial logistic regression has a slightly different loss function than binary lo-
gistic regression because it uses the softmax rather than sigmoid classiﬁer, The loss
function for a single example x is the sum of the logs of the K output classes:

LCE ( ˆy, y) = −

= −

K(cid:88)k=1
K(cid:88)k=1

1{y = k} log p(y = k|x)

1{y = k} log

ewk ·x+bk
j=1 ew j ·x+b j

(cid:80)K

This makes use of the function 1{} which evaluates to 1 if the condition in the
brackets is true and to 0 otherwise.
The gradient for a single example turns out to be very similar to the gradient for
logistic regression, although we don’t show the derivation here. It is the different
between the value for the true class k (which is 1) and the probability the classiﬁer
outputs for class k, weighted by the value of the input xk :
∂ LCE
∂ wk

= (1{y = k} − p(y = k|x))xk
= (cid:32)1{y = k} −

j=1 ew j ·x+b j (cid:33) xk
ewk ·x+bk

(cid:80)K

(5.35)

(5.36)

5.7

Interpreting models

interpretable

Often we want to know more than just the correct classiﬁcation of an observation.
We want to know why the classiﬁer made the decision it did. That is, we want our
decision to be interpretable. Interpretability can be hard to deﬁne strictly, but the
core idea is that as humans we should know why our algorithms reach the conclu-
sions they do. Because the features to logistic regression are often human-designed,
one way to understand a classiﬁer’s decision is to understand the role each feature it
plays in the decision. Logistic regression can be combined with statistical tests (the
likelihood ratio test, or the Wald test); investigating whether a particular feature is
signiﬁcant by one of these tests, or inspecting its magnitude (how large is the weight
w associated with the feature?) can help us interpret why the classiﬁer made the
decision it makes. This is enormously important for building transparent models.
Furthermore, in addition to its use as a classiﬁer, logistic regression in NLP and
many other ﬁelds is widely used as an analytic tool for testing hypotheses about the

98 CHA P TER 5

• LOG I S T IC R EGR E S S ION

effect of various explanatory variables (features). In text classiﬁcation, perhaps we
want to know if logically negative words (no, not, never) are more likely to be asso-
ciated with negative sentiment, or if negative reviews of movies are more likely to
discuss the cinematography. However, in doing so it’s necessary to control for po-
tential confounds: other factors that might inﬂuence sentiment (the movie genre, the
year it was made, perhaps the length of the review in words). Or we might be study-
ing the relationship between NLP-extracted linguistic features and non-linguistic
outcomes (hospital readmissions, political outcomes, or product sales), but need to
control for confounds (the age of the patient, the county of voting, the brand of the
product). In such cases, logistic regression allows us to test whether some feature is
associated with some outcome above and beyond the effect of other features.

5.8 Advanced: Deriving the Gradient Equation

In this section we give the derivation of the gradient of the cross-entropy loss func-
tion LCE for logistic regression. Let’s start with some quick calculus refreshers.
First, the derivative of ln(x):

d
1
d x
x
Second, the (very elegant) derivative of the sigmoid:

ln(x) =

(5.37)

chain rule

= σ (z)(1 − σ (z))

dσ (z)
d z
Finally, the chain rule of derivatives. Suppose we are computing the derivative
of a composite function f (x) = u(v(x)). The derivative of f (x) is the derivative of
u(x) with respect to v(x) times the derivative of v(x) with respect to x:
d f
d u
d v
d v ·
d x
d x
First, we want to know the derivative of the loss function with respect to a single
weight w j (we’ll need to compute it for each weight, and for the bias):

(5.38)

(5.39)

=

∂ LL(w, b)
∂ w j

=

∂

∂ w j

= − (cid:20) ∂

∂ w j − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))]
(1 − y) log [1 − σ (w · x + b)](cid:21)
y log σ (w · x + b) +
Next, using the chain rule, and relying on the derivative of log:
∂ LL(w, b)
y
σ (w · x + b) −
1 − y
∂ w j
σ (w · x + b)
1 − σ (w · x + b)

= −

∂ w j

∂ w j

∂

∂ w j

∂

∂

(5.40)

1 − σ (w · x + b)

(5.41)

Rearranging terms:
∂ LL(w, b)
∂ w j

= − (cid:20)

y
σ (w · x + b) −

1 − σ (w · x + b) (cid:21) ∂
1 − y
∂ w j

σ (w · x + b)

(5.42)

∂ LL(w, b)
∂ w j

And now plugging in the derivative of the sigmoid, and using the chain rule one
more time, we end up with Eq. 5.43:
σ (w · x + b)[1 − σ (w · x + b)] (cid:21) σ (w · x + b)[1 − σ (w · x + b)]
y − σ (w · x + b)
∂ (w · x + b)
∂ w j
σ (w · x + b)[1 − σ (w · x + b)] (cid:21) σ (w · x + b)[1 − σ (w · x + b)]x j
y − σ (w · x + b)
= −[y − σ (w · x + b)]x j
= [σ (w · x + b) − y]x j

= − (cid:20)
= − (cid:20)

5 .9

• SUMMARY

99

(5.43)

5.9 Summary

This chapter introduced the logistic regression model of classiﬁcation.
• Logistic regression is a supervised machine learning classiﬁer that extracts
real-valued features from the input, multiplies each by a weight, sums them,
and passes the sum through a sigmoid function to generate a probability. A
threshold is used to make a decision.
• Logistic regression can be used with two classes (e.g., positive and negative
sentiment) or with multiple classes (multinomial logistic regression, for ex-
ample for n-ary text classiﬁcation, part-of-speech labeling, etc.).
• Multinomial logistic regression uses the softmax function to compute proba-
bilities.
• The weights (vector w and bias b) are learned from a labeled training set via a
loss function, such as the cross-entropy loss, that must be minimized.
• Minimizing this loss function is a convex optimization problem, and iterative
algorithms like gradient descent are used to ﬁnd the optimal weights.
• Regularization is used to avoid overﬁtting.
• Logistic regression is also one of the most useful analytic tools, because of its
ability to transparently study the importance of individual features.

Bibliographical and Historical Notes

Logistic regression was developed in the ﬁeld of statistics, where it was used for
the analysis of binary data by the 1960s, and was particularly common in medicine
(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one
of the formal foundations of the study of linguistic variation (Sankoff and Labov,
1979).
Nonetheless, logistic regression didn’t become common in natural language pro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. The ﬁrst source was the neighboring ﬁelds of information retrieval and
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to NLP. Indeed a very early use of logistic
regression for document routing was one of the ﬁrst NLP applications to use (LSI)
embeddings as word representations (Sch ¨utze et al., 1995).
At the same time in the early 1990s logistic regression was developed and ap-
plied to NLP at IBM Research under the name maximum entropy modeling or

maximum
entropy

100 CHA PTER 5

• LOG I ST IC R EGR E S S ION

maxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-
der that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech
tagging ((Ratnaparkhi, 1996)), parsing (Ratnaparkhi, 1997), and text classiﬁcation
(Nigam et al., 1999).
More on classiﬁcation can be found in machine learning textbooks (Hastie et al. 2001,
Witten and Frank 2005, Bishop 2006, Murphy 2012).

Exercises

CHAPTER

6 Vector Semantics

The asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the
middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt
preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene
Epoch. One of these fossils is the Smilodon, or sabre-toothed tiger, instantly rec-
ognizable by its long canines. Five million years ago or so, a completely different
sabre-tooth tiger called Thylacosmilus lived
in Argentina and other parts of South Amer-
ica. Thylacosmilus was a marsupial whereas
Smilodon was a placental mammal, but Thy-
lacosmilus had the same long upper canines
and, like Smilodon, had a protective bone
ﬂange on the lower jaw. The similarity of
these two mammals is one of many examples
of parallel or convergent evolution, in which particular contexts or environments
lead to the evolution of very similar structures in different species (Gould, 1980).
The role of context is also important in the similarity of a less biological kind
of organism: the word. Words that occur in similar contexts tend to have similar
meanings. This link between similarity in how words are distributed and similarity
in what they mean is called the distributional hypothesis. The hypothesis was
ﬁrst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth
(1957), who noticed that words which are synonyms (like oculist and eye-doctor)
tended to occur in the same environment (e.g., near words like eye or examined)
with the amount of meaning difference between two words “corresponding roughly
to the amount of difference in their environments” (Harris, 1954, 157).
In this chapter we introduce a model known as vector semantics, which instan-
tiates this linguistic hypothesis by learning representations of the meaning of words
directly from their distributions in texts. These representations are used in every
natural language processing application that makes use of meaning. These word
representations are also the ﬁrst example we will see in the book of representation
learning, automatically learning useful representations of the input text. Finding
such unsupervised ways to learn representations of the input, instead of creating
representations by hand via feature engineering, is an important focus of recent
NLP research (Bengio et al., 2013).
We’ll begin, however, by introducing some basic principles of word meaning,
which will motivate the vector semantic models of this chapter as well as extensions
that we’ll return to in Appendix C, Chapter 19, and Chapter 18.

distributional
hypothesis

vector
semantics

representation
learning

102 CHA PTER 6

• V ECTOR S EMAN T IC S

6.1 Lexical Semantics

How should we represent the meaning of a word? In the N-gram models we saw in
Chapter 3, and in many traditional NLP applications, our only representation of a
word is as a string of letters, or perhaps as an index in a vocabulary list. This repre-
sentation is not that different from a tradition in philosophy, perhaps you’ve seen it
in introductory logic classes, in which the meaning of words is often represented by
just spelling the word with small capital letters; representing the meaning of “dog”
as DOG, and “cat” as CAT).
Representing the meaning of a word by capitalizing it is a pretty unsatisfactory
model. You might have seen the old philosophy joke:
Q: What’s the meaning of life?

A: L I FE

Surely we can do better than this! After all, we’ll want a model of word meaning
to do all sorts of things for us. It should tell us that some words have similar mean-
ings (cat is similar to dog), other words are antonyms (cold is the opposite of hot). It
should know that some words have positive connotations (happy) while others have
negative connotations (sad). It should represent the fact that the meanings of buy,
sell, and pay offer differing perspectives on the same underlying purchasing event
(If I buy something from you, you’ve probably sold it to me, and I likely paid you).
More generally, a model of word meaning should allow us to draw useful infer-
ences that will help us solve meaning-related tasks like question-answering, sum-
marization, paraphrase or plagiarism detection, and dialogue.
In this section we summarize some of these desiderata, drawing on results in the
linguistic study of word meaning, which is called lexical semantics.

Lemmas and Senses Let’s start by looking at how one word (we’ll choose mouse)
might be deﬁned in a dictionary: 1

mouse (N)
1.
any of numerous small rodents...
2.
a hand-operated device that controls a cursor...

Here the form mouse is the lemma, also called the citation form. The form
mouse would also be the lemma for the word mice; dictionaries don’t have separate
deﬁnitions for inﬂected forms like mice. Similarly sing is the lemma for sing, sang,
sung. In many languages the inﬁnitive form is used as the lemma for the verb, so
Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The speciﬁc forms
sung or carpets or sing or duermes are called wordforms.
As the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. We call each
of these aspects of the meaning of mouse a word sense. The fact that lemmas can be
homonymous (have multiple senses) can make interpretation difﬁcult (is someone
who types “mouse info” to a search engine looking for a pet or a tool?). Appendix C
will discuss the problem of homonymy, and introduce word sense disambiguation,
the task of determining which sense of a word is being used in a particular context.

Relationships between words or senses One important component of word mean-

ing is the relationship between word senses. For example when one word has a sense

1 This example shortened from the online dictionary WordNet, discussed in Appendix C.

lexical
semantics

lemma
citation form

wordform

6 .1

• L EX ICA L S EMAN T IC S

103

synonym

whose meaning is identical to a sense of another word, or nearly identical, we say
the two senses of those two words are synonyms. Synonyms include such pairs as

couch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile

propositional
meaning

principle of
contrast

antonym

reversives

similarity

A more formal deﬁnition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable one for the other in any sentence
without changing the truth conditions of the sentence, the situations in which the
sentence would be true. We often say in this case that the two words have the same

propositional meaning.

While substitutions between some pairs of words like car / automobile or water /
H2O are truth preserving, the words are still not identical in meaning. Indeed, proba-
bly no two words are absolutely identical in meaning. One of the fundamental tenets
of semantics, called the principle of contrast (Br ´eal 1897, ?, Clark 1987), is the as-
sumption that a difference in linguistic form is always associated with at least some
difference in meaning. For example, the word H2O is used in scientiﬁc contexts and
would be inappropriate in a hiking guide—water would be more appropriate— and
this difference in genre is part of the meaning of the word. In practice, the word
synonym is therefore commonly used to describe a relationship of approximate or
rough synonymy.
Where synonyms are words with identical or similar meanings, Antonyms are
words with an opposite meaning, like:

long/short big/little fast/slow cold/hot dark/light
rise/fall
up/down in/out

Two senses can be antonyms if they deﬁne a binary opposition or are at opposite
ends of some scale. This is the case for long/short, fast/slow, or big/little, which are
at opposite ends of the length or size scale. Another group of antonyms, reversives,
describe change or movement in opposite directions, such as rise/fall or up/down.
Antonyms thus differ completely with respect to one aspect of their meaning—
their position on a scale or their direction—but are otherwise very similar, sharing
almost all other aspects of meaning. Thus, automatically distinguishing synonyms
from antonyms can be difﬁcult.

Word Similarity: While words don’t have many synonyms, most words do have
lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly
similar words. In moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). Dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.
The notion of word similarity is very useful in larger semantic tasks. For exam-
ple knowing how similar two words are is helpful if we are trying to decide if two
phrases or sentences mean similar things. Phrase or sentence similarity is useful in
such natural language understanding tasks as question answering, paraphrasing, and
summarization.
One way of getting values for word similarity is to ask humans to judge how
similar one word is to another. A number of datasets have resulted from such ex-
periments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on
a scale from 0 to 10, like the examples below, which range from near-synonyms
(vanish, disappear) to pairs that scarcely seem to have anything in common (hole,
agreement):

104 CHA PTER 6

• V ECTOR S EMAN T IC S

vanish disappear
9.8
behave obey
7.3
belief
impression 5.95
muscle bone
3.65
modest ﬂexible
0.98
hole
agreement 0.3

relatedness
association

semantic ﬁeld

topic models

semantic frame

hyponym

hypernym

superordinate

Word Relatedness: The meaning of two words can be related in ways others than
similarity. One such class of connections is called word relatedness (Budanitsky
and Hirst, 2006), also traditionally called word association in psychology.
Consider the meanings of the words coffee and cup; Coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is an
manufactured object with a particular shape).
But coffee and cup are clearly related; they are associated in the world by com-
monly co-participating in a shared event (the event of drinking coffee out of a cup).
Similarly the nouns scalpel and surgeon are not similar but are related eventively (a
surgeon tends to make use of a scalpel).
One common kind of relatedness between words is if they belong to the same
semantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic
domain and bear structured relations with each other.
For example, words might be related by being in the semantic ﬁeld of hospitals
(surgeon, scalpel, nurse, anaesthetic, hospital), restaurants (waiter, menu, plate,
food, chef), or houses (door, roof, kitchen, family, bed).
Semantic ﬁelds are also related to topic models, like Latent Dirichlet Alloca-
tion, LDA, which apply unsupervised learning on large sets of texts to induce sets
of associated words from text. Semantic ﬁelds and topic models are a very useful
tool for discovering topical structure in documents.
Semantic Frames and Roles: Closely related to semantic ﬁelds is the idea of a
semantic frame. A semantic frame is a set of words that denote perspectives or
participants in a particular type of event. A commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service
is performed. This event can be encoded lexically by using verbs like buy (the
event from the perspective of the buyer) sell (from the perspective of the seller), pay
(focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles
(like buyer, seller, goods, money), and words in a sentence can take on these roles.
Knowing that buy and sell have this relation makes it possible for a system to
know that a sentence like Sam bought the book from Ling could be paraphrased as
Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and
Ling the seller. Being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.
Taxonomic Relations: Another way word senses can be related is taxonomically.
A word (or sense) is a hyponym of another word or sense if the ﬁrst is more speciﬁc,
denoting a subclass of the other. For example, car is a hyponym of vehicle; dog is
a hyponym of animal, and mango is a hyponym of fruit. Conversely, we say that
vehicle is a hypernym of car, and animal is a hypernym of dog. It is unfortunate that
the two words (hypernym and hyponym) are very similar and hence easily confused;
for this reason, the word superordinate is often used instead of hypernym.
furniture mammal
mango chair
dog

Superordinate vehicle fruit
Subordinate

car

IS-A

connotations

sentiment

6 .1

• L EX ICA L S EMAN T IC S

105

We can deﬁne hypernymy more formally by saying that the class denoted by the
superordinate extensionally includes the class denoted by the hyponym. Thus, the
class of animals includes as members all dogs, and the class of moving actions in-
cludes all walking actions. Hypernymy can also be deﬁned in terms of entailment.
Under this deﬁnition, a sense A is a hyponym of a sense B if everything that is A is
also B, and hence being an A entails being a B, or ∀x A(x) ⇒ B(x). Hyponymy/hy-
pernymy is usually a transitive relation; if A is a hyponym of B and B is a hyponym
of C, then A is a hyponym of C. Another name for the hypernym/hyponym structure
is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A.
Hypernymy is useful for tasks like textual entailment or question answering;
knowing that leukemia is a type of cancer, for example, would certainly be useful in
answering questions about leukemia.

Connotation: Finally, words have affective meanings or connotations. The word
connotation has different meanings in different ﬁelds, but here we use it to mean
the aspects of a word’s meaning that are related to a writer or reader’s emotions,
sentiment, opinions, or evaluations. For example some words have positive conno-
tations (happy) while others have negative connotations (sad). Some words describe
positive evaluation (great, love) and others negative evaluation (terrible, hate). Pos-
itive or negative evaluation expressed through language is called sentiment, as we
saw in Chapter 4, and word sentiment plays a role in important tasks like sentiment
analysis, stance detection, and many aspects of natural language processing to the
language of politics and consumer reviews.
Early work on affective meaning (Osgood et al., 1957) found that words varied
along three important dimensions of affective meaning. These are now generally
called valence, arousal, and dominance, deﬁned as follows:

valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus

Thus words like happy or satisﬁed are high on valence, while unhappy or an-
noyed are low on valence. Excited or frenzied are high on arousal, while relaxed
or calm are low on arousal. Important or controlling are high on dominance, while
awed or inﬂuenced are low on dominance. Each word is thus represented by three
numbers, corresponding to its value on each of the three dimensions, like the exam-
ples below:

Valence Arousal Dominance
courageous 8.05
5.5
7.38
music
7.67
5.57
6.5
heartbreak 2.45
5.65
3.58
cub
6.71
3.95
4.24
life
6.68
5.59
5.89

Osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word’s
rating on the three scales. This revolutionary idea that word meaning word could
be represented as a point in space (e.g., that part of the meaning of heartbreak can
be represented as the point [2.45, 5.65, 3.58]) was the ﬁrst expression of the vector
semantics models that we introduce next.

106 CHA PTER 6

• V ECTOR S EMAN T IC S

6.2 Vector Semantics

vector
semantics

How can we build a computational model that successfully deals with the different
aspects of word meaning we saw in the previous section (word senses, word simi-
larity and relatedness, lexical ﬁelds and frames, connotation)?
A perfect model that completely deals with each of these aspects of word mean-
ing turns out to be elusive. But the current best model, called vector semantics,
draws its inspiration from linguistic and philosophical work of the 1950’s.
During that period, the philosopher Ludwig Wittgenstein, skeptical of the possi-
bility of building a completely formal theory of meaning deﬁnitions for each word,
suggested instead that “the meaning of a word is its use in the language” (Wittgen-
stein, 1953, PI 43). That is, instead of using some logical language to deﬁne each
word, we should deﬁne words by some representation of how the word was used by
actual people in speaking and understanding.
Linguists of the period like Joos (1950), Harris (1954), and Firth (1957) (the
linguistic distributionalists), came up with a speciﬁc idea for realizing Wittgenstein’s
intuition: deﬁne a word by the environment or distribution it occurs in in language
use. A word’s distribution is the set of contexts in which it occurs, the neighboring
words or grammatical environments. The idea is that two words that occur in very
similar distributions (that occur together with very similar words) are likely to have
the same meaning.
Let’s see an example illustrating this distributionalist approach. Suppose you
didn’t know what the Cantonese word ongchoi meant, but you do see it in the fol-
lowing sentences or contexts:
(6.1) Ongchoi is delicious sauteed with garlic.
(6.2) Ongchoi is superb over rice.
(6.3) ...ongchoi leaves with salty sauces...
And furthermore let’s suppose that you had seen many of these context words
occurring in contexts like:
(6.4) ...spinach sauteed with garlic over rice...
(6.5) ...chard stems and leaves are delicious...
(6.6) ...collard greens and other salty leafy greens
The fact that ongchoi occurs with words like rice and garlic and delicious and
salty, as do words like spinach, chard, and collard greens might suggest to the reader
that ongchoi is a leafy green similar to these other leafy greens.2
We can do the same thing computationally by just counting words in the context
of ongchoi; we’ll tend to see words like sauteed and eaten and garlic. The fact that
these words and other similar context words also occur around the word spinach or
collard greens can help us discover the similarity between these words and ongchoi.
Vector semantics thus combines two intuitions:
the distributionalist intuition
(deﬁning a word by counting what other words occur in its environment), and the
vector intuition of Osgood et al. (1957) we saw in the last section on connotation:
deﬁning the meaning of a word w as a vector, a list of numbers, a point in N-
dimensional space. There are various versions of vector semantics, each deﬁning
the numbers in the vector somewhat differently, but in each case the numbers are
based in some way on counts of neighboring words.

2

It’s in fact Ipomoea aquatica, a relative of morning glory sometimes called water spinach in English.

6 .2

• V EC TOR S EMAN T IC S

107

embeddings

Figure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. The original 60-
dimensional embeddings were trained for a sentiment analysis task. Simpliﬁed from Li et al.
(2015).

The idea of vector semantics is thus to represent a word as a point in some multi-
dimensional semantic space. Vectors for representing words are generally called
embeddings, because the word is embedded in a particular vector space. Fig. 6.1
displays a visualization of embeddings that were learned for a sentiment analysis
task, showing the location of some selected words projected down from the original
60-dimensional space into a two dimensional space.
Notice that positive and negative words seem to be located in distinct portions of
the space (and different also from the neutral function words). This suggests one of
the great advantages of vector semantics: it offers a ﬁne-grained model of meaning
that lets us also implement word similarity (and phrase similarity). For example,
the sentiment analysis classiﬁer we saw in Chapter 4 only works if enough of the
important sentimental words that appear in the test set also appeared in the training
set. But if words were represented as embeddings, we could assign sentiment as
long as words with similar meanings as the test set words occurred in the training
set. Vector semantic models are also extremely practical because they can be learned
automatically from text without any complex labeling or supervision.
As a result of these advantages, vector models of meaning are now the standard
way to represent the meaning of words in NLP. In this chapter we’ll introduce the
two most commonly used models. . First is the tf-idf model, often used a a baseline,
in which the meaning of a word is deﬁned by a simple function of the counts of
nearby words. We will see that this method results in very long vectors that are
sparse, i.e. contain mostly zeros (since most words simply never occur in the context
of others).
Then we’ll introduce the word2vec model, one of a family of models that are
ways of constructing short, dense vectors that have useful semantic properties.
We’ll also introduce the cosine, the standard way to use embeddings (vectors)
to compute functions like semantic similarity, the similarity between two words,
two sentences, or two documents, an important tool in practical applications like
question answering, summarization, or automatic essay grading.

108 CHA PTER 6

• V ECTOR S EMAN T IC S

6.3 Words and Vectors

Vector or distributional models of meaning are generally based on a co-occurrence
matrix, a way of representing how often words co-occur. This matrix can be con-
structed in various ways; let’s s begin by looking at one such co-occurrence matrix,
a term-document matrix.

6.3.1 Vectors and documents

term-document
matrix

In a term-document matrix, each row represents a word in the vocabulary and each
column represents a document from some collection of documents. Fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by Shakespeare. Each cell in this matrix represents the number of times
a particular word (deﬁned by the row) occurs in a particular document (deﬁned by
the column). Thus fool appeared 58 times in Twelfth Night.

As You Like It

Twelfth Night

Julius Caesar

Henry V

battle
good
fool
wit

1
0
7
13
114
80
62
89
36
58
1
4
20
15
2
3
Figure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.

vector space
model

vector

vector space
dimension

The term-document matrix of Fig. 6.2 was ﬁrst deﬁned as part of the vector
space model of information retrieval (Salton, 1971). In this model, a document is
represented as a count vector, a column in Fig. 6.3.
To review some basic linear algebra, a vector is, at heart, just a list or array
of numbers. So As You Like It is represented as the list [1,114,36,20] and Julius
Caesar is represented as the list [7,62,1,2]. A vector space is a collection of vectors,
characterized by their dimension.
In the example in Fig. 6.3, the vectors are of
dimension 4, just so they ﬁt on the page; in real term-document matrices, the vectors
representing each document would have dimensionality |V |, the vocabulary size.
The ordering of the numbers in a vector space is not arbitrary; each position
indicates a meaningful dimension on which the documents can vary. Thus the ﬁrst
dimension for both these vectors corresponds to the number of times the word battle
occurs, and we can compare each dimension, noting for example that the vectors for
As You Like It and Twelfth Night have similar values (1 and 0, respectively) for the
ﬁrst dimension.

As You Like It

Twelfth Night

Julius Caesar

Henry V

battle
good
fool
wit

1
0
7
13
114
80
62
89
36
58
1
4
20
15
2
3
Figure 6.3 The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each document is represented as a column vector of length four.

We can think of the vector for a document as identifying a point in |V |-dimensional
space; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-
dimensional spaces are hard to draw in textbooks, Fig. 6.4 shows a visualization in

6 .3

• WORD S AND V EC TOR S

109

two dimensions; we’ve arbitrarily chosen the dimensions corresponding to the words
battle and fool.

Figure 6.4 A spatial visualization of the document vectors for the four Shakespeare play
documents, showing just two of the dimensions, corresponding to the words battle and fool.
The comedies have high values for the fool dimension and low values for the battle dimension.

Term-document matrices were originally deﬁned as a means of ﬁnding similar
documents for the task of document information retrieval. Two documents that are
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. The vectors for the comedies As You
Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other
(more fools and wit than battles) than they do like Julius Caesar [7,62,1,2] or Henry
V [13,89,4,3]. We can see the intuition with the raw numbers; in the ﬁrst dimension
(battle) the comedies have low numbers and the others have high numbers, and we
can see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition
more formally.
A real term-document matrix, of course, wouldn’t just have 4 rows and columns,
let alone 2. More generally, the term-document matrix X has |V | rows (one for each
word type in the vocabulary) and D columns (one for each document in the collec-
tion); as we’ll see, vocabulary sizes are generally at least in the tens of thousands,
and the number of documents can be enormous (think about all the pages on the
web).
Information retrieval (IR) is the task of ﬁnding the document d from the D
documents in some collection that best matches a query q. For IR we’ll therefore also
represent a query by a vector, also of length |V |, and we’ll need a way to compare
two vectors to ﬁnd how similar they are. (Doing IR will also require efﬁcient ways
to store and manipulate these vectors, which is accomplished by making use of the
convenient fact that these vectors are sparse, i.e., mostly zeros).
Later in the chapter we’ll introduce some of the components of this vector com-
parison process: the tf-idf term weighting, and the cosine similarity metric.

information
retrieval

6.3.2 Words as vectors

We’ve seen that documents can be represented as vectors in a vector space. But
vector semantics can also be used to represent the meaning of words, by associating
each word with a vector.
The word vector is now a row vector rather than a column vector, and hence the
dimensions of the vector are different. The four dimensions of the vector for fool,

row vector

110 CHA PTER 6

• V ECTOR S EMAN T IC S

term-term
matrix
word-word
matrix

[36,58,1,4], correspond to the four Shakespeare plays. The same four dimensions are
used to form the vectors for the other 3 words: wit, [20, 15, 2, 3]; battle, [1,0,7,13];
and good [114,80,62,89]. Each entry in the vector thus represents the counts of the
word’s occurrence in the document corresponding to that dimension.
For documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. This same principle applies to words:
similar words have similar vectors because they tend to occur in similar documents.
The term-document matrix thus lets us represent the meaning of a word by the doc-
uments it tends to occur in.
However, it is most common to use a different kind of context for the dimensions
of a word’s vector representation. Rather than the term-document matrix we use the

term-term matrix, more commonly called the word-word matrix or the term-

context matrix, in which the columns are labeled by words rather than documents.
This matrix is thus of dimensionality |V | × |V | and each cell records the number of
times the row (target) word and the column (context) word co-occur in some context
in some training corpus. The context could be the document, in which case the cell
represents the number of times the two words appear in the same document. It is
most common, however, to use smaller contexts, generally a window around the
word, for example of 4 words to the left and 4 words to the right, in which case
the cell represents the number of times (in some training corpus) the column word
occurs in such a ±4 word window around the row word.
For example here are 7-word windows surrounding four sample words from the
Brown corpus (just one example of each word):

sugar, a sliced lemon, a tablespoonful of apricot
jam, a pinch each of,
their enjoyment. Cautiously she sampled her ﬁrst pineapple
and another fruit whose taste she likened
well suited to programming on the digital computer.
In ﬁnding the optimal R-stage policy from
for the purpose of gathering data and information necessary for the study authorized in the

For each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. Fig. 6.5 shows a selection from the word-word
co-occurrence matrix computed from the Brown corpus for these four words.

data

pinch

result

aardvark

computer

apricot
pineapple
digital
information

...
0
...
0
0
1
0
1
0
...
0
0
1
0
1
0
...
2
1
0
1
0
0
...
1
6
0
4
0
Figure 6.5 Co-occurrence vectors for four words, computed from the Brown corpus, show-
ing only six of the dimensions (hand-picked for pedagogical purposes). The vector for the
word digital is outlined in red. Note that a real vector would have vastly more dimensions
and thus be much sparser.

sugar

...

Note in Fig. 6.5 that the two words apricot and pineapple are more similar to
each other (both pinch and sugar tend to occur in their window) than they are to
other words like digital; conversely, digital and information are more similar to each
other than, say, to apricot. Fig. 6.6 shows a spatial visualization.
Note that |V |, the length of the vector, is generally the size of the vocabulary,
usually between 10,000 and 50,000 words (using the most frequent words in the
training corpus; keeping words after about the most frequent 50,000 or so is gener-
ally not helpful). But of course since most of these numbers are zero these are sparse
vector representations, and there are efﬁcient algorithms for storing and computing
with sparse matrices.

6 .4

• CO S IN E FOR M EA SUR ING S IM I LAR I TY

111

Figure 6.6 A spatial visualization of word vectors for digital and information, showing just
two of the dimensions, corresponding to the words data and result.

Now that we have some intuitions, let’s move on to examine the details of com-
puting word similarity. Afterwards we’ll discuss the tf-idf method of weighting
cells.

6.4 Cosine for measuring similarity

dot product
inner product

To deﬁne similarity between two target words v and w, we need a measure for taking
two such vectors and giving a measure of vector similarity. By far the most common
similarity metric is the cosine of the angle between the vectors.
The cosine—like most measures for vector similarity used in NLP—is based on
the dot product operator from linear algebra, also called the inner product:

dot-product((cid:126)v, (cid:126)w) = (cid:126)v · (cid:126)w =

N(cid:88)i=1

viwi = v1w1 + v2w2 + ... + vN wN

(6.7)

As we will see, most metrics for similarity between vectors are based on the dot
product. The dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. Alternatively,
vectors that have zeros in different dimensions—orthogonal vectors—will have a
dot product of 0, representing their strong dissimilarity.
This raw dot-product, however, has a problem as a similarity metric: it favors
long vectors. The vector length is deﬁned as

vector length

|(cid:126)v| = (cid:118)(cid:117)(cid:117)(cid:116)

N(cid:88)i=1

v2

i

(6.8)

The dot product is higher if a vector is longer, with higher values in each dimension.
More frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. The raw dot product
thus will be higher for frequent words. But this is a problem; we’d like a similarity
metric that tells us how similar two words are regardless of their frequency.
The simplest way to modify the dot product to normalize for the vector length is
to divide the dot product by the lengths of each of the two vectors. This normalized
dot product turns out to be the same as the cosine of the angle between the two

112 CHA PTER 6

• V ECTOR S EMAN T IC S

vectors, following from the deﬁnition of the dot product between two vectors (cid:126)a and
(cid:126)b:

(cid:126)a ·(cid:126)b = |(cid:126)a||(cid:126)b| cos θ
(cid:126)a ·(cid:126)b
= cos θ

|(cid:126)a||(cid:126)b|

(6.9)

The cosine similarity metric between two vectors (cid:126)v and (cid:126)w thus can be computed as:

cosine

unit vector

cosine((cid:126)v, (cid:126)w) =

(cid:126)v · (cid:126)w

|(cid:126)v||(cid:126)w|

=

(6.10)

w2

i

N(cid:88)i=1
N(cid:88)i=1

v2

viwi

i (cid:118)(cid:117)(cid:117)(cid:116)

N(cid:88)i=1

(cid:118)(cid:117)(cid:117)(cid:116)

For some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. Thus we could compute a unit vector from (cid:126)a by
dividing it by |(cid:126)a|. For unit vectors, the dot product is the same as the cosine.
The cosine value ranges from 1 for vectors pointing in the same direction, through
0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.
But raw frequency values are non-negative, so the cosine for these vectors ranges
from 0–1.
Let’s see how the cosine computes which of the words apricot or digital is closer
in meaning to information, just using raw counts from the following simpliﬁed table:
large data computer

apricot
digital
information

2
0
1

0
1
6

0
2
1

cos(apricot, information) =

2 + 0 + 0
2
√4 + 0 + 0√1 + 36 + 1
2√38
0 + 6 + 2
8
√0 + 1 + 4√1 + 36 + 1
√38√5
The model decides that information is closer to digital than it is to apricot, a
result that seems sensible. Fig. 6.7 shows a visualization.

cos(digital, information) =

= .16

= .58

(6.11)

=

=

6.5 TF-IDF: Weighing terms in the vector

The co-occurrence matrix in Fig. 6.5 represented each cell by the raw frequency of
the co-occurrence of two words.
It turns out, however, that simple frequency isn’t the best measure of association
between words. One problem is that raw frequency is very skewed and not very
discriminative. If we want to know what kinds of contexts are shared by apricot
and pineapple but not by digital and information, we’re not going to get good dis-
crimination from words like the, it, or they, which occur frequently with all sorts of
words and aren’t informative about any particular word. We saw this also in Fig. 6.3
for the Shakespeare corpus; the dimension for the word good is not very discrimina-
tive between plays; good is simply a frequent word and has roughly equivalent high
frequencies in each of the plays.

6 .5

• TF - IDF : W E IGH ING TERM S IN THE V ECTOR

113

Figure 6.7 A graphical demonstration of cosine similarity, showing vectors for three words
(apricot, digital, and information) in the two dimensional space deﬁned by counts of the
words data and large in the neighborhood. Note that the angle between digital and informa-
tion is smaller than the angle between apricot and information. When two vectors are more
similar, the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the
angle between two vectors is smallest (0◦ ); the cosine of all other angles is less than 1.

It’s a bit of a paradox. Word that occur nearby frequently (maybe sugar appears
often in our corpus near apricot) are more important than words that only appear
once or twice. Yet words that are too frequent—ubiquitous, like the or good— are
unimportant. How can we balance these two conﬂicting constraints?
The tf-idf algorithm (the ‘-’ here is a hyphen, not a minus sign) algorithm is the
product of two terms, each term capturing one of these two intuitions:

if count(t , d ) > 0
otherwise

1. The ﬁrst is the term frequency (Luhn, 1957): the frequency of the word in the
document. Normally we want to downweight the raw frequency a bit, since
a word appearing 100 times in a document doesn’t make that word 100 times
more likely to be relevant to the meaning of the document. So we generally
use the log10 of the frequency, resulting in the following deﬁnition for the term
frequency weight:
tft ,d = (cid:26) 1 + log10 count(t , d )
0
Thus terms which occur 10 times in a document would have a tf=2, 100 times
in a document tf=3, 1000 times tf=4, and so on.
2. The second factor is used to give a higher weight to words that occur only
in a few documents. Terms that are limited to a few documents are useful
for discriminating those documents from the rest of the collection; terms that
occur frequently across the entire collection aren’t as helpful. The document
frequency dft of a term t is simply the number of documents it occurs in. By
contrast, the collection frequency of a term is the total number of times the
word appears in the whole collection in any document. Consider in the col-
lection Shakespeare’s 37 plays the two words Romeo and action. The words
have identical collection frequencies of 113 (they both occur 113 times in all
the plays) but very different document frequencies, since Romeo only occurs
in a single play. If our goal is ﬁnd documents about the romantic tribulations
of Romeo, the word Romeo should be highly weighted:

term frequency

document
frequency

114 CHA PTER 6

• V ECTOR S EMAN T IC S

idf

tf-idf

Collection Frequency Document Frequency

Romeo 113
1
action
113
31
We assign importance to these more discriminative words like Romeo via
the inverse document frequency or idf term weight (Sparck Jones, 1972).
The idf is deﬁned using the fraction N /dft , where N is the total number of
documents in the collection, and dft is the number of documents in which
term t occurs. The fewer documents in which a term occurs, the higher this
weight. The lowest weight of 1 is assigned to terms that occur in all the
documents.
It’s usually clear what counts as a document:
in Shakespeare
we would use a play; when processing a collection of encyclopedia articles
like Wikipedia, the document is a Wikipedia page; in processing newspaper
articles, the document is a single article. Occasionally your corpus might
not have appropriate document divisions and you might need to break up the
corpus into documents yourself for the purposes of computing idf.
Because of the large number of documents in many collections, this mea-
sure is usually squashed with a log function. The resulting deﬁnition for in-
verse document frequency (idf) is thus
idft = log10 (cid:18) N
dft (cid:19)
Here are some idf values for some words in the Shakespeare corpus, ranging
from extremely informative words which occur in only one play like Romeo, to
those that occur in a few like salad or Falstaff, to those which are very common like
fool or so common as to be completely non-discriminative since they occur in all 37
plays like good or sweet.3

(6.12)

Word

df

idf

Romeo
1
1.57
salad
2
1.27
Falstaff
4
0.967
forest
12
0.489
battle
21
0.074
fool
36
0.012
good
37
0
sweet
37
0
The tf-idf weighting of the value for word t in document d , wt ,d thus combines
term frequency with idf:

wt ,d = tft ,d × idft

(6.13)

Fig. 6.8 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2.
Note that the tf-idf values for the dimension corresponding to the word good have
now all become 0; since this word appears in every document, the tf-idf algorithm
leads it to be ignored in any comparison of the plays. Similarly, the word fool, which
appears in 36 out of the 37 plays, has a much lower weight.
The tf-idf weighting is by far the dominant way of weighting co-occurrence ma-
trices in information retrieval, but also plays a role in many other aspects of natural

3 Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

6 .6

• A P PL ICAT ION S O F TH E T F - ID F V EC TOR MODE L

115

As You Like It

Twelfth Night

Julius Caesar

Henry V

battle
good
fool
wit

0.074
0
0.22
0.28
0
0
0
0
0.019
0.021
0.0036
0.0083
0.049
0.044
0.018
0.022
Figure 6.8 A tf-idf weighted term-document matrix for four words in four Shakespeare
plays, using the counts in Fig. 6.2. Note that the idf weighting has eliminated the importance
of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word
fool.

language processing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll
look at other weightings like PPMI (Positive Pointwise Mutual Information) in Sec-
tion 6.7.

6.6 Applications of the tf-idf vector model

In summary, the vector semantics model we’ve described so far represents a target
word as a vector with dimensions corresponding to all the words in the vocabulary
(length |V |, with vocabularies of 20,000 to 50,000), which is also sparse (most values
are zero). The values in each dimension are the frequency with which the target
word co-occurs with each neighboring context word, weighted by tf-idf. The model
computes the similarity between two words x and y by taking the cosine of their
tf-idf vectors; high cosine, high similarity. This entire model is sometimes referred
to for short as the tf-idf model, after the weighting function.
One common use for a tf-idf model is to compute word similarity, a useful tool
for tasks like ﬁnding word paraphrases, tracking changes in word meaning, or au-
tomatically discovering meanings of words in different corpora. For example, we
can ﬁnd the 10 most similar words to any target word w by computing the cosines
between w and each of the V − 1 other words, sorting, and looking at the top 10.
The tf-idf vector model can also be used to decide if two documents are similar.
We represent a document by taking the vectors of all the words in the document, and
computing the centroid of all those vectors. The centroid is the multidimensional
version of the mean; the centroid of a set of vectors is a single vector that has the
minimum sum of squared distances to each of the vectors in the set. Given k word
vectors w1 , w2 , ..., wk , the centroid document vector d is:

d =

w1 + w2 + ... + wk

k

(6.14)

Given two documents, we can then compute their document vectors d1 and d2 ,
and estimate the similarity between the two documents by cos(d1 , d2 ).
Document similarity is also useful for all sorts of applications; information re-
trieval, plagiarism detection, news recommender systems, and even for digital hu-
manities tasks like comparing different versions of a text to see which are similar to
each other.

centroid

document
vector

116 CHA PTER 6

• V ECTOR S EMAN T IC S

6.7 Optional: Pointwise Mutual Information (PMI)

An alternative weighting function to tf-idf is called PPMI (positive pointwise mutual
information). PPMI draws on the intuition that best way to weigh the association
between two words is to ask how much more the two words co-occur in our corpus
than we would have a priori expected them to appear by chance.
Pointwise mutual information (Fano, 1961)4 is one of the most important con-
cepts in NLP. It is a measure of how often two events x and y occur, compared with
what we would expect if they were independent:

pointwise
mutual
information

P(x, y)
P(x)P(y)
The pointwise mutual information between a target word w and a context word
c (Church and Hanks 1989, Church and Hanks 1990) is then deﬁned as:

I (x, y) = log2

(6.16)

PMI(w, c) = log2

P(w, c)
P(w)P(c)

(6.17)

The numerator tells us how often we observed the two words together (assuming
we compute probability by using the MLE). The denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the probability of two independent events both occurring is just
the product of the probabilities of the two events. Thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. PMI is
a useful tool whenever we need to ﬁnd words that are strongly associated.
PMI values range from negative to positive inﬁnity. But negative PMI values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. To distinguish whether two
words whose individual probability is each 10−6 occur together more often than
chance, we would need to be certain that the probability of the two occurring to-
gether is signiﬁcantly different than 10−12 , and this kind of granularity would require
an enormous corpus. Furthermore it’s not clear whether it’s even possible to evalu-
ate such scores of ‘unrelatedness’ with human judgments. For this reason it is more
common to use Positive PMI (called PPMI) which replaces all negative PMI values
with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5 :

PPMI(w, c) = max(log2

P(w, c)
P(w)P(c)

, 0)

(6.18)

More formally, let’s assume we have a co-occurrence matrix F with W rows (words)
and C columns (contexts), where f i j gives the number of times word wi occurs in

PPMI

4 Pointwise mutual information is based on the mutual information between two random variables X
and Y , which is deﬁned as:

(cid:88)

(cid:88)

P(x, y) log2

P(x, y)
P(x)P(y)

(6.15)

I (X , Y ) =

x

y

In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information
5 Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the
−∞ from log(0).

6 .7

• O PT IONAL : PO INTW I S E MU TUA L IN FORMAT ION (PM I )

117

context c j . This can be turned into a PPMI matrix where p pmii j gives the PPMI
value of word wi with context c j as follows:

(6.19)

(6.20)

(cid:80)W

(cid:80)C

f i j

i=1

j=1 f i j

pi j =

pi∗ =

(cid:80)C
(cid:80)W
(cid:80)C

i=1

j=1 f i j
j=1 f i j

(cid:80)W
(cid:80)W
(cid:80)C

i=1

i=1 f i j
j=1 f i j

p∗ j =

PPMIi j = max(log2

pi j

, 0)

pi∗ p∗ j
Thus for example we could compute PPMI(w=information,c=data), assuming we
pretended that Fig. 6.5 encompassed all the relevant word contexts/dimensions, as
follows:

= .316

P(w=information) =

P(w=information,c=data) =

6
19
11
19
7
P(c=data) =
= .368
19
ppmi(information,data) = log 2(.316/(.368 ∗ .579)) = .568
Fig. 6.9 shows the joint probabilities computed from the counts in Fig. 6.5, and
Fig. 6.10 shows the PPMI values.

= .579

apricot
pineapple
digital
information

computer

0
0
0.11
0.05

p(w,context)
pinch

result

sugar

0.05
0.05
0
0

0
0
0.05
0.21

0.05
0.05
0
0

data

0
0
0.05
.32

p(w)
p(w)

0.11
0.11
0.21
0.58

p(context)

0.16
0.37
0.11
0.26
0.11
Figure 6.9 Replacing the counts in Fig. 6.5 with joint probabilities, showing the marginals
around the outside.

computer

data

pinch

result

sugar

apricot
pineapple
digital
information

0
0
2.25
0
2.25
0
0
2.25
0
2.25
1.66
0
0
0
0
0
0.57
0
0.47
0
Figure 6.10 The PPMI matrix showing the association between words and context words,
computed from the counts in Fig. 6.5 again showing ﬁve dimensions. Note that the 0
ppmi values are ones that had a negative pmi; for example pmi(information,computer) =
log 2(.05/(.16 ∗ .58)) = −0.618, meaning that information and computer co-occur in this
mini-corpus slightly less often than we would expect by chance, and with ppmi we re-
place negative values by zero. Many of the zero ppmi values had a pmi of −∞, like
pmi(apricot,computer) = log 2(0/(0.16 ∗ 0.11)) = log 2(0) = −∞.

PMI has the problem of being biased toward infrequent events; very rare words
tend to have very high PMI values. One way to reduce this bias toward low frequency
events is to slightly change the computation for P(c), using a different function Pα (c)
that raises contexts to the power of α :

PPMIα (w, c) = max(log2

P(w, c)

P(w)Pα (c)

, 0)

(6.21)

118 CHA PTER 6

• V ECTOR S EMAN T IC S

(6.22)

Pα (c) =

count (c)α
(cid:80)c count (c)α
Levy et al. (2015) found that a setting of α = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in Eq. 6.31). This works because raising the probability to
α = 0.75 increases the probability assigned to rare contexts, and hence lowers their
PMI (Pα (c) > P(c) when c is rare).
Another possible solution is Laplace smoothing: Before computing PMI, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. The larger the k, the more the non-zero counts
are discounted.

computer

data

pinch

result

sugar

apricot
pineapple
digital
information

2
2
3
2
2
2
3
2
4
3
2
3
3
8
2
6
Figure 6.11 Laplace (add-2) smoothing of the counts in Fig. 6.5.

3
3
2
2

computer

data

pinch

result

sugar

apricot
pineapple
digital
information

0
0
0.56
0
0.56
0
0
0.56
0
0.56
0.62
0
0
0
0
0
0.58
0
0.37
0
Figure 6.12 The Add-2 Laplace smoothed PPMI matrix from the add-2 smoothing counts
in Fig. 6.11.

6.8 Word2vec

In the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to the words in the vocabulary, and whose values were tf-
idf or PPMI functions of the count of the word co-occurring with each neighboring
word. In this section we turn to an alternative method for representing a word: the
use of vectors that are short (of length perhaps 50-500) and dense (most values are
non-zero).
It turns out that dense vectors work better in every NLP task than sparse vec-
tors. While we don’t complete understand all the reasons for this, we have some
intuitions. First, dense vectors may be more successfully included as features in
machine learning systems; for example if we use 100-dimensional word embed-
dings as features, a classiﬁer can just learn 100 weights to represent a function of
word meaning; if we instead put in a 50,000 dimensional vector, a classiﬁer would
have to learn tens of thousands of weights for each of the sparse dimensions. Sec-
ond, because they contain fewer parameters than sparse vectors of explicit counts,
dense vectors may generalize better and help avoid overﬁtting. Finally, dense vec-
tors may do a better job of capturing synonymy than sparse vectors. For example,
car and automobile are synonyms; but in a typical sparse vector representation, the
car dimension and the automobile dimension are distinct dimensions. Because the

skip-gram
SGNS
word2vec

6 .8

• WORD2V EC

119

relationship between these two dimensions is not modeled, sparse vectors may fail
to capture the similarity between a word with car as a neighbor and a word with
automobile as a neighbor.
In this section we introduce one method for very dense, short vectors, skip-
gram with negative sampling, sometimes called SGNS. The skip-gram algorithm
is one of two algorithms in a software package called word2vec, and so sometimes
the algorithm is loosely referred to as word2vec (Mikolov et al. 2013, Mikolov
et al. 2013a). The word2vec methods are fast, efﬁcient to train, and easily avail-
able online with code and pretrained embeddings. We point to other embedding
methods, like the equally popular GloVe (Pennington et al., 2014), at the end of the
chapter.
The intuition of word2vec is that instead of counting how often each word w oc-
curs near, say, apricot, we’ll instead train a classiﬁer on a binary prediction task: “Is
word w likely to show up near apricot?” We don’t actually care about this prediction
task; instead we’ll take the learned classiﬁer weights as the word embeddings.
The revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classiﬁer; a word s that occurs near the target
word apricot acts as gold ‘correct answer’ to the question “Is word w likely to show
up near apricot?” This avoids the need for any sort of hand-labeled supervision
signal. This idea was ﬁrst proposed in the task of neural language modeling, when
Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model
(a neural network that learned to predict the next word from prior words) could just
use the next word in running text as its supervision signal, and could be used to learn
an embedding representation for each word as part of doing this prediction task.
We’ll see how to do neural networks in the next chapter, but word2vec is a
much simpler model than the neural network language model, in two ways. First,
word2vec simpliﬁes the task (making it binary classiﬁcation instead of word pre-
diction). Second, word2vec simpliﬁes the architecture (training a logistic regression
classiﬁer instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). The intuition of skip-gram is:

1. Treat the target word and a neighboring context word as positive examples.
2. Randomly sample other words in the lexicon to get negative samples
3. Use logistic regression to train a classiﬁer to distinguish those two cases
4. Use the regression weights as the embeddings

6.8.1 The classiﬁer

Let’s start by thinking about the classiﬁcation task, and then turn to how to train.
Imagine a sentence like the following, with a target word apricot and assume we’re
using a window of ±2 context words:

... lemon,

a [tablespoon of apricot jam,
c1
c2
t
c3

a] pinch ...
c4

Our goal is to train a classiﬁer such that, given a tuple (t , c) of a target word
t paired with a candidate context word c (for example (apricot, jam), or perhaps
(apricot, aardvark) it will return the probability that c is a real context word (true
for jam, false for aardvark):

P(+|t , c)

(6.23)

120 CHA PTER 6

• V ECTOR S EMAN T IC S

The probability that word c is not a real context word for t is just 1 minus
Eq. 6.23:

P(−|t , c) = 1 − P(+|t , c)
(6.24)
How does the classiﬁer compute the probability P? The intuition of the skip-
gram model is to base this probability on similarity: a word is likely to occur near
the target if its embedding is similar to the target embedding. How can we compute
similarity between embeddings? Recall that two vectors are similar if they have a
high dot product (cosine, the most popular similarity metric, is just a normalized dot
product). In other words:

Simil arit y(t , c) ≈ t · c
Of course, the dot product t · c is not a probability, it’s just a number ranging from
0 to ∞. (Recall, for that matter, that cosine isn’t a probability either). To turn the
dot product into a probability, we’ll use the logistic or sigmoid function σ (x), the
fundamental core of logistic regression:

(6.25)

1
1 + e−x
The probability that word c is a real context word for target word t is thus computed
as:

σ (x) =

(6.26)

(6.27)

P(+|t , c) =

1
1 + e−t ·c
The sigmoid function just returns a number between 0 and 1, so to make it a proba-
bility we’ll need to make sure that the total probability of the two possible events (c
being a context word, and c not being a context word) sum to 1.
The probability that word c is not a real context word for t is thus:
P(−|t , c) = 1 − P(+|t , c)
e−t ·c
1 + e−t ·c
Equation 6.27 give us the probability for one word, but we need to take account of
the multiple context words in the window. Skip-gram makes the strong but very
useful simplifying assumption that all context words are independent, allowing us to
just multiply their probabilities:

=

(6.28)

1
1 + e−t ·ci

log

1
1 + e−t ·ci

(6.29)

(6.30)

k(cid:89)i=1
k(cid:88)i=1

P(+|t , c1:k ) =

log P(+|t , c1:k ) =

In summary, skip-gram trains a probabilistic classiﬁer that, given a test target word
t and its context window of k words c1:k , assigns a probability based on how similar
this context window is to the target word. The probability is based on applying the
logistic (sigmoid) function to the dot product of the embeddings of the target word
with each context word. We could thus compute this probability if only we had
embeddings for each word target and context word in the vocabulary. Let’s now turn
to learning these embeddings (which is the real goal of training this classiﬁer in the
ﬁrst place).

6 .8

• WORD2V EC

121

6.8.2 Learning skip-gram embeddings

Word2vec learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don’t occur nearby.
Let’s start by considering a single piece of the training data, from the sentence
above:

... lemon,

a [tablespoon of apricot jam,
c1
c2
t
c3

a] pinch ...
c4

This example has a target word t (apricot), and 4 context words in the L = ±2
window, resulting in 4 positive training instances (on the left below):

positive examples +

negative examples -

t
c
t
c
t
c
apricot
tablespoon
apricot aardvark apricot
twelve
apricot of
apricot puddle
apricot hello
apricot preserves
apricot where
apricot dear
apricot or
apricot coaxial
apricot forever
For training a binary classiﬁer we also need negative examples, and in fact skip-
gram uses more negative examples than positive examples, the ratio set by a param-
eter k. So for each of these (t , c) training instances we’ll create k negative samples,
each consisting of the target t plus a ‘noise word’. A noise word is a random word
from the lexicon, constrained not to be the target word t . The right above shows the
setting where k = 2, so we’ll have 2 negative examples in the negative training set
− for each positive example t , c.
The noise words are chosen according to their weighted unigram frequency
pα (w), where α is a weight. If we were sampling according to unweighted fre-
quency p(w), it would mean that with unigram probability p(“t he”) we would choose
the word the as a noise word, with unigram probability p(“aard vark”) we would
choose aardvark, and so on. But in practice it is common to set α = .75, i.e. use the
weighting p 3

4 (w):

Pα (w) =

count (w)α
(cid:80)w(cid:48) count (w(cid:48) )α
Setting α = .75 gives better performance because it gives rare noise words slightly
higher probability: for rare words, Pα (w) > P(w). To visualize this intuition, it
might help to work out the probabilities for an example with two events, P(a) = .99
and P(b) = .01:

(6.31)

Pα (a) =

Pα (b) =

.99.75
.99.75 + .01.75 = .97
.01.75
.99.75 + .01.75 = .03

(6.32)

Given the set of positive and negative training instances, and an initial set of
embeddings, the goal of the learning algorithm is to adjust those embeddings such
that we
• Maximize the similarity of the target word, context word pairs (t,c) drawn
from the positive examples

122 CHA PTER 6

• V ECTOR S EMAN T IC S

• Minimize the similarity of the (t,c) pairs drawn from the negative examples.
We can express this formally over the whole training set as:
log P(+|t , c) + (cid:88)(t ,c)∈−
log P(−|t , c)
Or, focusing in on one word/context pair (t , c) with its k noise words n1 ...nk , the
learning objective L is:

L(θ ) = (cid:88)(t ,c)∈+

(6.33)

L(θ ) = log P(+|t , c) +

log P(−|t , ni )

= log σ (c · t ) +

log σ (−ni · t )

k(cid:88)i=1
k(cid:88)i=1
k(cid:88)i=1

1

log

(6.34)

= log

1 + eni ·t

1
1 + e−c·t +
That is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the k negative sampled non-
neighbor words.
We can then use stochastic gradient descent to train to this objective, iteratively
modifying the parameters (the embeddings for each target word t and each context
word or noise word c in the vocabulary) to maximize the objective.
Note that the skip-gram model thus actually learns two separate embeddings

for each word w: the target embedding t and the context embedding c. These

embeddings are stored in two matrices, the target matrix T and the context matrix
C. So each row i of the target matrix T is the 1 × d vector embedding ti for word
i in the vocabulary V , and each column i of the context matrix C is a d × 1 vector
embedding ci for word i in V . Fig. 6.13 shows an intuition of the learning task for
the embeddings encoded in these two matrices.

target
embedding
context
embedding

Figure 6.13 The skip-gram model tries to shift embeddings so the target embedding (here
for apricot) are closer to (have a higher dot product with) context embeddings for nearby
words (here jam) and further from (have a lower dot product with) context embeddings for
words that don’t occur nearby (here aardvark).

Just as in logistic regression, then, the learning algorithm starts with randomly
initialized W and C matrices, and then walks through the training corpus using gra-
dient descent to move W and C so as to maximize the objective in Eq. 6.34. Thus
the matrices W and C function as the parameters θ that logistic regression is tuning.

6 .9

• V I SUA L I Z ING EMB EDD ING S

123

Once the embeddings are learned, we’ll have two embeddings for each word wi :
ti and ci . We can choose to throw away the C matrix and just keep W , in which case
each word i will be represented by the vector ti .
Alternatively we can add the two embeddings together, using the summed em-
bedding ti + ci as the new d -dimensional embedding, or we can concatenate them
into an embedding of dimensionality 2d .
As with the simple count-based methods like tf-idf, the context window size L
effects the performance of skip-gram embeddings, and experiments often tune the
parameter L on a dev set. One difference from the count-based methods is that for
skip-grams, the larger the window size the more computation the algorithm requires
for training (more neighboring words must be predicted).

6.9 Visualizing Embeddings

Visualizing embeddings is an important goal in helping understands, apply, and im-
prove these models of word meaning. But how can we visualize a (for example)
100-dimensional vector?
The simplest way to visualize the meaning of a word w embedded in a space
is to list the most similar words to w sorting all words in the vocabulary by their
cosines. For example the 7 closest words to frog using the GloVe embeddings are:
frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (Pennington
et al., 2014)
Yet another visualization method is to use a clus-
tering algorithm to show a hierarchical representa-
tion of which words are similar to others in the em-
bedding space. The example on the right uses hi-
erarchical clustering of some embedding vectors for
nouns as a visualization method (Rohde et al., 2006).
Probably the most common visualization method,
however, is to project the 100 dimensions of a word
down into 2 dimensions. Fig. 6.1 showed one such
visualization, using a projection method called t-
SNE (van der Maaten and Hinton, 2008).

6.10 Semantic properties of embeddings

Vector semantic models have a number of parameters. One parameter that is relevant
to both sparse tf-idf vectors and dense word2vec vectors is the size of the context
window used to collect counts. This is generally between 1 and 10 words on each
side of the target word (for a total context of 3-20 words).
The choice depends on on the goals of the representation. Shorter context win-
dows tend to lead to representations that are a bit more syntactic, since the infor-
mation is coming from immediately nearby words. When the vectors are computed
from short context windows, the most similar words to a target word w tend to be
semantically similar words with the same parts of speech. When vectors are com-
puted from long context windows, the highest cosine words to a target word w tend
to be words that are topically related but not similar.

124 CHA PTER 6

• V ECTOR S EMAN T IC S

For example Levy and Goldberg (2014a) showed that using skip-gram with a
window of ±2, the most similar words to the word Hogwarts (from the Harry Potter
series) were names of other ﬁctional schools: Sunnydale (from Buffy the Vampire
Slayer) or Evernight (from a vampire series). With a window of ±5, the most similar
words to Hogwarts were other words topically related to the Harry Potter series:
Dumbledore, Malfoy, and half-blood.
It’s also often useful to distinguish two kinds of similarity or association between
words (Sch ¨utze and Pedersen, 1993). Two words have ﬁrst-order co-occurrence
(sometimes called syntagmatic association) if they are typically nearby each other.
Thus wrote is a ﬁrst-order associate of book or poem. Two words have second-order

co-occurrence (sometimes called paradigmatic association) if they have similar

neighbors. Thus wrote is a second-order associate of words like said or remarked.

ﬁrst-order
co-occurrence

second-order
co-occurrence

Analogy Another semantic property of embeddings is their ability to capture re-
lational meanings. Mikolov et al. (2013b) and Levy and Goldberg (2014b) show
that the offsets between vector embeddings can capture some analogical relations
between words. For example, the result of the expression vector(‘king’) - vec-
tor(‘man’) + vector(‘woman’) is a vector close to vector(‘queen’); the left panel
in Fig. 6.14 visualizes this, again projected down into 2 dimensions. Similarly, they
found that the expression vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) results
in a vector that is very close to vector(‘Rome’).

(a)

(b)

Figure 6.14 Relational properties of the vector space, shown by projecting vectors onto two dimensions. (a)
’king’ - ’man’ + ’woman’ is close to ’queen’ (b) offsets seem to capture comparative and superlative morphology
(Pennington et al., 2014).

Embeddings and Historical Semantics: Embeddings can also be a useful tool

for studying how meaning changes over time, by computing multiple embedding
spaces, each from texts written in a particular time period. For example Fig. 6.15
shows a visualization of changes in meaning in English words over the last two
centuries, computed by building separate embedding spaces for each decade from
historical corpora like Google N-grams (Lin et al., 2012) and the Corpus of Histori-
cal American English (Davies, 2012).

6 .11

• B IA S AND EMB EDD ING S

125

Figure 6.15 A t-SNE visualization of the semantic change of 3 words in English using
word2vec vectors. The modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. Earlier points are com-
puted from earlier historical embedding spaces. The visualizations show the changes in the
word gay from meanings related to “cheerful” or “frolicsome” to referring to homosexuality,
the development of the modern “transmission” sense of broadcast from its original sense of
sowing seeds, and the pejoration of the word awful as it shifted from meaning “full of awe”
to meaning “terrible or appalling” (Hamilton et al., 2016b).

6.11 Bias and Embeddings

In addition to their ability to learn word meaning from text, embeddings, alas, also
reproduce the implicit biases and stereotypes that were latent in the text. Recall that
embeddings model analogical relations; ‘queen’ as the closest word to ’king’ - ’man’
+ ’woman’ implies the analogy man:woman::king:queen. But embedding analogies
also exhibit gender stereotypes. For example Bolukbasi et al. (2016) ﬁnd that the
closest occupation to ‘man’ - ‘computer programmer’ + ‘woman’ in word2vec em-
beddings trained on news text is ‘homemaker’, and that the embeddings similarly
suggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. Algorithms that
used embeddings as part of an algorithm to search for potential programmers or
doctors might thus incorrectly downweight documents with women’s names.
Embeddings also encode the implicit associations that are a property of human
reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-
ple’s associations between concepts (like ’ﬂowers’ or ’insects’) and attributes (like
‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with
which they label words in the various categories.6 Using such methods, people
in the United States have been shown to associate African-American names with
unpleasant words (more than European-American names), male names more with
mathematics and female names with the arts, and old people’s names with unpleas-
ant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan
et al. (2017) replicated all these ﬁndings of implicit associations using GloVe vec-
tors and cosine similarity instead of human latencies. For example Afrian American
names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words
while European American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine
with pleasant words. Any embedding-aware algorithm that made use of word senti-
ment could thus lead to bias against African Americans.

6 Roughly speaking, if humans associate ‘ﬂowers’ with ’pleasantness’ and ‘insects’ with ‘unpleasant-
ness’, when they are instructed to push a red button for ‘ﬂowers’ (daisy, iris, lilac) and ’pleasant words’
(love, laughter, pleasure) and a green button for ‘insects’ (ﬂea, spider, mosquito) and ‘unpleasant words’
(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for
‘ﬂowers’ and ‘unpleasant words’ and a green button for ‘insects’ and ‘pleasant words’.

126 CHA PTER 6

• V ECTOR S EMAN T IC S

Recent research focuses on ways to try to remove the kinds of biases, for example
by developing a transformation of the embedding space that removes gender stereo-
types but preserves deﬁnitional gender (Bolukbasi et al. 2016, Zhao et al. 2017).
Historical embeddings are also being used to measure biases in the past. Garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
ties or genders (for example the relative cosine similarity of women’s names versus
men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century.
They found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupation. Historical embeddings also replicated
old surveys of ethnic stereotypes; the tendency of experimental participants in 1933
to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese eth-
nicity, correlates with the cosine between Chinese last names and those adjectives
using embeddings trained on 1930s text. They also were able to document historical
gender biases, such as the fact that embeddings for adjectives related to competence
(‘smart’, ‘wise’, ‘thoughtful’, ’resourceful’) had a higher cosine with male than fe-
male words, and showed that this bias has been slowly decreasing since 1960.
We will return in later chapters to this question about the role of bias in natural
language processing and machine learning in general.

6.12 Evaluating Vector Models

The most important evaluation metric for vector models is extrinsic evaluation on
tasks; adding them as features into any NLP task and seeing whether this improves
performance over some other model.
Nonetheless it is useful to have intrinsic evaluations. The most common metric
is to test their performance on similarity, computing the correlation between an
algorithm’s word similarity scores and word similarity ratings assigned by humans.
WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.
SimLex-999 (Hill et al., 2015) is a more difﬁcult dataset that quantiﬁes similarity
(cup, mug) rather than relatedness (cup, coffee), and including both concrete and
abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions,
each consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: Levied is closest in meaning to:
imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these
datasets present words without context.
Slightly more realistic are intrinsic similarity tasks that include context. The
Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) offers a
richer evaluation scenario, giving human judgments on 2,003 pairs of words in their
sentential context, including nouns, verbs, and adjectives. This dataset enables the
evaluation of word similarity algorithms that can make use of context words. The
semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the
performance of sentence-level similarity algorithms, consisting of a set of pairs of
sentences, each pair with human-labeled similarity scores.
Another task used for evaluate is an analogy task, where the system has to solve
problems of the form a is to b as c is to d, given a, b, and c and having to ﬁnd d.
Thus given Athens is to Greece as Oslo is to
, the system must ﬁll in the word
Norway. Or more syntactically-oriented examples: given mouse, mice, and dollar

the system must return dollars. Large sets of such tuples have been created (Mikolov
et al. 2013, Mikolov et al. 2013b).

6 .13

• SUMMARY

127

6.13 Summary

frequency and inverse document frequency.

• In vector semantics, a word is modeled as a vector—a point in high-dimensional
space, also called an embedding.
• Vector semantic models fall into two classes: sparse and dense. In sparse
models like tf-idf each dimension corresponds to a word in the vocabulary V ;
• Cell in sparse models are functions of co-occurrence counts. The term-
document matrix has rows for each word (term) in the vocabulary and a
column for each document.
• The word-context matrix has a row for each (target) word in the vocabulary
and a column for each context term in the vocabulary.
• A common sparse weighting is tf-idf, which weights each cell by its term
• Word and document similarity is computed by computing the dot product
between vectors. The cosine of two vectors—a normalized dot product—is
the most popular such metric.
• PPMI (pointwise positive mutual information) is an alternative weighting
scheme to tf-idf.
• Dense vector models have dimensionality 50-300 and the dimensions are harder
to interpret.
• The word2vec family of models, including skip-gram and CBOW, is a pop-
ular efﬁcient way to compute dense embeddings.
• Skip-gram trains a logistic regression classiﬁer to compute the probability that
two words are ‘likely to occur nearby in text’. This probability is computed
from the dot product between the embeddings for the two words,
• Skip-gram uses stochastic gradient descent to train the classiﬁer, by learning
embeddings that have a high dot-product with embeddings of words that occur
nearby and a low dot-product with noise words.
• Other important embedding algorithms include GloVe, a method based on ra-
tios of word co-occurrence probabilities, and fasttext, an open-source library
for computing word embeddings by summing embeddings of the bag of char-
acter n-grams that make up a word.

Bibliographical and Historical Notes

The idea of vector semantics arose out of research in the 1950s in three distinct
ﬁelds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.
The idea that meaning was related to distribution of words in context was widespread
in linguistic theory of the 1950s, among distributionalists like Zellig Harris, Martin
Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos (1950) put it,

128 CHA PTER 6

• V ECTOR S EMAN T IC S

the linguist’s “meaning” of a morpheme. . . is by deﬁnition the set of conditional
probabilities of its occurrence in context with all other morphemes.

mechanical
indexing

semantic
feature

The idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like Charles E. Osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like happy/sad, or hard/soft. Osgood et al. (1957) proposed that
the meaning of a word in general could be modeled as a point in a multidimensional
Euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.
A ﬁnal intellectual source in the 1950s and early 1960s was the ﬁeld then called

mechanical indexing, now known as information retrieval. In what became known

as the vector space model for information retrieval (Salton 1971,Sparck Jones 1986),
researchers demonstrated new ways to deﬁne the meaning of words in terms of vec-
tors (Switzer, 1965), and reﬁned methods for word similarity based on measures
of statistical association between words like mutual information (Giuliano, 1965)
and idf (Sparck Jones, 1972), and showed that the meaning of documents could be
represented in the same vector spaces used for words.
More distantly related is the idea of deﬁning words by a vector of discrete fea-
tures, which has a venerable history in our ﬁeld, with roots at least as far back as
Descartes and Leibniz (Wierzbicka 1992, Wierzbicka 1996). By the middle of the
20th century, beginning with the work of Hjelmslev (Hjelmslev, 1969) and ﬂeshed
out in early models of generative grammar (Katz and Fodor, 1963), the idea arose of
representing meaning with semantic features, symbols that represent some sort of
primitive meaning. For example words like hen, rooster, or chick, have something
in common (they all describe chickens) and something different (their age and sex),
representable as:
hen

+female, +chicken, +adult
rooster -female, +chicken, +adult
+chicken, -adult

chick
The dimensions used by vector models of meaning to deﬁne words, however, are
only abstractly related to this idea of a small ﬁxed number of hand-built dimensions.
Nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speciﬁc compositional aspect of meaning like
these early semantic features.
The ﬁrst use of dense vectors to model word meaning was the latent seman-
tic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic
analysis) (Deerwester et al., 1990). In LSA SVD is applied to a term-document ma-
trix (each cell weighted by log frequency and normalized by entropy), and then using
the ﬁrst 300 dimensions as the embedding. LSA was then quickly widely applied:
as a cognitive model Landauer and Dumais (1997), and tasks like spell checking
(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-
rafsky 1998, Bellegarda 2000) morphology induction (Schone and Jurafsky 2000,
Schone and Jurafsky 2001), and essay grading (Rehder et al., 1998). Related mod-
els were simultaneously developed and applied to word sense disambiguation by
Sch ¨utze (1992b). LSA also led to the earliest use of embeddings to represent words
in a probabilistic classiﬁer, in the logistic regression document router of Sch ¨utze
et al. (1995). The idea of SVD on the term-term matrix (rather than the term-
document matrix) as a model of meaning for NLP was proposed soon after LSA
by Sch ¨utze (1992b). Sch ¨utze applied the low-rank (97-dimensional) embeddings
produced by SVD to the task of word sense disambiguation, analyzed the result-

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

129

ing semantic space, and also suggested possible techniques like dropping high-order
dimensions. See Sch ¨utze (1997a).
A number of alternative matrix models followed on from the early SVD work,
including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) Latent
Dirichlet Allocation (LDA) (Blei et al., 2003). Nonnegative Matrix Factorization
(NMF) (Lee and Seung, 1999).
By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and
Collobert et al. (2011) then demonstrated that embeddings could play a role for rep-
resenting word meanings for a number of NLP tasks. Turian et al. (2010) compared
the value of different kinds of embeddings for different NLP tasks. Mikolov et al.
(2011) showed that recurrent neural nets could be used as language models. The
idea of simplifying the hidden layer of these neural net language models to create
the skip-gram and CBOW algorithms was proposed by Mikolov et al. (2013). The
negative sampling training algorithm was proposed in Mikolov et al. (2013a).
Studies of embeddings include results showing an elegant mathematical relation-
ship between sparse and dense embeddings (Levy and Goldberg, 2014c), as well
as numerous surveys of embeddings and their parameterizations. (Bullinaria and
Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014,
Levy et al. 2015).
There are many other embedding algorithms, using methods like non-negative
matrix factorization (Fyshe et al., 2015), or by converting sparse PPMI embeddings
to dense vectors by using SVD (Levy and Goldberg, 2014c). The most widely-
used embedding model besides word2vec is GloVe (Pennington et al., 2014). The
name stands for Global Vectors, because the model is based on capturing global
corpus statistics. GloVe is based on ratios of probabilities from the word-word co-
occurrence matrix, combining the intuitions of count-based models like PPMI while
also capturing the linear structures used by methods like word2vec.
An extension of word2vec, fasttext (Bojanowski et al., 2017), deals with un-
known words and sparsity in languages with rich morphology, by using subword
models. Each word in fasttext is represented as itself plus a bag of constituent n-
grams, with special boundary symbols < and > added to each word. For example,
with n = 3 the word where would be represented by the character n-grams:

fasttext

<wh, whe, her, ere, re>

plus the sequence

<where>

Then a skipgram embedding is learned for each constituent n-gram, and the word
where is represented by the sum of all of the embeddings of its constituent n-grams.
A fasttext open-source library, including pretrained embeddings for 157 languages,

is available at https://fasttext.cc.

See Manning et al. (2008) for a deeper understanding of the role of vectors in in-
formation retrieval, including how to compare queries with documents, more details
on tf-idf, and issues of scaling to very large datasets.
Cruse (2004) is a useful introductory linguistic text on lexical semantics.

130 CHA PTER 6

• V ECTOR S EMAN T IC S

Exercises

CHAPTER

7 Neural Networks and Neural
Language Models

“[M]achines of this character can behave in a very complicated manner when
the number of units is large.”

Alan Turing (1948) “Intelligent Machines”, page 6

deep learning
deep

Neural networks are an essential computational tool for language processing, and
a very old one. They are called neural because their origins lie in the McCulloch-
Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the human neuron
as a kind of computing element that could be described in terms of propositional
logic. But the modern use in language processing no longer draws on these early
biological inspirations.
Instead, a modern neural network is a network of small computing units, each
of which takes a vector of input values and produces a single output value. In this
chapter we introduce the neural net applied to classiﬁcation. The architecture we
introduce is called a feed-forward network because the computation proceeds iter-
atively from one layer of units to the next. The use of modern neural nets is often
called deep learning, because modern networks are often deep (have many layers).
Neural networks share much of the same mathematics as logistic regression. But
neural networks are a more powerful classiﬁer than logistic regression, and indeed a
minimal neural network (technically one with a single ‘hidden layer’) can be shown
to learn any function.
Neural net classiﬁers are different from logistic regression in another way. With
logistic regression, we applied the regression classiﬁer to many different tasks by
developing many rich kinds of feature templates based on domain knowledge. When
working with neural networks, it is more common to avoid the use of rich hand-
derived features, instead building neural networks that take raw words as inputs
and learn to induce features as part of the process of learning to classify. We saw
examples of this kind of representation learning for embeddings in Chapter 6. Nets
that are very deep are particularly good at representation learning for that reason
deep neural nets are the right tool for large scale problems that offer sufﬁcient data
to learn features automatically.
In this chapter we’ll see feedforward networks as classiﬁers, and apply them to
the simple task of language modeling: assigning probabilities to word sequences and
predicting upcoming words. In later chapters we’ll introduce many other aspects of

neural models, such as the recurrent neural network and the encoder-decoder

model.

132 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

7.1 Units

The building block of a neural network is a single computational unit. A unit takes
a set of real valued numbers as input, performs some computation on them, and
produces an output.
At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-
tional term in the sum called a bias term. Thus given a set of inputs x1 ...xn , a unit
has a set of corresponding weights w1 ...wn and a bias b, so the weighted sum z can
be represented as:
z = b +(cid:88)i
Often it’s more convenient to express this weighted sum using vector notation;
recall from linear algebra that a vector is, at heart, just a list or array of numbers.
Thus we’ll talk about z in terms of a weight vector w, a scalar bias b, and an input
vector x, and we’ll replace the sum with the convenient dot product:

(7.1)

wi xi

z = w · x + b
As deﬁned in Eq. 7.2, z is just a real valued number.
Finally, instead of using z, a linear function of x, as the output, neural units
apply a non-linear function f to z. We will refer to the output of this function as
the activation value for the unit, a. Since we are just modeling a single unit, the
activation for the node is in fact the ﬁnal output of the network, which we’ll generally
call y. So the value y is deﬁned as:

(7.2)

bias term

vector

activation

y = a = f (z)

(7.3)

We’ll discuss three popular non-linear functions f () below (the sigmoid, the
tanh, and the rectiﬁed linear ReLU) but it’s pedagogically convenient to start with
the sigmoid function since we saw it in Chapter 5:

sigmoid

y = σ (z) =

1
1 + e−z
The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output
into the range [0, 1], which is useful in squashing outliers toward 0 or 1. And it’s
differentiable, which as we saw in Section 5.8 will be handy for learning.
Substituting the sigmoid equation into Eq. 7.2 gives us the ﬁnal value for the
output of a neural unit:

(7.4)

1
y = σ (w · x + b) =
1 + ex p(−(w · x + b))
Fig. 7.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit
takes 3 input values x1 , x2 , and x3 , and computes a weighted sum, multiplying each
value by a weight (w1 , w2 , and w3 , respectively), adds them to a bias term b, and then
passes the resulting sum through a sigmoid function to result in a number between 0
and 1.
Let’s walk through an example just to get an intuition. Let’s suppose we have a
unit with the following weight vector and bias:

(7.5)

7 .1

• UN I T S

133

Figure 7.1 The sigmoid function takes a real value and maps it to the range [0, 1]. Because
it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1.

Figure 7.2 A neural unit, taking 3 inputs x1 , x2 , and x3 (and a bias b that we represent as a
weight for an input clamped at +1) and producing an output y. We include some convenient
intermediate variables: the output of the summation, z, and the output of the sigmoid, a. In
this case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to
mean the ﬁnal output of the entire network, leaving a as the activation of an individual node.

w = [0.2, 0.3, 0.9]

b = 0.5

What would this unit do with the following input vector:

x = [0.5, 0.6, 0.1]

The resulting output y would be:
1

1 + e−(w·x+b)

=

1

= e−0.87 = .70

1 + e−(.5∗.2+.6∗.3+.1∗.9+.5)

y = σ (w · x + b) =
In practice, the sigmoid is not commonly used as an activation function. A
function that is very similar but almost always better is the tanh function shown
in Fig. 7.3a; tanh is a variant of the sigmoid that ranges from -1 to +1:
ez − e−z
ez + e−z
The simplest activation function, and perhaps the most commonly used, is the
rectiﬁed linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as x

(7.6)

y =

tanh

ReLU

134 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

when x is positive, and 0 otherwise:

y = max(x, 0)

(7.7)

(a)

(b)

Figure 7.3 The tanh and ReLU activation functions.

saturated

These activation functions have different properties that make them useful for
different language applications or network architectures. For example the rectiﬁer
function has nice properties that result from it being very close to linear. In the sig-
moid or tanh functions, very high values of z result in values of y that are saturated,
i.e., extremely close to 1, which causes problems for learning. Rectiﬁers don’t have
this problem, since the output of values close to 1 also approaches 1 in a nice gentle
linear way. By contrast, the tanh function has the nice properties of being smoothly
differentiable and mapping outlier values toward the mean.

7.2 The XOR problem

Early in the history of neural networks it was realized that the power of neural net-
works, as with the real neurons that inspired them, comes from combining these
units into larger networks.
One of the most clever demonstrations of the need for multi-layer networks was
the proof by Minsky and Papert (1969) that a single neural unit cannot compute
some very simple functions of its input. Consider the very simple task of computing
simple logical functions of two inputs, like AND, OR, and XOR. As a reminder,
here are the truth tables for those functions:

AND
x1 x2 y
0
0
0
0
1
0
1
0
0
1
1
1

OR
x1 x2 y
0
0
0
0
1
1
1
0
1
1
1
1

XOR
x1 x2 y
0
0
0
0
1
1
1
0
1
1
1
0

perceptron

This example was ﬁrst shown for the perceptron, which is a very simple neural
unit that has a binary output and no non-linear activation function. The output y of

7 .2

• TH E XOR PROB LEM 135

a perceptron is 0 or 1, and just computed as follows (using the same weight w, input
x, and bias b as in Eq. 7.2):
y = (cid:26) 0,
if w · x + b ≤ 0
1,
if w · x + b > 0
It’s very easy to build a perceptron that can compute the logical AND and OR
functions of its binary inputs; Fig. 7.4 shows the necessary weights.

(7.8)

decision
boundary

linearly
separable

(a)

(b)

Figure 7.4 The weights w and bias b for perceptrons for computing logical functions. The
inputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied
with the bias weight b. (a) logical AND, showing weights w1 = 1 and w2 = 1 and bias weight
b = −1. (b) logical OR, showing weights w1 = 1 and w2 = 1 and bias weight b = 0. These
weights/biases are just one from an inﬁnite number of possible sets of weights and biases that
would implement the functions.

It turns out, however, that it’s not possible to build a perceptron to compute
logical XOR! (It’s worth spending a moment to give it a try!)
The intuition behind this important result relies on understanding that a percep-
tron is a linear classiﬁer. For a two-dimensional input x0 and x1 , the perception
equation, w1 x1 + w2 x2 + b = 0 is the equation of a line (we can see this by putting
it in the standard linear format: x2 = −(w1/w2 )x1 − b.) This line acts as a decision
boundary in two-dimensional space in which the output 0 is assigned to all inputs
lying on one side of the line, and the output 1 to all input points lying on the other
side of the line. If we had more than 2 inputs, the decision boundary becomes a
hyperplane instead of a line, but the idea is the same, separating the space into two
categories.
Fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn
by one possible set of parameters for an AND and an OR classiﬁer. Notice that there
is simply no way to draw a line that separates the positive cases of XOR (01 and 10)
from the negative cases (00 and 11). We say that XOR is not a linearly separable
function. Of course we could draw a boundary with a curve, or some other function,
but not a single line.

7.2.1 The solution: neural networks

While the XOR function cannot be calculated by a single perceptron, it can be cal-
culated by a layered network of units. Let’s see an example of how to do this from
Goodfellow et al. (2016) that computes XOR using two layers of ReLU-based units.
Fig. 7.6 shows a ﬁgure with the input being processed by two layers of neural units.
The middle layer (called h) has two units, and the output layer (called y) has one
unit. A set of weights and biases are shown for each ReLU that correctly computes
the XOR function
Let’s walk through what happens with the input x = [0 0]. If we multiply each
input value by the appropriate weight, sum, and then add the bias b, we get the vector

136 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

Figure 7.5 The functions AND, OR, and XOR, represented with input x0 on the x-axis and input x1 on the
y axis, Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no
way to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig
(2002).

Figure 7.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in
two layers; we’ve called them h1 , h2 (h for “hidden layer”) and y1 . As before, the numbers
on the arrows represent the weights w for each unit, and we represent the bias b as a weight
on a unit clamped to +1, with the bias weights/units in gray.

[0 -1], and we then we apply the rectiﬁed linear transformation to give the output
of the h layer as [0 0]. Now we once again multiply by the weights, sum, and add
the bias (0 in this case) resulting in the value 0. The reader should work through the
computation of the remaining 3 possible input pairs to see that the resulting y values
correctly are 1 for the inputs [0 1] and [1 0] and 0 for [0 0] and [1 1].
It’s also instructive to look at the intermediate results, the outputs of the two
hidden nodes h0 and h1 . We showed in the previous paragraph that the h vector for
the inputs x = [0 0] was [0 0]. Fig. 7.7b shows the values of the h layer for all 4
inputs. Notice that hidden representations of the two input points x = [0 1] and x
= [1 0] (the two cases with XOR output = 1) are merged to the single point h = [1
0]. The merger makes it easy to linearly separate the positive and negative cases
of XOR. In other words, we can view the hidden layer of the network is forming a
representation for the input.
In this example we just stipulated the weights in Fig. 7.6. But for real exam-
ples the weights for neural networks are learned automatically using the error back-
propagation algorithm to be introduced in Section 7.4. That means the hidden layers
will learn to form useful representations. This intuition, that neural networks can au-
tomatically learn useful representations of the input, is one of their key advantages,

7 .3

• F EED -FORWARD N EURA L N ETWORK S

137

Figure 7.7 The hidden layer forming a new representation of the input. Here is the rep-
resentation of the hidden layer, h, compared to the original input representation x. Notice
that the input point [0 1] has been collapsed with the input point [1 0], making it possible to
linearly separate the positive and negative cases of XOR. After Goodfellow et al. (2016).

and one that we will return to again and again in later chapters.
Note that the solution to the XOR problem requires a network of units with non-
linear activation functions. A network made up of simple linear (perceptron) units
cannot solve the XOR problem. This is because a network formed by many layers
of purely linear units can always be reduced (shown to be computationally identical
to) a single layer of linear units with appropriate weights, and we’ve already shown
(visually, in Fig. 7.5) that a single unit cannot solve the XOR problem.

7.3 Feed-Forward Neural Networks

feed-forward
network

multi-layer
perceptrons
MLP

hidden layer

fully-connected

Let’s now walk through a slightly more formal presentation of the simplest kind of
neural network, the feed-forward network. A feed-forward network is a multilayer
network in which the units are connected with no cycles; the outputs from units in
each layer are passed to units in the next higher layer, and no outputs are passed
back to lower layers. (In Chapter 9 we’ll introduce networks with cycles, called

recurrent neural networks.)

For historical reasons multilayer networks, especially feedforward networks, are
sometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,
since the units in modern multilayer networks aren’t perceptrons (perceptrons are
purely linear, but modern networks are made up of units with non-linearities like
sigmoids), but at some point the name stuck.
Simple feed-forward networks have three kinds of nodes: input units, hidden
units, and output units. Fig. 7.8 shows a picture.
The input units are simply scalar values just as we saw in Fig. 7.2.
The core of the neural network is the hidden layer formed of hidden units,
each of which is a neural unit as described in Section 7.1, taking a weighted sum of
its inputs and then applying a non-linearity. In the standard architecture, each layer
is fully-connected, meaning that each unit in each layer takes as input the outputs
from all the units in the previous layer, and there is a link between every pair of units
from two adjacent layers. Thus each hidden unit sums over all the input units.

138 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

Figure 7.8 A simple 2-layer feed-forward network, with one hidden layer, one output layer,
and one input layer (the input layer is usually not counted when enumerating layers).

Recall that a single hidden unit has parameters w (the weight vector) and b (the
bias scalar). We represent the parameters for the entire hidden layer by combining
the weight vector wi and bias bi for each unit i into a single weight matrix W and
a single bias vector b for the whole layer (see Fig. 7.8). Each element Wi j of the
weight matrix W represents the weight of the connection from the ith input unit xi to
the the jth hidden unit h j .
The advantage of using a single matrix W for the weights of the entire layer is
that now that hidden layer computation for a feedforward network can be done very
efﬁciently with simple matrix operations. In fact, the computation only has three
steps: multiplying the weight matrix by the input vector x, adding the bias vector b,
and applying the activation function g (such as the sigmoid, tanh, or relu activation
function deﬁned above).
The output of the hidden layer, the vector h, is thus the following, using the
sigmoid function σ :

h = σ (W x + b)

(7.9)

Notice that we’re applying the σ function here to a vector, while in Eq. 7.4 it was
applied to a scalar. We’re thus allowing σ (·), and indeed any activation function
g(·), to apply to a vector element-wise, so g[z1 , z2 , z3 ] = [g(z1 ), g(z2 ), g(z3 )].
Let’s introduce some constants to represent the dimensionalities of these vectors
and matrices. We’ll refer to the input layer as layer 0 of the network, and use have
n0 represent the number of inputs, so x is a vector of real numbers of dimension
n0 , or more formally x ∈ Rn0 . Let’s call the hidden layer layer 1 and the output
layer layer 2. The hidden layer has dimensionality n1 , so h ∈ Rn1 and also b ∈ Rn1
(since each hidden unit can take a different bias value). And the weight matrix W
has dimensionality W ∈ Rn1×n0 .
Take a moment to convince yourself that the matrix multiplication in Eq. 7.9 will
compute the value of each hi j as (cid:80)nx
As we saw in Section 7.2, the resulting value h (for hidden but also for hypoth-
esis) forms a representation of the input. The role of the output layer is to take
this new representation h and compute a ﬁnal output. This output could be a real-
valued number, but in many cases the goal of the network is to make some sort of
classiﬁcation decision, and so we will focus on the case of classiﬁcation.
If we are doing a binary task like sentiment classiﬁcation, we might have a single

i=1 wi j xi + b j .

7 .3

• F EED -FORWARD N EURA L N ETWORK S

139

output node, and its value y is the probability of positive versus negative sentiment.
If we are doing multinomial classiﬁcation, such as assigning a part-of-speech tag, we
might have one output node for each potential part-of-speech, whose output value
is the probability of that part-of-speech, and the values of all the output nodes must
sum to one. The output layer thus gives a probability distribution across the output
nodes.
Let’s see how this happens. Like the hidden layer, the output layer has a weight
matrix (let’s call it U ), but output layers may not t have a bias vector b, so we’ll sim-
plify by eliminating the bias vector in this example. The weight matrix is multiplied
by its input vector (h) to produce the intermediate output z.

z = U h
There are n2 output nodes, so z ∈ Rn2 , weight matrix U has dimensionality U ∈
Rn2×n1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the
output layer.
However, z can’t be the output of the classiﬁer, since it’s a vector of real-valued
numbers, while what we need for classiﬁcation is a vector of probabilities. There is
a convenient function for normalizing a vector of real values, by which we mean
converting it to a vector that encodes a probability distribution (all the numbers lie
between 0 and 1 and sum to 1): the softmax function that we saw on page 96 of
Chapter 5. For a vector z of dimensionality d , the softmax is deﬁned as:

normalizing

softmax

ezi
j=1 ez j

(cid:80)d

(7.10)

softmax(zi ) =

1 ≤ i ≤ d
Thus for example given a vector z=[0.6 1.1 -1.5 1.2 3.2 -1.1], softmax(z) is [ 0.055
0.090 0.0067 0.10 0.74 0.010].
You may recall that softmax was exactly what is used to create a probability
distribution from a vector of real-valued numbers (computed from summing weights
times features) in logistic regression in Chapter 5.
That means we can think of a neural network classiﬁer with one hidden layer
as building a vector h which is a hidden layer representation of the input, and then
running standard logistic regression on the features that the network develops in h.
By contrast, in Chapter 5 the features were mainly designed by hand via feature
templates. So a neural network is like logistic regression, but (a) with many layers,
since a deep neural network is like layer after layer of logistic regression classiﬁers,
and (b) rather than forming the features by feature templates, the prior layers of the
network induce the feature representations themselves.
Here are the ﬁnal equations for a feed-forward network with a single hidden
layer, which takes an input vector x, outputs a probability distribution y, and is pa-
rameterized by weight matrices W and U and a bias vector b:
h = σ (W x + b)
z = U h
y = softmax(z)

We’ll call this network a 2-layer network (we traditionally don’t count the input
layer when numbering layers, but do count the output layer). So by this terminology
logistic regression is a 1-layer network.
Let’s now set up some notation to make it easier to talk about deeper networks
of depth more than 2. We’ll use superscripts in square brackets to mean layer num-
bers, starting at 0 for the input layer. So W [1] will mean the weight matrix for the

(7.11)

140 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. n j
will mean the number of units at layer j. We’ll use g(·) to stand for the activation
function, which will tend to be ReLU or tanh for intermediate layers and softmax
for output layers. We’ll use a[i] to mean the output from layer i, and z[i] to mean the
combination of weights and biases W [i]a[i−1] + b[i] . The 0th layer is for inputs, so the
inputs x we’ll refer to more generally as a[0] .
Thus we’ll represent a 3-layer net as follows:

z[1] = W [1]a[0] + b[1]
a[1] = g[1] (z[1] )
z[2] = W [2]a[1] + b[2]
a[2] = g[2] (z[2] )

ˆy = a[2]

(7.12)

Note that with this notation, the equations for the computation done at each layer are
the same. The algorithm for computing the forward step in an n-layer feed-forward
network, given the input vector a[0] is thus simply:

for i in 1..n

z[i] = W [i] a[i−1] + b[i]
a[i] = g[i] (z[i] )

ˆy = a[n]

The

activation functions g(·) are generally different at the ﬁnal layer. Thus g[2] might
be softmax for multinomial classiﬁcation or sigmoid for binary classiﬁcation, while
ReLU or tanh might be the activation function g() at the internal layers.

7.4 Training Neural Nets

A feedforward neural net is an instance of supervised machine learning in which we
know the correct output y for each observation x. What the system produces, via
Eq. 7.12, is ˆy, the system’s estimate of the true y. The goal of the training procedure
is to learn parameters W [i] and b[i] for each layer i that make ˆy for each training
observation as close as possible to the true y .
In general, we do all this by drawing on the methods we introduced in Chapter 5
for logistic regression, so the reader should be comfortable with that chapter before
proceeding.
First, we’ll need a loss function that models the distance between the system
output and the gold output, and it’s common to use the loss used for logistic regres-

sion, the cross-entropy loss.

Second, to ﬁnd the parameters that minimize this loss function, we’ll use the
gradient descent optimization algorithm introduced in Chapter 5. There are some
differences
Third, gradient descent requires knowing the gradient of the loss function, the
vector that contains the partial derivative of the loss function with respect to each of
the parameters. Here is one part where learning for neural networks is more complex
than for logistic logistic regression. In logistic regression, for each observation we
could directly compute the derivative of the loss function with respect to an individ-
ual w or b. But for neural networks, with millions of parameters in many layers, it’s

7 .4

• TRA IN ING N EURA L N E T S

141

much harder to see how to compute the partial derivative of some weight in layer 1
when the loss is attached to some much later layer. How do we partial out the loss
over all those intermediate layers?

The answer is the algorithm called error back-propagation or reverse differ-
entiation.

7.4.1 Loss function

cross entropy
loss

The cross entropy loss, that is used in neural networks is the same one we saw for
logistic regression.
In fact, if the neural network is being used as a binary classiﬁer, with the sig-
moid at the ﬁnal layer, the loss function is exactly the same as we saw with logistic
regression in Eq. 5.10:

negative log
likelihood loss

LCE ( ˆy, y) = − log p(y|x) = − [y log ˆy + (1 − y) log(1 − ˆy)]
What about if the neural network is being used as a multinomial classiﬁer? Let
y be a vector over the C classes representing the true output probability distribution.
The cross entropy loss here is

(7.13)

LCE ( ˆy, y) = −

yi log ˆyi

C(cid:88)i=1

(7.14)

We can simplify this equation further. Assume this is a hard classiﬁcation task,
meaning that only one class is the correct one, and that there is one output unit in y
for each class. If the true class is i, then y is a vector where yi = 1 and y j = 0 ∀ j (cid:54)= i.
A vector like this, with one value=1 and the rest 0, is called a one-hot vector. Now
let ˆy be the vector output from the network. The sum in Eq. 7.14 will be 0 except
for the true class. Hence the cross-entropy loss is simply the log probability of the
correct class, and we therefore also call this the negative log likelihood loss:

LCE ( ˆy, y) = − log ˆyi
Plugging in the softmax formula from Eq. 7.10, and with K the number of classes:

(7.15)

LCE ( ˆy, y) = − log

7.4.2 Computing the Gradient

ezi
j−1 ez j

(cid:80)K

(7.16)

How do we compute the gradient of this loss function? Computing the gradient
requires the partial derivative of the loss function with respect to each parameter.
For a network with one weight layer and sigmoid output (which is what logistic
regression is), we could simply use the derivative of the loss that we used for logistic
regression in: Eq. 7.17 (and derived in Section 5.8):

∂ LCE (w, b)

∂ w j

= ( ˆy − y) x j
= (σ (w · x + b) − y) x j

(7.17)

142 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

Or for a network with one hidden layer and softmax output, we could use the deriva-
tive of the softmax loss from Eq. 5.36:

∂ LCE
∂ wk

j=1 ew j ·x+b j (cid:33) xk
ewk ·x+bk

= (1{y = k} − p(y = k|x))xk
= (cid:32)1{y = k} −
But these derivatives only give correct updates for one weight layer: the last one!
For deep networks, computing the gradients for each weight is much more complex,
since we are computing the derivative with respect to weight parameters that appear
all the way back in the very early layers of the network, even though the loss is
computed only at the very end of the network.
The solution to computing this gradient is an algorithm called error backprop-
agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-
cially for neural networks, it turns out to be the same as a more general procedure

(cid:80)K

(7.18)

error back-
propagation

called backward differentiation, which depends on the notion of computation

graphs. Let’s see how that works in the next subsection.

7.4.3 Computation Graphs

A computation graph is a representation of the process of computing a mathematical
expression, in which the computation is broken down into separate operations, each
of which is modeled as a node in a graph.
Consider computing the function L(a, b, c) = c(a + 2b). If we make each of the
component addition and multiplication operations explicit, and add names (d and e)
for the intermediate outputs, the resulting series of computations is:

d = 2 ∗ b
e = a + d
L = c ∗ e
We can now represent this as a graph, with nodes for each operation, and di-
rected edges showing the outputs from each operation as the inputs to the next, as
in Fig. 7.9. The simplest use of computation graphs is to compute the value of the
function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,
b = 1, c = −1, and we’ve shown the result of the forward pass to compute the re-
sult L(3, 1, −1) = 10. In the forward pass of a computation graph, we apply each
operation left to right, passing the outputs of each computation as the input to the
next node.

7.4.4 Backward differentiation on computation graphs

The importance of the computation graph comes from the backward pass, which
is used to compute the derivatives that we’ll need for the weight update. In this
example our goal is to compute the derivative of the output function L with respect
to each of the input variables, i.e., ∂ L
∂ b , and ∂ L
∂ c . The derivative ∂ L
∂ a , tells us how
much a small change in a affects L.
Backwards differentiation makes use of the chain rule in calculus. Suppose we
are computing the derivative of a composite function f (x) = u(v(x)). The derivative

∂ a , ∂ L

chain rule

7 .4

• TRA IN ING N EURA L N E T S

143

Figure 7.9 Computation graph for the function L(a, b, c) = c(a + 2b), with values for input
nodes a = 3, b = 1, c = −1, showing the forward pass computation of L.

of f (x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with
respect to x:

d u
d v ·
The chain rule extends to more than two functions. If computing the derivative of a
composite function f (x) = u(v(w(x))), the derivative of f (x) is:

d f
d x

d v
d x

(7.19)

=

d u
d v
d v ·
dw ·
Let’s now compute the 3 derivatives we need. Since in the computation graph
L = ce, we can directly compute the derivative ∂ L

dw
d x

d f
d x

(7.20)

=

∂ c :

∂ L
∂ c

= e

For the other two, we’ll need to use the chain rule:

∂ L
∂ a
∂ L
∂ b

=

=

∂ L
∂ e
∂ L
∂ e

∂ e
∂ a
∂ e
∂ d

∂ d
∂ b

(7.21)

(7.22)

Eq. 7.22 thus requires four intermediate derivatives: ∂ L
∂ d , and ∂ d
∂ b , which
are as follows (making use of the fact that the derivative of a sum is the sum of the
derivatives):

∂ e , ∂ e
∂ a , ∂ e

L = ce :

e = a + d :

d = 2b :

∂ L
∂ c
∂ e
∂ d

= e

= 1

∂ L
∂ e
∂ e
∂ a
∂ d
∂ b

= c,

= 1,

= 2

(7.23)

In the backward pass, we compute each of these partials along each edge of
the graph from right to left, multiplying the necessary partials to result in the ﬁnal
derivative we need. Thus we begin by annotating the ﬁnal node with ∂ L
∂ L = 1. Moving
to the left, we then compute ∂ L
∂ e , and so on, until we have annotated the graph

∂ c and ∂ L

144 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

all the way to the input variables. The forward pass conveniently already will have
computed the values of the forward intermediate variables we need (like d and e)
to compute these derivatives. Fig. 7.10 shows the backward pass. At each node we
need to compute the local partial derivative with respect to the parent, multiply it by
the partial derivative that is being passed down from the parent, and then pass it to
the child.

Figure 7.10 Computation graph for the function L(a, b, c) = c(a + 2b), showing the back-
ward pass computation of ∂ L
∂ b , and ∂ L

∂ a , ∂ L

∂ c .

Of course computation graphs for real neural networks are much more complex.
Fig. 7.11 shows a sample computation graph for a 2-layer neural network with n0 =
2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid
output unit for simplicity. The weights that need updating (those for which we need
to know the partial derivative of the loss function) are shown in orange.

Figure 7.11 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer)
with two input dimensions and 2 hidden dimensions.

In order to do the backward pass, we’ll need to know the derivatives of all the
functions in the graph. We already saw in Section 5.8 the derivative of the sigmoid

σ :

dσ (z)
= σ (z)(1 − z)
d z
We’ll also need the derivatives of each of the other activation functions. The
derivative of tanh is:

(7.24)

d tanh(z)
d z

= 1 − tanh2 (z)

(7.25)

7 .5

• N EURA L LANGUAGE MOD EL S

145

The derivative of the ReLU is
d ReLU(z)
d z

= (cid:26) 0 f or x < 0
1 f or x ≥ 0

7.4.5 More details on learning

(7.26)

Optimization in neural networks is a non-convex optimization problem, more com-
plex than for logistic regression, and for that and other reasons there are many best
practices for successful learning.
For logistic regression we can initialize gradient descent with all the weights and
biases having the value 0. In neural networks, by contrast, we need to initialize the
weights with small random numbers. It’s also helpful to normalize the input values
to have 0 mean and unit variance.
Various forms of regularization are used to prevent overﬁtting. One of the most
important is dropout: randomly dropping some units and their connections from the
network during training (Hinton et al. 2012, Srivastava et al. 2014).
Hyperparameter tuning is also important. The parameters of a neural network
are the weights W and biases b; those are learned by gradient descent. The hyperpa-
rameters are things that are set by the algorithm designer and not learned in the same
way, although they must be tuned. Hyperparameters include the learning rate η , the
minibatch size, the model architecture (the number of layers, the number of hidden
nodes per layer, the choice of activation functions), how to regularize, and so on.
Gradient descent itself also has many architectural variants such as Adam (Kingma
and Ba, 2015).
Finally, most modern neural networks are built using computation graph for-
malisms that make all the work of gradient computation and parallelization onto
vector-based GPUs (Graphic Processing Units) very easy and natural. Pytorch (Paszke
et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The
interested reader should consult a neural network textbook for further details; some
suggestions are at the end of the chapter.

dropout

hyperparameter

7.5 Neural Language Models

As our ﬁrst application of neural networks, let’s consider language modeling: pre-
dicting upcoming words from prior word context.
Neural net-based language models turn out to have many advantages over the n-
gram language models of Chapter 3. Among these are that neural language models
don’t need smoothing, they can handle much longer histories, and they can general-
ize over contexts of similar words. For a training set of a given size, a neural lan-
guage model has much higher predictive accuracy than an n-gram language model
Furthermore, neural language models underlie many of the models we’ll introduce
for tasks like machine translation, dialog, and language generation.
On the other hand, there is a cost for this improved performance: neural net
language models are strikingly slower to train than traditional language models, and
so for many tasks an n-gram language model is still the right tool.
In this chapter we’ll describe simple feedforward neural language models, ﬁrst
introduced by Bengio et al. (2003). Modern neural language models are generally
not feedforward but recurrent, using the technology that we will introduce in Chap-
ter 9.

146 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

A feedforward neural LM is a standard feedforward network that takes as input
at time t a representation of some number of previous words (wt−1 , wt−2 , etc) and
outputs a probability distribution over possible next words. Thus—like the n-gram
LM—the feedforward neural LM approximates the probability of a word given the
entire prior context P(wt |wt−1
) by approximating based on the N previous words:
P(wt |wt−1
) ≈ P(wt |wt−1
(7.27)
In the following examples we’ll use a 4-gram example, so we’ll show a net to
estimate the probability P(wt = i|wt−1 , wt−2 , wt−3 ).

t−N+1 )

1

1

7.5.1 Embeddings

In neural language models, the prior context is represented by embeddings of the
previous words. Representing the prior context as embeddings, rather than by ex-
act words as used in n-gram language models, allows neural language models to
generalize to unseen data much better than n-gram language models. For example,
suppose we’ve seen this sentence in training:
I have to make sure when I get home to feed the cat.
but we’ve never seen the word “dog” after the words ”feed the”. In our test set we
are trying to predict what comes after the preﬁx “I forgot when I got home to feed
the”.
An n-gram language model will predict “cat”, but not “dog”. But a neural LM,
which can make use of the fact that “cat” and “dog” have similar embeddings, will
be able to assign a reasonably high probability to “dog” as well as “cat”, merely
because they have similar vectors.
Let’s see how this works in practice. Let’s assume we have an embedding dic-
tionary E that gives us, for each word in our vocabulary V , the embedding for that
word, perhaps precomputed by an algorithm like word2vec from Chapter 6.
Fig. 7.12 shows a sketch of this simpliﬁed FFNNLM with N=3; we have a mov-
ing window at time t with an embedding vector representing each of the 3 previous
words (words wt−1 , wt−2 , and wt−3 ). These 3 vectors are concatenated together to
produce x, the input layer of a neural network whose output is a softmax with a
probability distribution over words. Thus y42 , the value of output node 42 is the
probability of the next word wt being V42 , the vocabulary word with index 42.
The model shown in Fig. 7.12 is quite sufﬁcient, assuming we learn the embed-
dings separately by a method like the word2vec methods of Chapter 6. The method
of using another algorithm to learn the embedding representations we use for input
words is called pretraining. If those pretrained embeddings are sufﬁcient for your
purposes, then this is all you need.
However, often we’d like to learn the embeddings simultaneously with training
the network. This is true when whatever task the network is designed for (sentiment
classiﬁcation, or translation, or parsing) places strong constraints on what makes a
good representation.
Let’s therefore show an architecture that allows the embeddings to be learned.
To do this, we’ll add an extra layer to the network, and propagate the error all the
way back to the embedding vectors, starting with embeddings with random values
and slowly moving toward sensible representations.
For this to work at the input layer, instead of pre-trained embeddings, we’re
going to represent each of the N previous words as a one-hot vector of length |V |, i.e.,
with one dimension for each word in the vocabulary. A one-hot vector is a vector

pretraining

one-hot vector

7 .5

• N EURA L LANGUAGE MOD EL S

147

Figure 7.12 A simpliﬁed view of a feedforward neural language model moving through a text. At each
timestep t the network takes the 3 context words, converts each to a d -dimensional embeddings, and concate-
nates the 3 embeddings together to get the 1 × N d unit input layer x for the network. These units are multiplied
by a weight matrix W and bias vector b and then an activation function to produce a hidden layer h, which
is then multiplied by another weight matrix U . (For graphic simplicity we don’t show b in this and future
pictures). Finally, a softmax output layer predicts at each node i the probability that the next word wt will be
vocabulary word Vi . (This picture is simpliﬁed because it assumes we just look up in an embedding dictionary
E the d -dimensional embedding vector for each word, precomputed by an algorithm like word2vec.)

that has one element equal to 1—in the dimension corresponding to that word’s
index in the vocabulary— while all the other elements are set to zero.
Thus in a one-hot representation for the word “toothpaste”, supposing it happens
to have index 5 in the vocabulary, x5 is one and and xi = 0 ∀i (cid:54)= 5, as shown here:

[0 0 0 0 1 0 0 ... 0 0 0 0]
1 2 3 4 5 6 7 ...
... |V|

Fig. 7.13 shows the additional layers needed to learn the embeddings during LM
training. Here the N=3 context words are represented as 3 one-hot vectors, fully
connected to the embedding layer via 3 instantiations of the E embedding matrix.
Note that we don’t want to learn separate weight matrices for mapping each of the 3
previous words to the projection layer, we want one single embedding dictionary E
that’s shared among these three. That’s because over time, many different words will
appear as wt−2 or wt−1 , and we’d like to just represent each word with one vector,
whichever context position it appears in. The embedding weight matrix E thus has
a row for each word, each a vector of d dimensions, and hence has dimensionality
V × d .
Let’s walk through the forward pass of Fig. 7.13.
1. Select three embeddings from E: Given the three previous words, we look
up their indices, create 3 one-hot vectors, and then multiply each by the em-
bedding matrix E . Consider wt−3 . The one-hot vector for ‘the’ is (index 35) is
multiplied by the embedding matrix E , to give the ﬁrst part of the ﬁrst hidden
layer, called the projection layer. Since each row of the input matrix E is just
an embedding for a word, and the input is a one-hot columnvector xi for word

projection layer

148 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

Figure 7.13

learning all the way back to embeddings. notice that the embedding matrix E is shared among
the 3 context words.

Vi , the projection layer for input w will be E xi = ei , the embedding for word i.
We now concatenate the three embeddings for the context words.
2. Multiply by W: We now multiply by W (and add b) and pass through the
rectiﬁed linear (or other) activation function to get the hidden layer h.
3. Multiply by U: h is now multiplied by U
4. Apply softmax: After the softmax, each node i in the output layer estimates
the probability P(wt = i|wt−1 , wt−2 , wt−3 )
In summary, if we use e to represent the projection layer, formed by concatenat-
ing the 3 embedding for the three context vectors, the equations for a neural language
model become:

e = (E x1 , E x2 , ..., E x)

h = σ (We + b)
z = U h
y = softmax(z)

(7.28)
(7.29)
(7.30)
(7.31)

7.5.2 Training the neural language model

To train the model, i.e. to set all the parameters θ = E ,W ,U , b, we do gradient de-
scent (Fig. 5.5), using error back propagation on the computation graph to compute
the gradient. Training thus not only sets the weights W and U of the network, but
also as we’re predicting upcoming words, we’re learning the embeddings E for each
words that best predict upcoming words.

7 .6

• SUMMARY

149

Generally training proceedings by taking as input a very long text, concatenating
all the sentences, start with random weights, and then iteratively moving through
the text predicting each word wt . At each word wt , the cross-entropy (negative log
likelihood) loss is:

The gradient is for this loss is then:

L = − log p(wt |wt−1 , ..., wt−n+1 )

θt+1 = θt − η

∂ − log p(wt |wt−1 , ..., wt−n+1 )

∂ θ

(7.32)

(7.33)

This gradient can be computed in any standard neural network framework which
will then backpropagate through U , W , b, E .
Training the parameters to minimize loss will result both in an algorithm for
language modeling (a word predictor) but also a new set of embeddings E that can
be used as word representations for other tasks.

7.6 Summary

• Neural networks are built out of neural units, originally inspired by human
neurons but now simple an abstract computational device.
• Each neural unit multiplies input values by a weight vector, adds a bias, and
then applies a non-linear activation function like sigmoid, tanh, or rectiﬁed
linear.
• In a fully-connected, feedforward network, each unit in layer i is connected
to each unit in layer i + 1, and there are no cycles.
• The power of neural networks comes from the ability of early layers to learn
representations that can be utilized by later layers in the network.
• Neural networks are trained by optimization algorithms like gradient de-

scent.
• Error back propagation, backward differentiation on a computation graph,

is used to compute the gradients of the loss function for a network.
• Neural language models use a neural network as a probabilistic classiﬁer, to
compute the probability of the next word given the previous n words.
• Neural language models can use pretrained embeddings, or can learn embed-
dings from scratch in the process of language modeling.

Bibliographical and Historical Notes

The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-
loch and Pitts, 1943), a simpliﬁed model of the human neuron as a kind of com-
puting element that could be described in terms of propositional logic. By the late
1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and
Bernard Widrow at Stanford) developed research into neural networks; this phase
saw the development of the perceptron (Rosenblatt, 1958), and the transformation
of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).

150 CHA PTER 7

• N EURA L N ETWORK S AND N EURA L LANGUAGE MODE L S

connectionist

The ﬁeld of neural networks declined after it was shown that a single percep-
tron unit was unable to model functions as simple as XOR (Minsky and Papert,
1969). While some small amount of work continued during the next two decades,
a major revival for the ﬁeld didn’t come until the 1980s, when practical tools for
building deeper networks like error back propagation became widespread (Rumel-
hart et al., 1986). During the 1980s a wide variety of neural network and related
architectures were developed, particularly for applications in psychology and cog-
nitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986,
Rumelhart and McClelland 1986a,Elman 1990), for which the term connection-
ist or parallel distributed processing was often used (Feldman and Ballard 1982,
Smolensky 1988). Many of the principles and techniques developed in this period
are foundational to modern work, including the ideas of distributed representations
(Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for com-
positionality (Smolensky, 1990).
By the 1990s larger neural networks began to be applied to many practical lan-
guage processing tasks as well, like handwriting recognition (LeCun et al. 1989,
LeCun et al. 1990) and speech recognition (Morgan and Bourlard 1989, Morgan
and Bourlard 1990). By the early 2000s, improvements in computer hardware and
advances in optimization and training techniques made it possible to train even larger
and deeper networks, leading to the modern term deep learning (Hinton et al. 2006,
Bengio et al. 2007). We cover more related history in Chapter 9.
There are a number of excellent books on the subject. Goldberg (2017) has a
superb and comprehensive coverage of neural networks for natural language pro-
cessing. For neural networks in general see Goodfellow et al. (2016) and Nielsen
(2015).

CHAPTER

8 Part-of-Speech Tagging

parts-of-speech

POS

Dionysius Thrax of Alexandria (c. 100 B .C .), or perhaps someone else (it was a long
time ago), wrote a grammatical sketch of Greek (a “techn ¯e”) that summarized the
linguistic knowledge of his day. This work is the source of an astonishing proportion
of modern linguistic vocabulary, including words like syntax, diphthong, clitic, and
analogy. Also included are a description of eight parts-of-speech: noun, verb,
pronoun, preposition, adverb, conjunction, participle, and article. Although earlier
scholars (including Aristotle as well as the Stoics) had their own lists of parts-of-
speech, it was Thrax’s set of eight that became the basis for practically all subsequent
part-of-speech descriptions of most European languages for the next 2000 years.
Schoolhouse Rock was a series of popular animated educational television clips
from the 1970s. Its Grammar Rock sequence included songs about exactly 8 parts-
of-speech, including the late great Bob Dorough’s Conjunction Junction:
Conjunction Junction, what’s your function?
Hooking up words and phrases and clauses...
Although the list of 8 was slightly modiﬁed from Thrax’s original, the astonishing
durability of the parts-of-speech through two millenia is an indicator of both the
importance and the transparency of their role in human language.1
Parts-of-speech (also known as POS, word classes, or syntactic categories) are
useful because they reveal a lot about a word and its neighbors. Knowing whether a
word is a noun or a verb tells us about likely neighboring words (nouns are preceded
by determiners and adjectives, verbs by nouns) and syntactic structure word (nouns
are generally part of noun phrases), making part-of-speech tagging a key aspect
of parsing (Chapter 11). Parts of speech are useful features for labeling named
entities like people or organizations in information extraction (Chapter 17), or for
coreference resolution (Chapter 20). A word’s part-of-speech can even play a role
in speech recognition or synthesis, e.g., the word content is pronounced CONtent
when it is a noun and conTENT when it is an adjective.
This chapter introduces parts-of-speech, and then introduces two algorithms for
part-of-speech tagging, the task of assigning parts-of-speech to words. One is
generative— Hidden Markov Model (HMM)—and one is discriminative—the Max-
imum Entropy Markov Model (MEMM). Chapter 9 then introduces a third algorithm
based on the recurrent neural network (RNN). All three have roughly equal perfor-
mance but, as we’ll see, have different tradeoffs.

8.1

(Mostly) English Word Classes

Until now we have been using part-of-speech terms like noun and verb rather
freely. In this section we give a more complete deﬁnition of these and other classes.
While word classes do have semantic tendencies—adjectives, for example, often

1 Nonetheless, eight isn’t very many and, as we’ll see, recent tagsets have more.

152 CHA PTER 8

• PART-O F -S P EECH TAGG ING

closed class
open class

function word

noun

proper noun

common noun
count noun
mass noun

verb

adjective

adverb

describe properties and nouns people— parts-of-speech are traditionally deﬁned in-
stead based on syntactic and morphological function, grouping words that have sim-
ilar neighboring words (their distributional properties) or take similar afﬁxes (their
morphological properties).
Parts-of-speech can be divided into two broad supercategories: closed class
types and open class types. Closed classes are those with relatively ﬁxed member-
ship, such as prepositions—new prepositions are rarely coined. By contrast, nouns
and verbs are open classes—new nouns and verbs like iPhone or to fax are contin-
ually being created or borrowed. Any given speaker or corpus may have different
open class words, but all speakers of a language, and sufﬁciently large corpora,
likely share the set of closed class words. Closed class words are generally function
words like of, it, and, or you, which tend to be very short, occur frequently, and
often have structuring uses in grammar.
Four major open classes occur in the languages of the world: nouns, verbs,
adjectives, and adverbs. English has all four, although not every language does.
The syntactic class noun includes the words for most people, places, or things, but
others as well. Nouns include concrete terms like ship and chair, abstractions like
bandwidth and relationship, and verb-like terms like pacing as in His pacing to and
fro became quite annoying. What deﬁnes a noun in English, then, are things like its
ability to occur with determiners (a goat, its bandwidth, Plato’s Republic), to take
possessives (IBM’s annual revenue), and for most but not all nouns to occur in the
plural form (goats, abaci).
Open class nouns fall into two classes. Proper nouns, like Regina, Colorado,
and IBM, are names of speciﬁc persons or entities. In English, they generally aren’t
preceded by articles (e.g., the book is upstairs, but Regina is upstairs). In written
English, proper nouns are usually capitalized. The other class, common nouns, are
divided in many languages, including English, into count nouns and mass nouns.
Count nouns allow grammatical enumeration, occurring in both the singular and plu-
ral (goat/goats, relationship/relationships) and they can be counted (one goat, two
goats). Mass nouns are used when something is conceptualized as a homogeneous
group. So words like snow, salt, and communism are not counted (i.e., *two snows
or *two communisms). Mass nouns can also appear without articles where singular
count nouns cannot (Snow is white but not *Goat is white).
Verbs refer to actions and processes, including main verbs like draw, provide,
and go. English verbs have inﬂections (non-third-person-sg (eat), third-person-sg
(eats), progressive (eating), past participle (eaten)). While many researchers believe
that all human languages have the categories of noun and verb, others have argued
that some languages, such as Riau Indonesian and Tongan, don’t even make this
distinction (Broschart 1997; Evans 2000; Gil 2000) .
The third open class English form is adjectives, a class that includes many terms
for properties or qualities. Most languages have adjectives for the concepts of color
(white, black), age (old, young), and value (good, bad), but there are languages
without adjectives.
In Korean, for example, the words corresponding to English
adjectives act as a subclass of verbs, so what is in English an adjective “beautiful”
acts in Korean like a verb meaning “to be beautiful”.
The ﬁnal open class form, adverbs, is rather a hodge-podge in both form and
meaning. In the following all the italicized words are adverbs:
Actually, I ran home extremely quickly yesterday
What coherence the class has semantically may be solely that each of these
words can be viewed as modifying something (often verbs, hence the name “ad-

8 .1

•

(MO ST LY ) ENG L I SH WORD C LA S SE S

153

verb”, but also other adverbs and entire verb phrases). Directional adverbs or loca-
tive adverbs (home, here, downhill) specify the direction or location of some action;
degree adverbs (extremely, very, somewhat) specify the extent of some action, pro-
cess, or property; manner adverbs (slowly, slinkily, delicately) describe the manner
of some action or process; and temporal adverbs describe the time that some ac-
tion or event took place (yesterday, Monday). Because of the heterogeneous nature
of this class, some adverbs (e.g., temporal adverbs like Monday) are tagged in some
tagging schemes as nouns.
The closed classes differ more from language to language than do the open
classes. Some of the important closed classes in English include:
prepositions: on, under, over, near, by, at, from, to, with
particles: up, down, on, off, in, out, at, by

determiners: a, an, the

conjunctions: and, but, or, as, if, when
pronouns: she, who, I, others
auxiliary verbs: can, may, should, are
numerals: one, two, three, ﬁrst, second, third
Prepositions occur before noun phrases. Semantically they often indicate spatial
or temporal relations, whether literal (on it, before then, by the house) or metaphor-
ical (on time, with gusto, beside herself), but often indicate other relations as well,
like marking the agent in (Hamlet was written by Shakespeare, A particle resembles
a preposition or an adverb and is used in combination with a verb. Particles often
have extended meanings that aren’t quite the same as the prepositions they resemble,
as in the particle over in she turned the paper over.
A verb and a particle that act as a single syntactic and/or semantic unit are
called a phrasal verb. The meaning of phrasal verbs is often problematically non-
compositional—not predictable from the distinct meanings of the verb and the par-
ticle. Thus, turn down means something like ‘reject’, rule out ‘eliminate’, ﬁnd out
‘discover’, and go on ‘continue’.
A closed class that occurs with nouns, often marking the beginning of a noun
phrase, is the determiner. One small subtype of determiners is the article: English
has three articles: a, an, and the. Other determiners include this and that (this chap-
ter, that page). A and an mark a noun phrase as indeﬁnite, while the can mark it
as deﬁnite; deﬁniteness is a discourse property (Chapter 21). Articles are quite fre-
quent in English; indeed, the is the most frequently occurring word in most corpora
of written English, and a and an are generally right behind.
Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc-
tions like and, or, and but join two elements of equal status. Subordinating conjunc-
tions are used when one of the elements has some embedded status. For example,
that in “I thought that you might like some milk” is a subordinating conjunction
that links the main clause I thought with the subordinate clause you might like some
milk. This clause is called subordinate because this entire clause is the “content” of
the main verb thought. Subordinating conjunctions like that which link a verb to its
argument in this way are also called complementizers.
Pronouns are forms that often act as a kind of shorthand for referring to some
noun phrase or entity or event. Personal pronouns refer to persons or entities (you,
she, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that in-
dicate either actual possession or more often just an abstract relation between the
person and some object (my, your, his, her, its, one’s, our, their). Wh-pronouns
(what, who, whom, whoever) are used in certain question forms, or may also act as

locative
degree
manner
temporal

preposition

particle

phrasal verb

determiner
article

conjunctions

complementizer
pronoun
personal
possessive

wh

154 CHA PTER 8

• PART-O F -S P EECH TAGG ING

auxiliary

copula
modal

interjection
negative

complementizers (Frida, who married Diego. . . ).
A closed class subtype of English verbs are the auxiliary verbs. Cross-linguist-
ically, auxiliaries mark semantic features of a main verb: whether an action takes
place in the present, past, or future (tense), whether it is completed (aspect), whether
it is negated (polarity), and whether an action is necessary, possible, suggested, or
desired (mood). English auxiliaries include the copula verb be, the two verbs do and
have, along with their inﬂected forms, as well as a class of modal verbs. Be is called
a copula because it connects subjects with certain kinds of predicate nominals and
adjectives (He is a duck). The verb have can mark the perfect tenses (I have gone, I
had gone), and be is used as part of the passive (We were robbed) or progressive (We
are leaving) constructions. Modals are used to mark the mood associated with the
event depicted by the main verb: can indicates ability or possibility, may permission
or possibility, must necessity. There is also a modal use of have (e.g., I have to go).
English also has many words of more or less unique function, including inter-
jections (oh, hey, alas, uh, um), negatives (no, not), politeness markers (please,
thank you), greetings (hello, goodbye), and the existential there (there are two on
the table) among others. These classes may be distinguished or lumped together as
interjections or adverbs depending on the purpose of the labeling.

8.2 The Penn Treebank Part-of-Speech Tagset

An important tagset for English is the 45-tag Penn Treebank tagset (Marcus et al.,
1993), shown in Fig. 8.1, which has been used to label many corpora.
In such
labelings, parts-of-speech are generally represented by placing the tag after each
word, delimited by a slash:

Tag

Description

Example

Tag Description

Example Tag Description Example

CC

CD
DT
EX

IN

and, but, or PDT predeterminer

PRP

one, two
a, the
there
mea culpa RB
of, in, by

coordinating
conjunction
cardinal number
POS possessive ending ’s
determiner
personal pronoun
existential ‘there’
PRP$ possess. pronoun
FW foreign word
adverb
preposition/
RBR comparative
subordin-conj
adverb
RBS superlatv. adverb
adjective
comparative adj
particle
superlative adj
SYM symbol
list item marker
“to”
modal
interjection
sing or mass noun llama
verb base form
noun, plural
VBD verb past tense
proper noun, sing.
VBG verb gerund
NNPS proper noun, plu. Carolinas VBN verb past part.

fastest
up, off
+,%, &
to
ah, oops
eat
ate
eating
eaten
Figure 8.1 Penn Treebank part-of-speech tags (including punctuation).

yellow
bigger
wildest
1, 2, One
TO
can, should UH
VB

JJ
JJR
JJS
LS
MD
NN
NNS
NNP

llamas
IBM

RP

all, both

VBP verb non-3sg
present
VBZ verb 3sg pres

I, you, he WDT wh-determ.
your, one’s WP
quickly
faster

wh-pronoun
WP$ wh-possess.
WRB wh-adverb

eat

eats
which, that
what, who
whose
how, where

$
#
“
”
(
)
,
.
:

$
#
‘ or “
’ or ”
[, (, {, <
], ), }, >
,

dollar sign
pound sign
left quote
right quote
left paren
right paren
comma
sent-end punc . ! ?
sent-mid punc : ; ... – -

(8.1) The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN
other/JJ topics/NNS ./.
(8.2) There/EX are/VBP 70/CD children/NNS there/RB

Brown

WSJ
Switchboard

8 .2

• TH E P ENN TR EEBANK PART-O F -S P EECH TAG SET

155

(8.3) Preliminary/JJ ﬁndings/NNS were/VBD reported/VBN in/IN today/NN
’s/POS New/NNP England/NNP Journal/NNP of/IN Medicine/NNP ./.
Example (8.1) shows the determiners the and a, the adjectives grand and other,
the common nouns jury, number, and topics, and the past tense verb commented.
Example (8.2) shows the use of the EX tag to mark the existential there construction
in English, and, for comparison, another use of there which is tagged as an adverb
(RB). Example (8.3) shows the segmentation of the possessive morpheme ’s a pas-
sive construction, ‘were reported’, in which reported is marked as a past participle
(VBN). Note that since New England Journal of Medicine is a proper noun, the Tree-
bank tagging chooses to mark each noun in it separately as NNP, including journal
and medicine, which might otherwise be labeled as common nouns (NN).
Corpora labeled with parts-of-speech are crucial training (and testing) sets for
statistical tagging algorithms. Three main tagged corpora are consistently used for
training and testing part-of-speech taggers for English. The Brown corpus is a mil-
lion words of samples from 500 written texts from different genres published in the
United States in 1961. The WSJ corpus contains a million words published in the
Wall Street Journal in 1989. The Switchboard corpus consists of 2 million words
of telephone conversations collected in 1990-1991. The corpora were created by
running an automatic part-of-speech tagger on the texts and then human annotators
hand-corrected each tag.
There are some minor differences in the tagsets used by the corpora. For example
in the WSJ and Brown corpora, the single Penn tag TO is used for both the inﬁnitive
to (I like to race) and the preposition to (go to the store), while in Switchboard the
tag TO is reserved for the inﬁnitive use of to and the preposition is tagged IN:
Well/UH ,/, I/PRP ,/, I/PRP want/VBP to/TO go/VB to/IN a/DT restauran-
t/NN
Finally, there are some idiosyncracies inherent in any tagset. For example, be-
cause the Penn 45 tags were collapsed from a larger 87-tag tagset, the original
Brown tagset, some potential useful distinctions were lost. The Penn tagset was
designed for a treebank in which sentences were parsed, and so it leaves off syntac-
tic information recoverable from the parse tree. Thus for example the Penn tag IN is
used for both subordinating conjunctions like if, when, unless, after:
after/IN spending/VBG a/DT day/NN at/IN the/DT beach/NN
and prepositions like in, on, after:
after/IN sunrise/NN
Words are generally tokenized before tagging. The Penn Treebank and the
British National Corpus split contractions and the ’s-genitive from their stems:2
would/MD n’t/RB
children/NNS ’s/POS
The Treebank tagset assumes that tokenization of multipart words like New
York is done at whitespace, thus tagging. a New York City ﬁrm as a/DT New/NNP
York/NNP City/NNP ﬁrm/NN.
Another commonly used tagset, the Universal POS tag set of the Universal De-
pendencies project (Nivre et al., 2016a), is used when building systems that can tag
many languages. See Section 8.7.

2

Indeed, the Treebank tag POS is used only for ’s, which must be segmented in tokenization.

156 CHA PTER 8

• PART-O F -S P EECH TAGG ING

8.3 Part-of-Speech Tagging

part-of-speech
tagging

ambiguous

ambiguity
resolution

Part-of-speech tagging is the process of assigning a part-of-speech marker to each
word in an input text.3 The input to a tagging algorithm is a sequence of (tokenized)
words and a tagset, and the output is a sequence of tags, one per token.
Tagging is a disambiguation task; words are ambiguous —have more than one
possible part-of-speech—and the goal is to ﬁnd the correct tag for the situation.
For example, book can be a verb (book that ﬂight) or a noun (hand me that book).
That can be a determiner (Does that ﬂight serve dinner) or a complementizer (I
thought that your ﬂight was earlier). The goal of POS-tagging is to resolve these
ambiguities, choosing the proper tag for the context. How common is tag ambiguity?
Fig. 8.2 shows that most word types (80-86%) are unambiguous (Janet is always
NNP, funniest JJS, and hesitantly RB). But the ambiguous words, though accounting
for only 14-15% of the vocabulary, are very common words, and hence 55-67% of
word tokens in running text are ambiguous.4

Types:
Unambiguous (1 tag)
Ambiguous
Tokens:
Unambiguous (1 tag)
Ambiguous

(2+ tags)

(2+ tags)

WSJ

44,432 (86%)
7,025 (14%)

Brown

45,799 (85%)
8,050 (15%)

577,421 (45%) 384,349 (33%)
711,780 (55%) 786,646 (67%)

Figure 8.2

Tag ambiguity for word types in Brown and WSJ, using Treebank-3 (45-tag)
tagging. Punctuation were treated as words, and words were kept in their original case.

Some of the most ambiguous frequent words are that, back, down, put and set;
here are some examples of the 6 different parts-of-speech for the word back:
earnings growth took a back/JJ seat
a small building in the back/NN
a clear majority of senators back/VBP the bill
Dave began to back/VB toward the door
enable the country to buy back/RP about debt
I was twenty-one back/RB then
Nonetheless, many words are easy to disambiguate, because their different tags
aren’t equally likely. For example, a can be a determiner or the letter a, but the
determiner sense is much more likely. This idea suggests a simplistic baseline algo-
rithm for part-of-speech tagging: given an ambiguous word, choose the tag which is
most frequent in the training corpus. This is a key concept:
Most Frequent Class Baseline: Always compare a classiﬁer against a baseline at
least as good as the most frequent class baseline (assigning each token to the class
it occurred in most often in the training set).

accuracy

How good is this baseline? A standard way to measure the performance of part-
of-speech taggers is accuracy: the percentage of tags correctly labeled (matching

3 Tags are also applied to punctuation, so assumes tokenzing of commas, quotation marks, etc., and
disambiguating end-of-sentence periods from periods inside words (e.g., etc.).
4 Note the large differences across the two genres, especially in token frequency. Tags in the WSJ corpus
are less ambiguous; its focus on ﬁnancial news leads to a more limited distribution of word usages than
the diverse genres of the Brown corpus.

8 .4

• HMM PART-O F -S PE ECH TAGG ING

157

human labels on a test set). If we train on the WSJ training corpus and test on sec-
tions 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy
of 92.34%. By contrast, the state of the art in part-of-speech tagging on this dataset
is around 97% tag accuracy, a performance that is achievable by most algorithms
(HMMs, MEMMs, neural networks, rule-based algorithms). See Section 8.7 on
other languages and genres.

8.4 HMM Part-of-Speech Tagging

sequence model

Markov chain

In this section we introduce the use of the Hidden Markov Model for part-of-speech
tagging. The HMM is a sequence model. A sequence model or sequence classi-
ﬁer is a model whose job is to assign a label or class to each unit in a sequence,
thus mapping a sequence of observations to a sequence of labels. An HMM is a
probabilistic sequence model: given a sequence of units (words, letters, morphemes,
sentences, whatever), it computes a probability distribution over possible sequences
of labels and chooses the best label sequence.

8.4.1 Markov Chains

The HMM is based on augmenting the Markov chain. A Markov chain is a model
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. These sets can be words, or
tags, or symbols representing anything, for example the weather. A Markov chain
makes a very strong assumption that if we want to predict the future in the sequence,
all that matters is the current state. All the states before the current state have no im-
pact on the future except via the current state. It’s as if to predict tomorrow’s weather
you could examine today’s weather but you weren’t allowed to look at yesterday’s
weather.

(a)

(b)

Figure 8.3 A Markov chain for weather (a) and one for words (b), showing states and
transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.

Markov
assumption

More formally, consider a sequence of state variables q1 , q2 , ..., qi . A Markov
model embodies the Markov assumption on the probabilities of this sequence: that
when predicting the future, the past doesn’t matter, only the present.
Markov Assumption: P(qi = a|q1 ...qi−1 ) = P(qi = a|qi−1 )
Figure 8.3a shows a Markov chain for assigning a probability to a sequence of
weather events, for which the vocabulary consists of HOT, COLD, and WARM . The

(8.4)

158 CHA PTER 8

• PART-O F -S P EECH TAGG ING

states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. The transitions are probabilities: the values of arcs leaving a given
state must sum to 1. Figure 8.3b shows a Markov chain for assigning a probability to
a sequence of words w1 ...wn . This Markov chain should be familiar; in fact, it repre-
sents a bigram language model, with each edge expressing the probability p(wi |w j )!
Given the two models in Fig. 8.3, we can assign a probability to any sequence from
our vocabulary.
Formally, a Markov chain is speciﬁed by the following components:
a set of N states

Q = q1q2 . . . qN
A = a11 a12 . . . an1 . . . ann

a transition probability matrix A, each ai j represent-

ing the probability of moving from state i to state j, s.t.

j=1 ai j = 1 ∀i

(cid:80)n

π = π1 , π2 , ..., πN

an initial probability distribution over states. πi is the

probability that the Markov chain will start in state i.
Some states j may have π j = 0, meaning that they cannot
be initial states. Also, (cid:80)n
Before you go on, use the sample probabilities in Fig. 8.3a (with π = [.1, .7., 2])
to compute the probability of each of the following sequences:

i=1 πi = 1

(8.5) hot hot hot hot
(8.6) cold hot cold hot

What does the difference in these probabilities tell you about a real-world weather
fact encoded in Fig. 8.3a?

8.4.2 The Hidden Markov Model

A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden: we don’t observe them directly. For example we don’t normally observe
part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
word sequence. We call the tags hidden because they are not observed.
A hidden Markov model (HMM) allows us to talk about both observed events
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. An HMM is speciﬁed by
the following components:
a set of N states
a transition probability matrix A, each ai j representing the probability
of moving from state i to state j , s.t. (cid:80)N
a sequence of T observations, each one drawn from a vocabulary V =

j=1 ai j = 1 ∀i

v1 , v2 , ..., vV

hidden

Hidden
Markov model

Q = q1 q2 . . . qN
A = a11 . . . ai j . . . aNN

O = o1 o2 . . . oT

B = bi (ot )

π = π1 , π2 , ..., πN

a sequence of observation likelihoods, also called emission probabili-

ties, each expressing the probability of an observation ot being generated
from a state i
an initial probability distribution over states. πi is the probability that
the Markov chain will start in state i. Some states j may have π j = 0,
meaning that they cannot be initial states. Also, (cid:80)n

i=1 πi = 1

8 .4

• HMM PART-O F -S PE ECH TAGG ING

159

A ﬁrst-order hidden Markov model instantiates two simplifying assumptions.
First, as with a ﬁrst-order Markov chain, the probability of a particular state depends
only on the previous state:

Markov Assumption: P(qi |q1 ...qi−1 ) = P(qi |qi−1 )
Second, the probability of an output observation oi depends only on the state that
produced the observation qi and not on any other states or any other observations:

(8.7)

Output Independence: P(oi |q1 . . . qi , . . . , qT , o1 , . . . , oi , . . . , oT ) = P(oi |qi )

(8.8)

8.4.3 The components of an HMM tagger

Let’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use
it to tag. An HMM has two components, the A and B probabilities.
The A matrix contains the tag transition probabilities P(ti |ti−1 ) which represent
the probability of a tag occurring given the previous tag. For example, modal verbs
like will are very likely to be followed by a verb in the base form, a VB, like race, so
we expect this probability to be high. We compute the maximum likelihood estimate
of this transition probability by counting, out of the times we see the ﬁrst tag in a
labeled corpus, how often the ﬁrst tag is followed by the second:

P(ti |ti−1 ) =

C(ti−1 , ti )
C(ti−1 )

(8.9)

In the WSJ corpus, for example, MD occurs 13124 times of which it is followed
by VB 10471, for an MLE estimate of

=

= .80

C(MD,V B)
10471
P(V B|MD) =
C(MD)
13124
Let’s walk through an example, seeing how these probabilities are estimated and
used in a sample tagging task, before we return to the algorithm for decoding.
In HMM tagging, the probabilities are estimated by counting on a tagged training
corpus. For this example we’ll use the tagged WSJ corpus.
The B emission probabilities, P(wi |ti ), represent the probability, given a tag (say
MD), that it will be associated with a given word (say will). The MLE of the emis-
sion probability is

(8.10)

P(wi |ti ) =

C(ti , wi )
C(ti )

(8.11)

=

= .31

Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046
times:

C(MD, wil l )
4046
P(wil l |MD) =
C(MD)
13124
We saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood
term is not asking “which is the most likely tag for the word will?” That would be
the posterior P(MD|will). Instead, P(will|MD) answers the slightly counterintuitive
question “If we were going to generate a MD, how likely is it that this modal would
be will?”
The A transition probabilities, and B observation likelihoods of the HMM are
illustrated in Fig. 8.4 for three states in an HMM part-of-speech tagger; the full
tagger would have one state for each tag.

(8.12)

160 CHA PTER 8

• PART-O F -S P EECH TAGG ING

Figure 8.4 An illustration of the two parts of an HMM representation:
the A transition
probabilities used to compute the prior probability, and the B observation likelihoods that are
associated with each state, one likelihood for each possible observation word.

8.4.4 HMM tagging as decoding

decoding

For any model, such as an HMM, that contains hidden variables, the task of deter-
mining the hidden variables sequence corresponding to the sequence of observations
is called decoding. More formally,
Decoding: Given as input an HMM λ = (A, B) and a sequence of ob-
servations O = o1 , o2 , ..., oT , ﬁnd the most probable sequence of states

Q = q1q2q3 . . . qT .

For part of speech tagging, the goal of HMM decoding is to choose the tag
sequence t n
1 that is most probable given the observation sequence of n words words
wn
1 :
P(t n
1 |wn
The way we’ll do this in the HMM is to use Bayes’ rule to instead compute:
P(wn
1 |t n
P(wn

ˆt n
1 = argmax

ˆt n
1 = argmax

(8.14)

(8.13)

1 )

t n

1 )P(t n
1 )
1 )

1

t n

1

Furthermore, we simplify Eq. 8.14 by dropping the denominator P(wn
ˆt n
1 = argmax
P(wn
1 |t n
HMM taggers make two further simplifying assumptions. The ﬁrst is that the
probability of a word appearing depends only on its own tag and is independent of
neighboring words and tags:

1 )P(t n
1 )

(8.15)

1 ):

t n

1

P(wn
1 |t n
P(wi |ti )
The second assumption, the bigram assumption, is that the probability of a tag
is dependent only on the previous tag, rather than the entire tag sequence;

1 ) ≈

(8.16)

n(cid:89)i=1

P(t n

1 ) ≈

n(cid:89)i=1

P(ti |ti−1 )

(8.17)

8 .4

• HMM PART-O F -S PE ECH TAGG ING

161

Plugging the simplifying assumptions from Eq. 8.16 and Eq. 8.17 into Eq. 8.15
results in the following equation for the most probable tag sequence from a bigram
tagger:

ˆt n
1 = argmax

emission
transition
P(t n
1 |wn
1 ) ≈ argmax
P(wi |ti )
P(ti |ti−1 )
The two parts of Eq. 8.18 correspond neatly to the B emission probability and
A transition probability that we just deﬁned above!

(cid:122) (cid:125)(cid:124) (cid:123)

n(cid:89)i=1

(cid:125)(cid:124)

(8.18)

(cid:122)

(cid:123)

t n

1

t n

1

8.4.5 The Viterbi Algorithm

Viterbi
algorithm

The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.5. As
an instance of dynamic programming, Viterbi resembles the dynamic program-
ming minimum edit distance algorithm of Chapter 2.

function V I TERB I(observations of len T,state-graph of len N) returns best-path, path-prob

; recursion step

; initialization step

create a path probability matrix viterbi[N,T]
for each state s from 1 to N do
viterbi[s,1] ← πs ∗ bs (o1 )
backpointer[s,1] ← 0
for each time step t from 2 to T do
for each state s from 1 to N do
viterbi[s,t] ←
Nmax
viterbi[s(cid:48) , t − 1] ∗ as(cid:48) ,s ∗ bs (ot )
backpointer[s,t] ←
Nargmax
viterbi[s(cid:48) , t − 1] ∗ as(cid:48) ,s ∗ bs (ot )
bestpathprob ←
Nmax
viterbi[s, T ]
; termination step
Nargmax
bestpathpointer ←
viterbi[s, T ]
; termination step
bestpath ← the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob

s(cid:48) =1

s(cid:48) =1

s=1

s=1

Figure 8.5 Viterbi algorithm for ﬁnding the optimal sequence of tags. Given an observation sequence and an
HMM λ = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood to
the observation sequence.

The Viterbi algorithm ﬁrst sets up a probability matrix or lattice, with one col-
umn for each observation ot and one row for each state in the state graph. Each col-
umn thus has a cell for each state qi in the single combined automaton. Figure 8.6
shows an intuition of this lattice for the sentence Janet will back the bill.
Each cell of the trellis, vt ( j), represents the probability that the HMM is in state
j after seeing the ﬁrst t observations and passing through the most probable state
sequence q1 , ..., qt−1 , given the HMM λ . The value of each cell vt ( j) is computed
by recursively taking the most probable path that could lead us to this cell. Formally,
each cell expresses the probability

vt ( j) = max

q1 ,...,qt−1

P(q1 ...qt−1 , o1 , o2 . . . ot , qt = j|λ )

(8.19)

We represent the most probable path by taking the maximum over all possible
previous state sequences max
. Like other dynamic programming algorithms,

q1 ,...,qt−1

162 CHA PTER 8

• PART-O F -S P EECH TAGG ING

Figure 8.6 A sketch of the lattice for Janet will back the bill, showing the possible tags (qi )
for each word and highlighting the path corresponding to the correct tag sequence through the
hidden states. States (parts-of-speech) which have a zero probability of generating a particular
word according to the B matrix (such as the probability that a determiner DT will be realized
as Janet) are greyed out.

Viterbi ﬁlls each cell recursively. Given that we had already computed the probabil-
ity of being in every state at time t − 1, we compute the Viterbi probability by taking
the most probable of the extensions of the paths that lead to the current cell. For a
given state q j at time t , the value vt ( j) is computed as
Nmax

(8.20)

vt ( j) =

vt−1 (i) ai j b j (ot )

i=1

The three factors that are multiplied in Eq. 8.20 for extending the previous paths to
compute the Viterbi probability at time t are

vt−1 (i)

ai j

b j (ot )

the previous Viterbi path probability from the previous time step

the transition probability from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

8.4.6 Working through an example

Let’s tag the sentence Janet will back the bill; the goal is the correct series of tags
(see also Fig. 8.6):
(8.21) Janet/NNP will/MD back/VB the/DT bill/NN
Let the HMM be deﬁned by the two tables in Fig. 8.7 and Fig. 8.8. Figure 8.7
lists the ai j probabilities for transitioning between the hidden states (part-of-speech
tags). Figure 8.8 expresses the bi (ot ) probabilities, the observation likelihoods of
words given tags. This table is (slightly simpliﬁed) from counts in the WSJ corpus.
So the word Janet only appears as an NNP, back has 4 possible parts of speech, and
the word the can appear as a determiner or as an NNP (in titles like “Somewhere
Over the Rainbow” all words are tagged as NNP).
Figure 8.9 shows a ﬂeshed-out version of the sketch we saw in Fig. 8.6, the
Viterbi trellis for computing the best hidden state sequence for the observation se-
quence Janet will back the bill.

8 .4

• HMM PART-O F -S PE ECH TAGG ING

163

<s >

NNP
MD
VB
JJ
NN
RB
DT

NNP

0.2767
0.3777
0.0008
0.0322
0.0366
0.0096
0.0068
0.1147

MD

0.0006
0.0110
0.0002
0.0005
0.0004
0.0176
0.0102
0.0021

VB

JJ

NN

RB

DT

0.0031 0.0453 0.0449 0.0510 0.2026
0.0009 0.0084 0.0584 0.0090 0.0025
0.7968 0.0005 0.0008 0.1698 0.0041
0.0050 0.0837 0.0615 0.0514 0.2231
0.0001 0.0733 0.4509 0.0036 0.0036
0.0014 0.0086 0.1216 0.0177 0.0068
0.1011 0.1012 0.0120 0.0728 0.0479
0.0002 0.2157 0.4744 0.0102 0.0017

Figure 8.7 The A transition probabilities P(ti |ti−1 ) computed from the WSJ corpus without
smoothing. Rows are labeled with the conditioning event; thus P(V B|MD) is 0.7968.

NNP
MD
VB
JJ
NN
RB
DT

Janet

will

back

the

bill

0.000032 0
0
0.000048 0
0
0.308431 0
0
0
0
0.000028 0.000672 0
0.000028
0
0
0.000340 0
0
0
0.000200 0.000223 0
0.002337
0
0
0.010446 0
0
0
0
0
0.506099 0

Figure 8.8 Observation likelihoods B computed from the WSJ corpus without smoothing,
simpliﬁed slightly.

There are N = 5 state columns. We begin in column 1 (for the word Janet) by
setting the Viterbi value in each cell to the product of the π transition probability
(the start probability for that state i, which we get from the < s > entry of Fig. 8.7),
and the observation likelihood of the word Janet given the tag for that cell. Most of
the cells in the column are zero since the word Janet cannot be any of those tags.
The reader should ﬁnd this in Fig. 8.9.
Next, each cell in the will column gets updated. For each state, we compute the
value viterbi[s, t ] by taking the maximum over the extensions of all the paths from the
previous column that lead to the current cell according to Eq. 8.20. We have shown
the values for the MD, VB, and NN cells. Each cell gets the max of the 7 values
from the previous column, multiplied by the appropriate transition probability; as it
happens in this case, most of them are zero from the previous column. The remaining
value is multiplied by the relevant observation probability, and the (trivial) max is
taken. In this case the ﬁnal value, .0000002772, comes from the NNP state at the
previous column. The reader should ﬁll in the rest of the trellis in Fig. 8.9 and
backtrace to reconstruct the correct state sequence NNP MD VB DT NN.

8.4.7 Extending the HMM Algorithm to Trigrams

Practical HMM taggers have a number of extensions of this simple model. One
important missing feature is a wider tag context. In the tagger described above the
probability of a tag depends only on the previous tag:

P(t n

1 ) ≈

n(cid:89)i=1

P(ti |ti−1 )

(8.22)

In practice we use more of the history, letting the probability of a tag depend on

164 CHA PTER 8

• PART-O F -S P EECH TAGG ING

Figure 8.9 The ﬁrst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps the
probability of the best path so far and a pointer to the previous cell along that path. We have only ﬁlled out
columns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the
reader. After the cells are ﬁlled in, backtracing from the end state, we should be able to reconstruct the correct
state sequence NNP MD VB DT NN.

the two previous tags:

P(t n

1 ) ≈

n(cid:89)i=1

P(ti |ti−1 , ti−2 )

(8.23)

Extending the algorithm from bigram to trigram taggers gives a small (perhaps a
half point) increase in performance, but conditioning on two previous tags instead of
one requires a signiﬁcant change to the Viterbi algorithm. For each cell, instead of
taking a max over transitions from each cell in the previous column, we have to take
a max over paths through the cells in the previous two columns, thus considering N 2
rather than N hidden states at every observation.
In addition to increasing the context window, HMM taggers have a number of
other advanced features. One is to let the tagger know the location of the end of the
sentence by adding dependence on an end-of-sequence marker for tn+1 . This gives
the following equation for part-of-speech tagging:

ˆt n
1 = argmax

P(t n
1 |wn
1 ) ≈ argmax
In tagging any sentence with Eq. 8.24, three of the tags used in the context will
fall off the edge of the sentence, and hence will not match regular words. These tags,

P(wi |ti )P(ti |ti−1 , ti−2 )(cid:35) P(tn+1 |tn )

(cid:34) n(cid:89)i=1

t n

1

t n

1

(8.24)

8 .4

• HMM PART-O F -S PE ECH TAGG ING

165

t−1 , t0 , and tn+1 , can all be set to be a single special ‘sentence boundary’ tag that is
added to the tagset, which assumes sentences boundaries have already been marked.
One problem with trigram taggers as instantiated in Eq. 8.24 is data sparsity.
Any particular sequence of tags ti−2 , ti−1 , ti that occurs in the test set may simply
never have occurred in the training set. That means we cannot compute the tag
trigram probability just by the maximum likelihood estimate from counts, following
Eq. 8.25:

Just as we saw with language modeling, many of these counts will be zero
in any training set, and we will incorrectly predict that a given tag sequence will
never occur! What we need is a way to estimate P(ti |ti−1 , ti−2 ) even if the sequence
ti−2 , ti−1 , ti never occurs in the training data.
The standard approach to solving this problem is the same interpolation idea
we saw in language modeling: estimate the probability by combining more robust,
but weaker estimators. For example, if we’ve never seen the tag sequence PRP VB
TO, and so can’t compute P(TO|PRP,VB) from this frequency, we still could rely
on the bigram probability P(TO|VB), or even the unigram probability P(TO). The
maximum likelihood estimation of each of these probabilities can be computed from
a corpus with the following counts:

P(ti |ti−1 , ti−2 ) =

C(ti−2 , ti−1 , ti )
C(ti−2 , ti−1 )

(8.25)

Trigrams

ˆP(ti |ti−1 , ti−2 ) =

Bigrams

ˆP(ti |ti−1 ) =

Unigrams

ˆP(ti ) =

C(ti−2 , ti−1 , ti )
C(ti−2 , ti−1 )
C(ti−1 , ti )
C(ti−1 )
C(ti )

N

(8.26)

(8.27)

(8.28)

The standard way to combine these three estimators to estimate the trigram probabil-
ity P(ti |ti−1 , ti−2 ) is via linear interpolation. We estimate the probability P(ti |ti−1ti−2 )
by a weighted sum of the unigram, bigram, and trigram probabilities:

P(ti |ti−1ti−2 ) = λ3 ˆP(ti |ti−1ti−2 ) + λ2 ˆP(ti |ti−1 ) + λ1 ˆP(ti )

(8.29)

We require λ1 + λ2 + λ3 = 1, ensuring that the resulting P is a probability distri-
bution. The λ s are set by deleted interpolation (Jelinek and Mercer, 1980): we
successively delete each trigram from the training corpus and choose the λ s so as to
maximize the likelihood of the rest of the corpus. The deletion helps to set the λ s
in such a way as to generalize to unseen data and not overﬁt. Figure 8.10 gives a
deleted interpolation algorithm for tag trigrams.

deleted
interpolation

8.4.8 Beam Search

When the number of states grows very large, the vanilla Viterbi algorithm be slow.
The complexity of the algorithm is O(N 2T ); N (the number of states) can be large
for trigram taggers, which have to consider every previous pair of the 45 tags, re-
sulting in 453 = 91, 125 computations per column. N can be even larger for other
applications of Viterbi, for example to decoding in neural networks, as we will see
in future chapters.
One common solution to the complexity problem is the use of beam search
decoding. In beam search, instead of keeping the entire column of states at each

beam search

166 CHA PTER 8

• PART-O F -S P EECH TAGG ING

function D EL ET ED - IN TER PO LAT ION(corpus) returns λ1 , λ2 , λ3
λ1 , λ2 , λ3 ← 0
foreach trigram t1 , t2 , t3 with C(t1 , t2 , t3 ) > 0
depending on the maximum of the following three values
C(t1 ,t2 )−1 : increment λ3 by C(t1 , t2 , t3 )
C(t2 )−1 : increment λ2 by C(t1 , t2 , t3 )
N−1 : increment λ1 by C(t1 , t2 , t3 )

case C(t1 ,t2 ,t3 )−1
case C(t2 ,t3 )−1
case C(t3 )−1

end
end

normalize λ1 , λ2 , λ3

return λ1 , λ2 , λ3

Figure 8.10 The deleted interpolation algorithm for setting the weights for combining un-
igram, bigram, and trigram tag probabilities. If the denominator is 0 for any case, we deﬁne
the result of that case to be 0. N is the number of tokens in the corpus. After Brants (2000).

time point t , we just keep the best few hypothesis at that point. At time t this requires
computing the Viterbi score for each of the N cells, sorting the scores, and keeping
only the best-scoring states. The rest are pruned out and not continued forward to
time t + 1.
One way to implement beam search is to keep a ﬁxed number of states instead of
all N current states. Here the beam width β is a ﬁxed number of states. Alternatively
β can be modeled as a ﬁxed percentage of the N states, or as a probability threshold.
Figure 8.11 shows the search lattice using a beam width of 2 states.

beam width

Figure 8.11 A beam search version of Fig. 8.6, showing a beam width of 2. At each time
t , all (non-zero) states are computed, but then they are sorted and only the best 2 states are
propagated forward and the rest are pruned, shown in orange.

unknown
words

8 .5

• MAX IMUM EN TRO PY MARKOV MOD EL S

167

8.4.9 Unknown Words

words people
never use —
could be
only I
know them

Ishikawa Takuboku 1885–1912

To achieve high accuracy with part-of-speech taggers, it is also important to have
a good model for dealing with unknown words. Proper names and acronyms are
created very often, and even new common nouns and verbs enter the language at a
surprising rate. One useful feature for distinguishing parts of speech is word shape:
words starting with capital letters are likely to be proper nouns (NNP).
But the strongest source of information for guessing the part-of-speech of un-
known words is morphology. Words that end in -s are likely to be plural nouns
(NNS), words ending with -ed tend to be past participles (VBN), words ending with
-able adjectives (JJ), and so on. We store for each ﬁnal letter sequence (for sim-
plicity referred to as word sufﬁxes) of up to 10 letters the statistics of the tag it was
associated with in training. We are thus computing for each sufﬁx of length i the
probability of the tag ti given the sufﬁx letters (Samuelsson 1993, Brants 2000):

P(ti |ln−i+1 . . . ln )

(8.30)

Back-off is used to smooth these probabilities with successively shorter sufﬁxes.
Because unknown words are unlikely to be closed-class words like prepositions,
sufﬁx probabilities can be computed only for words whose training set frequency is
≤ 10, or only for open-class words. Separate sufﬁx tries are kept for capitalized and
uncapitalized words.
Finally, because Eq. 8.30 gives a posterior estimate p(ti |wi ), we can compute
the likelihood p(wi |ti ) that HMMs require by using Bayesian inversion (i.e., using
Bayes rule and computation of the two priors P(ti ) and P(ti |ln−i+1 . . . ln )).
In addition to using capitalization information for unknown words, Brants (2000)
also uses capitalization for known words by adding a capitalization feature to each
tag. Thus, instead of computing P(ti |ti−1 , ti−2 ) as in Eq. 8.26, the algorithm com-
putes the probability P(ti , ci |ti−1 , ci−1 , ti−2 , ci−2 ). This is equivalent to having a cap-
italized and uncapitalized version of each tag, doubling the size of the tagset.
Combining all these features, a trigram HMM like that of Brants (2000) has a
tagging accuracy of 96.7% on the Penn Treebank, perhaps just slightly below the
performance of the best MEMM and neural taggers.

8.5 Maximum Entropy Markov Models

While an HMM can achieve very high accuracy, we saw that it requires a number of
architectural innovations to deal with unknown words, backoff, sufﬁxes, and so on.
It would be so much easier if we could add arbitrary features directly into the model
in a clean way, but that’s hard for generative models like HMMs. Luckily, we’ve
already seen a model for doing this: the logistic regression model of Chapter 5! But
logistic regression isn’t a sequence model; it assigns a class to a single observation.
However, we could turn logistic regression into a discriminative sequence model
simply by running it on successive words, using the class assigned to the prior word

168 CHA PTER 8

• PART-O F -S P EECH TAGG ING

MEMM

as a feature in the classiﬁcation of the next word. When we apply logistic regression

in this way, it’s called the maximum entropy Markov model or MEMM5

Let the sequence of words be W = wn
1 and the sequence of tags T = t n
1 . In an
HMM to compute the best tag sequence that maximizes P(T |W ) we rely on Bayes’
rule and the likelihood P(W |T ):
ˆT = argmax
P(T |W )
P(W |T )P(T )
P(wordi |tagi )(cid:89)i
In an MEMM, by contrast, we compute the posterior P(T |W ) directly, training it to
discriminate among the possible tag sequences:
ˆT = argmax
P(T |W )

T
T
T (cid:89)i

P(tagi |tagi−1 )

= argmax

= argmax

(8.31)

= argmax

P(ti |wi , ti−1 )

(8.32)

T
T (cid:89)i

Consider tagging just one word. A multinomial logistic regression classiﬁer could
compute the single probability P(ti |wi , ti−1 ) in a different way that an HMM. Fig. 8.12
shows the intuition of the difference via the direction of the arrows; HMMs compute
likelihood (observation word conditioned on tags) but MEMMs compute posterior
(tags conditioned on observation words).

Figure 8.12 A schematic view of the HMM (top) and MEMM (bottom) representation of
the probability computation for the correct sequence of tags for the back sentence. The HMM
computes the likelihood of the observation given the hidden state, while the MEMM computes
the posterior of each state, conditioned on the previous state and current observation.

8.5.1 Features in a MEMM

Of course we don’t build MEMMs that condition just on wi and ti−1 . The reason to
use a discriminative sequence model is that it’s easier to incorporate a lots of fea-
tures.6 Figure 8.13 shows a graphical intuition of some of these additional features.

5

‘Maximum entropy model’ is an outdated name for logistic regression; see the history section.
6 Because in HMMs all computation is based on the two probabilities P(tag|tag) and P(word|tag), if
we want to include some source of knowledge into the tagging process, we must ﬁnd a way to encode
the knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of
complicated conditioning which gets harder and harder as we have more and more such features.

8 .5

• MAX IMUM EN TRO PY MARKOV MOD EL S

169

Figure 8.13 An MEMM for part-of-speech tagging showing the ability to condition on
more features.

A basic MEMM part-of-speech tagger conditions on the observation word it-
self, neighboring words, and previous tags, and various combinations, using feature
templates like the following:

templates

(cid:104)ti , wi−2 (cid:105), (cid:104)ti , wi−1 (cid:105), (cid:104)ti , wi (cid:105), (cid:104)ti , wi+1 (cid:105), (cid:104)ti , wi+2 (cid:105)
(cid:104)ti , ti−1 (cid:105), (cid:104)ti , ti−2 , ti−1 (cid:105),
(cid:104)ti , ti−1 , wi (cid:105), (cid:104)ti , wi−1 , wi (cid:105)(cid:104)ti , wi , wi+1 (cid:105),

(8.33)

Recall from Chapter 5 that feature templates are used to automatically populate the
set of features from every instance in the training and test set. Thus our example
Janet/NNP will/MD back/VB the/DT bill/NN, when wi is the word back, would gen-
erate the following features:
ti = VB and wi−2 = Janet
ti = VB and wi−1 = will
ti = VB and wi = back
ti = VB and wi+1 = the
ti = VB and wi+2 = bill
ti = VB and ti−1 = MD
ti = VB and ti−1 = MD and ti−2 = NNP
ti = VB and wi = back and wi+1 = the
Also necessary are features to deal with unknown words, expressing properties of
the word’s spelling or shape:

wi contains a particular preﬁx (from all preﬁxes of length ≤ 4)
wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4)
wi contains a number
wi contains an upper-case letter
wi contains a hyphen
wi is all upper case
wi ’s word shape
wi ’s short word shape
wi is upper case and has a digit and a dash (like CFC-12)
wi is upper case and followed within 3 words by Co., Inc., etc.

word shape

Word shape features are used to represent the abstract letter pattern of the word
by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and retaining
punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30 would
map to XXdd-dd. A second class of shorter word shape features is also used. In these
features consecutive character types are removed, so DC10-30 would be mapped to
Xd-d but I.M.F would still map to X.X.X. For example the word well-dressed would
generate the following non-zero valued feature values:

170 CHA PTER 8

• PART-O F -S P EECH TAGG ING

preﬁx(wi ) = w
preﬁx(wi ) = we
preﬁx(wi ) = wel
preﬁx(wi ) = well
sufﬁx(wi ) = ssed
sufﬁx(wi ) = sed
sufﬁx(wi ) = ed
sufﬁx(wi ) = d
has-hyphen(wi )
word-shape(wi ) = xxxx-xxxxxxx
short-word-shape(wi ) = x-x
Features for known words, like the templates in Eq. 8.33, are computed for every
word seen in the training set. The unknown word features can also be computed for
all words in training, or only on training words whose frequency is below some
threshold. The result of the known-word templates and word-signature features is a
very large set of features. Generally a feature cutoff is used in which features are
thrown out if they have count < 5 in the training set.

8.5.2 Decoding and Training MEMMs

The most likely sequence of tags is then computed by combining these features of
the input word wi , its neighbors within l words wi+l
i−l , and the previous k tags t i−1
i−k as
follows (using θ to refer to feature weights instead of w to avoid the confusion with
w meaning words):

ˆT = argmax

= argmax

P(ti |wi+l
i−l , t i−1
i−k )

= argmax

T (cid:89)i

T
T (cid:89)i

θ j f j (ti , wi+l
i−l , t i−1

P(T |W )
exp (cid:88)j
exp (cid:88)j
(cid:88)t (cid:48) ∈tagset
How should we decode to ﬁnd this optimal tag sequence ˆT ? The simplest way
to turn logistic regression into a sequence model is to build a local classiﬁer that
classiﬁes each word left to right, making a hard classiﬁcation of the ﬁrst word in
the sentence, then a hard decision on the second word, and so on. This is called a
greedy decoding algorithm, because we greedily choose the best tag for each word,
as shown in Fig. 8.14.

i−k )
i−k )

θ j f j (t (cid:48) , wi+l
i−l , t i−1

(8.34)

greedy

function GR EEDY S EQU ENC E D ECOD ING(words W, model P) returns tag sequence T

for i = 1 to length(W)
P(t (cid:48) | wi+l
ˆti = argmax
In greedy decoding we simply run the classiﬁer on each token, left to right,
each time making a hard decision of which is the best tag.

i−l , t i−1
i−k )

Figure 8.14

t (cid:48) ∈ T

Viterbi

8 .6

• B ID IREC T IONA L I TY

171

The problem with the greedy algorithm is that by making a hard decision on
each word before moving on to the next word, the classiﬁer can’t use evidence from
future decisions. Although the greedy algorithm is very fast, and occasionally has
sufﬁcient accuracy to be useful, in general the hard decision causes too much a drop
in performance, and we don’t use it.
Instead we decode an MEMM with the Viterbi algorithm just as with the HMM,
ﬁnding the sequence of part-of-speech tags that is optimal for the whole sentence.
For example, assume that our MEMM is only conditioning on the previous tag
ti−1 and observed word wi . Concretely, this involves ﬁlling an N × T array with
the appropriate values for P(ti |ti−1 , wi ), maintaining backpointers as we proceed. As
with HMM Viterbi, when the table is ﬁlled, we simply follow pointers back from the
maximum value in the ﬁnal column to retrieve the desired set of labels. The requisite
changes from the HMM-style application of Viterbi have to do only with how we
ﬁll each cell. Recall from Eq. 8.20 that the recursive step of the Viterbi equation
computes the Viterbi value of time t for state j as
Nmax

vt−1 (i) ai j b j (ot ); 1 ≤ j ≤ N , 1 < t ≤ T
which is the HMM implementation of
Nmax

vt ( j) =

(8.35)

i=1

(8.36)

vt ( j) =

vt−1 (i) P(s j |si ) P(ot |s j ) 1 ≤ j ≤ N , 1 < t ≤ T
The MEMM requires only a slight change to this latter formula, replacing the a and
b prior and likelihood probabilities with the direct posterior:
Nmax

vt−1 (i) P(s j |si , ot ) 1 ≤ j ≤ N , 1 < t ≤ T
Learning in MEMMs relies on the same supervised learning algorithms we presented
for logistic regression. Given a sequence of observations, feature functions, and cor-
responding hidden states, we use gradient descent to train the weights to maximize
the log-likelihood of the training corpus.

vt ( j) =

(8.37)

i=1

i=1

8.6 Bidirectionality

label bias
observation
bias

The one problem with the MEMM and HMM models as presented is that they are
exclusively run left-to-right. While the Viterbi algorithm still allows present deci-
sions to be inﬂuenced indirectly by future decisions, it would help even more if a
decision about word wi could directly use information about future tags ti+1 and ti+2 .
Adding bidirectionality has another useful advantage. MEMMs have a theoret-
ical weakness, referred to alternatively as the label bias or observation bias prob-
lem (Lafferty et al. 2001, Toutanova et al. 2003). These are names for situations
when one source of information is ignored because it is explained away by another
source. Consider an example from Toutanova et al. (2003), the sequence will/NN
to/TO ﬁght/VB. The tag TO is often preceded by NN but rarely by modals (MD),
and so that tendency should help predict the correct NN tag for will. But the previ-
ous transition P(twil l |(cid:104)s(cid:105)) prefers the modal, and because P(T O|t o, twil l ) is so close
to 1 regardless of twil l the model cannot make use of the transition probability and
incorrectly chooses MD. The strong information that to must have the tag TO has ex-
plained away the presence of TO and so the model doesn’t learn the importance of

172 CHA PTER 8

• PART-O F -S P EECH TAGG ING

CRF

Stanford tagger

the previous NN tag for predicting TO. Bidirectionality helps the model by making
the link between TO available when tagging the NN.
One way to implement bidirectionality is to switch to a more powerful model
called a conditional random ﬁeld or CRF. The CRF is an undirected graphical
model, which means that it’s not computing a probability for each tag at each time
step. Instead, at each time step the CRF computes log-linear functions over a clique,
a set of relevant features. Unlike for an MEMM, these might include output features
of words in future time steps. The probability of the best sequence is similarly
computed by the Viterbi algorithm. Because a CRF normalizes probabilities over all
tag sequences, rather than over all the tags at an individual time t , training requires
computing the sum over all possible labelings, which makes CRF training quite slow.
Simpler methods can also be used; the Stanford tagger uses a bidirectional
version of the MEMM called a cyclic dependency network (Toutanova et al., 2003).
Alternatively, any sequence model can be turned into a bidirectional model by
using multiple passes. For example, the ﬁrst pass would use only part-of-speech
features from already-disambiguated words on the left. In the second pass, tags for
all words, including those on the right, can be used. Alternately, the tagger can be run
twice, once left-to-right and once right-to-left. In greedy decoding, for each word
the classiﬁer chooses the highest-scoring of the tag assigned by the left-to-right and
right-to-left classiﬁer. In Viterbi decoding, the classiﬁer chooses the higher scoring
of the two sequences (left-to-right or right-to-left). These bidirectional models lead
directly into the bi-LSTM models that we will introduce in Chapter 9 as a standard
neural sequence model.

8.7 Part-of-Speech Tagging for Other Languages

Augmentations to tagging algorithms become necessary when dealing with lan-
guages with rich morphology like Czech, Hungarian and Turkish.
These productive word-formation processes result in a large vocabulary for these
languages: a 250,000 word token corpus of Hungarian has more than twice as many
word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while
a 10 million word token corpus of Turkish contains four times as many word types
as a similarly sized English corpus (Hakkani-T ¨ur et al., 2002). Large vocabular-
ies mean many unknown words, and these unknown words cause signiﬁcant per-
formance degradations in a wide variety of languages (including Czech, Slovene,
Estonian, and Romanian) (Haji ˇc, 2000).
Highly inﬂectional languages also have much more information than English
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). Because this information is important for tasks like pars-
ing and coreference resolution, part-of-speech taggers for morphologically rich lan-
guages need to label words with case and gender information. Tagsets for morpho-
logically rich languages are therefore sequences of morphological tags rather than a
single primitive tag. Here’s a Turkish example, in which the word izin has three pos-
sible morphological/part-of-speech tags and meanings (Hakkani-T ¨ur et al., 2002):

1. Yerdeki izin temizlenmesi gerek.
The trace on the ﬂoor should be cleaned.
¨Uzerinde parmak izin kalmis¸
Your ﬁnger print is left on (it).

2.

iz + Noun+A3sg+Pnon+Gen

iz + Noun+A3sg+P2sg+Nom

3. Ic¸ eri girmek ic¸ in izin alman gerekiyor.
You need a permission to enter.

8 .8

• SUMMARY

173

izin + Noun+A3sg+Pnon+Nom

Using a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-
of-speech tag greatly increases the number of parts-of-speech, and so tagsets can
be 4 to 10 times larger than the 50–100 tags we have seen for English. With such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. The
role of the tagger is then to disambiguate among these tags. This method also helps
with unknown words since morphological parsers can accept unknown stems and
still segment the afﬁxes properly.
For non-word-space languages like Chinese, word segmentation (Chapter 2) is
either applied before tagging or done jointly. Although Chinese words are on aver-
age very short (around 2.4 characters per unknown word compared with 7.7 for En-
glish) the problem of unknown words is still large. While English unknown words
tend to be proper nouns in Chinese the majority of unknown words are common
nouns and verbs because of extensive compounding. Tagging models for Chinese
use similar unknown word features to English, including character preﬁx and suf-
ﬁx features, as well as novel features like the radicals of each character in a word.
(Tseng et al., 2005b).
A stanford for multilingual tagging is the Universal POS tag set of the Universal
Dependencies project, which contains 16 tags plus a wide variety of features that
can be added to them to create a large tagset for any language (Nivre et al., 2016a).

8.8 Summary

This chapter introduced parts-of-speech and part-of-speech tagging:

• Languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words, and open-class words like
nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40
and 200 tags.
• Part-of-speech tagging is the process of assigning a part-of-speech label to
each of a sequence of words.
• Two common approaches to sequence modeling are a generative approach,
HMM tagging, and a discriminative approach, MEMM tagging. We will see
a third, discriminative neural approach in Chapter 9.
• The probabilities in HMM taggers are estimated by maximum likelihood es-
timation on tag-labeled training corpora. The Viterbi algorithm is used for
decoding, ﬁnding the most likely tag sequence
• Beam search is a variant of Viterbi decoding that maintains only a fraction of
high scoring states rather than all states during decoding.

• Maximum entropy Markov model or MEMM taggers train logistic regres-

sion models to pick the best tag given an observation word and its context and
the previous tags, and then use Viterbi to choose the best sequence of tags.
• Modern taggers are generally run bidirectionally.

174 CHA PTER 8

• PART-O F -S P EECH TAGG ING

Bibliographical and Historical Notes

What is probably the earliest part-of-speech tagger was part of the parser in Zellig
Harris’s Transformations and Discourse Analysis Project (TDAP), implemented be-
tween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),
although earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quences and the relative frequency of tags for a word preﬁgures all modern algo-
rithms. The parser was implemented essentially as a cascade of ﬁnite-state trans-
ducers; see Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.
The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had
three components: a lexicon, a morphological analyzer, and a context disambiguator.
The small 1500-word lexicon listed only function words and other irregular words.
The morphological analyzer used inﬂectional and derivational sufﬁxes to assign part-
of-speech classes. These were run over words to produce candidate parts-of-speech
which were then disambiguated by a set of 500 context rules by relying on sur-
rounding islands of unambiguous words. For example, one rule said that between an
ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-
ADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used
the same architecture as Klein and Simmons (1963), with a bigger dictionary and
more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis
and Ku ˇcera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the
Brown corpus was then tagged by hand. All these early algorithms were based on
a two-stage architecture in which a dictionary was ﬁrst used to assign each word a
set of potential parts-of-speech, and then lists of hand-written disambiguation rules
winnowed the set down to a single part-of-speech per word.
Soon afterwards probabilistic architectures began to be developed. Probabili-
ties were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger
with Viterbi decoding was sketched by Bahl and Mercer (1976). The Lancaster-
Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was
tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Marshall 1987;
Garside 1987), a probabilistic algorithm that approximated a simpliﬁed HMM tag-
ger. The algorithm used tag bigram probabilities, but instead of storing the word
likelihood of each tag, the algorithm marked tags either as rare (P(tag|word) < .01)
infrequent (P(tag|word) < .10) or normally frequent (P(tag|word) > .10).
DeRose (1988) developed a quasi-HMM algorithm, including the use of dy-
namic programming, although computing P(t |w)P(w) instead of P(w|t )P(w). The
same year, the probabilistic PART S tagger of Church (1988), (1989) was probably
the ﬁrst implemented HMM tagger, described correctly in Church (1989), although
Church (1988) also described the computation incorrectly as P(t |w)P(w) instead
of P(w|t )P(w). Church (p.c.) explained that he had simpliﬁed for pedagogical pur-
poses because using the probability P(t |w) made the idea seem more understandable
as “storing a lexicon in an almost standard form”.
Later taggers explicitly introduced the use of the hidden Markov model (Ku-
piec 1992; Weischedel et al. 1993; Sch ¨utze and Singer 1994). Merialdo (1994)
showed that fully unsupervised EM didn’t work well for the tagging task and that
reliance on hand-labeled data was important. Charniak et al. (1993) showed the im-
portance of the most frequent tag baseline; the 92.3% number we give above was
from Abney et al. (1999). See Brants (2000) for many implementation details of an
HMM tagger whose performance is still roughly close to state of the art taggers.

EX ERC I SE S

175

Ratnaparkhi (1996) introduced the MEMM tagger, called MXPOST, and the
modern formulation is very much based on his work.
The idea of using letter sufﬁxes for unknown words is quite old; the early Klein
and Simmons (1963) system checked all ﬁnal letter sufﬁxes of lengths 1-5. The
probabilistic formulation we described for HMMs comes from Samuelsson (1993).
The unknown word features described on page 169 come mainly from (Ratnaparkhi,
1996), with augmentations from Toutanova et al. (2003) and Manning (2011).
State of the art taggers use neural algorithms or (bidirectional) log-linear models
Toutanova et al. (2003). HMM (Brants 2000; Thede and Harper 1999) and MEMM
tagger accuracies are likely just a tad lower.
An alternative modern formalism, the English Constraint Grammar systems (Karls-
son et al. 1995; Voutilainen 1995; Voutilainen 1999), uses a two-stage formalism
much like the early taggers from the 1950s and 1960s. A morphological analyzer
with tens of thousands of English word stem entries returns all parts-of-speech for a
word, using a large feature-based tagset. So the word occurred is tagged with the op-
tions (cid:104)V PCP2 SV(cid:105) and (cid:104)V PAST VFIN SV(cid:105), meaning it can be a participle (PCP2)
for an intransitive (SV) verb, or a past (PAST) ﬁnite (VFIN) form of an intransitive
(SV) verb. A set of 3,744 constraints are then applied to the input sentence to rule
out parts-of-speech inconsistent with the context. For example here’s a rule for the
ambiguous word that that eliminates all tags except the ADV (adverbial intensiﬁer)
sense (this is the sense in the sentence it isn’t that odd):

ADVERB IA L - THAT RU L E Given input: “that”
if (+1 A/ADV/QUANT); /* if next word is adj, adverb, or quantiﬁer */
(+2 SENT-LIM);
/* and following which is a sentence boundary, */
(NOT -1 SVOC/A); /* and the previous word is not a verb like */
/* ‘consider’ which allows adjs as object complements */
then eliminate non-ADV tags else eliminate ADV tag

Manning (2011) investigates the remaining 2.7% of errors in a state-of-the-art
tagger, the bidirectional MEMM-style model described above (Toutanova et al.,
2003). He suggests that a third or half of these remaining errors are due to errors or
inconsistencies in the training data, a third might be solvable with richer linguistic
models, and for the remainder the task is underspeciﬁed or unclear.
Supervised tagging relies heavily on in-domain training data hand-labeled by
experts. Ways to relax this assumption include unsupervised algorithms for cluster-
ing words into part-of-speech-like classes, summarized in Christodoulopoulos et al.
(2010), and ways to combine labeled and unlabeled data, for example by co-training
(Clark et al. 2003; Søgaard 2010).
See Householder (1995) for historical notes on parts-of-speech, and Sampson
(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.

Exercises

8.1

Find one tagging error in each of the following sentences that are tagged with
the Penn Treebank tagset:
1. I/PRP need/VBP a/DT ﬂight/NN from/IN Atlanta/NN
2. Does/VBZ this/DT ﬂight/NN serve/VB dinner/NNS
3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP
4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN ﬂights/NNS

176 CHA PTER 8

• PART-O F -S P EECH TAGG ING

5.

8.4

8.2 Use the Penn Treebank tagset to tag each word in the following sentences
from Damon Runyon’s short stories. You may ignore punctuation. Some of
these are quite difﬁcult; do your best.
1. It is a nice night.
2. This crap game is over a garage in Fifty-second Street. . .
3.
. . . Nobody ever takes the newspapers she sells . . .
4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a
mournful voice.
. . . I am sitting in Mindy’s restaurant putting on the geﬁllte ﬁsh, which is
a dish I am very fond of, . . .
6. When a guy and a doll get to taking peeks back and forth at each other,
why there you are indeed.
8.3 Now compare your tags from the previous exercise with one or two friend’s
answers. On which words did you disagree the most? Why?
Implement the “most likely tag” baseline. Find a POS-tagged training set,
and use it to compute for each word the tag that maximizes p(t |w). You will
need to implement a simple tokenizer to deal with sentence boundaries. Start
by assuming that all unknown words are NN and compute your error rate on
known and unknown words. Now write at least ﬁve rules to do a better job of
tagging unknown words, and show the difference in error rates.
8.5 Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus.
First split the corpus into a training set and test set. From the labeled training
set, train the transition and observation probabilities of the HMM tagger di-
rectly on the hand-tagged data. Then implement the Viterbi algorithm so that
you can label an arbitrary test sentence. Now run your algorithm on the test
set. Report its error rate and compare its performance to the most frequent tag
baseline.
8.6 Do an error analysis of your tagger. Build a confusion matrix and investigate
the most frequent errors. Propose some features for improving the perfor-
mance of your tagger on these errors.

CHAPTER

9 Sequence
Processing with
Recurrent Networks

Time will explain.

Jane Austin, Persuasion

In Chapter 7, we explored feedforward neural networks along with their applications
to neural language models and text classiﬁcation. In the case of language models,
we saw that such networks can be trained to make predictions about the next word in
a sequence given a limited context of preceding words — an approach that is remi-
niscent of the Markov approach to language modeling discussed in Chapter 3. These
models operated by accepting a small ﬁxed-sized window of tokens as input; longer
sequences are processed by sliding this window over the input making incremental
predictions, with the end result being a sequence of predictions spanning the input.
Fig. 9.1, reproduced here from Chapter 7, illustrates this approach with a window
of size 3. Here, we’re predicting which word will come next given the window the
ground there. Subsequent words are predicted by sliding the window forward one
word at a time.
Unfortunately, the sliding window approach is problematic for a number of rea-
sons. First, it shares the primary weakness of Markov approaches in that it limits
the context from which information can be extracted; anything outside the context
window has no impact on the decision being made. This is problematic since there
are many language tasks that require access to information that can be arbitrarily dis-
tant from the point at which processing is happening. Second, the use of windows
makes it difﬁcult for networks to learn systematic patterns arising from phenomena
like constituency. For example, in Fig. 9.1 the phrase the ground appears twice in
different windows: once, as shown, in the ﬁrst and second positions in the window,
and in in the preceding step in the second and third slots, thus forcing the network
to learn two separate patterns for a single constituent.
The subject of this chapter is recurrent neural networks, a class of networks
designed to address these problems by processing sequences explicitly as sequences,
allowing us to handle variable length inputs without the use of arbitrary ﬁxed-sized
windows.

9.1 Simple Recurrent Networks

A recurrent neural network is any network that contains is a cycle within its network
connections. That is, any network where the value of a unit is directly, or indirectly,
dependent on its own output as an input. In general, such networks are difﬁcult to
reason about, and to train. However, within the general class of recurrent networks
there are constrained architectures that have proven to be extremely useful when

177

178 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.1 A simpliﬁed view of a feedforward neural language model moving through a text. At each
timestep t the network takes the 3 context words, converts each to a d -dimensional embeddings, and con-
catenates the 3 embeddings together to get the 1 × N d unit input layer x for the network.

Simple
Recurrent
Networks
Elman
Networks

applied to language problems. In this section, we’ll introduce a class of recurrent

networks referred to as Simple Recurrent Networks (SRNs) or Elman Networks

(Elman, 1990). These networks are useful in their own right, and will serve as the
basis for more complex approaches to be discussed later in this chapter and again in
Chapter 22.
Fig. 9.2 abstractly illustrates the recurrent structure of an SRN. As with ordinary
feed-forward networks, an input vector representing the current input element, xt ,
is multiplied by a weight matrix and then passed through an activation function to
compute an activation value for a layer of hidden of units. This hidden layer is,
in turn, used to calculate a corresponding output, yt . Sequences are processed by
presenting one element at a time to the network. The key difference from a feed-

Figure 9.2 Simple recurrent neural network after Elman (Elman, 1990). The hidden layer
includes a recurrent connection as part of its input. That is, the activation value of the hidden
layer depends on the current input as well as the activation value of the hidden layer from the
previous timestep.

9 .1

• S IM P LE R ECURR EN T N ETWORK S

179

Figure 9.3 Simple recurrent neural network illustrated as a feed-forward network.

forward network lies in the recurrent link shown in the ﬁgure with the dashed line.
This link augments the input to the hidden layer with the activation value of the
hidden layer from the preceding point in time.
The hidden layer from the previous timestep provides a form of memory, or
context, that encodes earlier processing and informs the decisions to be made at later
points in time. Importantly, the architecture does not impose a ﬁxed-length limit
on this prior context; the context embodied in the previous hidden layer includes
information extending back to the beginning of the sequence.
Adding this temporal dimension makes recurrent networks appear to be more
exotic than non-recurrent architectures. But in reality, they’re not all that different.
Given an input vector and the values for the hidden layer from the previous time
step, we’re still performing the standard feed-forward calculation. To see this, con-
sider Fig. 9.3 which clariﬁes the nature of the recurrence and how it factors into the
computation at the hidden layer. The most signiﬁcant addition lies in the new set of
weights, U , that connect the hidden layer from the previous timestep to the current
hidden layer. These weights determine how the network should make use of past
context in calculating the output for the current input. As with the other weights in
the network, these connections will be trained via backpropagation.

9.1.1

Inference in Simple RNNs

Forward inference (mapping a sequence of inputs to a sequence of outputs) in an
SRN is nearly identical to what we’ve already seen with feedforward networks. To
compute an output yt for an input xt , we need the activation value for the hidden
layer ht . To calculate this, we compute the dot product of the input xt with the weight
matrix W , and the dot product of the hidden layer from the previous time step ht−1
with the weight matrix U . We add these values together and pass them through a
suitable activation function, g, to arrive at the activation value for the current hidden
layer, ht . Once we have the values for the hidden layer, we proceed with the usual
computation to generate the output vector.

ht = g(U ht−1 + W xt )
yt = f (V ht )

In the commonly encountered case of soft classiﬁcation, ﬁnding yt consists of
a softmax computation that provides a normalized probability distribution over the

180 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.4 A simple recurrent neural network shown unrolled in time. Network layers are copied for each
timestep, while the weights U , V and W are shared in common across all timesteps.

possible output classes.

yt = softmax(V ht )

The sequential nature of simple recurrent networks can be illustrated by un-
rolling the network in time as is shown in Fig. 9.4.
In ﬁgures such as this, the
various layers of units are copied for each time step to illustrate that they will have
differing values over time. However the weights themselves are shared across the
various timesteps. Finally, the fact that the computation at time t requires the value
of the hidden layer from time t − 1 mandates an incremental inference algorithm that
proceeds from the start of the sequence to the end as shown in Fig. 9.5.

function FORWARDRNN(x, network) returns output sequence y
h0 ← 0
for i ← 1 to L ENG TH(x) do
hi ← g(U hi−1 + W xi )
yi ← f (V hi )

return y

Figure 9.5 Forward inference in a simple recurrent network.

9.1.2 Training

As we did with feed-forward networks, we’ll use a training set, a loss function, and
backpropagation to adjust the sets of weights in these recurrent networks. As shown
in Fig. 9.3, we now have 3 sets of weights to update: W , the weights from the input

9 .1

• S IM P LE R ECURR EN T N ETWORK S

181

layer to the hidden layer, U , the weights from the previous hidden layer to the current
hidden layer, and ﬁnally V , the weights from the hidden layer to the output layer.
Before going on, let’s ﬁrst review some of the notation that we introduced in
Chapter 7. Assuming a network with an input layer x and a non-linear activation
function g, we’ll use a[i] to refer to the activation value from a layer i, which is the
result of applying g to z[i] , the weighted sum of the inputs to that layer. A simple
two-layer feedforward network with W and V as the ﬁrst and second sets of weights
respectively, would be characterized as follows.

z[1] = W x
a[1] = g(z[1] )

z[2] = U a[1]

a[2] = g(z[2] )
y = a[2]

Fig. 9.4 illustrates the two considerations that we didn’t have to worry about with
backpropagation in feed-forward networks. First, to compute the loss function for
the output at time t we need the hidden layer from time t − 1. Second, the hidden
layer at time t inﬂuences both the output at time t and the hidden layer at time t + 1
(and hence the output and loss at t + 1). It follows from this that to assess the error
accruing to ht , we’ll need to know its inﬂuence on both the current output as well as
the next one.
Consider the situation where we are examining an input/output pair at time 2 as
shown in Fig. 9.4. What do we need to compute the gradients needed to update the
weights U , V , and W here? Let’s start by reviewing how we compute the gradients
required to update V (this computation is unchanged from feed-forward networks).
To review from Chapter 7, we need to compute the derivative of the loss function L
with respect to the weights V . However, since the loss is not expressed directly in
terms of the weights, we apply the chain rule to get there indirectly.

∂ L
∂ V

=

∂ L
∂ a

∂ a
∂ z

∂ z
∂ V

The ﬁrst term is just the derivative of the loss function with respect to the network
output, which is just the activation of the output layer, a. The second term is the
derivative of the network output with respect to the intermediate network activation
z, which is a function of the activation function g. The ﬁnal term in our application of
the chain rule is the derivative of the network activation with respect to the weights
V , which is just the activation value of the current hidden layer ht .
It’s useful here to use the ﬁrst two terms to deﬁne δ , an error term that represents
how much of the scalar loss is attributable to each of the units in the output layer.

∂ L
∂ a

∂ a
∂ z

δout =

δout = L(cid:48)g(cid:48) (z)

Therefore, the ﬁnal gradient we need to update the weight matrix V is just:

∂ L
∂ V

= δout ht

(9.1)

(9.2)

(9.3)

Moving on, we need to compute the corresponding gradients for the weight ma-
trices W and U : ∂ L
∂ U . Here we encounter the ﬁrst substantive change from

∂W and ∂ L

182 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.6 The backpropagation of errors in an SRN. The ti vectors represent the targets for each element
of the sequence from the training data. The red arrows illustrate the ﬂow of backpropagated errors required to
calculate the updates for U , V and W at time 2. The two incoming arrows converging on h2 signal that these
errors need to be summed.

feed-forward networks. The hidden state at time t contributes to the output and asso-
ciated error at time t and to the output and error at the next timestep, t + 1. Therefore,
the error term, δh , for the hidden layer must be the sum of the error term from the
current output and its error from the next time step.

δh = g(cid:48) (z)V δout + δnext

Given this total error term for the hidden layer, we can compute the gradients for
the weights U and W in the usual way using the chain rule as we did in Chapter 7.

dL
dW
dL
dU

=

=

dL
d z
dL
d z

d z
d a
d z
d a

d a
dW
d a
dU

∂ L
∂W
∂ L
∂ U

= δh xt

= δhht−1

These gradients provide us with the information needed to update the matrices U
and W through ordinary backpropagation.
We’re not quite done yet, we still need to assign proportional blame (compute
the error term) back to the previous hidden layer ht−1 for use in further processing.

9 .1

• S IM P LE R ECURR EN T N ETWORK S

183

function BACKPRO PTHROUGHT IM E(sequence, network) returns gradients for weight
updates
forward pass to gather the loss
backward pass compute error terms and assess blame

Figure 9.7 Backpropagation training through time. The forward pass computes the re-
quired loss values at each time step. The backward pass computes the gradients using the
values from the forward pass.

This involves backpropagating the error from δh to ht−1 proportionally based on the
weights in U .

δnext = g(cid:48) (z)U δh

(9.4)

At this point we have all the gradients needed to perform weight updates for each
of our three sets of weights. Note that in this simple case there is no need to back-
propagate the error through W to the input x, since the input training data is assumed
to be ﬁxed. If we wished to update our input word or character embeddings we
would backpropagate the error through to them as well. We’ll discuss this more in
Section 9.5.
Taken together, all of these considerations lead to a two-pass algorithm for train-
ing the weights in SRNs. In the ﬁrst pass, we perform forward inference, computing
ht , yt , and an loss at each step in time, saving the value of the hidden layer at each
step for use at the next time step. In the second phase, we process the sequence
in reverse, computing the required error terms gradients as we go, computing and
saving the error term for use in the hidden layer for each step backward.
Unfortunately, computing the gradients and updating weights for each item of a
sequence individually would be extremely time-consuming. Instead, much as we did
with mini-batch training in Chapter 7, we will accumulate gradients for the weights
incrementally over the sequence, and then use those accumulated gradients in per-
forming weight updates.

9.1.3 Unrolled Networks as Computational Graphs

We used the unrolled network shown in Fig. 9.4 as a way to understand the dynamic
behavior of these networks over time. However, with modern computational frame-
works and adequate computing resources, explicitly unrolling a recurrent network
into a deep feed-forward computational graph is quite practical for word-by-word
approaches to sentence-level processing. In such an approach, we provide a tem-
plate that speciﬁes the basic structure of the SRN, including all the necessary pa-
rameters for the input, output, and hidden layers, the weight matrices, as well as the
activation and output functions to be used. Then, when provided with an input se-
quence such as a training sentence, we can compile a feed-forward graph speciﬁc to
that input, and use that graph to perform forward inference or training via ordinary
backpropagation.
For applications that involve much longer input sequences, such as speech recog-
nition, character-by-character sentence processing, or streaming of continuous in-
puts, unrolling an entire input sequence may not be feasible. In these cases, we can
unroll the input into manageable ﬁxed-length segments and treat each segment as a
distinct training item. This approach is called Truncated Backpropagation Through
Time (TBTT).

184 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.8 Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained
word embeddings serve as inputs and a softmax layer provides a probability distribution over
the part-of-speech tags as output at each time step.

9.2 Applications of RNNs

Simple recurrent networks have proven to be an effective approach to language mod-
eling, sequence labeling tasks such as part-of-speech tagging, as well as sequence
classiﬁcation tasks such as sentiment analysis and topic classiﬁcation. And as we’ll
see in Chapter 22, they form the basic building blocks for sequence to sequence
approaches to applications such as summarization and machine translation.

9.2.1 Generation with Neural Language Models

[Coming soon]

9.2.2 Sequence Labeling

In sequence labeling, the network’s job is to assign a label to each element of a
sequence chosen from a small ﬁxed set of labels. The canonical example of such a
task is part-of-speech tagging, discussed in Chapter 8. In a recurrent network-based
approach to POS tagging, inputs are words and the outputs are tag probabilities
generated by a softmax layer over the POS tagset, as illustrated in Fig. 9.8.
In this ﬁgure, the inputs at each time step are pre-trained word embeddings cor-
responding to the input tokens. The RNN block is an abstraction that represents
an unrolled simple recurrent network consisting of an input layer, hidden layer, and
output layer at each time step, as well as the shared U , V and W weight matrices that
comprise the network. The outputs of the network at each time step represent the
distribution over the POS tagset generated by a softmax layer. To generate an actual
tag sequence as output, we can run forward inference over the input sequence and
select the most likely tag from the softmax at each step. Since we’re using a softmax
layer to generate the probability distribution over the output tagset at each timestep,
we’ll rely on the cross entropy loss introduced in Chapter 7 to train the network.
A closely related, and extremely useful, application of sequence labeling is to
ﬁnd and classify spans of text corresponding to items of interest in some task do-
main. An example of such a task is named entity recognition — the problem of

named entity
recognition

9 .2

• A P P L ICAT ION S O F RNN S

185

ﬁnding all the spans in a text that correspond to names of people, places or organi-
zations (a problem we’ll study in gory detail in Chapter 17).
To turn a problem like this into a per-word sequence labeling task, we’ll use a
technique called IOB encoding (Ramshaw and Marcus, 1995). In its simplest form,
we’ll label any token that begins a span of interest with the label B, tokens that occur
inside a span are tagged with an I, and any tokens outside of any span of interest are
labeled O. Consider the following example:
(9.5) United
cancelled
the
ﬂight
from
Denver
B
O
O
O
O
B

Francisco.
I

to
O

San
B

Here, the spans of interest are United, Denver and San Francisco.
In applications where we are interested in more than one class of entity (e.g.,
ﬁnding and distinguishing names of people, locations, or organizations), we can
specialize the B and I tags to represent each of the more speciﬁc classes, thus ex-
panding the tagset from 3 tags to 2 ∗ N + 1 where N is the number of classes we’re
interested in.
(9.6) United
cancelled
the
ﬂight
from
Denver
to
San
Francisco.
B-ORG
O
O
O
O
B-LOC
O
B-LOC
I-LOC

With such an encoding, the inputs are the usual word embeddings and the output
consistes of a sequence of softmax distributions over the tags at each point in the
sequence.

9.2.3 Viterbi and Conditional Random Fields (CRFs)

As we saw with applying logistic regression to part-of-speech tagging, choosing the
maximum probability label for each element in a sequence does not necessarily re-
sult in an optimal (or even very good) tag sequence. In the case of IOB tagging, it
doesn’t even guarantee that the resulting sequence will be well-formed. For exam-
ple, nothing in approach described in the last section prevents an output sequence
from containing an I following an O, even though such a transition is illegal. Simi-
larly, when dealing with multiple classes nothing would prevent an I -LOC tag from
following a B -P ER tag.
A simple solution to this problem is to use combine the sequence of probability
distributions provided by the softmax outputs with a tag-level language model as we
did with MEMMs in Chapter 8. Thereby allowing the use of the Viterbi algorithm
to select the most likely tag sequence.
[Or a CRF layer... Coming soon]

9.2.4 RNNs for Sequence Classiﬁcation

Another use of RNNs is to classify entire sequences rather than the tokens within
a sequence. We’ve already encountered this task in Chapter 4 with our discussion
of sentiment analysis. Other examples include document-level topic classiﬁcation,
spam detection, message routing for customer service applications, and deception
detection. In all of these applications, sequences of text are classiﬁed as belonging
to one of a small number of categories.
To apply RNNs in this setting, the hidden layer from the ﬁnal state of the network
is taken to constitute a compressed representation of the entire sequence. This com-
pressed sequence representation can then in turn serve as the input to a feed-forward
network trained to select the correct class. Fig. 9.10 illustrates this approach.

186 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.9 Sequence classiﬁcation using a simple RNN combined with a feedforward net-
work.

Note that in this approach, there are no intermediate outputs for the items in the
sequence preceding the last element, and therefore there are no loss terms associ-
ated with those individual items. Instead, the loss used to train the network weights
is based on the loss from the ﬁnal classiﬁcation task. Speciﬁcally, we use the output
from the softmax layer from the ﬁnal classiﬁer along with a cross-entropy loss func-
tion to drive our network training. The loss is backpropagated all the way through
the weights in the feedforward classiﬁer through to its input, and then through to the
three sets of weights in the RNN as described earlier in Section 9.1.2. This combina-
tion of a simple recurrent network with a feedforward classiﬁer is our ﬁrst example
of a deep neural network.

9.3 Deep Networks: Stacked and Bidirectional RNNs

As suggested by the sequence classiﬁcation architecture shown in Fig. 9.9, recur-
rent networks are in fact quite ﬂexible. Combining the feedforward nature of un-
rolled computational graphs with vectors as common inputs and outputs, complex
networks can be treated as modules that can be combined in creative ways. This
section introduces two of the more common network architectures used in language
processing with RNNs.

9.3.1 Stacked RNNs

In our examples thus far, the inputs to our RNNs have consisted of sequences of
word or character embeddings (vectors) and the outputs have been vectors useful for
predicting words, tags or sequence labels. However, nothing prevents us from using
the entire sequence of outputs from one RNN as an input sequence to another one.
Stacked RNNs consist of multiple networks where the output of one layer serves as
the input to a subsequent layer, as shown in Fig. 9.10.
It has been demonstrated across numerous tasks that stacked RNNs can outper-

Stacked RNNs

9 .3

• D E E P N E TWORK S : S TACK ED AND B ID IREC T IONA L RNN S

187

Figure 9.10 Stacked recurrent networks. The output of a lower level serves as the input to
higher levels with the output of the last network serving as the ﬁnal output.

form single-layer networks. One reason for this success has to do with the networks
ability to induce representations at differing levels of abstraction across layers. Just
as the early stages of the human visual system detects edges that are then used for
ﬁnding larger regions and shapes, the initial layers of stacked networks can induce
representations that serve as useful abstractions for further layers — representations
that might prove difﬁcult to induce in a single RNN.

9.3.2 Bidirectional RNNs

In an simple recurrent network, the hidden state at a given time t represents every-
thing the network knows about the sequence up to that point in the sequence. That
is, the hidden state at time t is the result of a function of the inputs from the start up
through time t . We can think of this as the context of the network to the left of the
current time.

h f orward
t

= SRN f orward (x1 : xt )

Where h f orward
t

corresponds to the normal hidden state at time t , and represents
everything the network has gleaned from the sequence to that point.
Of course, in text-based applications we have access to the entire input sequence
all at once. We might ask whether its helpful to take advantage of the context to
the right of the current input as well. One way to recover such information is to
train a recurrent network on an input sequence in reverse, using the same kind of
network that we’ve been discussing. With this approach, the hidden state at time t
now represents information about the sequence to the right of the current input.

hbackward
t

= SRNbackward (xn : xt )

bidirectional
RNN

Here, the hidden state hbackward
represents all the information we have discerned
about the sequence from t to the end of the sequence.
Putting these networks together results in a bidirectional RNN. A Bi-RNN con-
sists of two independent recurrent networks, one where the input is processed from

t

188 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.11 A bidirectional RNN. Separate models are trained in the forward and backward
directions with the output of each model at each time point concatenated to represent the state
of affairs at that point in time. The box wrapped around the forward and backward network
emphasizes the modular nature of this architecture.

the start to the end, and the other from the end to the start. We can then combine the
outputs of the two networks into a single representation that captures the both the
left and right contexts of an input at each point in time.

ht = h f orward
t

⊕ hbackward
t

(9.7)

Fig. 9.11 illustrates a bidirectional network where the outputs of the forward and
backward pass are concatenated. Other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. The output at
each step in time thus captures information to the left and to the right of the current
input. In sequence labeling applications, these concatenated outputs can serve as the
basis for a local labeling decision.
Bidirectional RNNs have also proven to be quite effective for sequence classi-
ﬁcation. Recall from Fig. 9.10, that for sequence classiﬁcation we used the ﬁnal
hidden state of the RNN as the input to a subsequent feedforward classiﬁer. A dif-
ﬁculty with this approach is that the ﬁnal state naturally reﬂects more information
about the end of the sentence than its beginning. Bidirectional RNNs provide a
simple solution to this problem; as shown in Fig. 9.12, we simply combine the ﬁnal
hidden states from the forward and backward passes and use that as input for follow-
on processing. Again, concatenation is a common approach to combining the two
outputs but element-wise summation, multiplication or averaging are also used.

9.4 Managing Context in RNNs: LSTMs and GRUs

In practice, it is quite difﬁcult to train simple RNNs for tasks that require a network
to make use of information distant from the current point of processing. Despite hav-
ing access to the entire preceding sequence, the information encoded in hidden states
tends to be fairly local, more relevant to the most recent parts of the input sequence
and recent decisions. However, it is often the case that long-distance information is
critical to many language applications.

9 .4

• MANAG ING CON TEXT IN RNN S : LSTM S AND GRU S

189

Figure 9.12 A bidirectional RNN for sequence classiﬁcation. The ﬁnal hidden units from
the forward and backward passes are combined to represent the entire sequence. This com-
bined representation serves as input to the subsequent classiﬁer.

Consider the following example in the context of language models.
(9.8) The ﬂights the airline was cancelling were full.
Assigning a high probability to was following airline is straightforward since was
provides a strong local context for the singular agreement. However, assigning an
appropriate probability to were is quite difﬁcult, not only because the plural ﬂights
is quite distant, but also because the more recent context contains singular con-
stituents. Ideally, a network should be able to retain the distant information about
plural ﬂights until it is needed, all the while processing intermediate parts of the
sequence correctly.
One reason for the inability of SRNs to carry forward critical information is that
the hidden layer in SRNs, and, by extension, the weights that determine the values
in the hidden layer, are being asked to perform two tasks simultaneously: provide
information useful to the decision being made in the current context, and updating
and carrying forward information useful for future decisions.
A second difﬁculty to successfully training simple recurrent networks arises
from the need to backpropagate training error back in time through the hidden lay-
ers. Recall from Section 9.1.2 that the hidden layer at time t contributes to the loss
at the next time step since it takes part in that calculation. As a result, during the
backward pass of training, the hidden layers are subject to repeated dot products, as
determined by the length of the sequence. A frequent result of this process is that
the gradients are either driven to zero or saturate. Situations that are referred to as
vanishing gradients or exploding gradients, respectively.
To address these issues more complex network architectures have been designed
to explicitly manage the task of maintaining contextual information over time. These
approaches treat context as a kind of memory unit that needs to be managed explic-
itly. More speciﬁcally, the network needs to forget information that is no longer
needed and to remember information as needed for later decisions.

190 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.13 A single LSTM memory unit displayed as a computation graph.

9.4.1 Long Short-Term Memory

Long short-term memory (LSTM) networks, divide the context management prob-
lem into two sub-problems: removing information no longer needed from the con-
text, and adding information likely to be needed for later decision making. The key
to the approach is to learn how to manage this context rather than hard-coding a
strategy into the architecture.
LSTMs accomplish this through the use of specialized neural units that make use
of gates that control the ﬂow of information into and out of the units that comprise
the network layers. These gates are implemented through the use of additional sets
of weights that operate sequentially on the context layer.

gt = t anh(Ug ht−1 + Wg xt )

it = σ (Uiht−1 + Wi xt )
ft = σ (U f ht−1 + W f xt )
ot = σ (Uoht−1 + Wo xt )

ct = ft (cid:12) ct−1 + it (cid:12) gt
ht = ot (cid:12) t anh(ct )

[More on this]

9 .4

• MANAG ING CON TEXT IN RNN S : LSTM S AND GRU S

191

Figure 9.14 Basic neural units used in feed-forward, simple recurrent networks (SRN),
long short-term memory (LSTM) and gate recurrent units.

9.4.2 Gated Recurrent Units

While relatively easy to deploy, LSTMs introduce a considerable number of param-
eters to our networks, and hence carry a much larger training burden. Gated Recur-
rent Units (GRUs) try to ease this burden by collapsing the forget and add gates of
LSTMs into a single update gate with a single set of weights.
[coming soon]

9.4.3 Gated Units, Layers and Networks

The neural units used in LSTMs and GRUs are obviously much more complex than
basic feed-forward networks. Fortunately, this complexity is largely encapsulated
within the basic processing units, allowing us to maintain modularity and to eas-
ily experiment with different architectures. To see this, consider Fig. 9.14 which
illustrates the inputs/outputs and weights associated with each kind of unit.
At the far left, (a) is the basic feed-forward unit h = g(W x + b). A single set of
weights and a single activation function determine its output, and when arranged in
a layer there is no connection between the units in the layer. Next, (b) represents the
unit in an SRN. Now there are two inputs and additional set of weights to go with it.
However, there is still a single activation function and output. When arranged as a
layer the hidden layer from each unit feeds in as an input to the next.
Fortunately, the increased complexity of the LSTM and GRU units is encapsu-
lated within the units themselves. The only additional external complexity over the
basic recurrent unit (b) is the presence of the additional context vector input and out-
put. This modularity is key to the power and widespread applicability of LSTM and
GRU units. Speciﬁcally, LSTM and GRU units can be substituted into any of the
network architectures described in Section 9.3. And, as with SRNs, multi-layered
networks making use of gated units can be unrolled into deep feed-forward networks
and trained in the usual fashion with backpropagation.

192 CHA PTER 9

• S EQU ENC E PROCE S S ING W I TH R ECURREN T N ETWORK S

Figure 9.15 Sequence labeling RNN that accepts distributional word embeddings aug-
mented with character-level word embeddings.

9.5 Words, Characters and Byte-Pairs

To this point, we’ve assumed that the inputs to our networks would be either pre-
trained or trained word embeddings. As we’ve seen, word-based embeddings are
great at ﬁnding distributional (syntactic and semantic) similarity between words.
However, there are signiﬁcant issues with any solely word-based approach:
• For some languages and applications, the lexicon is simply too large to prac-
tically represent every possible word as an embedding. Some means of com-
posing words from smaller bits is needed.
• No matter how large the lexicon, we will always encounter unknown words
due to new words entering the language, misspellings and borrowings from
other languages.
• Morphological information, below the word level, is clearly an important
source of information for many applications. Word-based methods are blind
to such regularities.
We can overcome some of these issues by augmenting our input word repre-
sentations with embeddings derived from the characters that make up the words.
Fig. 9.15 illustrates an approach in the context of part-of-speech tagging. The upper
part of the diagram consists of an RNN that accepts an input sequence and outputs
a softmax distribution over the tags for each element of the input. Note that this
RNN can be arbitrarily complex, consisting of stacked and/or bidirectional network
layers.
The inputs to this network consist of ordinary word embeddings enriched with
character information. Speciﬁcally, each input consists of the concatenation of the
normal word embedding with embeddings derived from a bidirectional RNN that
accepts the character sequences for each word as input, as shown in the lower part
of the ﬁgure.
The character sequence for each word in the input is run through a bidirectional
RNN consisting of two independent RNNs — one that processes the sequence left-

9 .6

• SUMMARY

193

Figure 9.16 Bi-RNN accepts word character sequences and emits embeddings derived
from a forward and backward pass over the sequence. The network itself is trained in the
context of a larger end-application where the loss is propagated all the way through to the
character vector embeddings.

to-right and the other right-to-left. As discussed in Section ??, the ﬁnal hidden
states of the left-to-right and right-to-left networks are concatenated to represent the
composite character-level representation of each word. Critically, these character
embeddings are trained in the context of the overall task; the loss from the part-of-
speech softmax layer is propagated all the way back to the character embeddings.
[more on byte-pair encoding approach]

9.6 Summary

• Simple recurrent networks
• Inference and training in SRNs.
• Common use cases for RNNs
– language modeling
– sequence labeling
– sequence classiﬁcation
• LSTMs and GRUs
• Characters as inputs

194 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

CHAPTER

10 Formal Grammars of English

syntax

The study of grammar has an ancient pedigree; Panini’s grammar of Sanskrit was
written over two thousand years ago and is still referenced today in teaching San-
skrit. Despite this history, knowledge of grammar remains spotty at best. In this
chapter, we make a preliminary stab at addressing some of these gaps in our knowl-
edge of grammar and syntax, as well as introducing some of the formal mechanisms
that are available for capturing this knowledge in a computationally useful manner.
The word syntax comes from the Greek s ´yntaxis, meaning “setting out together
or arrangement”, and refers to the way words are arranged together. We have seen
various syntactic notions in previous chapters. The regular languages introduced
in Chapter 2 offered a simple way to represent the ordering of strings of words, and
Chapter 3 showed how to compute probabilities for these word sequences. Chapter 8
showed that part-of-speech categories could act as a kind of equivalence class for
words. In this chapter and next few we introduce a variety of syntactic phenomena
and models for syntax and grammar that go well beyond these simpler approaches.
The bulk of this chapter is devoted to the topic of context-free grammars. Context-
free grammars are the backbone of many formal models of the syntax of natural
language (and, for that matter, of computer languages). As such, they are integral to
many computational applications, including grammar checking, semantic interpreta-
tion, dialogue understanding, and machine translation. They are powerful enough to
express sophisticated relations among the words in a sentence, yet computationally
tractable enough that efﬁcient algorithms exist for parsing sentences with them (as
we show in Chapter 11). In Chapter 12, we show that adding probability to context-
free grammars gives us a powerful model of disambiguation. And in Chapter 15 we
show how they provide a systematic framework for semantic interpretation.
In addition to an introduction to this grammar formalism, this chapter also pro-
vides a brief overview of the grammar of English. To illustrate our grammars, we
have chosen a domain that has relatively simple sentences, the Air Trafﬁc Informa-
tion System (ATIS) domain (Hemphill et al., 1990). ATIS systems were an early
example of spoken language systems for helping book airline reservations. Users
try to book ﬂights by conversing with the system, specifying constraints like I’d like
to ﬂy from Atlanta to Denver.

10.1 Constituency

The fundamental notion underlying the idea of constituency is that of abstraction —
groups of words behaving as a single units, or constituents. A signiﬁcant part of
developing a grammar involves discovering the inventory of constituents present in
the language.
How do words group together in English? Consider the noun phrase, a sequence
of words surrounding at least one noun. Here are some examples of noun phrases

noun phrase

10 .2

• CONT EX T-FR EE GRAMMAR S

195

(thanks to Damon Runyon):

Harry the Horse
a high-class spot such as Mindy’s
the Broadway coppers
the reason he comes into the Hot Box
they
three parties from Brooklyn
What evidence do we have that these words group together (or “form constituents”)?
One piece of evidence is that they can all appear in similar syntactic environments,
for example, before a verb.

three parties from Brooklyn arrive. . .
a high-class spot such as Mindy’s attracts. . .
the Broadway coppers love. . .
they sit
But while the whole noun phrase can occur before a verb, this is not true of each
of the individual words that make up a noun phrase. The following are not grammat-
ical sentences of English (recall that we use an asterisk (*) to mark fragments that
are not grammatical English sentences):

*from arrive. . . *as attracts. . .
*the is. . .
*spot sat. . .
Thus, to correctly describe facts about the ordering of these words in English, we
must be able to say things like “Noun Phrases can occur before verbs”.
Other kinds of evidence for constituency come from what are called preposed or
postposed constructions. For example, the prepositional phrase on September sev-
enteenth can be placed in a number of different locations in the following examples,
including at the beginning (preposed) or at the end (postposed):
On September seventeenth, I’d like to ﬂy from Atlanta to Denver
I’d like to ﬂy on September seventeenth from Atlanta to Denver
I’d like to ﬂy from Atlanta to Denver on September seventeenth
But again, while the entire phrase can be placed differently, the individual words
making up the phrase cannot be
*On September, I’d like to ﬂy seventeenth from Atlanta to Denver
*On I’d like to ﬂy September seventeenth from Atlanta to Denver
*I’d like to ﬂy on September from Atlanta to Denver seventeenth

See Radford (1988) for further examples of groups of words behaving as a single
constituent.

preposed
postposed

10.2 Context-Free Grammars

The most widely used formal system for modeling constituent structure in English
and other natural languages is the Context-Free Grammar, or CFG. Context-
free grammars are also called Phrase-Structure Grammars, and the formalism
is equivalent to Backus-Naur Form, or BNF. The idea of basing a grammar on
constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was
not formalized until Chomsky (1956) and, independently, Backus (1959).
A context-free grammar consists of a set of rules or productions, each of which

CFG

rules

196 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

lexicon
NP

expresses the ways that symbols of the language can be grouped and ordered to-
gether, and a lexicon of words and symbols. For example, the following productions
express that an NP (or noun phrase) can be composed of either a ProperNoun or
a determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or
more Nouns.

NP → Det Nominal
NP → ProperNoun
Nominal → Noun | Nominal Noun
Context-free rules can be hierarchically embedded, so we can combine the pre-
vious rules with others, like the following, that express facts about the lexicon:

Det → a
Det → the
Noun → ﬂight
The symbols that are used in a CFG are divided into two classes. The symbols
that correspond to words in the language (“the”, “nightclub”) are called terminal
symbols; the lexicon is the set of rules that introduce these terminal symbols. The
symbols that express abstractions over these terminals are called non-terminals. In
each context-free rule, the item to the right of the arrow (→) is an ordered list of one
or more terminals and non-terminals; to the left of the arrow is a single non-terminal
symbol expressing some cluster or generalization. Notice that in the lexicon, the
non-terminal associated with each word is its lexical category, or part-of-speech,
which we deﬁned in Chapter 8.
A CFG can be thought of in two ways: as a device for generating sentences
and as a device for assigning a structure to a given sentence. Viewing a CFG as a
generator, we can read the → arrow as “rewrite the symbol on the left with the string
of symbols on the right”.
So starting from the symbol:
NP
we can use our ﬁrst rule to rewrite NP as:
Det Nominal
and then rewrite Nominal as:
Det Noun
and ﬁnally rewrite these parts-of-speech as:
a ﬂight
We say the string a ﬂight can be derived from the non-terminal NP. Thus, a CFG
can be used to generate a set of strings. This sequence of rule expansions is called a
derivation of the string of words. It is common to represent a derivation by a parse
tree (commonly shown inverted with the root at the top). Figure 10.1 shows the tree
representation of this derivation.
In the parse tree shown in Fig. 10.1, we can say that the node NP dominates
all the nodes in the tree (Det, Nom, Noun, a, ﬂight). We can say further that it
immediately dominates the nodes Det and Nom.
The formal language deﬁned by a CFG is the set of strings that are derivable
from the designated start symbol. Each grammar must have one designated start
symbol, which is often called S. Since context-free grammars are often used to deﬁne
sentences, S is usually interpreted as the “sentence” node, and the set of strings that
are derivable from S is the set of sentences in some simpliﬁed version of English.
Let’s add a few additional rules to our inventory. The following rule expresses
the fact that a sentence can consist of a noun phrase followed by a verb phrase:
S → NP VP I prefer a morning ﬂight

terminal

non-terminal

derivation
parse tree

dominates

start symbol

verb phrase

10 .2

• CONT EX T-FR EE GRAMMAR S

197

NP

Det

Nom

a

Noun

ﬂight

Figure 10.1 A parse tree for “a ﬂight”.

A verb phrase in English consists of a verb followed by assorted other things;
for example, one kind of verb phrase consists of a verb followed by a noun phrase:

VP → Verb NP prefer a morning ﬂight
Or the verb may be followed by a noun phrase and a prepositional phrase:

VP → Verb NP PP leave Boston in the morning
Or the verb phrase may have a verb followed by a prepositional phrase alone:

VP → Verb PP leaving on Thursday
A prepositional phrase generally has a preposition followed by a noun phrase.
For example, a common type of prepositional phrase in the ATIS corpus is used to
indicate location or direction:

PP → Preposition NP from Los Angeles
The NP inside a PP need not be a location; PPs are often used with times and
dates, and with other nouns as well; they can be arbitrarily complex. Here are ten
examples from the ATIS corpus:
to Seattle
on these ﬂights
in Minneapolis
about the ground transportation in Chicago
on Wednesday
of the round trip ﬂight on United Airlines
in the evening
of the AP ﬁfty seven ﬂight
on the ninth of July
with a stopover in Nashville
Figure 10.2 gives a sample lexicon, and Fig. 10.3 summarizes the grammar rules
we’ve seen so far, which we’ll call L0 . Note that we can use the or-symbol | to
indicate that a non-terminal has alternate possible expansions.
We can use this grammar to generate sentences of this “ATIS-language”. We
start with S, expand it to NP VP, then choose a random expansion of NP (let’s say, to
I), and a random expansion of VP (let’s say, to Verb NP), and so on until we generate
the string I prefer a morning ﬂight. Figure 10.4 shows a parse tree that represents a
complete derivation of I prefer a morning ﬂight.
It is sometimes convenient to represent a parse tree in a more compact format
called bracketed notation; here is the bracketed representation of the parse tree of
Fig. 10.4:

(10.1)

[S [NP [Pro I]] [VP [V prefer] [NP [Det a] [Nom [N morning] [Nom [N ﬂight]]]]]]

bracketed
notation

198 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

Noun → ﬂights | breeze | t ri p | morning
Verb → is | prefer | l ike | need | want | ﬂy
Adjective → chea pest | non-stop | ﬁrst | latest
| ot her | direct
Pronoun → me | I | you | it
Proper-Noun → Alaska | Baltimore | Los Angeles
| Chicago | United | American
Determiner → t he | a | an | t his | t hese | t hat
Preposition → from | t o | on | near
Conjunction → and | or | but

Figure 10.2 The lexicon for L0 .

Grammar Rules

S → NP VP
NP → Pronoun
I
| Proper-Noun
Los Angeles
| Det Nominal
a + ﬂight
Nominal → Nominal Noun morning + ﬂight
| Noun
ﬂights
VP → Verb
| Verb NP
| Verb NP PP
| Verb PP
PP → Preposition NP from + Los Angeles

Figure 10.3 The grammar for L0 , with example phrases for each rule.

Examples

I + want a morning ﬂight

do
want + a ﬂight
leave + Boston + in the morning
leaving + on Thursday

A CFG like that of L0 deﬁnes a formal language. We saw in Chapter 2 that a for-
mal language is a set of strings. Sentences (strings of words) that can be derived by a
grammar are in the formal language deﬁned by that grammar, and are called gram-
matical sentences. Sentences that cannot be derived by a given formal grammar are
not in the language deﬁned by that grammar and are referred to as ungrammatical.
This hard line between “in” and “out” characterizes all formal languages but is only
a very simpliﬁed model of how natural languages really work. This is because de-
termining whether a given sentence is part of a given natural language (say, English)
often depends on the context. In linguistics, the use of formal languages to model
natural languages is called generative grammar since the language is deﬁned by
the set of possible sentences “generated” by the grammar.

grammatical
ungrammatical

generative
grammar

10.2.1 Formal Deﬁnition of Context-Free Grammar

We conclude this section with a quick, formal description of a context-free gram-
mar and the language it generates. A context-free grammar G is deﬁned by four
parameters: N , Σ, R, S (technically this is a “4-tuple”).

10 .2

• CONT EX T-FR EE GRAMMAR S

199

S

NP

VP

Pro

Verb

NP

I

prefer

Det

Nom

a

Nom

Noun

Noun

ﬂight

morning

Figure 10.4 The parse tree for “I prefer a morning ﬂight” according to grammar L0 .

N a set of non-terminal symbols (or variables)

Σ a set of terminal symbols (disjoint from N )
R a set of rules or productions, each of the form A → β ,
where A is a non-terminal,
β is a string of symbols from the inﬁnite set of strings (Σ ∪ N )∗
S a designated start symbol and a member of N

For the remainder of the book we adhere to the following conventions when dis-
cussing the formal properties of context-free grammars (as opposed to explaining
particular facts about English or other languages).
Capital letters like A, B, and S
S
Lower-case Greek letters like α , β , and γ
Lower-case Roman letters like u, v, and w

Non-terminals
The start symbol
Strings drawn from (Σ ∪ N )∗
Strings of terminals

A language is deﬁned through the concept of derivation. One string derives an-
other one if it can be rewritten as the second one by some series of rule applications.
More formally, following Hopcroft and Ullman (1979),
if A → β is a production of R and α and γ are any strings in the set
Derivation is then a generalization of direct derivation:
Let α1 , α2 , . . . , αm be strings in (Σ ∪ N )∗, m ≥ 1, such that

(Σ ∪ N )∗, then we say that α Aγ directly derives α β γ , or α Aγ ⇒ α β γ .

α1 ⇒ α2 , α2 ⇒ α3 , . . . , αm−1 ⇒ αm

We say that α1 derives αm , or α1 ∗⇒ αm .
We can then formally deﬁne the language LG generated by a grammar G as the
set of strings composed of terminal symbols that can be derived from the designated

directly derives

derives

200 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

start symbol S.

LG = {w|w is in Σ ∗ and S ∗⇒ w}
The problem of mapping from a string of words to its parse tree is called syn-
tactic parsing; we deﬁne algorithms for parsing in Chapter 11.

syntactic
parsing

10.3 Some Grammar Rules for English

declarative

imperative

yes-no question

In this section, we introduce a few more aspects of the phrase structure of English;
for consistency we will continue to focus on sentences from the ATIS domain. Be-
cause of space limitations, our discussion is necessarily limited to highlights. Read-
ers are strongly advised to consult a good reference grammar of English, such as
Huddleston and Pullum (2002).

10.3.1 Sentence-Level Constructions

In the small grammar L0 , we provided only one sentence-level construction for
declarative sentences like I prefer a morning ﬂight. Among the large number of
constructions for English sentences, four are particularly common and important:
declaratives, imperatives, yes-no questions, and wh-questions.
Sentences with declarative structure have a subject noun phrase followed by
a verb phrase, like “I prefer a morning ﬂight”. Sentences with this structure have
a great number of different uses that we follow up on in Chapter 24. Here are a
number of examples from the ATIS domain:
I want a ﬂight from Ontario to Chicago
The ﬂight should be eleven a.m. tomorrow
The return ﬂight should leave at around seven p.m.
Sentences with imperative structure often begin with a verb phrase and have
no subject. They are called imperative because they are almost always used for
commands and suggestions; in the ATIS domain they are commands to the system.
Show the lowest fare
Give me Sunday’s ﬂights arriving in Las Vegas from New York City
List all ﬂights between ﬁve and seven p.m.
We can model this sentence structure with another rule for the expansion of S:

S → VP
Sentences with yes-no question structure are often (though not always) used to
ask questions; they begin with an auxiliary verb, followed by a subject NP, followed
by a VP. Here are some examples. Note that the third example is not a question at
all but a request; Chapter 24 discusses the uses of these question forms to perform
different pragmatic functions such as asking, requesting, or suggesting.
Do any of these ﬂights have stops?
Does American’s ﬂight eighteen twenty ﬁve serve dinner?
Can you give me the same information for United?
Here’s the rule:

S → Aux NP VP

wh-phrase
wh-word

wh-non-subject-
question

long-distance
dependencies

clause

10 .3

• SOM E GRAMMAR RU LE S FOR ENGL I SH

201

The most complex sentence-level structures we examine here are the various wh-
structures. These are so named because one of their constituents is a wh-phrase, that
is, one that includes a wh-word (who, whose, when, where, what, which, how, why).
These may be broadly grouped into two classes of sentence-level structures. The
wh-subject-question structure is identical to the declarative structure, except that
the ﬁrst noun phrase contains some wh-word.
What airlines ﬂy from Burbank to Denver?
Which ﬂights depart Burbank after noon and arrive in Denver by six p.m?
Whose ﬂights serve breakfast?
Here is a rule. Exercise 10.7 discusses rules for the constituents that make up the
Wh-NP.

S → Wh-NP VP
In the wh-non-subject-question structure, the wh-phrase is not the subject of the
sentence, and so the sentence includes another subject. In these types of sentences
the auxiliary appears before the subject NP, just as in the yes-no question structures.
Here is an example followed by a sample rule:
What ﬂights do you have from Burbank to Tacoma Washington?

S → Wh-NP Aux NP VP
Constructions like the wh-non-subject-question contain what are called long-
distance dependencies because the Wh-NP what ﬂights is far away from the predi-
cate that it is semantically related to, the main verb have in the VP. In some models
of parsing and understanding compatible with the grammar rule above, long-distance
dependencies like the relation between ﬂights and have are thought of as a semantic
relation. In such models, the job of ﬁguring out that ﬂights is the argument of have
is done during semantic interpretation. In other models of parsing, the relationship
between ﬂights and have is considered to be a syntactic relation, and the grammar is
modiﬁed to insert a small marker called a trace or empty category after the verb.
We return to such empty-category models when we introduce the Penn Treebank on
page 208.

10.3.2 Clauses and Sentences

Before we move on, we should clarify the status of the S rules in the grammars we
just described. S rules are intended to account for entire sentences that stand alone
as fundamental units of discourse. However, S can also occur on the right-hand side
of grammar rules and hence can be embedded within larger sentences. Clearly then,
there’s more to being an S than just standing alone as a unit of discourse.
What differentiates sentence constructions (i.e., the S rules) from the rest of the
grammar is the notion that they are in some sense complete. In this way they corre-
spond to the notion of a clause, which traditional grammars often describe as form-
ing a complete thought. One way of making this notion of “complete thought” more
precise is to say an S is a node of the parse tree below which the main verb of the S
has all of its arguments. We deﬁne verbal arguments later, but for now let’s just see
an illustration from the tree for I prefer a morning ﬂight in Fig. 10.4 on page 199.
The verb prefer has two arguments: the subject I and the object a morning ﬂight.
One of the arguments appears below the VP node, but the other one, the subject NP,
appears only below the S node.

202 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

10.3.3 The Noun Phrase

Our L0 grammar introduced three of the most frequent types of noun phrases that
occur in English: pronouns, proper nouns and the NP → Det Nominal construction.
The central focus of this section is on the last type since that is where the bulk of
the syntactic complexity resides. These noun phrases consist of a head, the central
noun in the noun phrase, along with various modiﬁers that can occur before or after
the head noun. Let’s take a close look at the various parts.

The Determiner

Noun phrases can begin with simple lexical determiners, as in the following exam-
ples:
a stop
those ﬂights

this ﬂight
some ﬂights

the ﬂights
any ﬂights

The role of the determiner in English noun phrases can also be ﬁlled by more
complex expressions, as follows:

United’s ﬂight
United’s pilot’s union
Denver’s mayor’s mother’s canceled ﬂight

In these examples, the role of the determiner is ﬁlled by a possessive expression
consisting of a noun phrase followed by an ’s as a possessive marker, as in the
following rule.

Det → NP (cid:48) s
The fact that this rule is recursive (since an NP can start with a Det) helps us
model the last two examples above, in which a sequence of possessive expressions
serves as a determiner.
Under some circumstances determiners are optional in English. For example,
determiners may be omitted if the noun they modify is plural:

(10.2) Show me ﬂights from San Francisco to Denver on weekdays
As we saw in Chapter 8, mass nouns also don’t require determination. Recall that
mass nouns often (not always) involve something that is treated like a substance
(including e.g., water and snow), don’t take the indeﬁnite article “a”, and don’t tend
to pluralize. Many abstract nouns are mass nouns (music, homework). Mass nouns
in the ATIS domain include breakfast, lunch, and dinner:

(10.3) Does this ﬂight serve dinner?

The Nominal

The nominal construction follows the determiner and contains any pre- and post-
head noun modiﬁers. As indicated in grammar L0 , in its simplest form a nominal
can consist of a single noun.

Nominal → Noun
As we’ll see, this rule also provides the basis for the bottom of various recursive
rules used to capture more complex nominal constructions.

10 .3

• SOM E GRAMMAR RU LE S FOR ENGL I SH

203

Before the Head Noun

Cardinal
numbers

ordinal
numbers
quantiﬁers

A number of different kinds of word classes can appear before the head noun (the
“postdeterminers”) in a nominal. These include cardinal numbers, ordinal num-
bers, quantiﬁers, and adjectives. Examples of cardinal numbers:
two friends
one stop

the second leg

Ordinal numbers include ﬁrst, second, third, and so on, but also words like next,
last, past, other, and another:
the ﬁrst one
the next day
the last ﬂight
the other American ﬂight
Some quantiﬁers (many, (a) few, several) occur only with plural count nouns:
many fares
Adjectives occur after quantiﬁers but before nouns.
a ﬁrst-class fare
a non-stop ﬂight
the longest layover
the earliest lunch ﬂight
Adjectives can also be grouped into a phrase called an adjective phrase or AP.
APs can have an adverb before the adjective (see Chapter 8 for deﬁnitions of adjec-
tives and adverbs):
the least expensive fare

After the Head Noun

A head noun can be followed by postmodiﬁers. Three kinds of nominal postmodi-
ﬁers are common in English:
prepositional phrases
all ﬂights from Cleveland
non-ﬁnite clauses
any ﬂights arriving after eleven a.m.
relative clauses
a ﬂight that serves breakfast
common in the ATIS corpus since they are used to mark the origin and destina-
tion of ﬂights.
Here are some examples of prepositional phrase postmodiﬁers, with brackets
inserted to show the boundaries of each PP; note that two or more PPs can be strung
together within a single NP:
all ﬂights [from Cleveland] [to Newark]
arrival [in San Jose] [before seven p.m.]
a reservation [on ﬂight six oh six] [from Tampa] [to Montreal]
Here’s a new nominal rule to account for postnominal PPs:

Nominal → Nominal PP
The three most common kinds of non-ﬁnite postmodiﬁers are the gerundive (-
ing), -ed, and inﬁnitive forms.
Gerundive postmodiﬁers are so called because they consist of a verb phrase that
begins with the gerundive (-ing) form of the verb. Here are some examples:
any of those [leaving on Thursday]
any ﬂights [arriving after eleven a.m.]
ﬂights [arriving within thirty minutes of each other]

adjective
phrase

non-ﬁnite

gerundive

204 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

relative
pronoun

We can deﬁne the Nominals with gerundive modiﬁers as follows, making use of
a new non-terminal GerundVP:
Nominal → Nominal GerundVP
We can make rules for GerundVP constituents by duplicating all of our VP pro-
ductions, substituting GerundV for V.
GerundVP → GerundV NP
| GerundV PP | GerundV | GerundV NP PP
GerundV can then be deﬁned as
GerundV → being | arriving | l eaving | . . .
The phrases in italics below are examples of the two other common kinds of
non-ﬁnite clauses, inﬁnitives and -ed forms:
the last ﬂight to arrive in Boston
I need to have dinner served
Which is the aircraft used by this ﬂight?
A postnominal relative clause (more correctly a restrictive relative clause), is
a clause that often begins with a relative pronoun (that and who are the most com-
mon). The relative pronoun functions as the subject of the embedded verb in the
following examples:

a ﬂight that serves breakfast
ﬂights that leave in the morning
the one that leaves at ten thirty ﬁve
We might add rules like the following to deal with these:
Nominal → Nominal RelClause
RelClause → (who | that) VP
The relative pronoun may also function as the object of the embedded verb, as
in the following example; we leave for the reader the exercise of writing grammar
rules for more complex relative clauses of this kind.

the earliest American Airlines ﬂight that I can get
Various postnominal modiﬁers can be combined, as the following examples
show:

a ﬂight [from Phoenix to Detroit] [leaving Monday evening]
evening ﬂights [from Nashville to Houston] [that serve dinner]
a friend [living in Denver] [that would like to visit me here in Washington DC]

Before the Noun Phrase

predeterminers

Word classes that modify and appear before NPs are called predeterminers. Many
of these have to do with number or amount; a common predeterminer is all:
all the ﬂights
all ﬂights
all non-stop ﬂights
The example noun phrase given in Fig. 10.5 illustrates some of the complexity
that arises when these rules are combined.

10 .3

• SOM E GRAMMAR RU LE S FOR ENGL I SH

205

NP

PreDet

NP

all

Det

the

Nom

Nom

GerundiveVP

Nom

PP

leaving before 10

Nom

PP

to Tampa

Nom

Noun

from Denver

Noun

ﬂights

morning

Figure 10.5 A parse tree for “all the morning ﬂights from Denver to Tampa leaving before 10”.

sentential
complements

10.3.4 The Verb Phrase

The verb phrase consists of the verb and a number of other constituents.
In the
simple rules we have built so far, these other constituents include NPs and PPs and
combinations of the two:

VP → Verb
disappear
VP → Verb NP prefer a morning ﬂight
VP → Verb NP PP leave Boston in the morning
VP → Verb PP leaving on Thursday
Verb phrases can be signiﬁcantly more complicated than this. Many other kinds
of constituents, such as an entire embedded sentence, can follow the verb. These are

called sentential complements:

You [VP [V said [S you had a two hundred sixty six dollar fare]]
[VP [V Tell] [NP me] [S how to get from the airport in Philadelphia to down-
town]]
I [VP [V think [S I would like to take the nine thirty ﬂight]]
Here’s a rule for these:

VP → Verb S
Similarly, another potential constituent of the VP is another VP. This is often the
case for verbs like want, would like, try, intend, need:
I want [VP to ﬂy from Milwaukee to Orlando]
Hi, I want [VP to arrange three ﬂights]

206 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

Frame

/0
NP
NP NP

PPfrom PPto

NP PPwith
VPto
VPbrst
S

Verb

eat, sleep
prefer, ﬁnd, leave
show, give
ﬂy, travel
help, load
prefer, want, need
can, would, might
mean

Example

I ate
Find [NP the ﬂight from Pittsburgh to Boston]
Show [NP me] [NP airlines with ﬂights from Pittsburgh]
I would like to ﬂy [PP from Boston] [PP to Philadelphia]
Can you help [NP me] [PP with a ﬂight]
I would prefer [VPto to go by United airlines]
I can [VPbrst go from Boston]
Does this mean [S AA has a hub in Boston]

Figure 10.6 Subcategorization frames for a set of example verbs.

transitive
intransitive

subcategorize

Subcategorizes
for

complements

Subcategorization
frame

While a verb phrase can have many possible kinds of constituents, not every
verb is compatible with every verb phrase. For example, the verb want can be used
either with an NP complement (I want a ﬂight . . . ) or with an inﬁnitive VP comple-
ment (I want to ﬂy to . . . ). By contrast, a verb like ﬁnd cannot take this sort of VP
complement (* I found to ﬂy to Dallas).
This idea that verbs are compatible with different kinds of complements is a very
old one; traditional grammar distinguishes between transitive verbs like ﬁnd, which
take a direct object NP (I found a ﬂight), and intransitive verbs like disappear,
which do not (*I disappeared a ﬂight).
Where traditional grammars subcategorize verbs into these two categories (tran-
sitive and intransitive), modern grammars distinguish as many as 100 subcategories.
We say that a verb like ﬁnd subcategorizes for an NP, and a verb like want sub-
categorizes for either an NP or a non-ﬁnite VP. We also call these constituents the
complements of the verb (hence our use of the term sentential complement above).
So we say that want can take a VP complement. These possible sets of complements
are called the subcategorization frame for the verb. Another way of talking about
the relation between the verb and these other constituents is to think of the verb as
a logical predicate and the constituents as logical arguments of the predicate. So we
can think of such predicate-argument relations as FIND ( I , A FL IGHT ) or WANT ( I , TO
FLY ). We talk more about this view of verbs and arguments in Chapter 14 when we
talk about predicate calculus representations of verb semantics. Subcategorization
frames for a set of example verbs are given in Fig. 10.6.
We can capture the association between verbs and their complements by making
separate subtypes of the class Verb (e.g., Verb-with-NP-complement, Verb-with-Inf-
VP-complement, Verb-with-S-complement, and so on):

Verb-with-NP-complement → ﬁnd | leave | repeat | . . .
Verb-with-S-complement → think | believe | say | . . .
Verb-with-Inf-VP-complement → want | try | need | . . .
Each VP rule could then be modiﬁed to require the appropriate verb subtype:

VP → Verb-with-no-complement
disappear
VP → Verb-with-NP-comp NP prefer a morning ﬂight
VP → Verb-with-S-comp S said there were two ﬂights
A problem with this approach is the signiﬁcant increase in the number of rules
and the associated loss of generality.

conjunctions
coordinate

metarules

10 .4

• TR EEBANK S

207

10.3.5 Coordination

The major phrase types discussed here can be conjoined with conjunctions like and,
or, and but to form larger constructions of the same type. For example, a coordinate
noun phrase can consist of two other noun phrases separated by a conjunction:
Please repeat [NP [NP the ﬂights] and [NP the costs]]
I need to know [NP [NP the aircraft] and [NP the ﬂight number]]
Here’s a rule that allows these structures:
NP → NP and NP
Note that the ability to form coordinate phrases through conjunctions is often
used as a test for constituency. Consider the following examples, which differ from
the ones given above in that they lack the second determiner.
Please repeat the [Nom [Nom ﬂights] and [Nom costs]]
I need to know the [Nom [Nom aircraft] and [Nom ﬂight number]]
The fact that these phrases can be conjoined is evidence for the presence of the
underlying Nominal constituent we have been making use of. Here’s a new rule for
this:

Nominal → Nominal and Nominal
The following examples illustrate conjunctions involving VPs and Ss.
What ﬂights do you have [VP [VP leaving Denver] and [VP arriving in
San Francisco]]
[S [S I’m interested in a ﬂight from Dallas to Washington] and [S I’m
also interested in going to Baltimore]]
The rules for VP and S conjunctions mirror the NP one given above.
VP → VP and VP
S → S and S
Since all the major phrase types can be conjoined in this fashion, it is also pos-
sible to represent this conjunction fact more generally; a number of grammar for-
malisms such as GPSG ((Gazdar et al., 1985)) do this using metarules such as the
following:

X → X and X
This metarule simply states that any non-terminal can be conjoined with the same
non-terminal to yield a constituent of the same type. Of course, the variable X
must be designated as a variable that stands for any non-terminal rather than a non-
terminal itself.

10.4 Treebanks

Sufﬁciently robust grammars consisting of context-free grammar rules can be used
to assign a parse tree to any sentence. This means that it is possible to build a
corpus where every sentence in the collection is paired with a corresponding parse

208 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

treebank

Penn Treebank

tree. Such a syntactically annotated corpus is called a treebank. Treebanks play
an important role in parsing, as we discuss in Chapter 11, as well as in linguistic
investigations of syntactic phenomena.
A wide variety of treebanks have been created, generally through the use of
parsers (of the sort described in the next few chapters) to automatically parse each
sentence, followed by the use of humans (linguists) to hand-correct the parses. The
Penn Treebank project (whose POS tagset we introduced in Chapter 8) has pro-
duced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal cor-
pora of English, as well as treebanks in Arabic and Chinese. A number of treebanks
use the dependency representation we will introduce in Chapter 13, including many
that are part of the Universal Dependencies project (Nivre et al., 2016b).

10.4.1 Example: The Penn Treebank Project

Figure 10.7 shows sentences from the Brown and ATIS portions of the Penn Tree-
bank.1 Note the formatting differences for the part-of-speech tags; such small dif-
ferences are common and must be dealt with in processing treebanks. The Penn
Treebank part-of-speech tagset was deﬁned in Chapter 8. The use of LISP-style
parenthesized notation for trees is extremely common and resembles the bracketed
notation we saw earlier in (10.1). For those who are not familiar with it we show a
standard node-and-line tree representation in Fig. 10.8.

((S
(NP-SBJ (DT That)
(JJ cold) (, ,)
(JJ empty) (NN sky) )
(VP (VBD was)
(ADJP-PRD (JJ full)
(PP (IN of)
(NP (NN fire)
(CC and)
(NN light) ))))

(. .) ))

(a)

((S
(NP-SBJ The/DT flight/NN )
(VP should/MD
(VP arrive/VB
(PP-TMP at/IN
(NP eleven/CD a.m/RB ))
(NP-TMP tomorrow/NN )))))

(b)

traces
syntactic
movement

Figure 10.7 Parsed sentences from the LDC Treebank3 version of the Brown (a) and ATIS
(b) corpora.

Figure 10.9 shows a tree from the Wall Street Journal. This tree shows an-
other feature of the Penn Treebanks:
the use of traces (-NONE- nodes) to mark
long-distance dependencies or syntactic movement. For example, quotations often
follow a quotative verb like say. But in this example, the quotation “We would have
to wait until we have collected on those assets” precedes the words he said. An
empty S containing only the node -NONE- marks the position after said where the
quotation sentence often occurs. This empty node is marked (in Treebanks II and
III) with the index 2, as is the quotation S at the beginning of the sentence. Such
co-indexing may make it easier for some parsers to recover the fact that this fronted
or topicalized quotation is the complement of the verb said. A similar -NONE- node

1 The Penn Treebank project released treebanks in multiple languages and in various stages; for ex-
ample, there were Treebank I (Marcus et al., 1993), Treebank II (Marcus et al., 1994), and Treebank III
releases of English treebanks. We use Treebank III for our examples.

10 .4

• TR EEBANK S

209

NP-SBJ

S

VP

DT

JJ

That

cold

,

,

JJ

NN

VBD

ADJP-PRD

empty

sky

was

JJ

PP

.

.

full

IN

NP

of

NN

CC

NN

ﬁre

and

light

Figure 10.8 The tree corresponding to the Brown corpus sentence in the previous ﬁgure.

marks the fact that there is no syntactic subject right before the verb to wait; instead,
the subject is the earlier NP We. Again, they are both co-indexed with the index 1.

( (S (‘‘ ‘‘)
(S-TPC-2
(NP-SBJ-1 (PRP We) )
(VP (MD would)
(VP (VB have)
(S

(NP-SBJ (-NONE- *-1) )
(VP (TO to)
(VP (VB wait)
(SBAR-TMP (IN until)
(S
(NP-SBJ (PRP we) )
(VP (VBP have)
(VP (VBN collected)
(PP-CLR (IN on)
(NP (DT those)(NNS assets)))))))))))))

(, ,) (’’ ’’)
(NP-SBJ (PRP he) )
(VP (VBD said)
(S (-NONE- *T*-2) ))
(. .) ))

Figure 10.9 A sentence from the Wall Street Journal portion of the LDC Penn Treebank.
Note the use of the empty -NONE- nodes.

The Penn Treebank II and Treebank III releases added further information to
make it easier to recover the relationships between predicates and arguments. Cer-

210 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

Grammar

Lexicon

PRP → we | he
DT → the | that | those
JJ → cold | empty | full
NN → sky | ﬁre | light | ﬂight | tomorrow
NNS → assets
CC → and
IN → of | at | until | on
CD → eleven
RB → a.m.
VB → arrive | have | wait
VBD → was | said
VBP → have
VBN → collected
MD → should | would
TO → to

S → NP VP .
S → NP VP
S → “ S ” , NP VP .
S → -NONE-
NP → DT NN
NP → DT NNS
NP → NN CC NN
NP → CD RB
NP → DT JJ , JJ NN
NP → PRP
NP → -NONE-
VP → MD VP
VP → VBD ADJP
VP → VBD S
VP → VBN PP
VP → VB S
VP → VB SBAR
VP → VBP VP
VP → VBN PP
VP → TO VP
SBAR → IN S
ADJP → JJ PP
PP → IN NP
Figure 10.10 A sample of the CFG grammar rules and lexical entries that would be ex-
tracted from the three treebank sentences in Fig. 10.7 and Fig. 10.9.

tain phrases were marked with tags indicating the grammatical function of the phrase
(as surface subject, logical topic, cleft, non-VP predicates) its presence in particular
text categories (headlines, titles), and its semantic function (temporal phrases, lo-
cations) (Marcus et al. 1994, Bies et al. 1995). Figure 10.9 shows examples of the
-SBJ (surface subject) and -TMP (temporal phrase) tags. Figure 10.8 shows in addi-
tion the -PRD tag, which is used for predicates that are not VPs (the one in Fig. 10.8
is an ADJP). We’ll return to the topic of grammatical function when we consider
dependency grammars and parsing in Chapter 13.

10.4.2 Treebanks as Grammars

The sentences in a treebank implicitly constitute a grammar of the language repre-
sented by the corpus being annotated. For example, from the three parsed sentences
in Fig. 10.7 and Fig. 10.9, we can extract each of the CFG rules in them. For sim-
plicity, let’s strip off the rule sufﬁxes (-SBJ and so on). The resulting grammar is
shown in Fig. 10.10.
The grammar used to parse the Penn Treebank is relatively ﬂat, resulting in very
many and very long rules. For example, among the approximately 4,500 different
rules for expanding VPs are separate rules for PP sequences of any length and every
possible arrangement of verb arguments:

VP → VBD PP
VP → VBD PP PP
VP → VBD PP PP PP
VP → VBD PP PP PP PP
VP → VB ADVP PP
VP → VB PP ADVP
VP → ADVP VB PP

10 .4

• TR EEBANK S

211

as well as even longer rules, such as

VP → VBP PP PP PP PP PP ADVP PP

which comes from the VP marked in italics:
This mostly happens because we go from football in the fall to lifting in the
winter to football again in the spring.
Some of the many thousands of NP rules include

NP → DT JJ NN
NP → DT JJ NNS
NP → DT JJ NN NN
NP → DT JJ JJ NN
NP → DT JJ CD NNS
NP → RB DT JJ NN NN
NP → RB DT JJ JJ NNS
NP → DT JJ JJ NNP NNS
NP → DT NNP NNP NNP NNP JJ NN
NP → DT JJ NNP CC JJ JJ NN NNS
NP → RB DT JJS NN NN SBAR
NP → DT VBG JJ NNP NNP CC NNP
NP → DT JJ NNS , NNS CC NN NNS NN
NP → DT JJ JJ VBG NN NNP NNP FW NNP
NP → NP JJ , JJ ‘‘ SBAR ’’ NNS

The last two of those rules, for example, come from the following two noun phrases:

[DT The] [JJ state-owned] [JJ industrial] [VBG holding] [NN company] [NNP Instituto]
[NNP Nacional] [FW de] [NNP Industria]
[NP Shearson’s] [JJ easy-to-ﬁlm], [JJ black-and-white] “[SBAR Where We Stand]”
[NNS commercials]
Viewed as a large grammar in this way, the Penn Treebank III Wall Street Journal
corpus, which contains about 1 million words, also has about 1 million non-lexical
rule tokens, consisting of about 17,500 distinct rule types.
Various facts about the treebank grammars, such as their large numbers of ﬂat
rules, pose problems for probabilistic parsing algorithms. For this reason, it is com-
mon to make various modiﬁcations to a grammar extracted from a treebank. We
discuss these further in Chapter 12.

10.4.3 Heads and Head Finding

We suggested informally earlier that syntactic constituents could be associated with
a lexical head; N is the head of an NP, V is the head of a VP. This idea of a head for
each constituent dates back to Bloomﬁeld (1914). It is central to constituent-based
grammar formalisms such as Head-Driven Phrase Structure Grammar (Pollard and
Sag, 1994), as well as the dependency-based approaches to grammar we’ll discuss
in Chapter 13. Heads and head-dependent relations have also come to play a central
role in computational linguistics with their use in probabilistic parsing (Chapter 12)
and in dependency parsing (Chapter 13).
In one simple model of lexical heads, each context-free rule is associated with
a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is
grammatically the most important. Heads are passed up the parse tree; thus, each
non-terminal in a parse tree is annotated with a single word, which is its lexical head.

212 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

S(dumped)

NP(workers)

VP(dumped)

NNS(workers)

VBD(dumped)

NP(sacks)

PP(into)

workers

dumped

NNS(sacks)

P

NP(bin)

sacks

into

DT(a)

NN(bin)

a

bin

Figure 10.11 A lexicalized tree from Collins (1999).

Figure 10.11 shows an example of such a tree from Collins (1999), in which each
non-terminal is annotated with its head.
For the generation of such a tree, each CFG rule must be augmented to identify
one right-side constituent to be the head daughter. The headword for a node is
then set to the headword of its head daughter. Choosing these head daughters is
simple for textbook examples (NN is the head of NP) but is complicated and indeed
controversial for most phrases. (Should the complementizer to or the verb be the
head of an inﬁnite verb-phrase?) Modern linguistic theories of syntax generally
include a component that deﬁnes heads (see, e.g., (Pollard and Sag, 1994)).
An alternative approach to ﬁnding a head is used in most practical computational
systems. Instead of specifying head rules in the grammar itself, heads are identiﬁed
dynamically in the context of trees for speciﬁc sentences.
In other words, once
a sentence is parsed, the resulting tree is walked to decorate each node with the
appropriate head. Most current systems rely on a simple set of hand-written rules,
such as a practical one for Penn Treebank grammars given in Collins (1999) but
developed originally by Magerman (1995). For example, the rule for ﬁnding the
head of an NP is as follows (Collins, 1999, p. 238):

• If the last word is tagged POS, return last-word.
• Else search from right to left for the ﬁrst child which is an NN, NNP, NNPS, NX, POS,
• Else search from left to right for the ﬁrst child which is an NP.
or JJR.
• Else search from right to left for the ﬁrst child which is a $, ADJP, or PRN.
• Else search from right to left for the ﬁrst child which is a CD.
• Else search from right to left for the ﬁrst child which is a JJ, JJS, RB or QP.
• Else return the last word

Selected other rules from this set are shown in Fig. 10.12. For example, for VP
rules of the form VP → Y1 · · · Yn , the algorithm would start from the left of Y1 · · ·
Yn looking for the ﬁrst Yi of type TO; if no TOs are found, it would search for the
ﬁrst Yi of type VBD; if no VBDs are found, it would search for a VBN, and so on.
See Collins (1999) for more details.

Parent Direction

ADJP

Left

ADVP
PRN
PRT
QP
S
SBAR
VP

Right
Left
Right
Left
Left
Left
Left

10 . 5

• GRAMMAR EQU IVA L ENCE AND NORMAL FORM 213

Priority List

NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS
SBAR RB
RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN

RP
$ IN NNS NN JJ RB DT CD NCD QP JJR JJS
TO IN VP S SBAR ADJP UCP NP
WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG
TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP

Figure 10.12 Selected head rules from Collins (1999). The set of head rules is often called a head percola-

tion table.

10.5 Grammar Equivalence and Normal Form

normal form

Chomsky
normal form

binary
branching

A formal language is deﬁned as a (possibly inﬁnite) set of strings of words. This
suggests that we could ask if two grammars are equivalent by asking if they gener-
ate the same set of strings. In fact, it is possible to have two distinct context-free
grammars generate the same language.
We usually distinguish two kinds of grammar equivalence: weak equivalence
and strong equivalence. Two grammars are strongly equivalent if they generate the
same set of strings and if they assign the same phrase structure to each sentence
(allowing merely for renaming of the non-terminal symbols). Two grammars are
weakly equivalent if they generate the same set of strings but do not assign the same
phrase structure to each sentence.
It is sometimes useful to have a normal form for grammars, in which each of
the productions takes a particular form. For example, a context-free grammar is in
Chomsky normal form (CNF) (Chomsky, 1963) if it is -free and if in addition
each production is either of the form A → B C or A → a. That is, the right-hand side
of each rule either has two non-terminal symbols or one terminal symbol. Chomsky
normal form grammars are binary branching, that is they have binary trees (down
to the prelexical nodes). We make use of this binary branching property in the CKY
parsing algorithm in Chapter 11.
Any context-free grammar can be converted into a weakly equivalent Chomsky
normal form grammar. For example, a rule of the form

A → B C D
can be converted into the following two CNF rules (Exercise 10.8 asks the reader to
formulate the complete algorithm):

A → B X
X → C D
Sometimes using binary branching can actually produce smaller grammars. For
example, the sentences that might be characterized as

VP -> VBD NP PP*

are represented in the Penn Treebank by this series of rules:

VP → VBD NP PP
VP → VBD NP PP PP

214 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

VP → VBD NP PP PP PP
VP → VBD NP PP PP PP PP

...

but could also be generated by the following two-rule grammar:

VP → VBD NP PP
VP → VP PP

The generation of a symbol A with a potentially inﬁnite sequence of symbols B with
a rule of the form A → A B is known as Chomsky-adjunction.

Chomsky-
adjunction

10.6 Lexicalized Grammars

The approach to grammar presented thus far emphasizes phrase-structure rules while
minimizing the role of the lexicon. However, as we saw in the discussions of
agreement, subcategorization, and long distance dependencies, this approach leads
to solutions that are cumbersome at best, yielding grammars that are redundant,
hard to manage, and brittle. To overcome these issues, numerous alternative ap-
proaches have been developed that all share the common theme of making bet-
ter use of the lexicon. Among the more computationally relevant approaches are
Lexical-Functional Grammar (LFG) (Bresnan, 1982), Head-Driven Phrase Structure
Grammar (HPSG) (Pollard and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi,
1985), and Combinatory Categorial Grammar (CCG). These approaches differ with
respect to how lexicalized they are — the degree to which they rely on the lexicon
as opposed to phrase structure rules to capture facts about the language.
The following section provides an introduction to CCG, a heavily lexicalized
approach motivated by both syntactic and semantic considerations, which we will
return to in Chapter 14. Chapter 13 discusses dependency grammars, an approach
that eliminates phrase-structure rules entirely.

10.6.1 Combinatory Categorial Grammar

combinatory
categorial

categorial
grammar

In this section, we provide an overview of categorial grammar (Ajdukiewicz 1935,
Bar-Hillel 1953), an early lexicalized grammar model, as well as an important mod-
ern extension, combinatory categorial grammar, or CCG (Steedman 1996,Steed-
grammar man 1989,Steedman 2000).
The categorial approach consists of three major elements: a set of categories,
a lexicon that associates words with categories, and a set of rules that govern how
categories combine in context.

Categories

Categories are either atomic elements or single-argument functions that return a cat-
egory as a value when provided with a desired category as argument. More formally,
we can deﬁne C , a set of categories for a grammar as follows:
• A ⊆ C , where A is a given set of atomic elements
• (X/Y), (X\Y) ∈ C , if X, Y ∈ C
The slash notation shown here is used to deﬁne the functions in the grammar.
It speciﬁes the type of the expected argument, the direction it is expected be found,
and the type of the result. Thus, (X/Y) is a function that seeks a constituent of type

10 .6

• L EX ICA L I Z ED GRAMMAR S

215

Y to its right and returns a value of X; (X\Y) is the same except it seeks its argument
to the left.
The set of atomic categories is typically very small and includes familiar el-
ements such as sentences and noun phrases. Functional categories include verb
phrases and complex noun phrases among others.

The Lexicon

The lexicon in a categorial approach consists of assignments of categories to words.
These assignments can either be to atomic or functional categories, and due to lexical
ambiguity words can be assigned to multiple categories. Consider the following
sample lexical entries.

ﬂight :
N
Miami :
NP
cancel : (S\NP)/NP
Nouns and proper nouns like ﬂight and Miami are assigned to atomic categories,
reﬂecting their typical role as arguments to functions. On the other hand, a transitive
verb like cancel is assigned the category (S\NP)/NP: a function that seeks an NP on
its right and returns as its value a function with the type (S\NP). This function can,
in turn, combine with an NP on the left, yielding an S as the result. This captures the
kind of subcategorization information discussed in Section 10.3.4, however here the
information has a rich, computationally useful, internal structure.
Ditransitive verbs like give, which expect two arguments after the verb, would
have the category ((S\NP)/NP)/NP: a function that combines with an NP on its
right to yield yet another function corresponding to the transitive verb (S\NP)/NP
category such as the one given above for cancel.

Rules

The rules of a categorial grammar specify how functions and their arguments com-
bine. The following two rule templates constitute the basis for all categorial gram-
mars.

(10.4)
(10.5)

X /Y Y ⇒ X
Y X \Y ⇒ X
The ﬁrst rule applies a function to its argument on the right, while the second
looks to the left for its argument. We’ll refer to the ﬁrst as forward function appli-
cation, and the second as backward function application. The result of applying
either of these rules is the category speciﬁed as the value of the function being ap-
plied.
Given these rules and a simple lexicon, let’s consider an analysis of the sentence
United serves Miami. Assume that serves is a transitive verb with the category
(S\NP)/NP and that United and Miami are both simple NPs. Using both forward
and backward function application, the derivation would proceed as follows:
Unit ed
serves
Miami
NP
(S\NP)/NP NP
S\NP

S

>

<

216 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

Categorial grammar derivations are illustrated growing down from the words,
rule applications are illustrated with a horizontal line that spans the elements in-
volved, with the type of the operation indicated at the right end of the line. In this
example, there are two function applications: one forward function application indi-
cated by the > that applies the verb serves to the NP on its right, and one backward
function application indicated by the < that applies the result of the ﬁrst to the NP
United on its left.
With the addition of another rule, the categorial approach provides a straight-
forward way to implement the coordination metarule described earlier on page 207.
Recall that English permits the coordination of two constituents of the same type,
resulting in a new constituent of the same type. The following rule provides the
mechanism to handle such examples.

S\NP

S\NP

>

<Φ>

X CON J X ⇒ X
This rule states that when two constituents of the same category are separated by a
constituent of type CONJ they can be combined into a single larger constituent of
the same type. The following derivation illustrates the use of this rule.
We
fl ew
to
Geneva
and
drove
to
Chamonix
NP (S\NP)/PP PP/NP
NP
CONJ (S\NP)/PP PP/NP
NP
PP

(10.6)

PP

>

>

>

<

S\NP
S
Here the two S\NP constituents are combined via the conjunction operator <Φ>
to form a larger constituent of the same type, which can then be combined with the
subject NP via backward function application.
These examples illustrate the lexical nature of the categorial grammar approach.
The grammatical facts about a language are largely encoded in the lexicon, while the
rules of the grammar are boiled down to a set of three rules. Unfortunately, the basic
categorial approach does not give us any more expressive power than we had with
traditional CFG rules; it just moves information from the grammar to the lexicon. To
move beyond these limitations CCG includes operations that operate over functions.
The ﬁrst pair of operators permit us to compose adjacent functions.
X /Y Y /Z ⇒ X /Z
Y \Z X \Y ⇒ X \Z
The ﬁrst rule, called forward composition, can be applied to adjacent con-
stituents where the ﬁrst is a function seeking an argument of type Y to its right, and
the second is a function that providesY as a result. This rule allows us to compose
these two functions into a single one with the type of the ﬁrst constituent and the
argument of the second. Although the notation is a little awkward, the second rule,
backward composition is the same, except that we’re looking to the left instead of
to the right for the relevant arguments. Both kinds of composition are signalled by a
B in CCG diagrams, accompanied by a < or > to indicate the direction.
The next operator is type raising. Type raising elevates simple categories to the
status of functions. More speciﬁcally, type raising takes a category and converts
it to function that seeks as an argument a function that takes the original category

(10.7)
(10.8)

forward
composition

backward
composition

type raising

10 .6

• L EX ICA L I Z ED GRAMMAR S

217

(10.9)
(10.10)

as its argument. The following schema show two versions of type raising: one for
arguments to the right, and one for the left.
X ⇒ T /(T \X )
X ⇒ T \(T /X )
The category T in these rules can correspond to any of the atomic or functional
categories already present in the grammar.
A particularly useful example of type raising transforms a simple NP argument
in subject position to a function that can compose with a following VP. To see how
this works, let’s revisit our earlier example of United serves Miami. Instead of clas-
sifying United as an NP which can serve as an argument to the function attached to
serve, we can use type raising to reinvent it as a function in its own right as follows.
NP ⇒ S/(S\NP)
Combining this type-raised constituent with the forward composition rule (10.7)
permits the following alternative to our previous derivation.
United
serves
Miami
NP
(S\NP)/NP NP
S/(S\NP)
S/NP

>T

>B

S
By type raising United to S/(S\NP), we can compose it with the transitive verb
serves to yield the (S/NP) function needed to complete the derivation.
There are several interesting things to note about this derivation. First, is it
provides a left-to-right, word-by-word derivation that more closely mirrors the way
humans process language. This makes CCG a particularly apt framework for psy-
cholinguistic studies. Second, this derivation involves the use of an intermediate
unit of analysis, United serves, that does not correspond to a traditional constituent
in English. This ability to make use of such non-constituent elements provides CCG
with the ability to handle the coordination of phrases that are not proper constituents,
as in the following example.
(10.11) We ﬂew IcelandAir to Geneva and SwissAir to London.
Here, the segments that are being coordinated are IcelandAir to Geneva and
SwissAir to London, phrases that would not normally be considered constituents, as
can be seen in the following standard derivation for the verb phrase ﬂew IcelandAir
to Geneva.

>

ﬂew
(VP/PP)/NP
VP/PP

IcelandAir
NP

>

to
PP/NP
PP

Geneva
NP

>

>

VP
In this derivation, there is no single constituent that corresponds to IcelandAir
to Geneva, and hence no opportunity to make use of the <Φ> operator. Note that
complex CCG categories can can get a little cumbersome, so we’ll use VP as a
shorthand for (S\NP) in this and the following derivations.
The following alternative derivation provides the required element through the
use of both backward type raising (10.10) and backward function composition (10.8).

218 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

Geneva
NP

ﬂew
(V P/PP)/NP

IcelandAir
NP
(V P/PP)\((V P/PP)/NP)

to
PP/NP
PP
V P\(V P/PP)
V P\((V P/PP)/NP)
V P
Applying the same analysis to SwissAir to London satisﬁes the requirements
for the <Φ> operator, yielding the following derivation for our original example
(10.11).

<T

<T

<B

>

<

ﬂew
(V P/PP)/NP

Geneva
NP

IcelandAir
NP
(V P/PP)\((V P/PP)/NP)

to
PP/NP
PP
V P\(V P/PP)
V P\((V P/PP)/NP)

<T

<T

>

<

and
CON J

London
NP

SwissAir
NP
(V P/PP)\((V P/PP)/NP)

to
PP/NP
PP
V P\(V P/PP)
V P\((V P/PP)/NP)

<T

<T

<Φ>

>

<

V P\((V P/PP)/NP)
V P

<

Finally, let’s examine how these advanced operators can be used to handle long-
distance dependencies (also referred to as syntactic movement or extraction). As
mentioned in Section 10.3.1, long-distance dependencies arise from many English
constructions including wh-questions, relative clauses, and topicalization. What
these constructions have in common is a constituent that appears somewhere dis-
tant from its usual, or expected, location. Consider the following relative clause as
an example.
the ﬂight that United diverted
Here, divert is a transitive verb that expects two NP arguments, a subject NP to its
left and a direct object NP to its right; its category is therefore (S\NP)/NP. However,
in this example the direct object the ﬂight has been “moved” to the beginning of the
clause, while the subject United remains in its normal position. What is needed is a
way to incorporate the subject argument, while dealing with the fact that the ﬂight is
not in its expected location.
The following derivation accomplishes this, again through the combined use of
type raising and function composition.
the
ﬂight
that
NP/N N (NP\NP)/(S/NP)
NP

diverted
(S\NP)/NP

>

United
NP
S/(S\NP)
S/NP

>T

NP\NP

>B

>

<

NP
As we saw with our earlier examples, the ﬁrst step of this derivation is type raising
United to the category S/(S\NP) allowing it to combine with diverted via forward
composition. The result of this composition is S/NP which preserves the fact that we
are still looking for an NP to ﬁll the missing direct object. The second critical piece
is the lexical category assigned to the word that: (NP\NP)/(S/NP). This function
seeks a verb phrase missing an argument to its right, and transforms it into an NP
seeking a missing element to its left, precisely where we ﬁnd the ﬂight.

10 .7

• SUMMARY

219

CCGBank

As with phrase-structure approaches, treebanks play an important role in CCG-
based approaches to parsing. CCGBank (Hockenmaier and Steedman, 2007) is the
largest and most widely used CCG treebank. It was created by automatically trans-
lating phrase-structure trees from the Penn Treebank via a rule-based approach. The
method produced successful translations of over 99% of the trees in the Penn Tree-
bank resulting in 48,934 sentences paired with CCG derivations. It also provides
a lexicon of 44,000 words with over 1200 categories. Chapter 12 will discuss how
these resources can be used to train CCG parsers.

10.7 Summary

This chapter has introduced a number of fundamental concepts in syntax through

the use of context-free grammars.

• In many languages, groups of consecutive words act as a group or a con-
stituent, which can be modeled by context-free grammars (which are also

known as phrase-structure grammars).

• A context-free grammar consists of a set of rules or productions, expressed
over a set of non-terminal symbols and a set of terminal symbols. Formally,
a particular context-free language is the set of strings that can be derived

from a particular context-free grammar.

• A generative grammar is a traditional name in linguistics for a formal lan-
guage that is used to model the grammar of a natural language.
• There are many sentence-level grammatical constructions in English; declar-
these can be modeled with context-free rules.

ative, imperative, yes-no question, and wh-question are four common types;

• An English noun phrase can have determiners, numbers, quantiﬁers, and
ber of postmodiﬁers; gerundive VPs, inﬁnitives VPs, and past participial

adjective phrases preceding the head noun, which can be followed by a num-

VPs are common possibilities.
• Subjects in English agree with the main verb in person and number.
• Verbs can be subcategorized by the types of complements they expect. Sim-
ple subcategories are transitive and intransitive; most grammars include
many more categories than these.
• Treebanks of parsed sentences exist for many genres of English and for many
languages. Treebanks can be searched with tree-search tools.
• Any context-free grammar can be converted to Chomsky normal form, in
which the right-hand side of each rule has either two non-terminals or a single
terminal.
• Lexicalized grammars place more emphasis on the structure of the lexicon,
lessening the burden on pure phrase-structure rules.
• Combinatorial categorial grammar (CCG) is an important computationally
relevant lexicalized approach.

220 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

Bibliographical and Historical Notes

[The origin of the idea of phrasal constituency, cited in Percival (1976)]:
den sprachlichen Ausdruck f ¨ur die willk ¨urliche
Gliederung einer Gesammtvorstellung in ihre
in logische Beziehung zueinander gesetzten Bestandteile’
[the linguistic expression for the arbitrary division of a total idea
into its constituent parts placed in logical relations to one another]
W. Wundt

According to Percival (1976), the idea of breaking up a sentence into a hierar-
chy of constituents appeared in the V ¨olkerpsychologie of the groundbreaking psy-
chologist Wilhelm Wundt (Wundt, 1900). Wundt’s idea of constituency was taken
up into linguistics by Leonard Bloomﬁeld in his early book An Introduction to the
Study of Language (Bloomﬁeld, 1914). By the time of his later book, Language
(Bloomﬁeld, 1933a), what was then called “immediate-constituent analysis” was a
well-established method of syntactic study in the United States. By contrast, tra-
ditional European grammar, dating from the Classical period, deﬁned relations be-
tween words rather than constituents, and European syntacticians retained this em-
phasis on such dependency grammars, the subject of Chapter 13.
American Structuralism saw a number of speciﬁc deﬁnitions of the immediate
constituent, couched in terms of their search for a “discovery procedure”: a method-
ological algorithm for describing the syntax of a language. In general, these attempt
to capture the intuition that “The primary criterion of the immediate constituent is the
degree in which combinations behave as simple units” (Bazell, 1966, p. 284). The
most well known of the speciﬁc deﬁnitions is Harris’ idea of distributional similarity
to individual units, with the substitutability test. Essentially, the method proceeded
by breaking up a construction into constituents by attempting to substitute simple
structures for possible constituents—if a substitution of a simple form, say, man,
was substitutable in a construction for a more complex set (like intense young man),
then the form intense young man was probably a constituent. Harris’s test was the
beginning of the intuition that a constituent is a kind of equivalence class.
The ﬁrst formalization of this idea of hierarchical constituency was the phrase-
structure grammar deﬁned in Chomsky (1956) and further expanded upon (and
argued against) in Chomsky (1957) and Chomsky (1975). From this time on, most
generative linguistic theories were based at least in part on context-free grammars or
generalizations of them (such as Head-Driven Phrase Structure Grammar (Pollard
and Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), Government and
Binding (Chomsky, 1981), and Construction Grammar (Kay and Fillmore, 1999),
inter alia); many of these theories used schematic context-free templates known as
X-bar schemata, which also relied on the notion of syntactic head.
Shortly after Chomsky’s initial work, the context-free grammar was reinvented
by Backus (1959) and independently by Naur et al. (1960) in their descriptions of
the ALGOL programming language; Backus (1996) noted that he was inﬂuenced by
the productions of Emil Post and that Naur’s work was independent of his (Backus’)
own. (Recall the discussion on page ?? of multiple invention in science.) After this
early work, a great number of computational models of natural language processing
were based on context-free grammars because of the early development of efﬁcient
algorithms to parse these grammars (see Chapter 11).

X-bar
schemata

EX ERC I SE S

221

As we have already noted, grammars based on context-free rules are not ubiqui-
tous. Various classes of extensions to CFGs are designed speciﬁcally to handle long-
distance dependencies. We noted earlier that some grammars treat long-distance-
dependent items as being related semantically but not syntactically; the surface syn-
tax does not represent the long-distance link (Kay and Fillmore 1999, Culicover and
Jackendoff 2005). But there are alternatives.
One extended formalism is Tree Adjoining Grammar (TAG)
(Joshi, 1985).
The primary TAG data structure is the tree, rather than the rule. Trees come in two
kinds: initial trees and auxiliary trees. Initial trees might, for example, represent
simple sentential structures, and auxiliary trees add recursion into a tree. Trees are
combined by two operations called substitution and adjunction. The adjunction
operation handles long-distance dependencies. See Joshi (1985) for more details.
An extension of Tree Adjoining Grammar, called Lexicalized Tree Adjoining Gram-
mars is discussed in Chapter 12. Tree Adjoining Grammar is a member of the family

of mildly context-sensitive languages.

We mentioned on page 208 another way of handling long-distance dependencies,
based on the use of empty categories and co-indexing. The Penn Treebank uses
this model, which draws (in various Treebank corpora) from the Extended Standard
Theory and Minimalism (Radford, 1997).
Readers interested in the grammar of English should get one of the three large
reference grammars of English: Huddleston and Pullum (2002), Biber et al. (1999),
and Quirk et al. (1985). Another useful reference is McCawley (1998).
There are many good introductory textbooks on syntax from different perspec-
tives. Sag et al. (2003) is an introduction to syntax from a generative perspective,
focusing on the use of phrase-structure rules, uniﬁcation, and the type hierarchy in
Head-Driven Phrase Structure Grammar. Van Valin, Jr. and La Polla (1997) is an
introduction from a functional perspective, focusing on cross-linguistic data and on
the functional motivation for syntactic structures.

generative

functional

Exercises

10.1 Draw tree structures for the following ATIS phrases:
1. Dallas
2. from Denver
3. after ﬁve p.m.
4. arriving in Washington
5. early ﬂights
6. all redeye ﬂights
7. on Thursday
8. a one-way fare
9. any delays in Denver
10.2 Draw tree structures for the following ATIS sentences:
1. Does American airlines have a ﬂight between ﬁve a.m. and six a.m.?
2. I would like to ﬂy on American airlines.
3. Please repeat that.
4. Does American 487 have a ﬁrst-class section?
5. I need to ﬂy between Philadelphia and Atlanta.
6. What is the fare from Atlanta to Denver?

222 CHA PTER 10

• FORMAL GRAMMAR S O F ENG L I SH

7. Is there an American airlines ﬂight from Philadelphia to Dallas?
10.3 Assume a grammar that has many VP rules for different subcategorizations,
as expressed in Section 10.3.4, and differently subcategorized verb rules like
Verb-with-NP-complement. How would the rule for postnominal relative clauses
(10.4) need to be modiﬁed if we wanted to deal properly with examples like
the earliest ﬂight that you have? Recall that in such examples the pronoun
that is the object of the verb get. Your rules should allow this noun phrase but
should correctly rule out the ungrammatical S *I get.
10.4 Does your solution to the previous problem correctly model the NP the earliest
ﬂight that I can get? How about the earliest ﬂight that I think my mother
wants me to book for her? Hint: this phenomenon is called long-distance

dependency.

10.5 Write rules expressing the verbal subcategory of English auxiliaries; for ex-
ample, you might have a rule verb-with-bare-stem-VP-complement → can.
10.6 NPs like Fortune’s ofﬁce or my uncle’s marks are called possessive or genitive
noun phrases. We can model possessive noun phrases by treating the sub-NP
like Fortune’s or my uncle’s as a determiner of the following head noun. Write
grammar rules for English possessives. You may treat ’s as if it were a separate
word (i.e., as if there were always a space before ’s).
10.7 Page 201 discussed the need for a Wh-NP constituent. The simplest Wh-NP
is one of the Wh-pronouns (who, whom, whose, which). The Wh-words what
and which can be determiners: which four will you have?, what credit do you
have with the Duke? Write rules for the different types of Wh-NPs.
10.8 Write an algorithm for converting an arbitrary context-free grammar into Chom-
sky normal form.

possessive
genitive

CHAPTER

11 Syntactic Parsing

One morning I shot an elephant in my pajamas.
How he got into my pajamas I don’t know.

Groucho Marx, Animal Crackers, 1930

Syntactic parsing is the task of recognizing a sentence and assigning a syntactic
structure to it. This chapter focuses on the structures assigned by context-free gram-
mars of the kind described in Chapter 10. Since they are based on a purely declar-
ative formalism, context-free grammars don’t specify how the parse tree for a given
sentence should be computed. We therefore need to specify algorithms that employ
these grammars to efﬁciently produce correct trees.
Parse trees are directly useful in applications such as grammar checking in
word-processing systems: a sentence that cannot be parsed may have grammatical
errors (or at least be hard to read). More typically, however, parse trees serve as an
important intermediate stage of representation for semantic analysis (as we show in
Chapter 15) and thus play an important role in applications like question answering
and information extraction. For example, to answer the question
What books were written by British women authors before 1800?
we’ll need to know that the subject of the sentence was what books and that the by-
adjunct was British women authors to help us ﬁgure out that the user wants a list of
books (and not a list of authors).
Before presenting any algorithms, we begin by discussing how the ambiguity
arises again in this context and the problems it presents. The section that fol-
lows then presents the Cocke-Kasami-Younger (CKY) algorithm (Kasami 1965,
Younger 1967), the standard dynamic programming approach to syntactic parsing.
Recall that we’ve already seen applications of dynamic programming algorithms in
the Minimum-Edit-Distance and Viterbi algorithms of earlier chapters. Finally, we
discuss partial parsing methods, for use in situations in which a superﬁcial syntac-
tic analysis of an input may be sufﬁcient.

11.1 Ambiguity

Structural
ambiguity

Ambiguity is perhaps the most serious problem faced by syntactic parsers. Chap-

ter 8 introduced the notions of part-of-speech ambiguity and part-of-speech dis-

ambiguation. Here, we introduce a new kind of ambiguity, called structural ambi-
guity, which arises from many commonly used rules in phrase-structure grammars.
To illustrate the issues associated with structural ambiguity, we’ll make use of a new
toy grammar L1 , shown in Figure 11.1, which consists of the L0 grammar from the
last chapter augmented with a few additional rules.
Structural ambiguity occurs when the grammar can assign more than one parse
to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal

224 CHA PTER 11

• SYNTAC T IC PAR S ING

Grammar

S → NP VP
S → Aux NP VP
S → VP
NP → Pronoun
NP → Proper-Noun
NP → Det Nominal
Nominal → Noun
Nominal → Nominal Noun
Nominal → Nominal PP
VP → Verb
VP → Verb NP
VP → Verb NP PP
VP → Verb PP
VP → VP PP
PP → Preposition NP

Lexicon

Det → that | this | the | a
Noun → book | ﬂight | meal | money
Verb → book | include | prefer
Pronoun → I | she | me
Proper-Noun → Houston | NWA
Aux → does
Preposition → from | to | on | near | through

Figure 11.1 The L1 miniature English grammar and lexicon.

S

S

NP

VP

NP

VP

Pronoun

Verb

NP

Pronoun

VP

PP

I

shot

Det

Nominal

I

Verb

NP

in my pajamas

an

Nominal

PP

shot

Det

Nominal

Noun

in my pajamas

an

Noun

elephant

elephant

Figure 11.2 Two parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous
reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which
Captain Spaulding did the shooting in his pajamas.

Attachment
ambiguity

Crackers is ambiguous because the phrase in my pajamas can be part of the NP
headed by elephant or a part of the verb phrase headed by shot. Figure 11.2 illus-
trates these two analyses of Marx’s line using rules from L1 .
Structural ambiguity, appropriately enough, comes in many forms. Two common

kinds of ambiguity are attachment ambiguity and coordination ambiguity.

A sentence has an attachment ambiguity if a particular constituent can be at-
tached to the parse tree at more than one place. The Groucho Marx sentence is
an example of PP-attachment ambiguity. Various kinds of adverbial phrases are
also subject to this kind of ambiguity. For instance, in the following example the
gerundive-VP ﬂying to Paris can be part of a gerundive sentence whose subject is
the Eiffel Tower or it can be an adjunct modifying the VP headed by saw:

Coordination
ambiguity

Syntactic
disambiguation

11 . 2

• CKY PAR S ING : A DYNAM IC PROGRAMM ING A P PROACH

225

(11.1) We saw the Eiffel Tower ﬂying to Paris.
In coordination ambiguity different sets of phrases can be conjoined by a con-
junction like and. For example, the phrase old men and women can be bracketed as
[old [men and women]], referring to old men and old women, or as [old men] and
[women], in which case it is only the men who are old.
These ambiguities combine in complex ways in real sentences. A program that
summarized the news, for example, would need to be able to parse sentences like
the following from the Brown corpus:
(11.2) President Kennedy today pushed aside other White House business to
devote all his time and attention to working on the Berlin crisis address he
will deliver tomorrow night to the American people over nationwide
television and radio.
This sentence has a number of ambiguities, although since they are semantically
unreasonable, it requires a careful reading to see them. The last noun phrase could be
parsed [nationwide [television and radio]] or [[nationwide television] and radio].
The direct object of pushed aside should be other White House business but could
also be the bizarre phrase [other White House business to devote all his time and
attention to working] (i.e., a structure like Kennedy afﬁrmed [his intention to propose
a new budget to address the deﬁcit]). Then the phrase on the Berlin crisis address he
will deliver tomorrow night to the American people could be an adjunct modifying
the verb pushed. A PP like over nationwide television and radio could be attached
to any of the higher VPs or NPs (e.g., it could modify people or night).
The fact that there are many grammatically correct but semantically unreason-
able parses for naturally occurring sentences is an irksome problem that affects all
parsers. Ultimately, most natural language processing systems need to be able to
choose a single correct parse from the multitude of possible parses through a process
of syntactic disambiguation. Effective disambiguation algorithms require statisti-
cal, semantic, and contextual knowledge sources that vary in how well they can be
integrated into parsing algorithms.
Fortunately, the CKY algorithm presented in the next section is designed to efﬁ-
ciently handle structural ambiguities of the kind we’ve been discussing. And as we’ll
see in Chapter 12, there are straightforward ways to integrate statistical techniques
into the basic CKY framework to produce highly accurate parsers.

11.2 CKY Parsing: A Dynamic Programming Approach

The previous section introduced some of the problems associated with ambiguous
grammars. Fortunately, dynamic programming provides a powerful framework for
addressing these problems, just as it did with the Minimum Edit Distance, Viterbi,
and Forward algorithms. Recall that dynamic programming approaches systemati-
cally ﬁll in tables of solutions to sub-problems. When complete, the tables contain
the solution to all the sub-problems needed to solve the problem as a whole.
In
the case of syntactic parsing, these sub-problems represent parse trees for all the
constituents detected in the input.
The dynamic programming advantage arises from the context-free nature of our
grammar rules — once a constituent has been discovered in a segment of the input
we can record its presence and make it available for use in any subsequent derivation
that might require it. This provides both time and storage efﬁciencies since subtrees

226 CHA PTER 11

• SYNTAC T IC PAR S ING

Unit
productions

can be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-
Younger (CKY) algorithm, the most widely used dynamic-programming based ap-
proach to parsing. Related approaches include the Earley algorithm (Earley, 1970)
and chart parsing (Kaplan 1973, Kay 1982).

11.2.1 Conversion to Chomsky Normal Form

We begin our investigation of the CKY algorithm by examining the requirement
that grammars used with it must be in Chomsky Normal Form (CNF). Recall from
Chapter 10 that grammars in CNF are restricted to rules of the form A → B C or
A → w. That is, the right-hand side of each rule must expand either to two non-
terminals or to a single terminal. Restricting a grammar to CNF does not lead to
any loss in expressiveness, since any context-free grammar can be converted into
a corresponding CNF grammar that accepts exactly the same set of strings as the
original grammar.
Let’s start with the process of converting a generic CFG into one represented in
CNF. Assuming we’re dealing with an -free grammar, there are three situations we
need to address in any generic grammar: rules that mix terminals with non-terminals
on the right-hand side, rules that have a single non-terminal on the right-hand side,
and rules in which the length of the right-hand side is greater than 2.
The remedy for rules that mix terminals and non-terminals is to simply introduce
a new dummy non-terminal that covers only the original terminal. For example, a
rule for an inﬁnitive verb phrase such as INF-VP → to VP would be replaced by the
two rules INF-VP → TO VP and TO → to.
Rules with a single non-terminal on the right are called unit productions. We
can eliminate unit productions by rewriting the right-hand side of the original rules
with the right-hand side of all the non-unit production rules that they ultimately lead
to. More formally, if A ∗⇒ B by a chain of one or more unit productions and B → γ
is a non-unit production in our grammar, then we add A → γ for each such rule in
the grammar and discard all the intervening unit productions. As we demonstrate
with our toy grammar, this can lead to a substantial ﬂattening of the grammar and a
consequent promotion of terminals to fairly high levels in the resulting trees.
Rules with right-hand sides longer than 2 are normalized through the introduc-
tion of new non-terminals that spread the longer sequences over several new rules.
Formally, if we have a rule like

A → B C γ
we replace the leftmost pair of non-terminals with a new non-terminal and introduce
a new production result in the following new rules:
A → X1 γ
X1 → B C
In the case of longer right-hand sides, we simply iterate this process until the of-
fending rule has been replaced by rules of length 2. The choice of replacing the
leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results
in binary rules would sufﬁce.
In our current grammar, the rule S → Aux NP VP would be replaced by the two
rules S → X1 VP and X1 → Aux NP.
The entire conversion process can be summarized as follows:
1. Copy all conforming rules to the new grammar unchanged.

11 . 2

• CKY PAR S ING : A DYNAM IC PROGRAMM ING A P PROACH

227

L1 in CNF

S → NP VP
S → X1 VP
X1 → Aux NP
S → book | include | prefer
S → Verb NP
S → X2 PP
S → Verb PP
S → VP PP
NP → I | she | me
NP → TWA | Houston
NP → Det Nominal
Nominal → book | ﬂight | meal | money
Nominal → Nominal Noun
Nominal → Nominal PP
VP → book | include | prefer
VP → Verb NP
VP → X2 PP
X2 → Verb NP
VP → Verb PP
VP → VP PP
PP → Preposition NP

L1 Grammar

S → NP VP
S → Aux NP VP
S → VP

NP → Pronoun
NP → Proper-Noun
NP → Det Nominal
Nominal → Noun
Nominal → Nominal Noun
Nominal → Nominal PP
VP → Verb
VP → Verb NP
VP → Verb NP PP
VP → Verb PP
VP → VP PP
PP → Preposition NP

Figure 11.3 L1 Grammar and its conversion to CNF. Note that although they aren’t shown
here, all the original lexical entries from L1 carry over unchanged as well.

2. Convert terminals within rules to dummy non-terminals.
3. Convert unit-productions.
4. Make all rules binary and add them to new grammar.
Figure 11.3 shows the results of applying this entire conversion procedure to
the L1 grammar introduced earlier on page 224. Note that this ﬁgure doesn’t show
the original lexical rules; since these original lexical rules are already in CNF, they
all carry over unchanged to the new grammar. Figure 11.3 does, however, show
the various places where the process of eliminating unit productions has, in effect,
created new lexical rules. For example, all the original verbs have been promoted to
both VPs and to Ss in the converted grammar.

11.2.2 CKY Recognition

With our grammar now in CNF, each non-terminal node above the part-of-speech
level in a parse tree will have exactly two daughters. A two-dimensional matrix can
be used to encode the structure of an entire tree. For a sentence of length n, we will
work with the upper-triangular portion of an (n + 1) × (n + 1) matrix. Each cell [i, j]
in this matrix contains the set of non-terminals that represent all the constituents that
span positions i through j of the input. Since our indexing scheme begins with 0,
it’s natural to think of the indexes as pointing at the gaps between the input words
(as in 0 Book 1 t hat 2 ﬂight 3 ). It follows then that the cell that represents the entire
input resides in position [0, n] in the matrix.
Since each non-terminal entry in our table has two daughters in the parse, it fol-
lows that for each constituent represented by an entry [i, j], there must be a position
in the input, k, where it can be split into two parts such that i < k < j . Given such

228 CHA PTER 11

• SYNTAC T IC PAR S ING

a position k, the ﬁrst constituent [i, k] must lie to the left of entry [i, j] somewhere
along row i, and the second entry [k, j] must lie beneath it, along column j .
To make this more concrete, consider the following example with its completed
parse matrix, shown in Fig. 11.4.

(11.3) Book the ﬂight through Houston.

The superdiagonal row in the matrix contains the parts of speech for each input word
in the input. The subsequent diagonals above that superdiagonal contain constituents
that cover all the spans of increasing length in the input.

Figure 11.4 Completed parse table for Book the ﬂight through Houston.

Given this setup, CKY recognition consists of ﬁlling the parse table in the right
way. To do this, we’ll proceed in a bottom-up fashion so that at the point where
we are ﬁlling any cell [i, j], the cells containing the parts that could contribute to
this entry (i.e., the cells to the left and the cells below) have already been ﬁlled.
The algorithm given in Fig. 11.5 ﬁlls the upper-triangular matrix a column at a time
working from left to right, with each column ﬁlled from bottom to top, as the right
side of Fig. 11.4 illustrates. This scheme guarantees that at each point in time we
have all the information we need (to the left, since all the columns to the left have
already been ﬁlled, and below since we’re ﬁlling bottom to top). It also mirrors on-
line parsing since ﬁlling the columns from left to right corresponds to processing
each word one at a time.

function CKY-PAR S E(words, grammar) returns table
for j ← from 1 to L ENG TH(words) do
for all {A | A → word s[ j] ∈ grammar}
table[ j − 1, j] ← table[ j − 1, j] ∪ A
for all {A | A → BC ∈ grammar and B ∈ t abl e[i, k] and C ∈ t abl e[k, j]}
table[i,j] ← table[i,j] ∪ A
Figure 11.5 The CKY algorithm.

for i ← from j − 2 downto 0 do
for k ← i + 1 to j − 1 do

11 . 2

• CKY PAR S ING : A DYNAM IC PROGRAMM ING A P PROACH

229

Figure 11.6 All the ways to ﬁll the [i, j]th cell in the CKY table.

The outermost loop of the algorithm given in Fig. 11.5 iterates over the columns,
and the second loop iterates over the rows, from the bottom up. The purpose of the
innermost loop is to range over all the places where a substring spanning i to j in
the input might be split in two. As k ranges over the places where the string can be
split, the pairs of cells we consider move, in lockstep, to the right along row i and
down along column j . Figure 11.6 illustrates the general case of ﬁlling cell [i, j]. At
each such split, the algorithm considers whether the contents of the two cells can be
combined in a way that is sanctioned by a rule in the grammar. If such a rule exists,
the non-terminal on its left-hand side is entered into the table.
Figure 11.7 shows how the ﬁve cells of column 5 of the table are ﬁlled after the
word Houston is read. The arrows point out the two spans that are being used to add
an entry to the table. Note that the action in cell [0, 5] indicates the presence of three
alternative parses for this input, one where the PP modiﬁes the ﬂight, one where
it modiﬁes the booking, and one that captures the second argument in the original
VP → Verb NP PP rule, now captured indirectly with the VP → X2 PP rule.

230 CHA PTER 11

• SYNTAC T IC PAR S ING

Figure 11.7 Filling the cells of column 5 after reading the word Houston.

11 .3

• PART IA L PAR S ING

231

11.2.3 CKY Parsing

The algorithm given in Fig. 11.5 is a recognizer, not a parser; for it to succeed, it
simply has to ﬁnd an S in cell [0, n]. To turn it into a parser capable of returning all
possible parses for a given input, we can make two simple changes to the algorithm:
the ﬁrst change is to augment the entries in the table so that each non-terminal is
paired with pointers to the table entries from which it was derived (more or less as
shown in Fig. 11.7), the second change is to permit multiple versions of the same
non-terminal to be entered into the table (again as shown in Fig. 11.7). With these
changes, the completed table contains all the possible parses for a given input. Re-
turning an arbitrary single parse consists of choosing an S from cell [0, n] and then
recursively retrieving its component constituents from the table.
Of course, returning all the parses for a given input may incur considerable cost
since an exponential number of parses may be associated with a given input. In such
cases, returning all the parses will have an unavoidable exponential cost. Looking
forward to Chapter 12, we can also think about retrieving the best parse for a given
input by further augmenting the table to contain the probabilities of each entry. Re-
trieving the most probable parse consists of running a suitably modiﬁed version of
the Viterbi algorithm from Chapter 8 over the completed parse table.

11.2.4 CKY in Practice

Finally, we should note that while the restriction to CNF does not pose a prob-
lem theoretically, it does pose some non-trivial problems in practice. Obviously, as
things stand now, our parser isn’t returning trees that are consistent with the grammar
given to us by our friendly syntacticians. In addition to making our grammar devel-
opers unhappy, the conversion to CNF will complicate any syntax-driven approach
to semantic analysis.
One approach to getting around these problems is to keep enough information
around to transform our trees back to the original grammar as a post-processing step
of the parse. This is trivial in the case of the transformation used for rules with length
greater than 2. Simply deleting the new dummy non-terminals and promoting their
daughters restores the original tree.
In the case of unit productions, it turns out to be more convenient to alter the ba-
sic CKY algorithm to handle them directly than it is to store the information needed
to recover the correct trees. Exercise 11.3 asks you to make this change. Many of
the probabilistic parsers presented in Chapter 12 use the CKY algorithm altered in
just this manner. Another solution is to adopt a more complex dynamic program-
ming solution that simply accepts arbitrary CFGs. The next section presents such an
approach.

11.3 Partial Parsing

partial parse
shallow parse

Many language processing tasks do not require complex, complete parse trees for all
inputs. For these tasks, a partial parse, or shallow parse, of input sentences may
be sufﬁcient. For example, information extraction systems generally do not extract
all the possible information from a text: they simply identify and classify the seg-
ments in a text that are likely to contain valuable information. Similarly, information
retrieval systems may index texts according to a subset of the constituents found in

232 CHA PTER 11

• SYNTAC T IC PAR S ING

them.
There are many different approaches to partial parsing. Some make use of
cascades of ﬁnite state transducers to produce tree-like representations. These ap-
proaches typically produce ﬂatter trees than the ones we’ve been discussing in this
chapter and the previous one. This ﬂatness arises from the fact that ﬁnite state trans-
ducer approaches generally defer decisions that may require semantic or contex-
tual factors, such as prepositional phrase attachments, coordination ambiguities, and
nominal compound analyses. Nevertheless, the intent is to produce parse trees that
link all the major constituents in an input.
An alternative style of partial parsing is known as chunking. Chunking is the
process of identifying and classifying the ﬂat, non-overlapping segments of a sen-
tence that constitute the basic non-recursive phrases corresponding to the major
content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and
prepositional phrases. THe task of ﬁnding all the base noun phrases in a text is
particularly common. Since chunked texts lack a hierarchical structure, a simple
bracketing notation is sufﬁcient to denote the location and the type of the chunks in
a given example:
(11.4)
[NP The morning ﬂight] [PP from] [NP Denver] [VP has arrived.]
This bracketing notation makes clear the two fundamental tasks that are involved
in chunking: segmenting (ﬁnding the non-overlapping extents of the chunks) and
labeling (assigning the correct tag to the discovered chunks).
Some input words may not be part of any chunk, particularly in tasks like base
NP:
(11.5)
[NP The morning ﬂight] from [NP Denver] has arrived.
What constitutes a syntactic base phrase depends on the application (and whether
the phrases come from a treebank). Nevertheless, some standard guidelines are fol-
lowed in most systems. First and foremost, base phrases of a given type do not
recursively contain any constituents of the same type. Eliminating this kind of recur-
sion leaves us with the problem of determining the boundaries of the non-recursive
phrases. In most approaches, base phrases include the headword of the phrase, along
with any pre-head material within the constituent, while crucially excluding any
post-head material. Eliminating post-head modiﬁers obviates the need to resolve at-
tachment ambiguities. This exclusion does lead to certain oddities, such as PPs and
VPs often consisting solely of their heads. Thus, our earlier example a ﬂight from
Indianapolis to Houston on NWA is reduced to the following:
(11.6) [NP a ﬂight] [PP from] [NP Indianapolis][PP to][NP Houston][PP on][NP
NWA]

11.3.1 Machine Learning-Based Approaches to Chunking

State-of-the-art approaches to chunking use supervised machine learning to train a
chunker by using annotated data as a training set and training any sequence labeler.
It’s common to model chunking as IOB tagging. In IOB tagging we introduce a tag
for the beginning (B) and inside (I) of each chunk type, and one for tokens outside
(O) any chunk. The number of tags is thus 2n + 1 tags, where n is the number
of chunk types.
IOB tagging can represent exactly the same information as the
bracketed notation. The following example shows the bracketing notation of (11.4)
on page 232 reframed as a tagging task:
(11.7) The
morning
ﬂight
from
Denver
B NP
I NP
I NP
B PP
B NP

arrived
I VP

has
B VP

chunking

IOB

11 .3

• PART IA L PAR S ING

233

Figure 11.8 A sequence model for chunking. The chunker slides a context window over the sentence, clas-
sifying words as it proceeds. At this point, the classiﬁer is attempting to label ﬂight, using features like words,
embeddings, part-of-speech tags and previously assigned chunk tags.

The same sentence with only the base-NPs tagged illustrates the role of the O tags.

(11.8) The
B NP

morning
I NP

ﬂight
I NP

from
O

Denver
B NP

has
O

arrived.
O

There is no explicit encoding of the end of a chunk in IOB tagging; the end of any
chunk is implicit in any transition from an I or B to a B or O tag. This encoding
reﬂects the notion that when sequentially labeling words, it is generally easier (at
least in English) to detect the beginning of a new chunk than it is to know when a
chunk has ended.
Since annotation efforts are expensive and time consuming, chunkers usually
rely on existing treebanks like the Penn Treebank (Chapter 10), extracting syntactic
phrases from the full parse constituents of a sentence, ﬁnding the appropriate heads
and then including the material to the left of the head, ignoring the text to the right.
This is somewhat error-prone since it relies on the accuracy of the head-ﬁnding rules
described in Chapter 10.
Given a training set, any sequence model can be used. Figure 11.8 shows an
illustration of a simple feature-based model, using features like the words and parts-
of-speech within a 2 word window, and the chunk tags of the preceding inputs in the
window. In training, each training vector would consist of the values of 13 features;
the two words to the left of the decision point, their parts-of-speech and chunk tags,
the word to be tagged along with its part-of-speech, the two words that follow along
with their parts-of speech, and the correct chunk tag, in this case, I N P. During
classiﬁcation, the classiﬁer is given the same vector without the answer and assigns
the most appropriate tag from its tagset. Viterbi decoding is commonly used.

234 CHA PTER 11

• SYNTAC T IC PAR S ING

11.3.2 Chunking-System Evaluations

As with the evaluation of part-of-speech taggers, the evaluation of chunkers pro-
ceeds by comparing chunker output with gold-standard answers provided by human
annotators. However, unlike part-of-speech tagging, word-by-word accuracy mea-
sures are not appropriate. Instead, chunkers are evaluated according to the notions of
precision, recall, and the F-measure borrowed from the ﬁeld of information retrieval.
Precision measures the percentage of system-provided chunks that were correct.
Correct here means that both the boundaries of the chunk and the chunk’s label are
correct. Precision is therefore deﬁned as
Precision: = Number of correct chunks given by system
Total number of chunks given by system
Recall measures the percentage of chunks actually present in the input that were
correctly identiﬁed by the system. Recall is deﬁned as
Recall: = Number of correct chunks given by system
Total number of actual chunks in the text
The F-measure (van Rijsbergen, 1975) provides a way to combine these two
measures into a single metric. The F-measure is deﬁned as

precision

recall

F-measure

Fβ =

(β 2 + 1)PR
β 2P + R
The β parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β > 1 favor recall, while
values of β < 1 favor precision. When β = 1, precision and recall are equally bal-
anced; this is sometimes called Fβ =1 or just F1 :
2PR
P + R
F-measure comes from a weighted harmonic mean of precision and recall. The
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:

(11.9)

F1 =

HarmonicMean(a1 , a2 , a3 , a4 , ..., an ) =

n

1
a1

+ 1

a2

+ 1

a3

+ ... + 1

an

and hence F-measure is

F =

1

α 1

P + (1 − α ) 1

R

or (cid:18)with β 2 =

1 − α
α (cid:19) F =

(β 2 + 1)PR
β 2P + R

(11.10)

(11.11)

11.4 Summary

The two major ideas introduced in this chapter are those of parsing and partial
parsing. Here’s a summary of the main points we covered about these ideas:
• Structural ambiguity is a signiﬁcant problem for parsers. Common sources

of structural ambiguity include PP-attachment, coordination ambiguity,
and noun-phrase bracketing ambiguity.

• Dynamic programming parsing algorithms, such as CKY, use a table of
partial parses to efﬁciently parse ambiguous sentences.

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

235

• CKY restricts the form of the grammar to Chomsky normal form (CNF).
• Many practical problems, including information extraction problems, can be
solved without full parsing.
• Partial parsing and chunking are methods for identifying shallow syntactic
constituents in a text.
• State-of-the-art methods for partial parsing use supervised machine learning
techniques.

Bibliographical and Historical Notes

Writing about the history of compilers, Knuth notes:

WFST

In this ﬁeld there has been an unusual amount of parallel discovery of
the same technique by people working independently.

Well, perhaps not unusual, since multiple discovery is the norm in science (see
page ??). But there has certainly been enough parallel publication that this his-
tory errs on the side of succinctness in giving only a characteristic early mention of
each algorithm; the interested reader should see Aho and Ullman (1972).
Bottom-up parsing seems to have been ﬁrst described by Yngve (1955), who
gave a breadth-ﬁrst, bottom-up parsing algorithm as part of an illustration of a ma-
chine translation procedure. Top-down approaches to parsing and translation were
described (presumably independently) by at least Glennie (1960), Irons (1961), and
Kuno and Oettinger (1963). Dynamic programming parsing, once again, has a his-
tory of independent discovery. According to Martin Kay (personal communication),
a dynamic programming parser containing the roots of the CKY algorithm was ﬁrst
implemented by John Cocke in 1960. Later work extended and formalized the algo-
rithm, as well as proving its time complexity (Kay 1967,Younger 1967,Kasami 1965).
The related well-formed substring table (WFST) seems to have been indepen-
dently proposed by Kuno (1965) as a data structure that stores the results of all pre-
vious computations in the course of the parse. Based on a generalization of Cocke’s
work, a similar data structure had been independently described in Kay 1967, Kay 1973.
The top-down application of dynamic programming to parsing was described in
Earley’s Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the
equivalence of the WFST and the Earley algorithm. Norvig (1991) shows that the
efﬁciency offered by dynamic programming can be captured in any language with a
memoization function (such as in LISP) simply by wrapping the memoization oper-
ation around a simple top-down parser.
While parsing via cascades of ﬁnite-state automata had been common in the
early history of parsing (Harris, 1962), the focus shifted to full CFG parsing quite
soon afterward. Church (1980) argued for a return to ﬁnite-state grammars as a
processing model for natural language understanding; other early ﬁnite-state parsing
models include Ejerhed (1988). Abney (1991) argued for the important practical role
of shallow parsing.
The classic reference for parsing algorithms is Aho and Ullman (1972); although
the focus of that book is on computer languages, most of the algorithms have been
applied to natural language. A good programming languages textbook such as Aho
et al. (1986) is also useful.

236 CHA PTER 11

• SYNTAC T IC PAR S ING

Exercises

11.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.
Apply your program to the L1 grammar.
11.2 Implement the CKY algorithm and test it with your converted L1 grammar.
11.3 Rewrite the CKY algorithm given in Fig. 11.5 on page 228 so that it can accept
grammars that contain unit productions.
11.4 Discuss the relative advantages and disadvantages of partial versus full pars-
ing.
11.5 Discuss how to augment a parser to deal with input that may be incorrect, for
example, containing spelling errors or mistakes arising from automatic speech
recognition.

CHAPTER

12 Statistical Parsing

The characters in Damon Runyon’s short stories are willing to bet “on any propo-
sition whatever”, as Runyon says about Sky Masterson in The Idyll of Miss Sarah
Brown, from the probability of getting aces back-to-back to the odds against a man
being able to throw a peanut from second base to home plate. There is a moral here
for language processing: with enough knowledge we can ﬁgure the probability of
just about anything. The last two chapters have introduced sophisticated models of
syntactic structure and its parsing. Here, we show that it is possible to build proba-
bilistic models of syntactic knowledge and use some of this probabilistic knowledge
to build efﬁcient probabilistic parsers.
One crucial use of probabilistic parsing is to solve the problem of disambigua-
tion. Recall from Chapter 11 that sentences on average tend to be syntactically
ambiguous because of phenomena like coordination ambiguity and attachment
ambiguity. The CKY parsing algorithm can represent these ambiguities in an efﬁ-
cient way but is not equipped to resolve them. A probabilistic parser offers a solution
to the problem: compute the probability of each interpretation and choose the most
probable interpretation. Thus, due to the prevalence of ambiguity, most modern
parsers used for natural language understanding tasks (semantic analysis, summa-
rization, question-answering, machine translation) are of necessity probabilistic.
The most commonly used probabilistic grammar formalism is the probabilistic
context-free grammar (PCFG), a probabilistic augmentation of context-free gram-
mars in which each rule is associated with a probability. We introduce PCFGs in the
next section, showing how they can be trained on Treebank grammars and how they
can be parsed. We present the most basic parsing algorithm for PCFGs, which is the
probabilistic version of the CKY algorithm that we saw in Chapter 11.
We then show a number of ways that we can improve on this basic probability
model (PCFGs trained on Treebank grammars). One method of improving a trained
Treebank grammar is to change the names of the non-terminals. By making the
non-terminals sometimes more speciﬁc and sometimes more general, we can come
up with a grammar with a better probability model that leads to improved parsing
scores. Another augmentation of the PCFG works by adding more sophisticated
conditioning factors, extending PCFGs to handle probabilistic subcategorization
information and probabilistic lexical dependencies.
Heavily lexicalized grammar formalisms such as Lexical-Functional Grammar
(LFG) (Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard
and Sag, 1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory
Categorial Grammar (CCG) pose additional problems for probabilistic parsers. Sec-
tion 12.7 introduces the task of supertagging and the use of heuristic search methods
based on the A* algorithm in the context of CCG parsing.
Finally, we describe the standard techniques and metrics for evaluating parsers
and discuss some relevant psychological results on human parsing.

238 CHA PTER 12

• S TAT I ST ICAL PAR S ING

12.1 Probabilistic Context-Free Grammars

PCFG
SCFG

consistent

The simplest augmentation of the context-free grammar is the Probabilistic Context-

Free Grammar (PCFG), also known as the Stochastic Context-Free Grammar

(SCFG), ﬁrst proposed by Booth (1969). Recall that a context-free grammar G is
deﬁned by four parameters (N , Σ, R, S); a probabilistic context-free grammar is also
deﬁned by four parameters, with a slight augmentation to each of the rules in R:

N a set of non-terminal symbols (or variables)

Σ a set of terminal symbols (disjoint from N )
R a set of rules or productions, each of the form A → β [ p],
where A is a non-terminal,
β is a string of symbols from the inﬁnite set of strings (Σ ∪ N )∗,
and p is a number between 0 and 1 expressing P(β |A)
S a designated start symbol

That is, a PCFG differs from a standard CFG by augmenting each rule in R with
a conditional probability:

A → β [ p]
Here p expresses the probability that the given non-terminal A will be expanded
to the sequence β . That is, p is the conditional probability of a given expansion β
given the left-hand-side (LHS) non-terminal A. We can represent this probability as
P(A → β )

(12.1)

or as

or as

P(A → β |A)

(cid:88)β

P(RH S|LH S)
Thus, if we consider all the possible expansions of a non-terminal, the sum of their
probabilities must be 1:

P(A → β ) = 1
Figure 12.1 shows a PCFG: a probabilistic augmentation of the L1 miniature En-
glish CFG grammar and lexicon. Note that the probabilities of all of the expansions
of each non-terminal sum to 1. Also note that these probabilities were made up
for pedagogical purposes. A real grammar has a great many more rules for each
non-terminal; hence, the probabilities of any particular rule would tend to be much
smaller.
A PCFG is said to be consistent if the sum of the probabilities of all sentences
in the language equals 1. Certain kinds of recursive rules cause a grammar to be
inconsistent by causing inﬁnitely looping derivations for some sentences. For ex-
ample, a rule S → S with probability 1 would lead to lost probability mass due to
derivations that never terminate. See Booth and Thompson (1973) for more details
on consistent and inconsistent grammars.

[.80]
[.15]
[.05]
[.35]
[.30]
[.20]
[.15]
[.75]

S → NP VP
S → Aux NP VP
S → VP
NP → Pronoun
NP → Proper-Noun
NP → Det Nominal
NP → Nominal
Nominal → Noun
Nominal → Nominal Noun [.20]
Nominal → Nominal PP
VP → Verb
VP → Verb NP
VP → Verb NP PP
VP → Verb PP
VP → Verb NP NP
VP → VP PP
PP → Preposition NP

[.05]
[.35]
[.20]
[.10]
[.15]
[.05]
[.15]
[1.0]

Lexicon

Det → that [.10] | a [.30] | the [.60]
Noun → book [.10] | ﬂight [.30]
| meal [.015] | money [.05]
| ﬂight [.40] | dinner [.10]
Verb → book [.30] | include [.30]
| prefer [.40]
Pronoun → I [.40] | she [.05]
| me [.15] | you [.40]
Proper-Noun → Houston [.60]
| NWA [.40]
Aux → does [.60] | can [40]
Preposition → from [.30] | to [.30]
| on [.20] | near [.15]
| through [.05]

12 . 1

• PROBAB I L I S T IC CON TEX T-FR EE GRAMMAR S

239

Grammar

Figure 12.1 A PCFG that is a probabilistic augmentation of the L1 miniature English CFG
grammar and lexicon of Fig. 11.1. These probabilities were made up for pedagogical purposes
and are not based on a corpus (since any real corpus would have many more rules, so the true
probabilities of each rule would be much smaller).

How are PCFGs used? A PCFG can be used to estimate a number of useful
probabilities concerning a sentence and its parse tree(s), including the probability of
a particular parse tree (useful in disambiguation) and the probability of a sentence
or a piece of a sentence (useful in language modeling). Let’s see how this works.

12.1.1 PCFGs for Disambiguation

A PCFG assigns a probability to each parse tree T (i.e., each derivation) of a sen-
tence S. This attribute is useful in disambiguation. For example, consider the two
parses of the sentence “Book the dinner ﬂight” shown in Fig. 12.2. The sensible
parse on the left means “Book a ﬂight that serves dinner”. The nonsensical parse
on the right, however, would have to mean something like “Book a ﬂight on behalf
of ‘the dinner”’ just as a structurally similar sentence like “Can you book John a
ﬂight?” means something like “Can you book a ﬂight on behalf of John?”
The probability of a particular parse T is deﬁned as the product of the probabil-
ities of all the n rules used to expand each of the n non-terminal nodes in the parse
tree T, where each rule i can be expressed as LH Si → RH Si :

P(T , S) =

n(cid:89)i=1

P(RH Si |LH Si )

(12.2)

The resulting probability P(T , S) is both the joint probability of the parse and the
sentence and also the probability of the parse P(T ). How can this be true? First, by
the deﬁnition of joint probability:

P(T , S) = P(T )P(S|T )

(12.3)

240 CHA PTER 12

• S TAT I ST ICAL PAR S ING

But since a parse tree includes all the words of the sentence, P(S|T ) is 1. Thus,
P(T , S) = P(T )P(S|T ) = P(T )
(12.4)

S

VP

S

VP

Verb

NP

Verb

NP

NP

Book

Det

Nominal

Book

Det

Nominal

Nominal

the

Nominal

Noun

the

Noun

Noun

Noun

ﬂight

dinner

ﬂight

dinner

Rules
P
Rules
P
S
→ VP
.05
S
→ VP
.05
VP
→ Verb NP
.20
VP
→ Verb NP NP .10
NP
→ Det Nominal
.20
NP
→ Det Nominal
.20
Nominal → Nominal Noun .20
NP
→ Nominal
.15
Nominal → Noun
.75
Nominal → Noun
.75
Nominal → Noun
.75
Verb
→ book
Verb
→ book
.30
Det
→ the
Det
→ the
.60
Noun → dinner
Noun → dinner
.10
Noun → ﬂight
Noun → ﬂight
.40
Figure 12.2 Two parse trees for an ambiguous sentence. The parse on the left corresponds
to the sensible meaning “Book a ﬂight that serves dinner”, while the parse on the right corre-
sponds to the nonsensical meaning “Book a ﬂight on behalf of ‘the dinner’ ”.

.30
.60
.10
.40

We can compute the probability of each of the trees in Fig. 12.2 by multiplying
the probabilities of each of the rules used in the derivation. For example, the proba-
bility of the left tree in Fig. 12.2a (call it Tl e f t ) and the right tree (Fig. 12.2b or Tright )
can be computed as follows:

P(Tl e f t ) = .05 ∗ .20 ∗ .20 ∗ .20 ∗ .75 ∗ .30 ∗ .60 ∗ .10 ∗ .40 = 2.2 × 10−6
P(Tright ) = .05 ∗ .10 ∗ .20 ∗ .15 ∗ .75 ∗ .75 ∗ .30 ∗ .60 ∗ .10 ∗ .40 = 6.1 × 10−7
We can see that the left tree in Fig. 12.2 has a much higher probability than the
tree on the right. Thus, this parse would correctly be chosen by a disambiguation
algorithm that selects the parse with the highest PCFG probability.
Let’s formalize this intuition that picking the parse with the highest probability
is the correct way to do disambiguation. Consider all the possible parse trees for a
given sentence S. The string of words S is called the yield of any parse tree over S.

yield

12 . 1

• PROBAB I L I S T IC CON TEX T-FR EE GRAMMAR S

241

Thus, out of all parse trees with a yield of S, the disambiguation algorithm picks the
parse tree that is most probable given S:

ˆT (S) = argmax

P(T |S)
By deﬁnition, the probability P(T |S) can be rewritten as P(T , S)/P(S), thus lead-
ing to

T s.t .S=yield(T )

(12.5)

ˆT (S) = argmax

T s.t .S=yield(T )

P(T , S)
P(S)

(12.6)

Since we are maximizing over all parse trees for the same sentence, P(S) will be
a constant for each tree, so we can eliminate it:

ˆT (S) = argmax

T s.t .S=yield(T )

P(T , S)

(12.7)

Furthermore, since we showed above that P(T , S) = P(T ), the ﬁnal equation
for choosing the most likely parse neatly simpliﬁes to choosing the parse with the
highest probability:

ˆT (S) = argmax

T s.t .S=yield(T )

P(T )

(12.8)

12.1.2 PCFGs for Language Modeling

A second attribute of a PCFG is that it assigns a probability to the string of words
constituting a sentence. This is important in language modeling, whether for use
in speech recognition, machine translation, spelling correction, augmentative com-
munication, or other applications. The probability of an unambiguous sentence is
P(T , S) = P(T ) or just the probability of the single parse tree for that sentence. The
probability of an ambiguous sentence is the sum of the probabilities of all the parse
trees for the sentence:

P(S) = (cid:88)T s.t .S=yield(T )

= (cid:88)T s.t .S=yield(T )

P(T , S)

P(T )

(12.9)

(12.10)

An additional feature of PCFGs that is useful for language modeling is their
ability to assign a probability to substrings of a sentence. For example, suppose we
want to know the probability of the next word wi in a sentence given all the words
we’ve seen so far w1 , ..., wi−1 . The general formula for this is

P(wi |w1 , w2 , ..., wi−1 ) =

P(w1 , w2 , ..., wi−1 , wi )
P(w1 , w2 , ..., wi−1 )

(12.11)

We saw in Chapter 3 a simple approximation of this probability using N -grams,
conditioning on only the last word or two instead of the entire context; thus, the

bigram approximation would give us

P(wi |w1 , w2 , ..., wi−1 ) ≈

P(wi−1 , wi )
P(wi−1 )

(12.12)

242 CHA PTER 12

• S TAT I ST ICAL PAR S ING

But the fact that the N -gram model can only make use of a couple words of
context means it is ignoring potentially useful prediction cues. Consider predicting
the word after in the following sentence from Chelba and Jelinek (2000):
(12.13) the contract ended with a loss of 7 cents after trading as low as 9 cents
A trigram grammar must predict after from the words 7 cents, while it seems clear
that the verb ended and the subject contract would be useful predictors that a PCFG-
based parser could help us make use of. Indeed, it turns out that PCFGs allow us to
condition on the entire previous context w1 , w2 , ..., wi−1 shown in Eq. 12.11.
In summary, this section and the previous one have shown that PCFGs can be
applied both to disambiguation in syntactic parsing and to word prediction in lan-
guage modeling. Both of these applications require that we be able to compute the
probability of parse tree T for a given sentence S. The next few sections introduce
some algorithms for computing this probability.

12.2 Probabilistic CKY Parsing of PCFGs

probabilistic
CKY

The parsing problem for PCFGs is to produce the most-likely parse ˆT for a given
sentence S, that is,

ˆT (S) = argmax

T s.t .S=yield(T )

P(T )

(12.14)

The algorithms for computing the most likely parse are simple extensions of the
standard algorithms for parsing; most modern probabilistic parsers are based on the
probabilistic CKY algorithm, ﬁrst described by Ney (1991).
As with the CKY algorithm, we assume for the probabilistic CKY algorithm that
the PCFG is in Chomsky normal form. Recall from page 213 that grammars in CNF
are restricted to rules of the form A → B C, or A → w. That is, the right-hand side
of each rule must expand to either two non-terminals or to a single terminal.
For the CKY algorithm, we represented each sentence as having indices between
the words. Thus, an example sentence like
(12.15) Book the ﬂight through Houston.
would assume the following indices between each word:
(12.16)
0(cid:13) Book 1(cid:13) the 2(cid:13) ﬂight 3(cid:13) through 4(cid:13) Houston 5(cid:13)
Using these indices, each constituent in the CKY parse tree is encoded in a
two-dimensional matrix. Speciﬁcally, for a sentence of length n and a grammar
that contains V non-terminals, we use the upper-triangular portion of an (n + 1) ×
(n + 1) matrix. For CKY, each cell t abl e[i, j] contained a list of constituents that
could span the sequence of words from i to j . For probabilistic CKY, it’s slightly
simpler to think of the constituents in each cell as constituting a third dimension of
maximum length V . This third dimension corresponds to each non-terminal that can
be placed in this cell, and the value of the cell is then a probability for that non-
terminal/constituent rather than a list of constituents. In summary, each cell [i, j, A]
in this (n + 1) × (n + 1) × V matrix is the probability of a constituent of type A that
spans positions i through j of the input.
Figure 12.3 gives pseudocode for this probabilistic CKY algorithm, extending
the basic CKY algorithm from Fig. 11.5.

12 . 3

• WAY S TO L EARN PCFG RU LE PROBAB I L I T I E S

243

function PROBAB I L I S T IC -CKY(words,grammar) returns most probable parse
and its probability

for j ← from 1 to L ENG TH(words) do
for all { A | A → word s[ j] ∈ grammar}
table[ j − 1, j, A] ← P(A → word s[ j])

for i ← from j − 2 downto 0 do
for k ← i + 1 to j − 1 do

for all { A | A → BC ∈ grammar,
and t abl e[i, k, B] > 0 and t abl e[k, j,C] > 0 }
if (table[i,j,A] < P(A → BC) × table[i,k,B] × table[k,j,C]) then
table[i,j,A] ← P(A → BC) × table[i,k,B] × table[k,j,C]
back[i,j,A] ← {k, B,C}
return BU I LD TRE E(back[1, L ENG TH(words), S]), table[1, L ENG TH(words), S]

Figure 12.3 The probabilistic CKY algorithm for ﬁnding the maximum probability parse
of a string of num words words given a PCFG grammar with num rules rules in Chomsky
normal form. back is an array of backpointers used to recover the best parse. The build tree
function is left as an exercise to the reader.

Like the basic CKY algorithm, the probabilistic CKY algorithm as shown in
Fig. 12.3 requires a grammar in Chomsky normal form. Converting a probabilistic
grammar to CNF requires that we also modify the probabilities so that the probability
of each parse remains the same under the new CNF grammar. Exercise 12.2 asks
you to modify the algorithm for conversion to CNF in Chapter 11 so that it correctly
handles rule probabilities.
In practice, a generalized CKY algorithm that handles unit productions directly
is typically used. Recall that Exercise 13.3 asked you to make this change in CKY;
Exercise 12.3 asks you to extend this change to probabilistic CKY.
Let’s see an example of the probabilistic CKY chart, using the following mini-
grammar, which is already in CNF:
S → NP VP
.80
Det → t he
NP → Det N
.30
Det → a
V P → V NP
.20
N → meal
V → incl ud es .05
N → f l ight
Given this grammar, Fig. 12.4 shows the ﬁrst steps in the probabilistic CKY
parse of the following example:
(12.17) The ﬂight includes a meal

.40
.40
.01
.02

12.3 Ways to Learn PCFG Rule Probabilities

Where do PCFG rule probabilities come from? There are two ways to learn proba-
bilities for the rules of a grammar. The simplest way is to use a treebank, a corpus
of already parsed sentences. Recall that we introduced in Chapter 10 the idea of
treebanks and the commonly used Penn Treebank (Marcus et al., 1993), a collec-
tion of parse trees in English, Chinese, and other languages that is distributed by the
Linguistic Data Consortium. Given a treebank, we can compute the probability of
each expansion of a non-terminal by counting the number of times that expansion

244 CHA PTER 12

• S TAT I ST ICAL PAR S ING

Figure 12.4 The beginning of the probabilistic CKY matrix. Filling out the rest of the chart
is left as Exercise 12.4 for the reader.

occurs and then normalizing.

=

(12.18)

P(α → β |α ) =

Count(α → β )
Count(α )

Count(α → β )
(cid:80)γ Count(α → γ )
If we don’t have a treebank but we do have a (non-probabilistic) parser, we can
generate the counts we need for computing PCFG rule probabilities by ﬁrst parsing
a corpus of sentences with the parser. If sentences were unambiguous, it would be
as simple as this: parse the corpus, increment a counter for every rule in the parse,
and then normalize to get probabilities.
But wait! Since most sentences are ambiguous, that is, have multiple parses, we
don’t know which parse to count the rules in. Instead, we need to keep a separate
count for each parse of a sentence and weight each of these partial counts by the
probability of the parse it appears in. But to get these parse probabilities to weight
the rules, we need to already have a probabilistic parser.
The intuition for solving this chicken-and-egg problem is to incrementally im-
prove our estimates by beginning with a parser with equal rule probabilities, then
parse the sentence, compute a probability for each parse, use these probabilities to

inside-outside

expectation
step
maximization
step

12 .4

• PROB L EM S W I TH PCFG S

245

weight the counts, re-estimate the rule probabilities, and so on, until our proba-
bilities converge. The standard algorithm for computing this solution is called the
inside-outside algorithm; it was proposed by Baker (1979) as a generalization of the
forward-backward algorithm for HMMs. Like forward-backward, inside-outside is
a special case of the Expectation Maximization (EM) algorithm, and hence has two
steps: the expectation step, and the maximization step. See Lari and Young (1990)
or Manning and Sch ¨utze (1999) for a complete description of the algorithm.
This use of the inside-outside algorithm to estimate the rule probabilities for
a grammar is actually a kind of limited use of inside-outside. The inside-outside
algorithm can actually be used not only to set the rule probabilities but even to induce
the grammar rules themselves. It turns out, however, that grammar induction is so
difﬁcult that inside-outside by itself is not a very successful grammar inducer; see
the Historical Notes at the end of the chapter for pointers to other grammar induction
algorithms.

12.4 Problems with PCFGs

While probabilistic context-free grammars are a natural extension to context-free
grammars, they have two main problems as probability estimators:
Poor independence assumptions: CFG rules impose an independence assumption
on probabilities, resulting in poor modeling of structural dependencies across
the parse tree.
Lack of lexical conditioning: CFG rules don’t model syntactic facts about speciﬁc
words, leading to problems with subcategorization ambiguities, preposition
attachment, and coordinate structure ambiguities.

Because of these problems, most current probabilistic parsing models use some
augmented version of PCFGs, or modify the Treebank-based grammar in some way.
In the next few sections after discussing the problems in more detail we introduce
some of these augmentations.

12.4.1

Independence Assumptions Miss Structural Dependencies
Between Rules

Let’s look at these problems in more detail. Recall that in a CFG the expansion of a
non-terminal is independent of the context, that is, of the other nearby non-terminals
in the parse tree. Similarly, in a PCFG, the probability of a particular rule like
NP → Det N is also independent of the rest of the tree. By deﬁnition, the probability
of a group of independent events is the product of their probabilities. These two facts
explain why in a PCFG we compute the probability of a tree by just multiplying the
probabilities of each non-terminal expansion.
Unfortunately, this CFG independence assumption results in poor probability
estimates. This is because in English the choice of how a node expands can after all
depend on the location of the node in the parse tree. For example, in English it turns
out that NPs that are syntactic subjects are far more likely to be pronouns, and NPs
that are syntactic objects are far more likely to be non-pronominal (e.g., a proper
noun or a determiner noun sequence), as shown by these statistics for NPs in the

246 CHA PTER 12

• S TAT I ST ICAL PAR S ING

Switchboard corpus (Francis et al., 1999):1

Pronoun Non-Pronoun
Subject 91%
9%
Object 34%
66%

Unfortunately, there is no way to represent this contextual difference in the prob-
abilities in a PCFG. Consider two expansions of the non-terminal NP as a pronoun
or as a determiner+noun. How shall we set the probabilities of these two rules? If
we set their probabilities to their overall probability in the Switchboard corpus, the
two rules have about equal probability.
NP → DT NN .28
NP → PRP
.25
Because PCFGs don’t allow a rule probability to be conditioned on surrounding
context, this equal probability is all we get; there is no way to capture the fact that in
subject position, the probability for NP → PRP should go up to .91, while in object
position, the probability for NP → DT NN should go up to .66.
These dependencies could be captured if the probability of expanding an NP as
a pronoun (e.g., NP → PRP) versus a lexical NP (e.g., NP → DT NN) were condi-
tioned on whether the NP was a subject or an object. Section 12.5 introduces the
technique of parent annotation for adding this kind of conditioning.

12.4.2 Lack of Sensitivity to Lexical Dependencies

A second class of problems with PCFGs is their lack of sensitivity to the words in
the parse tree. Words do play a role in PCFGs since the parse probability includes
the probability of a word given a part-of-speech (i.e., from rules like V → sl ee p,
NN → book, etc.).
But it turns out that lexical information is useful in other places in the grammar,
such as in resolving prepositional phrase (PP) attachment ambiguities. Since prepo-
sitional phrases in English can modify a noun phrase or a verb phrase, when a parser
ﬁnds a prepositional phrase, it must decide where to attach it into the tree. Consider
the following example:
(12.19) Workers dumped sacks into a bin.
Figure 12.5 shows two possible parse trees for this sentence; the one on the left is
the correct parse; Fig. 12.6 shows another perspective on the preposition attachment
problem, demonstrating that resolving the ambiguity in Fig. 12.5 is equivalent to
deciding whether to attach the prepositional phrase into the rest of the tree at the
NP or VP nodes; we say that the correct parse requires VP attachment, and the
incorrect parse implies NP attachment.
Why doesn’t a PCFG already deal with PP attachment ambiguities? Note that
the two parse trees in Fig. 12.5 have almost exactly the same rules; they differ only
in that the left-hand parse has this rule:

V P → V BD NP PP

1 Distribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sen-
tences. This tendency is caused by the use of subject position to realize the topic or old information
in a sentence (Giv ´on, 1990). Pronouns are a way to talk about old information, while non-pronominal
(“lexical”) noun-phrases are often used to introduce new referents. We talk more about new and old
information in Chapter 21.

VP attachment
NP attachment

12 .4

• PROB L EM S W I TH PCFG S

247

S

S

NP

VP

NP

VP

NNS

VBD

NP

PP

NNS

VBD

NP

workers

dumped

NNS

P

NP

workers

dumped

NP

PP

sacks

into

DT

NN

NNS

P

NP

a

bin

sacks

into

DT

NN

Figure 12.5 Two possible parse trees for a prepositional phrase attachment ambiguity. The left parse is

the sensible one, in which “into a bin” describes the resulting location of the sacks. In the right incorrect parse,
the sacks to be dumped are the ones which are already “into a bin”, whatever that might mean.

a

bin

S

NP

VP

NNS

VBD

NP

workers

dumped

NNS

sacks

PP

P

NP

into

DT

NN

a

bin

Figure 12.6 Another view of the preposition attachment problem. Should the PP on the right attach to the
VP or NP nodes of the partial parse tree on the left?

while the right-hand parse has these:

V P → V BD NP
NP → NP PP
Depending on how these probabilities are set, a PCFG will always either prefer
NP attachment or VP attachment. As it happens, NP attachment is slightly more
common in English, so if we trained these rule probabilities on a corpus, we might
always prefer NP attachment, causing us to misparse this sentence.
But suppose we set the probabilities to prefer the VP attachment for this sen-
tence. Now we would misparse the following sentence, which requires NP attach-
ment:
(12.20) ﬁshermen caught tons of herring

248 CHA PTER 12

• S TAT I ST ICAL PAR S ING

lexical
dependency

What information in the input sentence lets us know that (12.20) requires NP
attachment while (12.19) requires VP attachment?
It should be clear that these preferences come from the identities of the verbs,
nouns, and prepositions. It seems that the afﬁnity between the verb dumped and the
preposition into is greater than the afﬁnity between the noun sacks and the preposi-
tion into, thus leading to VP attachment. On the other hand, in (12.20) the afﬁnity
between tons and of is greater than that between caught and of, leading to NP attach-
ment.
Thus, to get the correct parse for these kinds of examples, we need a model that
somehow augments the PCFG probabilities to deal with these lexical dependency
statistics for different verbs and prepositions.
Coordination ambiguities are another case in which lexical dependencies are
the key to choosing the proper parse. Figure 12.7 shows an example from Collins
(1999) with two parses for the phrase dogs in houses and cats. Because dogs is
semantically a better conjunct for cats than houses (and because most dogs can’t ﬁt
inside cats), the parse [dogs in [NP houses and cats]] is intuitively unnatural and
should be dispreferred. The two parses in Fig. 12.7, however, have exactly the same
PCFG rules, and thus a PCFG will assign them the same probability.

NP

NP

NP

Conj

NP

NP

PP

NP

PP

and

Noun

Noun

Prep

NP

Noun

Prep

NP

cats

dogs

in

NP

Conj

NP

dogs

in

Noun

houses

Noun

and

Noun

houses

cats

Figure 12.7 An instance of coordination ambiguity. Although the left structure is intu-
itively the correct one, a PCFG will assign them identical probabilities since both structures
use exactly the same set of rules. After Collins (1999).

In summary, we have shown in this section and the previous one that probabilistic
context-free grammars are incapable of modeling important structural and lexical
dependencies. In the next two sections we sketch current methods for augmenting
PCFGs to deal with both these issues.

12.5

Improving PCFGs by Splitting Non-Terminals

Let’s start with the ﬁrst of the two problems with PCFGs mentioned above: their
inability to model structural dependencies, like the fact that NPs in subject position
tend to be pronouns, whereas NPs in object position tend to have full lexical (non-
pronominal) form. How could we augment a PCFG to correctly model this fact?
One idea would be to split the NP non-terminal into two versions: one for sub-

split

parent
annotation

12 . 5

•

IM PROV ING PCFG S BY S PL I TT ING NON -T ERM INAL S

249

jects, one for objects. Having two nodes (e.g., NPsubject and NPobject ) would allow
us to correctly model their different distributional properties, since we would have
different probabilities for the rule NPsubject → PRP and the rule NPobject → PRP.
One way to implement this intuition of splits is to do parent annotation (John-
son, 1998), in which we annotate each node with its parent in the parse tree. Thus,
an NP node that is the subject of the sentence and hence has parent S would be anno-
tated NPˆS, while a direct object NP whose parent is VP would be annotated NPˆVP.
Figure 12.8 shows an example of a tree produced by a grammar that parent-annotates
the phrasal non-terminals (like NP and VP).

a)

S

b)

S

NP

VP

NPˆS

VPˆS

PRP

VBD

NP

PRP

VBD

NPˆVP

I

need

DT

NN

I

need

DT

NN

a

ﬂight

a

ﬂight

Figure 12.8 A standard PCFG parse tree (a) and one which has parent annotation on the
nodes which aren’t pre-terminal (b). All the non-terminal nodes (except the pre-terminal
part-of-speech nodes) in parse (b) have been annotated with the identity of their parent.

In addition to splitting these phrasal nodes, we can also improve a PCFG by
splitting the pre-terminal part-of-speech nodes (Klein and Manning, 2003b). For ex-
ample, different kinds of adverbs (RB) tend to occur in different syntactic positions:
the most common adverbs with ADVP parents are also and now, with VP parents
n’t and not, and with NP parents only and just. Thus, adding tags like RBˆADVP,
RBˆVP, and RBˆNP can be useful in improving PCFG modeling.
Similarly, the Penn Treebank tag IN can mark a wide variety of parts-of-speech,
including subordinating conjunctions (while, as, if), complementizers (that, for), and
prepositions (of, in, from). Some of these differences can be captured by parent an-
notation (subordinating conjunctions occur under S, prepositions under PP), while
others require speciﬁcally splitting the pre-terminal nodes. Figure 12.9 shows an ex-
ample from Klein and Manning (2003b) in which even a parent-annotated grammar
incorrectly parses works as a noun in to see if advertising works. Splitting pre-
terminals to allow if to prefer a sentential complement results in the correct verbal
parse.
To deal with cases in which parent annotation is insufﬁcient, we can also hand-
write rules that specify a particular node split based on other features of the tree. For
example, to distinguish between complementizer IN and subordinating conjunction
IN, both of which can have the same parent, we could write rules conditioned on
other aspects of the tree such as the lexical identity (the lexeme that is likely to be a
complementizer, as a subordinating conjunction).
Node-splitting is not without problems; it increases the size of the grammar and
hence reduces the amount of training data available for each grammar rule, leading
to overﬁtting. Thus, it is important to split to just the correct level of granularity for a
particular training set. While early models employed hand-written rules to try to ﬁnd
an optimal number of non-terminals (Klein and Manning, 2003b), modern models

250 CHA PTER 12

• S TAT I ST ICAL PAR S ING

split and merge

automatically search for the optimal splits. The split and merge algorithm of Petrov
et al. (2006), for example, starts with a simple X-bar grammar, alternately splits the
non-terminals, and merges non-terminals, ﬁnding the set of annotated nodes that
maximizes the likelihood of the training set treebank. As of the time of this writing,
the performance of the Petrov et al. (2006) algorithm was the best of any known
parsing algorithm on the Penn Treebank.

12.6 Probabilistic Lexicalized CFGs

The previous section showed that a simple probabilistic CKY algorithm for pars-
ing raw PCFGs can achieve extremely high parsing accuracy if the grammar rule
symbols are redesigned by automatic splits and merges.
In this section, we discuss an alternative family of models in which instead of
modifying the grammar rules, we modify the probabilistic model of the parser to
allow for lexicalized rules. The resulting family of lexicalized parsers includes the
well-known Collins parser (Collins, 1999) and Charniak parser (Charniak, 1997),
both of which are publicly available and widely used throughout natural language
processing.
We saw in Section 10.4.3 that syntactic constituents could be associated with a
lexical head, and we deﬁned a lexicalized grammar in which each non-terminal
in the tree is annotated with its lexical head, where a rule like V P → V BD NP PP
would be extended as

VP(dumped) → VBD(dumped) NP(sacks) PP(into)
In the standard type of lexicalized grammar, we actually make a further exten-
sion, which is to associate the head tag, the part-of-speech tags of the headwords,
with the non-terminal symbols as well. Each rule is thus lexicalized by both the

(12.21)

VPˆS

Collins parser
Charniak
parser

lexicalized
grammar

head tag

VPˆS

TO

VPˆVP

TOˆVP

VPˆVP

to

VB

PPˆVP

to

VBˆVP

SBARˆVP

see

IN

NPˆPP

see

INˆSBAR

SˆSBAR

if

NN

NNS

if

NPˆS

VPˆS

advertising

works

NNˆNP

VBZˆVP

Figure 12.9 An incorrect parse even with a parent-annotated parse (left). The correct parse (right), was
produced by a grammar in which the pre-terminal nodes have been split, allowing the probabilistic grammar to
capture the fact that if prefers sentential complements. Adapted from Klein and Manning (2003b).

advertising

works

12 .6

• PROBAB I L I ST IC L EX ICA L I Z ED CFG S

251

headword and the head tag of each constituent resulting in a format for lexicalized
rules like

VP(dumped,VBD) → VBD(dumped,VBD) NP(sacks,NNS) PP(into,P)
We show a lexicalized parse tree with head tags in Fig. 12.10, extended from Fig. 10.11.

(12.22)

TOP

S(dumped,VBD)

NP(workers,NNS)

VP(dumped,VBD)

NNS(workers,NNS)

VBD(dumped,VBD)

NP(sacks,NNS)

PP(into,P)

workers

dumped

NNS(sacks,NNS)

P(into,P)

NP(bin,NN)

sacks

into

DT(a,DT)

NN(bin,NN)

a

bin

Internal Rules

Lexical Rules

TOP
→ S(dumped,VBD)
NNS(workers,NNS) → workers
S(dumped,VBD) → NP(workers,NNS)
VBD(dumped,VBD) → dumped
NP(workers,NNS) → NNS(workers,NNS)
NNS(sacks,NNS)
→ sacks
VP(dumped,VBD) → VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) P(into,P)
→ into
PP(into,P)
→ P(into,P)
NP(bin,NN)
DT(a,DT)
NP(bin,NN)
→ DT(a,DT)
NN(bin,NN)
NN(bin,NN)
→ bin

VP(dumped,VBD)

→ a

Figure 12.10 A lexicalized tree, including head tags, for a WSJ sentence, adapted from Collins (1999). Below
we show the PCFG rules that would be needed for this parse tree, internal rules on the left, and lexical rules on
the right.

To generate such a lexicalized tree, each PCFG rule must be augmented to iden-
tify one right-hand constituent to be the head daughter. The headword for a node is
then set to the headword of its head daughter, and the head tag to the part-of-speech
tag of the headword. Recall that we gave in Fig. 10.12 a set of hand-written rules for
identifying the heads of particular constituents.
A natural way to think of a lexicalized grammar is as a parent annotation, that
is, as a simple context-free grammar with many copies of each rule, one copy for
each possible headword/head tag for each constituent. Thinking of a probabilistic
lexicalized CFG in this way would lead to the set of simple PCFG rules shown below
the tree in Fig. 12.10.
Note that Fig. 12.10 shows two kinds of rules:
lexical rules, which express
the expansion of a pre-terminal to a word, and internal rules, which express the
other rule expansions. We need to distinguish these kinds of rules in a lexicalized
grammar because they are associated with very different kinds of probabilities. The
lexical rules are deterministic, that is, they have probability 1.0 since a lexicalized
pre-terminal like NN (bin, NN ) can only expand to the word bin. But for the internal
rules, we need to estimate probabilities.

lexical rules
internal rules

252 CHA PTER 12

• S TAT I ST ICAL PAR S ING

Suppose we were to treat a probabilistic lexicalized CFG like a really big CFG
that just happened to have lots of very complex non-terminals and estimate the
probabilities for each rule from maximum likelihood estimates. Thus, according
to Eq. 12.18, the MLE estimate for the probability for the rule P(VP(dumped,VBD)
→ VBD(dumped, VBD) NP(sacks,NNS) PP(into,P)) would be

Count(VP(dumped,VBD) → VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))
Count(VP(dumped,VBD))

(12.23)

But there’s no way we can get good estimates of counts like those in (12.23)
because they are so speciﬁc: we’re unlikely to see many (or even any) instances of a
sentence with a verb phrase headed by dumped that has one NP argument headed by
sacks and a PP argument headed by into. In other words, counts of fully lexicalized
PCFG rules like this will be far too sparse, and most rule probabilities will come out
0.

The idea of lexicalized parsing is to make some further independence assump-
tions to break down each rule so that we would estimate the probability

P(VP(dumped,VBD) → VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))

as the product of smaller independent probability estimates for which we could
acquire reasonable counts. The next section summarizes one such method, the
Collins parsing method.

12.6.1 The Collins Parser

Modern statistical parsers differ in exactly which independence assumptions they
make. In this section we describe a simpliﬁed version of Collins’s worth knowing
about; see the summary at the end of the chapter.
The ﬁrst intuition of the Collins parser is to think of the right-hand side of every
(internal) CFG rule as consisting of a head non-terminal, together with the non-
terminals to the left of the head and the non-terminals to the right of the head. In the
abstract, we think about these rules as follows:

LH S → Ln Ln−1 ... L1 H R1 ... Rn−1 Rn

(12.24)

Since this is a lexicalized grammar, each of the symbols like L1 or R3 or H or
LH S is actually a complex symbol representing the category and its head and head
tag, like VP(dumped,VP) or NP(sacks,NNS).
Now, instead of computing a single MLE probability for this rule, we are going
to break down this rule via a neat generative story, a slight simpliﬁcation of what is
called Collins Model 1. This new generative story is that given the left-hand side,
we ﬁrst generate the head of the rule and then generate the dependents of the head,
one by one, from the inside out. Each of these generation steps will have its own
probability.
We also add a special STO P non-terminal at the left and right edges of the rule;
this non-terminal allows the model to know when to stop generating dependents on a
given side. We generate dependents on the left side of the head until we’ve generated
S TO P on the left side of the head, at which point we move to the right side of the
head and start generating dependents there until we generate STO P. So it’s as if we

12 .6

• PROBAB I L I ST IC L EX ICA L I Z ED CFG S

253

are generating a rule augmented as follows:

P(VP(dumped,VBD) →
STOP VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) STOP)

(12.25)

Let’s see the generative story for this augmented rule. We make use of three
kinds of probabilities: PH for generating heads, PL for generating dependents on the
left, and PR for generating dependents on the right.

1. Generate the head VBD(dumped,VBD) with probability
P(H|LHS) = P(VBD(dumped,VBD) | VP(dumped,VBD))

2. Generate the left dependent (which is STOP, since there isn’t
one) with probability
P(STOP| VP(dumped,VBD) VBD(dumped,VBD))

3. Generate right dependent NP(sacks,NNS) with probability
Pr (NP(sacks,NNS| VP(dumped,VBD), VBD(dumped,VBD))

4. Generate the right dependent PP(into,P) with probability
Pr (PP(into,P) | VP(dumped,VBD), VBD(dumped,VBD))

STOP

VP(dumped,VBD)

VBD(dumped,VBD)

VP(dumped,VBD)

STOP

VBD(dumped,VBD)

VP(dumped,VBD)

STOP

VBD(dumped,VBD)

NP(sacks,NNS)

VP(dumped,VBD)

VBD(dumped,VBD)

NP(sacks,NNS)

PP(into,P)

VP(dumped,VBD)

5) Generate the right dependent STOP with probability
Pr (STOP | VP(dumped,VBD), VBD(dumped,VBD))

STOP

VBD(dumped,VBD)

NP(sacks,NNS)

PP(into,P)

STOP

In summary, the probability of this rule

P(VP(dumped,VBD) →
VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))

(12.26)

is estimated as

PH (VBD|VP, dumped) × PL (STOP|VP, VBD, d um ped )
× PR (NP(sacks,NNS)|VP, VBD, dumped)
× PR (PP(into,P)|VP, VBD, d um ped )
× PR (STOP|VP, VBD, d um ped )
Each of these probabilities can be estimated from much smaller amounts of data
than the full probability in (12.26). For example, the maximum likelihood estimate

(12.27)

254 CHA PTER 12

• S TAT I ST ICAL PAR S ING

for the component probability PR (NP(sacks,NNS)|VP, VBD, d um ped ) is

Count( VP(dumped,VBD) with NNS(sacks)as a daughter somewhere on the right )
Count( VP(dumped,VBD) )

(12.28)
These counts are much less subject to sparsity problems than are complex counts
like those in (12.26).
More generally, if H is a head with head word hw and head tag ht , lw/lt and
rw/rt are the word/tag on the left and right respectively, and P is the parent, then the
probability of an entire rule can be expressed as follows:

1. Generate the head of the phrase H (hw, ht ) with probability:

PH (H (hw, ht )|P, hw, ht )
2. Generate modiﬁers to the left of the head with total probability

n+1(cid:89)i=1

PL (Li (lwi , lti )|P, H , hw, ht )
such that Ln+1 (lwn+1 , ltn+1 ) =STO P, and we stop generating once we’ve gen-
erated a STO P token.
3. Generate modiﬁers to the right of the head with total probability:

n+1(cid:89)i=1

PP (Ri (rwi , rti )|P, H , hw, ht )
such that Rn+1 (rwn+1 , rtn+1 ) = ST OP, and we stop generating once we’ve
generated a STO P token.

12.6.2 Advanced: Further Details of the Collins Parser

distance

(12.29)
(12.30)

The actual Collins parser models are more complex (in a couple of ways) than the
simple model presented in the previous section. Collins Model 1 includes a distance
feature. Thus, instead of computing PL and PR as follows,
PL (Li (lwi , lti )|P, H , hw, ht )
PR (Ri (rwi , rti )|P, H , hw, ht )
Collins Model 1 conditions also on a distance feature:
PL (Li (lwi , lti )|P, H , hw, ht , d ist anceL (i − 1))
PR (Ri (rwi , rti )|P, H , hw, ht , d ist anceR (i − 1))
The distance measure is a function of the sequence of words below the previous
modiﬁers (i.e., the words that are the yield of each modiﬁer non-terminal we have
already generated on the left).
The simplest version of this distance measure is just a tuple of two binary fea-
tures based on the surface string below these previous dependencies: (1) Is the string
of length zero? (i.e., were no previous words generated?) (2) Does the string contain
a verb?

(12.31)
(12.32)

12 .7

• PROBAB I L I S T IC CCG PAR S ING

255

Collins Model 2 adds more sophisticated features, conditioning on subcatego-
rization frames for each verb and distinguishing arguments from adjuncts.
Finally, smoothing is as important for statistical parsers as it was for N -gram
models. This is particularly true for lexicalized parsers, since the lexicalized rules
will otherwise condition on many lexical items that may never occur in training
(even using the Collins or other methods of independence assumptions).
Consider the probability PR (Ri (rwi , rti )|P, hw, ht ). What do we do if a particular
right-hand constituent never occurs with this head? The Collins model addresses this
problem by interpolating three backed-off models: fully lexicalized (conditioning on
the headword), backing off to just the head tag, and altogether unlexicalized.
Backoff PR (Ri (rwi , rti |...)
1
PR (Ri (rwi , rti )|P, hw, ht )
2
PR (Ri (rwi , rti )|P, ht )
3

PR (NP(sacks,NNS)|VP, VBD, dumped)
PR (NP(sacks, NN S)|V P,V BD)
PR (NP(sacks, NN S)|V P)
Similar backoff models are built also for PL and PH . Although we’ve used the
word “backoff ”, in fact these are not backoff models but interpolated models. The
three models above are linearly interpolated, where e1 , e2 , and e3 are the maximum
likelihood estimates of the three backoff models above:

Example

PR (Ri (rwi , rti )|P)

PR (...) = λ1 e1 + (1 − λ1 )(λ2 e2 + (1 − λ2 )e3 )

(12.33)
The values of λ1andλ2 are set to implement Witten-Bell discounting (Witten and
Bell, 1991) following Bikel et al. (1997).
The Collins model deals with unknown words by replacing any unknown word
in the test set, and any word occurring less than six times in the training set, with a
special UNKNOWN word token. Unknown words in the test set are assigned a part-
of-speech tag in a preprocessing step by the Ratnaparkhi (1996) tagger; all other
words are tagged as part of the parsing process.
The parsing algorithm for the Collins model is an extension of probabilistic
CKY; see Collins (2003a). Extending the CKY algorithm to handle basic lexicalized
probabilities is left as Exercises 14.5 and 14.6 for the reader.

12.7 Probabilistic CCG Parsing

Lexicalized grammar frameworks such as CCG pose problems for which the phrase-
based methods we’ve been discussing are not particularly well-suited. To quickly
review, CCG consists of three major parts: a set of categories, a lexicon that asso-
ciates words with categories, and a set of rules that govern how categories combine
in context. Categories can be either atomic elements, such as S and NP, or functions
such as (S\NP)/NP which speciﬁes the transitive verb category. Rules specify how
functions, their arguments, and other functions combine. For example, the following
rule templates, forward and backward function application, specify the way that
functions apply to their arguments.
X /Y Y ⇒ X
Y X \Y ⇒ X
The ﬁrst rule applies a function to its argument on the right, while the second
looks to the left for its argument. The result of applying either of these rules is the

256 CHA PTER 12

• S TAT I ST ICAL PAR S ING

category speciﬁed as the value of the function being applied. For the purposes of
this discussion, we’ll rely on these two rules along with the forward and backward
composition rules and type-raising, as described in Chapter 10.

12.7.1 Ambiguity in CCG

As is always the case in parsing, managing ambiguity is the key to successful CCG
parsing. The difﬁculties with CCG parsing arise from the ambiguity caused by the
large number of complex lexical categories combined with the very general nature of
the grammatical rules. To see some of the ways that ambiguity arises in a categorial
framework, consider the following example.

(12.34) United diverted the ﬂight to Reno.

Our grasp of the role of the ﬂight in this example depends on whether the prepo-
sitional phrase to Reno is taken as a modiﬁer of the ﬂight, as a modiﬁer of the entire
verb phrase, or as a potential second argument to the verb divert. In a context-free
grammar approach, this ambiguity would manifest itself as a choice among the fol-
lowing rules in the grammar.

Nominal → Nominal PP
VP → VP PP
VP → Verb NP PP
In a phrase-structure approach we would simply assign the word to to the cate-
gory P allowing it to combine with Reno to form a prepositional phrase. The sub-
sequent choice of grammar rules would then dictate the ultimate derivation. In the
categorial approach, we can associate to with distinct categories to reﬂect the ways
in which it might interact with other elements in a sentence. The fairly abstract
combinatoric rules would then sort out which derivations are possible. Therefore,
the source of ambiguity arises not from the grammar but rather from the lexicon.
Let’s see how this works by considering several possible derivations for this
example. To capture the case where the prepositional phrase to Reno modiﬁes the
ﬂight, we assign the preposition to the category (NP\NP)/NP, which gives rise to
the following derivation.

United
NP

diverted
the
ﬂight
to
Reno
(S\NP)/NP NP/N N (NP\NP)/NP NP
NP
NP\NP

>

>

NP

S\NP

S

<

>

<

Here, the category assigned to to expects to ﬁnd two arguments: one to the right as
with a traditional preposition, and one to the left that corresponds to the NP to be
modiﬁed.
Alternatively, we could assign to to the category (S\S)/NP, which permits the
following derivation where to Reno modiﬁes the preceding verb phrase.

12 .7

• PROBAB I L I S T IC CCG PAR S ING

257

United
NP

diverted
the
ﬂight
to
Reno
(S\NP)/NP NP/N N (S\S)/NP NP
NP
S\S

>

>

>

S\NP

S\NP

S
A third possibility is to view divert as a ditransitive verb by assigning it to the
category ((S\NP)/PP)/NP, while treating to Reno as a simple prepositional phrase.

<B

<

>

<

United
NP

diverted
the
ﬂight
to
Reno
((S\NP)/PP)/NP NP/N N PP/NP NP
NP
PP

>

>

>

(S\NP)/PP
S\NP

S
While CCG parsers are still subject to ambiguity arising from the choice of
grammar rules, including the kind of spurious ambiguity discussed in Chapter 10,
it should be clear that the choice of lexical categories is the primary problem to be
addressed in CCG parsing.

12.7.2 CCG Parsing Frameworks

Since the rules in combinatory grammars are either binary or unary, a bottom-up,
tabular approach based on the CKY algorithm should be directly applicable to CCG
parsing. Recall from Fig. 12.3 that PCKY employs a table that records the location,
category and probability of all valid constituents discovered in the input. Given an
appropriate probability model for CCG derivations, the same kind of approach can
work for CCG parsing.
Unfortunately, the large number of lexical categories available for each word,
combined with the promiscuity of CCG’s combinatoric rules, leads to an explosion
in the number of (mostly useless) constituents added to the parsing table. The key
to managing this explosion of zombie constituents is to accurately assess and ex-
ploit the most likely lexical categories possible for each word — a process called
supertagging.
The following sections describe two approaches to CCG parsing that make use of
supertags. Section 12.7.4, presents an approach that structures the parsing process
as a heuristic search through the use of the A* algorithm. The following section
then brieﬂy describes a more traditional maximum entropy approach that manages
the search space complexity through the use of adaptive supertagging — a process
that iteratively considers more and more tags until a parse is found.

12.7.3 Supertagging

supertagging

Chapter 8 introduced the task of part-of-speech tagging, the process of assigning the
correct lexical category to each word in a sentence. Supertagging is the correspond-
ing task for highly lexicalized grammar frameworks, where the assigned tags often
dictate much of the derivation for a sentence.

258 CHA PTER 12

• S TAT I ST ICAL PAR S ING

CCG supertaggers rely on treebanks such as CCGbank to provide both the over-
all set of lexical categories as well as the allowable category assignments for each
word in the lexicon. CCGbank includes over 1000 lexical categories, however, in
practice, most supertaggers limit their tagsets to those tags that occur at least 10
times in the training corpus. This results in an overall total of around 425 lexical
categories available for use in the lexicon. Note that even this smaller number is
large in contrast to the 45 POS types used by the Penn Treebank tagset.
As with traditional part-of-speech tagging, the standard approach to building a
CCG supertagger is to use supervised machine learning to build a sequence classi-
ﬁer using labeled training data. A common approach is to use the maximum entropy
Markov model (MEMM), as described in Chapter 8, to ﬁnd the most likely sequence
of tags given a sentence. The features in such a model consist of the current word
wi , its surrounding words within l words wi+l
i−l , as well as the k previously assigned
supertags t i−1
i−k . This type of model is summarized in the following equation from
Chapter 8. Training by maximizing log-likelihood of the training corpus and decod-
ing via the Viterbi algorithm are the same as described in Chapter 8.

ˆT = argmax

P(T |W )

= argmax

P(ti |wi+l
i−l , t i−1
i−k )

T
T (cid:89)i
T (cid:89)i

(12.35)

= argmax

i−k )(cid:33)
wi f i (ti , wi+l
i−l , t i−1
i−k )(cid:33)
wi f i (t (cid:48) , wi+l
i−l , t i−1

exp (cid:32)(cid:88)i
(cid:88)t (cid:48) ∈tagset
exp (cid:32)(cid:88)i
Word and tag-based features with k and l both set to 2 provides reasonable results
given sufﬁcient training data. Additional features such as POS tags and short char-
acter sufﬁxes are also commonly used to improve performance.
Unfortunately, even with additional features the large number of possible su-
pertags combined with high per-word ambiguity leads to error rates that are too
high for practical use in a parser. More speciﬁcally, the single best tag sequence
ˆT will typically contain too many incorrect tags for effective parsing to take place.
To overcome this, we can instead return a probability distribution over the possible
supertags for each word in the input. The following table illustrates an example dis-
tribution for a simple example sentence. In this table, each column represents the
probability of each supertag for a given word in the context of the input sentence.
The “...” represent all the remaining supertags possible for each word.

United
serves
Denver
N /N : 0.4 (S\NP)/NP: 0.8 NP: 0.9
NP: 0.3
N : 0.1
N /N : 0.05
S/S: 0.1
...
...
S\S: .05
...

In a MEMM framework, the probability of the optimal tag sequence deﬁned in
Eq. 12.35 is efﬁciently computed with a suitably modiﬁed version of the Viterbi
algorithm. However, since Viterbi only ﬁnds the single best tag sequence it doesn’t

12 .7

• PROBAB I L I S T IC CCG PAR S ING

259

provide exactly what we need here; we need to know the probability of each pos-
sible word/tag pair. The probability of any given tag for a word is the sum of the
probabilities of all the supertag sequences that contain that tag at that location. A
table representing these values can be computed efﬁciently by using a version of the
forward-backward algorithm used for HMMs.
The same result can also be achieved through the use of deep learning approaches
based on recurrent neural networks (RNNs). Recent efforts have demonstrated con-
siderable success with RNNs as alternatives to HMM-based methods. These ap-
proaches differ from traditional classiﬁer-based methods in the following ways:
• The use of vector-based word representations (embeddings) rather than word-
based feature functions.
• Input representations that span the entire sentence, as opposed to size-limited
sliding windows.
• Avoiding the use of high-level features, such as part of speech tags, since
errors in tag assignment can propagate to errors in supertags.

As with the forward-backward algorithm, RNN-based methods can provide a prob-
ability distribution over the lexical categories for each word in the input.

12.7.4 CCG Parsing using the A* Algorithm

The A* algorithm is a heuristic search method that employs an agenda to ﬁnd an
optimal solution. Search states representing partial solutions are added to an agenda
based on a cost function, with the least-cost option being selected for further ex-
ploration at each iteration. When a state representing a complete solution is ﬁrst
selected from the agenda, it is guaranteed to be optimal and the search terminates.
The A* cost function, f (n), is used to efﬁciently guide the search to a solution.
The f -cost has two components: g(n), the exact cost of the partial solution repre-
sented by the state n, and h(n) a heuristic approximation of the cost of a solution
that makes use of n. When h(n) satisﬁes the criteria of not overestimating the actual
cost, A* will ﬁnd an optimal solution. Not surprisingly, the closer the heuristic can
get to the actual cost, the more effective A* is at ﬁnding a solution without having
to explore a signiﬁcant portion of the solution space.
When applied to parsing, search states correspond to edges representing com-
pleted constituents. As with the PCKY algorithm, edges specify a constituent’s start
and end positions, its grammatical category, and its f -cost. Here, the g component
represents the current cost of an edge and the h component represents an estimate
of the cost to complete a derivation that makes use of that edge. The use of A*
for phrase structure parsing originated with (Klein and Manning, 2003a), while the
CCG approach presented here is based on (Lewis and Steedman, 2014).
Using information from a supertagger, an agenda and a parse table are initial-
ized with states representing all the possible lexical categories for each word in the
input, along with their f -costs. The main loop removes the lowest cost edge from
the agenda and tests to see if it is a complete derivation. If it reﬂects a complete
derivation it is selected as the best solution and the loop terminates. Otherwise, new
states based on the applicable CCG rules are generated, assigned costs, and entered
into the agenda to await further processing. The loop continues until a complete
derivation is discovered, or the agenda is exhausted, indicating a failed parse. The
algorithm is given in Fig. 12.11.

260 CHA PTER 12

• S TAT I ST ICAL PAR S ING

function CCG -AS TAR -PAR S E(words) returns table or failure
supertags ← SU PERTAGG ER(words)
for i ← from 1 to L ENG TH(words) do
for all {A | (word s[i], A, score) ∈ su pert ags}
edge ← MAK EEDGE(i − 1, i, A, score)
table ← IN SERTEDG E(table, edge)
agenda ← IN SERTEDG E(agenda, edge)

loop do
if EM P TY ?(agenda) return failure

current ← PO P(agenda)
if COM PL ET EDPAR S E ?(current) return table
table ← IN SERTEDGE(chart, edge)
for each rule in A P PL ICAB LERU LE S(edge) do
successor ← A P PLY(rule, edge)
if successor not ∈ in agenda or chart
agenda ← IN SERTEDGE(agenda, successor)
else if successor ∈ agenda with higher cost
agenda ← R E P LAC EEDGE(agenda, successor)
Figure 12.11 A*-based CCG parsing.

Heuristic Functions

Before we can deﬁne a heuristic function for our A* search, we need to decide how
to assess the quality of CCG derivations. For the generic PCFG model, we deﬁned
the probability of a tree as the product of the probability of the rules that made up
the tree. Given CCG’s lexical nature, we’ll make the simplifying assumption that the
probability of a CCG derivation is just the product of the probability of the supertags
assigned to the words in the derivation, ignoring the rules used in the derivation.
More formally, given a sentence S and derivation D that contains suptertag sequence
T , we have:

P(D, S) = P(T , S)

=

n(cid:89)i=1

P(ti |si )

(12.36)

(12.37)

To better ﬁt with the traditional A* approach, we’d prefer to have states scored
by a cost function where lower is better (i.e., we’re trying to minimize the cost of
a derivation). To achieve this, we’ll use negative log probabilities to score deriva-
tions; this results in the following equation, which we’ll use to score completed CCG
derivations.

P(D, S) = P(T , S)

=

n(cid:88)i=1

− log P(ti |si )

(12.38)

(12.39)

Given this model, we can deﬁne our f -cost as follows. The f -cost of an edge is
the sum of two components: g(n), the cost of the span represented by the edge, and

12 .7

• PROBAB I L I S T IC CCG PAR S ING

261

h(n), the estimate of the cost to complete a derivation containing that edge (these
are often referred to as the inside and outside costs). We’ll deﬁne g(n) for an edge
using Equation 12.39. That is, it is just the sum of the costs of the supertags that
comprise the span.
For h(n), we need a score that approximates but never overestimates the actual
cost of the ﬁnal derivation. A simple heuristic that meets this requirement assumes
that each of the words in the outside span will be assigned its most probable su-
pertag. If these are the tags used in the ﬁnal derivation, then its score will equal
the heuristic. If any other tags are used in the ﬁnal derivation the f -cost will be
higher since the new tags must have higher costs, thus guaranteeing that we will not
overestimate.
Putting this all together, we arrive at the following deﬁnition of a suitable f -cost
for an edge.

f (wi, j , ti, j ) = g(wi, j ) + h(wi, j )

(12.40)

=

j(cid:88)k=i
i−1(cid:88)k=1

− log P(tk |wk ) +

max

t ∈t ags

(− log P(t |wk )) +

N(cid:88)k= j+1

max

t ∈t ags

(− log P(t |wk ))

As an example, consider an edge representing the word serves with the supertag
N in the following example.
(12.41) United serves Denver.
The g-cost for this edge is just the negative log probability of the tag, or X. The
outside h-cost consists of the most optimistic supertag assignments for United and
Denver. The resulting f -cost for this edge is therefore x+y+z = 1.494.

An Example

Fig. 12.12 shows the initial agenda and the progress of a complete parse for this
example. After initializing the agenda and the parse table with information from the
supertagger, it selects the best edge from the agenda — the entry for United with
the tag N /N and f -cost 0.591. This edge does not constitute a complete parse and is
therefore used to generate new states by applying all the relevant grammar rules. In
this case, applying forward application to United: N/N and serves: N results in the
creation of the edge United serves: N[0,2], 1.795 to the agenda.
Skipping ahead, at the the third iteration an edge representing the complete
derivation United serves Denver, S[0,3], .716 is added to the agenda. However,
the algorithm does not terminate at this point since the cost of this edge (.716) does
not place it at the top of the agenda. Instead, the edge representing Denver with the
category NP is popped. This leads to the addition of another edge to the agenda
(type-raising Denver). Only after this edge is popped and dealt with does the ear-
lier state representing a complete derivation rise to the top of the agenda where it is
popped, goal tested, and returned as a solution.
The effectiveness of the A* approach is reﬂected in the coloring of the states
in Fig. 12.12 as well as the ﬁnal parsing table. The edges shown in blue (includ-
ing all the initial lexical category assignments not explicitly shown) reﬂect states in
the search space that never made it to the top of the agenda and, therefore, never

262 CHA PTER 12

• S TAT I ST ICAL PAR S ING

Figure 12.12 Example of an A* search for the example “United serves Denver”. The circled numbers on the
white boxes indicate the order in which the states are popped from the agenda. The costs in each state are based
on f-costs using negative l og10 probabilities.

contributed any edges to the ﬁnal table. This is in contrast to the PCKY approach
where the parser systematically ﬁlls the parse table with all possible constituents for
all possible spans in the input, ﬁlling the table with myriad constituents that do not
contribute to the ﬁnal analysis.

12.8 Evaluating Parsers

12 .8

• EVALUAT ING PAR S ER S

263

The standard techniques for evaluating parsers and grammars are called the PAR-
SEVAL measures; they were proposed by Black et al. (1991) and were based on
the same ideas from signal-detection theory that we saw in earlier chapters. The
intuition of the PARSEVAL metric is to measure how much the constituents in the
hypothesis parse tree look like the constituents in a hand-labeled, gold-reference
parse. PARSEVAL thus assumes we have a human-labeled “gold standard” parse
tree for each sentence in the test set; we generally draw these gold-standard parses
from a treebank like the Penn Treebank.
Given these gold-standard reference parses for a test set, a given constituent in
a hypothesis parse Ch of a sentence s is labeled “correct” if there is a constituent in
the reference parse Cr with the same starting point, ending point, and non-terminal
symbol.
We can then measure the precision and recall just as we did for chunking in the
previous chapter.
labeled recall: = # of correct constituents in hypothesis parse of s
# of correct constituents in reference parse of s
labeled precision: = # of correct constituents in hypothesis parse of s
# of total constituents in hypothesis parse of s

As with other uses of precision and recall, instead of reporting them separately,
we often report a single number, the F-measure (van Rijsbergen, 1975): The F-
measure is deﬁned as

Fβ =

(β 2 + 1)PR
β 2P + R
The β parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β > 1 favor recall and values
of β < 1 favor precision. When β = 1, precision and recall are equally balanced;
this is sometimes called Fβ =1 or just F1 :

F-measure

2PR
P + R
The F-measure derives from a weighted harmonic mean of precision and recall.
Remember that the harmonic mean of a set of numbers is the reciprocal of the arith-
metic mean of the reciprocals:

(12.42)

F1 =

HarmonicMean(a1 , a2 , a3 , a4 , ..., an ) =

n

1
a1

+ 1

a2

+ 1

a3

+ ... + 1

an

(12.43)

and hence the F-measure is
1

F =

or (cid:18)with β 2 =
We additionally use a new metric, crossing brackets, for each sentence s:

1 − α
α (cid:19) F =

(β 2 + 1)PR
β 2P + R

α 1

P + (1 − α ) 1

R

(12.44)

cross-brackets: the number of constituents for which the reference parse has a
bracketing such as ((A B) C) but the hypothesis parse has a bracketing such
as (A (B C)).

264 CHA PTER 12

• S TAT I ST ICAL PAR S ING

evalb

As of the time of this writing, the performance of modern parsers that are trained
and tested on the Wall Street Journal treebank was somewhat higher than 90% recall,
90% precision, and about 1% cross-bracketed constituents per sentence.
For comparing parsers that use different grammars, the PARSEVAL metric in-
cludes a canonicalization algorithm for removing information likely to be grammar-
speciﬁc (auxiliaries, pre-inﬁnitival “to”, etc.) and for computing a simpliﬁed score
(Black et al., 1991). The canonical implementation of the PARSEVAL metrics is
called evalb (Sekine and Collins, 1997).
Nonetheless, phrasal constituents are not always an appropriate unit for parser
evaluation. In lexically-oriented grammars, such as CCG and LFG, the ultimate goal
is to extract the appropriate predicate-argument relations or grammatical dependen-
cies, rather than a speciﬁc derivation. Such relations are also more directly relevant
to further semantic processing. For these purposes, we can use alternative evaluation
metrics based on measuring the precision and recall of labeled dependencies, where
the labels indicate the grammatical relations (Lin 1995, Carroll et al. 1998, Collins
et al. 1999).
Finally, you might wonder why we don’t evaluate parsers by measuring how
many sentences are parsed correctly instead of measuring component accuracy in
the form of constituents or dependencies. The reason we use components is that it
gives us a more ﬁne-grained metric. This is especially true for long sentences, where
most parsers don’t get a perfect parse. If we just measured sentence accuracy, we
wouldn’t be able to distinguish between a parse that got most of the parts wrong and
one that just got one part wrong.

12.9 Human Parsing

Human
sentence
processing

Reading time

Are the kinds of probabilistic parsing models we have been discussing also used by
humans when they are parsing? The answer to this question lies in a ﬁeld called
human sentence processing. Recent studies suggest that there are at least two
ways in which humans apply probabilistic parsing algorithms, although there is still
disagreement on the details.
One family of studies has shown that when humans read, the predictability of a
word seems to inﬂuence the reading time; more predictable words are read more
quickly. One way of deﬁning predictability is from simple bigram measures. For
example, Scott and Shillcock (2003) used an eye-tracker to monitor the gaze of
participants reading sentences. They constructed the sentences so that some would
have a verb-noun pair with a high bigram probability (such as (12.45a)) and others
a verb-noun pair with a low bigram probability (such as (12.45b)).
(12.45)
a) HIGH PROB: One way to avoid confusion is to make the changes
during vacation
b) LOW PROB: One way to avoid discovery is to make the changes
during vacation
They found that the higher the bigram predictability of a word, the shorter the
time that participants looked at the word (the initial-ﬁxation duration).
While this result provides evidence only for N -gram probabilities, more recent
experiments have suggested that the probability of an upcoming word given the
syntactic parse of the preceding sentence preﬁx also predicts word reading time
(Hale 2001, Levy 2008).

12 .9

• HUMAN PAR S ING

265

garden-path

The second family of studies has examined how humans disambiguate sentences
that have multiple possible parses, suggesting that humans prefer whichever parse
is more probable. These studies often rely on a speciﬁc class of temporarily am-
biguous sentences called garden-path sentences. These sentences, ﬁrst described
by Bever (1970), are sentences that are cleverly constructed to have three properties
that combine to make them very difﬁcult for people to parse:

1. They are temporarily ambiguous: The sentence is unambiguous, but its ini-
tial portion is ambiguous.
2. One of the two or more parses in the initial portion is somehow preferable to
the human parsing mechanism.
3. But the dispreferred parse is the correct one for the sentence.

The result of these three properties is that people are “led down the garden path”
toward the incorrect parse and then are confused when they realize it’s the wrong
one. Sometimes this confusion is quite conscious, as in Bever’s example (12.46);
in fact, this sentence is so hard to parse that readers often need to be shown the
correct structure. In the correct structure, raced is part of a reduced relative clause
modifying The horse, and means “The horse [which was raced past the barn] fell”;
this structure is also present in the sentence “Students taught by the Berlitz method
do worse when they get to France”.

(12.46) The horse raced past the barn fell.

(a)

S

NP

VP

?

V

(b)

S

NP

Det

N

V

PP

fell

NP

VP

The

horse

raced

P

NP

Det

N

V

PP

VP

V

fell

past

Det

N

The

horse

raced

P

NP

the

barn

past

Det

N

the

barn

Other times, the confusion caused by a garden-path sentence is so subtle that it
can only be measured by a slight increase in reading time. Thus, in (12.47) readers
often misparse the solution as the direct object of forgot rather than as the subject
of an embedded sentence. This misparse is subtle, and is only noticeable because
experimental participants take longer to read the word was than in control sentences.
This “mini garden path” effect at the word was suggests that subjects had chosen the
direct object parse and had to reanalyze or rearrange their parse now that they realize
they are in a sentential complement.

(12.47) The student forgot the solution was in the back of the book.

266 CHA PTER 12

• S TAT I ST ICAL PAR S ING

S

S

NP

VP

NP

VP

Det

N

V

NP

Det

N

V

S

The

students

forgot

Det

N

The

students

forgot

NP

VP

the

solution

Det

N

V

the

solution

was

While many factors seem to play a role in these preferences for a particular (in-
correct) parse, at least one factor seems to be syntactic probabilities, especially lex-
icalized (subcategorization) probabilities. For example, the probability of the verb
forgot taking a direct object (VP → V NP) is higher than the probability of it taking a
sentential complement (VP → V S); this difference causes readers to expect a direct
object after forget and be surprised (longer reading times) when they encounter a
sentential complement. By contrast, a verb which prefers a sentential complement
(like hope) didn’t cause extra reading time at was. The garden path in (12.46) is at
least partially caused by the low probability of the reduced relative clause construc-
tion.

12.10 Summary

This chapter has sketched the basics of probabilistic parsing, concentrating on

probabilistic context-free grammars and probabilistic lexicalized context-free
grammars.

• Probabilistic grammars assign a probability to a sentence or string of words
while attempting to capture more sophisticated syntactic information than the
N -gram grammars of Chapter 3.

context-free grammar

(PCFG)

• A probabilistic

is
a
context-free
grammar in which every rule is annotated with the probability of that rule
being chosen. Each PCFG rule is treated as if it were conditionally inde-
pendent; thus, the probability of a sentence is computed by multiplying the
probabilities of each rule in the parse of the sentence.
• The probabilistic CKY (Cocke-Kasami-Younger) algorithm is a probabilistic
version of the CKY parsing algorithm. There are also probabilistic versions
of other parsers like the Earley algorithm.
• PCFG probabilities can be learned by counting in a parsed corpus or by pars-
ing a corpus. The inside-outside algorithm is a way of dealing with the fact
that the sentences being parsed are ambiguous.
• Raw PCFGs suffer from poor independence assumptions among rules and lack
of sensitivity to lexical dependencies.
• One way to deal with this problem is to split and merge non-terminals (auto-
matically or by hand).

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

267

• Probabilistic lexicalized CFGs are another solution to this problem in which
the basic PCFG model is augmented with a lexical head for each rule. The
probability of a rule can then be conditioned on the lexical head or nearby
heads.
• Parsers for lexicalized PCFGs (like the Charniak and Collins parsers) are
based on extensions to probabilistic CKY parsing.
• Parsers are evaluated with three metrics: labeled recall, labeled precision,
• Evidence from garden-path sentences and other on-line sentence-processing
experiments suggest that the human parser uses some kinds of probabilistic
information about grammar.

and cross-brackets.

Bibliographical and Historical Notes

Many of the formal properties of probabilistic context-free grammars were ﬁrst
worked out by Booth (1969) and Salomaa (1969). Baker (1979) proposed the inside-
outside algorithm for unsupervised training of PCFG probabilities, and used a CKY-
style parsing algorithm to compute inside probabilities. Jelinek and Lafferty (1991)
extended the CKY algorithm to compute probabilities for preﬁxes. Stolcke (1995)
drew on both of these algorithms in adapting the Earley algorithm to use with
PCFGs.
A number of researchers starting in the early 1990s worked on adding lexical de-
pendencies to PCFGs and on making PCFG rule probabilities more sensitive to sur-
rounding syntactic structure. For example, Schabes et al. (1988) and Schabes (1990)
presented early work on the use of heads. Many papers on the use of lexical depen-
dencies were ﬁrst presented at the DARPA Speech and Natural Language Workshop
in June 1990. A paper by Hindle and Rooth (1990) applied lexical dependencies
to the problem of attaching prepositional phrases; in the question session to a later
paper, Ken Church suggested applying this method to full parsing (Marcus, 1990).
Early work on such probabilistic CFG parsing augmented with probabilistic depen-
dency information includes Magerman and Marcus (1991), Black et al. (1992), Bod
(1993), and Jelinek et al. (1994), in addition to Collins (1996), Charniak (1997), and
Collins (1999) discussed above. Other recent PCFG parsing models include Klein
and Manning (2003a) and Petrov et al. (2006).
This early lexical probabilistic work led initially to work focused on solving
speciﬁc parsing problems like preposition-phrase attachment by using methods in-
cluding transformation-based learning (TBL) (Brill and Resnik, 1994), maximum
entropy (Ratnaparkhi et al., 1994), memory-based Learning (Zavrel and Daelemans,
1997), log-linear models (Franz, 1997), decision trees that used semantic distance
between heads (computed from WordNet) (Stetina and Nagao, 1997), and boosting
(Abney et al., 1999).
Another direction extended the lexical probabilistic parsing work to build prob-
abilistic formulations of grammars other than PCFGs, such as probabilistic TAG
grammar (Resnik 1992, Schabes 1992), based on the TAG grammars discussed in
Chapter 10, probabilistic LR parsing (Briscoe and Carroll, 1993), and probabilistic
link grammar (Lafferty et al., 1992). An approach to probabilistic parsing called
supertagging extends the part-of-speech tagging metaphor to parsing by using very
complex tags that are, in fact, fragments of lexicalized parse trees (Bangalore and

supertagging

268 CHA PTER 12

• S TAT I ST ICAL PAR S ING

Joshi 1999, Joshi and Srinivas 1994), based on the lexicalized TAG grammars of
Schabes et al. (1988). For example, the noun purchase would have a different tag
as the ﬁrst noun in a noun compound (where it might be on the left of a small tree
dominated by Nominal) than as the second noun (where it might be on the right).
Goodman (1997), Abney (1997), and Johnson et al. (1999) gave early discus-
sions of probabilistic treatments of feature-based grammars. Other recent work
on building statistical models of feature-based grammar formalisms like HPSG and
LFG includes (Riezler et al. 2002, Kaplan et al. 2004), and Toutanova et al. (2005).
We mentioned earlier that discriminative approaches to parsing fall into the two
broad categories of dynamic programming methods and discriminative reranking
methods. Recall that discriminative reranking approaches require N -best parses.
Parsers based on A* search can easily be modiﬁed to generate N -best lists just by
continuing the search past the ﬁrst-best parse (Roark, 2001). Dynamic programming
algorithms like the ones described in this chapter can be modiﬁed by the elimina-
tion of the dynamic programming with heavy pruning (Collins 2000, Collins and
Koo 2005, Bikel 2004), or through new algorithms (Jim ´enez and Marzal 2000,Char-
niak and Johnson 2005,Huang and Chiang 2005), some adapted from speech recog-
nition algorithms such as those of Schwartz and Chow (1990) (see Section ??).
In dynamic programming methods, instead of outputting and then reranking an
N -best list, the parses are represented compactly in a chart, and log-linear and other
methods are applied for decoding directly from the chart. Such modern methods
include (Johnson 2001, Clark and Curran 2004), and Taskar et al. (2004). Other
reranking developments include changing the optimization criterion (Titov and Hen-
derson, 2006).
Collins’ (1999) dissertation includes a very readable survey of the ﬁeld and an
introduction to his parser. Manning and Sch ¨utze (1999) extensively cover proba-
bilistic parsing.
The ﬁeld of grammar induction is closely related to statistical parsing, and a
parser is often used as part of a grammar induction algorithm. One of the earliest
statistical works in grammar induction was Horning (1969), who showed that PCFGs
could be induced without negative evidence. Early modern probabilistic grammar
work showed that simply using EM was insufﬁcient (Lari and Young 1990, Carroll
and Charniak 1992). Recent probabilistic work, such as Yuret (1998), Clark (2001),
Klein and Manning (2002), and Klein and Manning (2004), are summarized in Klein
(2005) and Adriaans and van Zaanen (2004). Work since that summary includes
Smith and Eisner (2005), Haghighi and Klein (2006), and Smith and Eisner (2007).

Exercises

12.1 Implement the CKY algorithm.
12.2 Modify the algorithm for conversion to CNF from Chapter 11 to correctly
handle rule probabilities. Make sure that the resulting CNF assigns the same
total probability to each parse tree.
12.3 Recall that Exercise 13.3 asked you to update the CKY algorithm to han-
dle unit productions directly rather than converting them to CNF. Extend this
change to probabilistic CKY.
12.4 Fill out the rest of the probabilistic CKY chart in Fig. 12.4.

EX ERC I SE S

269

12.5 Sketch how the CKY algorithm would have to be augmented to handle lexi-
calized probabilities.
12.6 Implement your lexicalized extension of the CKY algorithm.
12.7 Implement the PARSEVAL metrics described in Section 12.8. Next, either
use a treebank or create your own hand-checked parsed test set. Now use your
CFG (or other) parser and grammar, parse the test set and compute labeled
recall, labeled precision, and cross-brackets.

270 CHA PTER 13

• D E P END ENCY PAR S ING

CHAPTER

13 Dependency Parsing

dependency
grammars

The focus of the three previous chapters has been on context-free grammars and
their use in automatically generating constituent-based representations. Here we
present another family of grammar formalisms called dependency grammars that
are quite important in contemporary speech and language processing systems. In
these formalisms, phrasal constituents and phrase-structure rules do not play a direct
role. Instead, the syntactic structure of a sentence is described solely in terms of the
words (or lemmas) in a sentence and an associated set of directed binary grammatical
relations that hold among the words.
The following diagram illustrates a dependency-style analysis using the standard
graphical method favored in the dependency-parsing community.

typed
dependency

free word order

root

dobj

det

nsubj

nmod

nmod

case

I prefer the morning ﬂight through Denver

(13.1)
Relations among the words are illustrated above the sentence with directed, la-
beled arcs from heads to dependents. We call this a typed dependency structure
because the labels are drawn from a ﬁxed inventory of grammatical relations. It also
includes a root node that explicitly marks the root of the tree, the head of the entire
structure.
Figure 13.1 shows the same dependency analysis as a tree alongside its corre-
sponding phrase-structure analysis of the kind given in Chapter 10. Note the ab-
sence of nodes corresponding to phrasal constituents or lexical categories in the
dependency parse; the internal structure of the dependency parse consists solely
of directed relations between lexical items in the sentence. These relationships di-
rectly encode important information that is often buried in the more complex phrase-
structure parses. For example, the arguments to the verb prefer are directly linked to
it in the dependency structure, while their connection to the main verb is more dis-
tant in the phrase-structure tree. Similarly, morning and Denver, modiﬁers of ﬂight,
are linked to it directly in the dependency structure.
A major advantage of dependency grammars is their ability to deal with lan-
guages that are morphologically rich and have a relatively free word order. For
example, word order in Czech can be much more ﬂexible than in English; a gram-
matical object might occur before or after a location adverbial. A phrase-structure
grammar would need a separate rule for each possible place in the parse tree where
such an adverbial phrase could occur. A dependency-based approach would just
have one link type representing this particular adverbial relation. Thus, a depen-
dency grammar approach abstracts away from word-order information, representing
only the information that is necessary for the parse.
An additional practical motivation for a dependency-based approach is that the
head-dependent relations provide an approximation to the semantic relationship be-

13 .1

• D E P END ENCY R ELAT ION S

271

prefer

S

I

ﬂight

NP

VP

the

morning

Denver

Pro

Verb

NP

I

prefer

Det

Nom

through

the

Nom

PP

Nom

Noun

P

NP

Noun

ﬂight

through

Pro

morning

Denver

Figure 13.1 A dependency-style parse alongside the corresponding constituent-based analysis for I prefer the
morning ﬂight through Denver.

tween predicates and their arguments that makes them directly useful for many ap-
plications such as coreference resolution, question answering and information ex-
traction. Constituent-based approaches to parsing provide similar information, but it
often has to be distilled from the trees via techniques such as the head ﬁnding rules
discussed in Chapter 10.
In the following sections, we’ll discuss in more detail the inventory of relations
used in dependency parsing, as well as the formal basis for these dependency struc-
tures. We’ll then move on to discuss the dominant families of algorithms that are
used to automatically produce these structures. Finally, we’ll discuss how to eval-
uate dependency parsers and point to some of the ways they are used in language
processing applications.

13.1 Dependency Relations

grammatical
relation

head
dependent

grammatical
function

The traditional linguistic notion of grammatical relation provides the basis for the
binary relations that comprise these dependency structures. The arguments to these
relations consist of a head and a dependent. We’ve already discussed the notion of
heads in Chapter 10 and Chapter 12 in the context of constituent structures. There,
the head word of a constituent was the central organizing word of a larger constituent
(e.g, the primary noun in a noun phrase, or verb in a verb phrase). The remaining
words in the constituent are either direct, or indirect, dependents of their head. In
dependency-based approaches, the head-dependent relationship is made explicit by
directly linking heads to the words that are immediately dependent on them, bypass-
ing the need for constituent structures.
In addition to specifying the head-dependent pairs, dependency grammars allow
us to further classify the kinds of grammatical relations, or grammatical function,

272 CHA PTER 13

• D E P END ENCY PAR S ING

Clausal Argument Relations Description

Nominal subject
Direct object
Indirect object
Clausal complement
Open clausal complement

Nominal Modiﬁer Relations Description

N SUB J
DOB J
IOB J
CCOM P
XCOM P

NMOD
AMOD
NUMMOD
A P PO S
D ET
CA S E

Other Notable Relations

CON J
CC

Nominal modiﬁer
Adjectival modiﬁer
Numeric modiﬁer
Appositional modiﬁer
Determiner
Prepositions, postpositions and other case markers

Description

Conjunct
Coordinating conjunction

Universal
Dependencies

Figure 13.2 Selected dependency relations from the Universal Dependency set. (de Marn-
effe et al., 2014)

in terms of the role that the dependent plays with respect to its head. Familiar notions
such as subject, direct object and indirect object are among the kind of relations we
have in mind. In English these notions strongly correlate with, but by no means de-
termine, both position in a sentence and constituent type and are therefore somewhat
redundant with the kind of information found in phrase-structure trees. However, in
more ﬂexible languages the information encoded directly in these grammatical rela-
tions is critical since phrase-based constituent syntax provides little help.
Not surprisingly, linguists have developed taxonomies of relations that go well
beyond the familiar notions of subject and object. While there is considerable vari-
ation from theory to theory, there is enough commonality that efforts to develop a
computationally useful standard are now possible. The Universal Dependencies
project (Nivre et al., 2016b) provides an inventory of dependency relations that are
linguistically motivated, computationally useful, and cross-linguistically applicable.
Fig. 13.2 shows a subset of the relations from this effort. Fig. 13.3 provides some
example sentences illustrating selected relations.
The motivation for all of the relations in the Universal Dependency scheme is
beyond the scope of this chapter, but the core set of frequently used relations can be
broken into two sets: clausal relations that describe syntactic roles with respect to a
predicate (often a verb), and modiﬁer relations that categorize the ways that words
that can modify their heads.
Consider the following example sentence:

root

dobj

det

nsubj

nmod

nmod

case

United canceled the morning ﬂights to Houston

(13.2)
The clausal relations N SUB J and DOB J identify the subject and direct object of
the predicate cancel, while the NMOD, D ET, and CA S E relations denote modiﬁers of
the nouns ﬂights and Houston.

13 .2

• D E PEND ENCY FORMA L I SM S

273

Relation

N SUB J
DOB J

IOB J
NMOD
AMOD
NUMMOD
A P PO S
D ET

CON J
CC
CA S E

Examples with head and dependent
United canceled the ﬂight.
United diverted the ﬂight to Reno.
We booked her the ﬁrst ﬂight to Miami.
We booked her the ﬂight to Miami.
We took the morning ﬂight.
Book the cheapest ﬂight.
Before the storm JetBlue canceled 1000 ﬂights.
United, a unit of UAL, matched the fares.
The ﬂight was canceled.
Which ﬂight was delayed?
We ﬂew to Denver and drove to Steamboat.
We ﬂew to Denver and drove to Steamboat.
Book the ﬂight through Houston.

Figure 13.3 Examples of core Universal Dependency relations.

13.2 Dependency Formalisms

dependency
tree

In their most general form, the dependency structures we’re discussing are simply
directed graphs. That is, structures G = (V, A) consisting of a set of vertices V , and
a set of ordered pairs of vertices A, which we’ll refer to as arcs.
For the most part we will assume that the set of vertices, V , corresponds exactly
to the set of words in a given sentence. However, they might also correspond to
punctuation, or when dealing with morphologically complex languages the set of
vertices might consist of stems and afﬁxes. The set of arcs, A, captures the head-
dependent and grammatical function relationships between the elements in V .
Further constraints on these dependency structures are speciﬁc to the underlying
grammatical theory or formalism. Among the more frequent restrictions are that the
structures must be connected, have a designated root node, and be acyclic or planar.
Of most relevance to the parsing approaches discussed in this chapter is the common,
computationally-motivated, restriction to rooted trees. That is, a dependency tree
is a directed graph that satisﬁes the following constraints:

1. There is a single designated root node that has no incoming arcs.
2. With the exception of the root node, each vertex has exactly one incoming arc.
3. There is a unique path from the root node to each vertex in V .

Taken together, these constraints ensure that each word has a single head, that the
dependency structure is connected, and that there is a single root node from which
one can follow a unique directed path to each of the words in the sentence.

13.2.1 Projectivity

The notion of projectivity imposes an additional constraint that is derived from the
order of the words in the input, and is closely related to the context-free nature of
human languages discussed in Chapter 10. An arc from a head to a dependent is
said to be projective if there is a path from the head to every word that lies between
the head and the dependent in the sentence. A dependency tree is then said to be
projective if all the arcs that make it up are projective. All the dependency trees
we’ve seen thus far have been projective. There are, however, many perfectly valid

274 CHA PTER 13

• D E P END ENCY PAR S ING

constructions which lead to non-projective trees, particularly in languages with a
relatively ﬂexible word order.
Consider the following example.

root

mod

nsubj

dobj

det

nmod

det

case

mod

adv

JetBlue canceled our ﬂight this morning which was already late

(13.3)

In this example, the arc from ﬂight to its modiﬁer was is non-projective since
there is no path from ﬂight to the intervening words this and morning. As we can
see from this diagram, projectivity (and non-projectivity) can be detected in the way
we’ve been drawing our trees. A dependency tree is projective if it can be drawn
with no crossing edges. Here there is no way to link ﬂight to its dependent was
without crossing the arc that links morning to its head.
Our concern with projectivity arises from two related issues. First, the most
widely used English dependency treebanks were automatically derived from phrase-
structure treebanks through the use of head-ﬁnding rules (Chapter 10). The trees
generated in such a fashion are guaranteed to be projective since they’re generated
from context-free grammars.
Second, there are computational limitations to the most widely used families of
parsing algorithms. The transition-based approaches discussed in Section 13.4 can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. This limitation is one of the motivations for
the more ﬂexible graph-based parsing approach described in Section 13.5.

13.3 Dependency Treebanks

As with constituent-based methods, treebanks play a critical role in the development
and evaluation of dependency parsers. Dependency treebanks have been created
using similar approaches to those discussed in Chapter 10 — having human annota-
tors directly generate dependency structures for a given corpus, or using automatic
parsers to provide an initial parse and then having annotators hand correct those
parsers. We can also use a deterministic process to translate existing constituent-
based treebanks into dependency trees through the use of head rules.
For the most part, directly annotated dependency treebanks have been created for
morphologically rich languages such as Czech, Hindi and Finnish that lend them-
selves to dependency grammar approaches, with the Prague Dependency Treebank
(Bej ˇcek et al., 2013) for Czech being the most well-known effort. The major English
dependency treebanks have largely been extracted from existing resources such as
the Wall Street Journal sections of the Penn Treebank(Marcus et al., 1993). The
more recent OntoNotes project (Hovy et al. 2006,Weischedel et al. 2011) extends
this approach going beyond traditional news text to include conversational telephone
speech, weblogs, usenet newsgroups, broadcast, and talk shows in English, Chinese
and Arabic.
The translation process from constituent to dependency structures has two sub-
tasks: identifying all the head-dependent relations in the structure and identifying
the correct dependency relations for these relations. The ﬁrst task relies heavily on

13 . 4

• TRAN S I T ION -BA SED D E PEND ENCY PAR S ING

275

the use of head rules discussed in Chapter 10 ﬁrst developed for use in lexicalized
probabilistic parsers (Magerman 1994,Collins 1999,Collins 2003b). Here’s a simple
and effective algorithm from Xia and Palmer (2001).
1. Mark the head child of each node in a phrase structure, using the appropriate
head rules.
2. In the dependency structure, make the head of each non-head child depend on
the head of the head-child.
When a phrase-structure parse contains additional information in the form of
grammatical relations and function tags, as in the case of the Penn Treebank, these
tags can be used to label the edges in the resulting tree. When applied to the parse
tree in Fig. 13.4, this algorithm would produce the dependency structure in Fig. 13.4.

root

sbj

aux

dobj

nmod

clr

tmp

case

nmod

amod

num

Vinken will join the board as a nonexecutive director Nov 29

(13.4)
The primary shortcoming of these extraction methods is that they are limited by
the information present in the original constituent trees. Among the most impor-
tant issues are the failure to integrate morphological information with the phrase-
structure trees, the inability to easily represent non-projective structures, and the
lack of internal structure to most noun-phrases, as reﬂected in the generally ﬂat
rules used in most treebank grammars. For these reasons, outside of English, most
dependency treebanks are developed directly using human annotators.

13.4 Transition-Based Dependency Parsing

shift-reduce
parsing

conﬁguration

Our ﬁrst approach to dependency parsing is motivated by a stack-based approach
called shift-reduce parsing originally developed for analyzing programming lan-
guages (Aho and Ullman, 1972). This classic approach is simple and elegant, em-
ploying a context-free grammar, a stack, and a list of tokens to be parsed. Input
tokens are successively shifted onto the stack and the top two elements of the stack
are matched against the right-hand side of the rules in the grammar; when a match is
found the matched elements are replaced on the stack (reduced) by the non-terminal
from the left-hand side of the rule being matched. In adapting this approach for
dependency parsing, we forgo the explicit use of a grammar and alter the reduce
operation so that instead of adding a non-terminal to a parse tree, it introduces a
dependency relation between a word and its head. More speciﬁcally, the reduce ac-
tion is replaced with two possible actions: assert a head-dependent relation between
the word at the top of the stack and the word below it, or vice versa. Figure 13.5
illustrates the basic operation of such a parser.
A key element in transition-based parsing is the notion of a conﬁguration which
consists of a stack, an input buffer of words, or tokens, and a set of relations rep-
resenting a dependency tree. Given this framework, the parsing process consists of
a sequence of transitions through the space of possible conﬁgurations. The goal of

276 CHA PTER 13

• D E P END ENCY PAR S ING

S

NP-SBJ

VP

NNP

MD

VP

Vinken

will

VB

NP

PP-CLR

NP-TMP

join

DT

NN

IN

NP

NNP

CD

the

board

as

DT

JJ

NN

Nov

29

a

nonexecutive

director

S(join)

NP-SBJ(Vinken)

VP(join)

NNP

MD

VP(join)

Vinken

will

VB

NP(board)

PP-CLR(director)

NP-TMP(29)

join

DT

NN

IN

NP(director)

NNP

CD

the

board

as

DT

JJ

NN

Nov

29

a

nonexecutive

director

join

Vinken

will

board

director

29

the

as

a

nonexecutive

Nov

Figure 13.4 A phrase-structure tree from the Wall Street Journal component of the Penn Treebank 3.

this process is to ﬁnd a ﬁnal conﬁguration where all the words have been accounted
for and an appropriate dependency tree has been synthesized.
To implement such a search, we’ll deﬁne a set of transition operators, which
when applied to a conﬁguration produce new conﬁgurations. Given this setup, we
can view the operation of a parser as a search through a space of conﬁgurations for
a sequence of transitions that leads from a start state to a desired goal state. At the
start of this process we create an initial conﬁguration in which the stack contains the

13 . 4

• TRAN S I T ION -BA SED D E PEND ENCY PAR S ING

277

Figure 13.5 Basic transition-based parser. The parser examines the top two elements of the
stack and selects an action based on consulting an oracle that examines the current conﬁgura-
tion.

ROOT node, the word list is initialized with the set of the words or lemmatized tokens
in the sentence, and an empty set of relations is created to represent the parse. In the
ﬁnal goal state, the stack and the word list should be empty, and the set of relations
will represent the ﬁnal parse.
In the standard approach to transition-based parsing, the operators used to pro-
duce new conﬁgurations are surprisingly simple and correspond to the intuitive ac-
tions one might take in creating a dependency tree by examining the words in a
single pass over the input from left to right (Covington, 2001):
• Assign the current word as the head of some previously seen word,
• Assign some previously seen word as the head of the current word,
• Or postpone doing anything with the current word, adding it to a store for later
processing.
To make these actions more precise, we’ll create three transition operators that
will operate on the top two elements of the stack:
• LE FTARC: Assert a head-dependent relation between the word at the top of
stack and the word directly beneath it; remove the lower word from the stack.
• R IGH TARC: Assert a head-dependent relation between the second word on
the stack and the word at the top; remove the word at the top of the stack;
• SH I F T: Remove the word from the front of the input buffer and push it onto
the stack.
This particular set of operators implements what is known as the arc standard
approach to transition-based parsing (Covington 2001,Nivre 2003). There are two
notable characteristics to this approach: the transition operators only assert relations
between elements at the top of the stack, and once an element has been assigned
its head it is removed from the stack and is not available for further processing.
As we’ll see, there are alternative transition systems which demonstrate different
parsing behaviors, but the arc standard approach is quite effective and is simple to
implement.

arc standard

278 CHA PTER 13

• D E P END ENCY PAR S ING

To assure that these operators are used properly we’ll need to add some pre-
conditions to their use. First, since, by deﬁnition, the ROOT node cannot have any
incoming arcs, we’ll add the restriction that the LE FTARC operator cannot be ap-
plied when ROOT is the second element of the stack. Second, both reduce operators
require two elements to be on the stack to be applied. Given these transition opera-
tors and preconditions, the speciﬁcation of a transition-based parser is quite simple.
Fig. 13.6 gives the basic algorithm.

function D E P END ENCYPAR SE(words) returns dependency tree
state ← {[root], [words], [] } ; initial conﬁguration
t ← ORACL E(state)
; choose a transition operator to apply
state ← A P P LY(t, state) ; apply it, creating a new state

while state not ﬁnal

return state

Figure 13.6 A generic transition-based dependency parser

At each step, the parser consults an oracle (we’ll come back to this shortly) that
provides the correct transition operator to use given the current conﬁguration. It then
applies that operator to the current conﬁguration, producing a new conﬁguration.
The process ends when all the words in the sentence have been consumed and the
ROOT node is the only element remaining on the stack.
The efﬁciency of transition-based parsers should be apparent from the algorithm.
The complexity is linear in the length of the sentence since it is based on a single left
to right pass through the words in the sentence. More speciﬁcally, each word must
ﬁrst be shifted onto the stack and then later reduced.
Note that unlike the dynamic programming and search-based approaches dis-
cussed in Chapters 12 and 13, this approach is a straightforward greedy algorithm
— the oracle provides a single choice at each step and the parser proceeds with that
choice, no other options are explored, no backtracking is employed, and a single
parse is returned in the end.
Figure 13.7 illustrates the operation of the parser with the sequence of transitions
leading to a parse for the following example.

root

iobj

dobj

det

nmod

Book me the morning ﬂight

(13.5)
Let’s consider the state of the conﬁguration at Step 2, after the word me has been
pushed onto the stack.

Stack
Word List
[root, book, me] [the, morning, ﬂight]

Relations

The correct operator to apply here is R IGH TARC which assigns book as the head of
me and pops me from the stack resulting in the following conﬁguration.

Stack
Word List
Relations
[root, book] [the, morning, ﬂight] (book → me)

13 . 4

• TRAN S I T ION -BA SED D E PEND ENCY PAR S ING

279

Step
0
1
2
3
4
5
6
7
8
9
10

Stack Word List
[root]
[book, me, the, morning, ﬂight]
[root, book]
[me, the, morning, ﬂight]
[root, book, me]
[the, morning, ﬂight]
[root, book]
[the, morning, ﬂight]
[root, book, the]
[morning, ﬂight]
[root, book, the, morning]
[ﬂight]
[root, book, the, morning, ﬂight]
[]
[root, book, the, ﬂight]
[]
[root, book, ﬂight]
[]
[root, book]
[]
[root]
[]

Action

SH I F T
SH I F T
R IGH TARC
SH I F T
SH I F T
SH I F T
LE FTARC
LE FTARC
R IGH TARC
R IGH TARC

Done

Relation Added

(book → me)

(morning ← ﬂight)
(the ← ﬂight)
(book → ﬂight)
(root → book)

Figure 13.7 Trace of a transition-based parse.

After several subsequent applications of the SH I F T and LE FTARC operators, the con-
ﬁguration in Step 6 looks like the following:

Stack
[root, book, the, morning, ﬂight]

Relations
(book → me)
Here, all the remaining words have been passed onto the stack and all that is left
to do is to apply the appropriate reduce operators. In the current conﬁguration, we
employ the LE FTARC operator resulting in the following state.

Word List
[]

Stack
[root, book, the, ﬂight]

Relations
(book → me)
(morning ← ﬂight)
At this point, the parse for this sentence consists of the following structure.

Word List
[]

dobj

nmod

Book me the morning ﬂight

(13.6)
There are several important things to note when examining sequences such as
the one in Figure 13.7. First, the sequence given is not the only one that might lead
to a reasonable parse. In general, there may be more than one path that leads to the
same result, and due to ambiguity, there may be other transition sequences that lead
to different equally valid parses.
Second, we are assuming that the oracle always provides the correct operator
at each point in the parse — an assumption that is unlikely to be true in practice.
As a result, given the greedy nature of this algorithm, incorrect choices will lead to
incorrect parses since the parser has no opportunity to go back and pursue alternative
choices. Section 13.4.2 will introduce several techniques that allow transition-based
approaches to explore the search space more fully.
Finally, for simplicity, we have illustrated this example without the labels on
the dependency relations. To produce labeled trees, we can parameterize the LE FT-
ARC and R IGHTARC operators with dependency labels, as in LE FTARC (N SUB J ) or
R IGHTARC (DOB J ). This is equivalent to expanding the set of transition operators
from our original set of three to a set that includes L E FTARC and R IGH TARC opera-
tors for each relation in the set of dependency relations being used, plus an additional
one for the SH I F T operator. This, of course, makes the job of the oracle more difﬁcult
since it now has a much larger set of operators from which to choose.

280 CHA PTER 13

• D E P END ENCY PAR S ING

13.4.1 Creating an Oracle

training oracle

State-of-the-art transition-based systems use supervised machine learning methods
to train classiﬁers that play the role of the oracle. Given appropriate training data,
these methods learn a function that maps from conﬁgurations to transition operators.
As with all supervised machine learning methods, we will need access to appro-
priate training data and we will need to extract features useful for characterizing the
decisions to be made. The source for this training data will be representative tree-
banks containing dependency trees. The features will consist of many of the same
features we encountered in Chapter 8 for part-of-speech tagging, as well as those
used in Chapter 12 for statistical parsing models.

Generating Training Data

Let’s revisit the oracle from the algorithm in Fig. 13.6 to fully understand the learn-
ing problem. The oracle takes as input a conﬁguration and returns as output a tran-
sition operator. Therefore, to train a classiﬁer, we will need conﬁgurations paired
with transition operators (i.e., LE FTARC, R IGHTARC, or SH I F T). Unfortunately,
treebanks pair entire sentences with their corresponding trees, and therefore they
don’t directly provide what we need.
To generate the required training data, we will employ the oracle-based parsing
algorithm in a clever way. We will supply our oracle with the training sentences
to be parsed along with their corresponding reference parses from the treebank. To
produce training instances, we will then simulate the operation of the parser by run-
ning the algorithm and relying on a new training oracle to give us correct transition
operators for each successive conﬁguration.
To see how this works, let’s ﬁrst review the operation of our parser. It begins with
a default initial conﬁguration where the stack contains the ROOT, the input list is just
the list of words, and the set of relations is empty. The LE FTARC and R IGH TARC
operators each add relations between the words at the top of the stack to the set of
relations being accumulated for a given sentence. Since we have a gold-standard
reference parse for each training sentence, we know which dependency relations are
valid for a given sentence. Therefore, we can use the reference parse to guide the
selection of operators as the parser steps through a sequence of conﬁgurations.
To be more precise, given a reference parse and a conﬁguration, the training
oracle proceeds as follows:
• Choose LE FTARC if it produces a correct head-dependent relation given the
reference parse and the current conﬁguration,
• Otherwise, choose R IGH TARC if (1) it produces a correct head-dependent re-
lation given the reference parse and (2) all of the dependents of the word at
the top of the stack have already been assigned,
• Otherwise, choose SH I F T.
The restriction on selecting the R IGH TARC operator is needed to ensure that a
word is not popped from the stack, and thus lost to further processing, before all its
dependents have been assigned to it.
More formally, during training the oracle has access to the following informa-
tion:
• A current conﬁguration with a stack S and a set of dependency relations Rc
• A reference parse consisting of a set of vertices V and a set of dependency
relations R p

13 . 4

• TRAN S I T ION -BA SED D E PEND ENCY PAR S ING

281

Step
0
1
2
3
4
5
6
7
8
9
10

Predicted Action

Stack
Word List
[root]
[book, the, ﬂight, through, houston]
[root, book]
[the, ﬂight, through, houston]
[root, book, the]
[ﬂight, through, houston]
[root, book, the, ﬂight]
[through, houston]
[root, book, ﬂight]
[through, houston]
[root, book, ﬂight, through]
[houston]
[root, book, ﬂight, through, houston]
[]
[root, book, ﬂight, houston ]
[]
[root, book, ﬂight]
[]
[root, book]
[]
[root]
[]
Done
Figure 13.8 Generating training items consisting of conﬁguration/predicted action pairs by
simulating a parse with a given reference parse.

SH I F T
SH I F T
SH I F T
LE FTARC
SH I F T
SH I F T
LE FTARC
R IGHTARC
R IGHTARC
R IGHTARC

Given this information, the oracle chooses transitions as follows:
LE FTARC(r): if (S1 r S2 ) ∈ R p
R IGH TARC(r): if (S2 r S1 ) ∈ R p and ∀r (cid:48) , w s.t .(S1 r (cid:48) w) ∈ R p then (S1 r (cid:48) w) ∈
Rc

SH I F T: otherwise

Let’s walk through some the steps of this process with the following example as
shown in Fig. 13.8.

root

dobj

det

nmod

case

Book the ﬂight through Houston

(13.7)
At Step 1, LE FTARC is not applicable in the initial conﬁguration since it asserts
a relation, (root ← book), not in the reference answer; R IGH TARC does assert a
relation contained in the ﬁnal answer (root → book), however book has not been
attached to any of its dependents yet, so we have to defer, leaving SH I F T as the only
possible action. The same conditions hold in the next two steps. In step 3, LE FTARC
is selected to link the to its head.
Now consider the situation in Step 4.

Stack
Word buffer
Relations
[root, book, ﬂight] [through, Houston] (the ← ﬂight)
Here, we might be tempted to add a dependency relation between book and ﬂight,
which is present in the reference parse. But doing so now would prevent the later
attachment of Houston since ﬂight would have been removed from the stack. For-
tunately, the precondition on choosing R IGHTARC prevents this choice and we’re
again left with SH I F T as the only viable option. The remaining choices complete the
set of operators needed for this example.
To recap, we derive appropriate training instances consisting of conﬁguration-
transition pairs from a treebank by simulating the operation of a parser in the con-
text of a reference dependency tree. We can deterministically record correct parser
actions at each step as we progress through each training example, thereby creating
the training set we require.

282 CHA PTER 13

• D E P END ENCY PAR S ING

Features

feature
template

Having generated appropriate training instances (conﬁguration-transition pairs), we
need to extract useful features from the conﬁgurations so what we can train classi-
ﬁers. The features that are used to train transition-based systems vary by language,
genre, and the kind of classiﬁer being employed. For example, morphosyntactic
features such as case marking on subjects or direct objects may be more or less im-
portant depending on the language being processed. That said, the basic features that
we have already seen with part-of-speech tagging and partial parsing have proven to
be useful in training dependency parsers across a wide range of languages. Word
forms, lemmas and parts of speech are all powerful features, as are the head, and
dependency relation to the head.
In the transition-based parsing framework, such features need to be extracted
from the conﬁgurations that make up the training data. Recall that conﬁgurations
consist of three elements: the stack, the buffer and the current set of relations. In
principle, any property of any or all of these elements can be represented as features
in the usual way for training. However, to avoid sparsity and encourage generaliza-
tion, it is best to focus the learning algorithm on the most useful aspects of decision
making at each point in the parsing process. The focus of feature extraction for
transition-based parsing is, therefore, on the top levels of the stack, the words near
the front of the buffer, and the dependency relations already associated with any of
those elements.
By combining simple features, such as word forms or parts of speech, with spe-
ciﬁc locations in a conﬁguration, we can employ the notion of a feature template
that we’ve already encountered with sentiment analysis and part-of-speech tagging.
Feature templates allow us to automatically generate large numbers of speciﬁc fea-
tures from a training set. As an example, consider the following feature templates
that are based on single positions in a conﬁguration.

(13.8)

(cid:104)s1 .w, op(cid:105), (cid:104)s2 .w, op(cid:105)(cid:104)s1 .t , op(cid:105), (cid:104)s2 .t , op(cid:105)
(cid:104)b1 .w, op(cid:105), (cid:104)b1 .t , op(cid:105)(cid:104)s1 .wt , op(cid:105)
In these examples, individual features are denoted as l ocat ion. pro pert y, where s
denotes the stack, b the word buffer, and r the set of relations. Individual properties
of locations include w for word forms, l for lemmas, and t for part-of-speech. For
example, the feature corresponding to the word form at the top of the stack would be
denoted as s1 .w, and the part of speech tag at the front of the buffer b1 .t . We can also
combine individual features via concatenation into more speciﬁc features that may
prove useful. For example, the feature designated by s1 .wt represents the word form
concatenated with the part of speech of the word at the top of the stack. Finally, o p
stands for the transition operator for the training example in question (i.e., the label
for the training instance).
Let’s consider the simple set of single-element feature templates given above
in the context of the following intermediate conﬁguration derived from a training
oracle for Example 13.2.

Stack
Word buffer
Relations
[root, canceled, ﬂights] [to Houston] (canceled → United)
(ﬂights → morning)
(ﬂights → the)
The correct transition here is SH I F T (you should convince yourself of this before

13 . 4

• TRAN S I T ION -BA SED D E PEND ENCY PAR S ING

283

proceeding). The application of our set of feature templates to this conﬁguration
would result in the following set of instantiated features.

(13.9)

(cid:104)s1 .w = ﬂights, o p = shift(cid:105)
(cid:104)s2 .w = canceled, o p = shift(cid:105)
(cid:104)s1 .t = NNS, o p = shift(cid:105)
(cid:104)s2 .t = VBD, o p = shift(cid:105)
(cid:104)b1 .w = to, o p = shift(cid:105)
(cid:104)b1 .t = TO, o p = shift(cid:105)
(cid:104)s1 .wt = ﬂightsNNS, o p = shift(cid:105)
Given that the left and right arc transitions operate on the top two elements of
the stack, features that combine properties from these positions are even more useful.
For example, a feature like s1 .t ◦ s2 .t concatenates the part of speech tag of the word
at the top of the stack with the tag of the word beneath it.

(cid:104)s1 .t ◦ s2 .t = NNSVBD, o p = shift(cid:105)
Not surprisingly, if two properties are useful then three or more should be even
better. Figure 13.9 gives a baseline set of feature templates that have been employed
in various state-of-the-art systems (Zhang and Clark 2008,Huang and Sagae 2010,Zhang
and Nivre 2011).
Note that some of these features make use of dynamic features — features such
as head words and dependency relations that have been predicted at earlier steps in
the parsing process, as opposed to features that are derived from static properties of
the input.

(13.10)

Source
Feature templates
One word s1 .w

s2 .w
b1 .w

Two word s1 .w ◦ s2 .w

s1 .t ◦ s2 .wt
s1 .w ◦ s1 .t ◦ s2 .t

s1 .t
s1 .wt
s2 .t
s2 .wt
b1 .w
b0 .wt
s1 .t ◦ s2 .t
s1 .t ◦ b1 .w
s1 .w ◦ s2 .w ◦ s2 .t s1 .w ◦ s1 .t ◦ s2 .t
s1 .w ◦ s1 .t

Figure 13.9 Standard feature templates for training transition-based dependency parsers.
In the template speciﬁcations sn refers to a location on the stack, bn refers to a location in the
word buffer, w refers to the wordform of the input, and t refers to the part of speech of the
input.

Learning

Over the years, the dominant approaches to training transition-based dependency
parsers have been multinomial logistic regression and support vector machines, both
of which can make effective use of large numbers of sparse features of the kind
described in the last section. More recently, neural network, or deep learning,
approaches of the kind described in Chapter 8 have been applied successfully to
transition-based parsing (Chen and Manning, 2014). These approaches eliminate the
need for complex, hand-crafted features and have been particularly effective at over-
coming the data sparsity issues normally associated with training transition-based
parsers.

284 CHA PTER 13

• D E P END ENCY PAR S ING

13.4.2 Advanced Methods in Transition-Based Parsing

The basic transition-based approach can be elaborated in a number of ways to im-
prove performance by addressing some of the most obvious ﬂaws in the approach.

Alternative Transition Systems

arc eager

The arc-standard transition system described above is only one of many possible sys-
tems. A frequently used alternative is the arc eager transition system. The arc eager
approach gets its name from its ability to assert rightward relations much sooner
than in the arc standard approach. To see this, let’s revisit the arc standard trace of
Example 13.7, repeated here.

root

dobj

det

nmod

case

Book the ﬂight through Houston

Consider the dependency relation between book and ﬂight in this analysis. As
is shown in Fig. 13.8, an arc-standard approach would assert this relation at Step 8,
despite the fact that book and ﬂight ﬁrst come together on the stack much earlier at
Step 4. The reason this relation can’t be captured at this point is due to the presence
of the post-nominal modiﬁer through Houston. In an arc-standard approach, depen-
dents are removed from the stack as soon as they are assigned their heads. If ﬂight
had been assigned book as its head in Step 4, it would no longer be available to serve
as the head of Houston.
While this delay doesn’t cause any issues in this example, in general the longer
a word has to wait to get assigned its head the more opportunities there are for
something to go awry. The arc-eager system addresses this issue by allowing words
to be attached to their heads as early as possible, before all the subsequent words
dependent on them have been seen. This is accomplished through minor changes to
the LE FTARC and R IGH TARC operators and the addition of a new R EDUCE operator.
• LE FTARC: Assert a head-dependent relation between the word at the front of
the input buffer and the word at the top of the stack; pop the stack.
• R IGH TARC: Assert a head-dependent relation between the word on the top of
the stack and the word at front of the input buffer; shift the word at the front
of the input buffer to the stack.
• SH I F T: Remove the word from the front of the input buffer and push it onto
the stack.
• R EDUCE: Pop the stack.
The LE FTARC and R IGHTARC operators are applied to the top of the stack and
the front of the input buffer, instead of the top two elements of the stack as in the
arc-standard approach. The R IGH TARC operator now moves the dependent to the
stack from the buffer rather than removing it, thus making it available to serve as the
head of following words. The new R EDUC E operator removes the top element from
the stack. Together these changes permit a word to be eagerly assigned its head and
still allow it to serve as the head for later dependents. The trace shown in Fig. 13.10
illustrates the new decision sequence for this example.
In addition to demonstrating the arc-eager transition system, this example demon-
strates the power and ﬂexibility of the overall transition-based approach. We were
able to swap in a new transition system without having to make any changes to the

13 . 4

• TRAN S I T ION -BA SED D E PEND ENCY PAR S ING

285

Step
0
1
2
3
4
5
6
7
8
9
10

Stack Word List
Action
[root]
[book, the, ﬂight, through, houston] R IGH TARC
[root, book]
[the, ﬂight, through, houston]
[root, book, the]
[ﬂight, through, houston]
[root, book]
[ﬂight, through, houston]
[root, book, ﬂight]
[through, houston]
[root, book, ﬂight, through]
[houston]
[root, book, ﬂight]
[houston]
[root, book, ﬂight, houston]
[]
[root, book, ﬂight]
[]
[root, book]
[]
[root]
[]
Done
Figure 13.10 A processing trace of Book the ﬂight through Houston using the arc-eager
transition operators.

Relation Added
(root → book)
(the ← ﬂight)
(book → ﬂight)
(through ← houston)
(ﬂight → houston)

SH I F T
LE FTARC
R IGH TARC
SH I F T
LE FTARC
R IGH TARC
R EDUCE
R EDUCE
R EDUCE

Beam search

beam width

underlying parsing algorithm. This ﬂexibility has led to the development of a di-
verse set of transition systems that address different aspects of syntax and semantics
including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the
generation of non-projective dependency structures (Nivre, 2009), assigning seman-
tic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages
(Bhat et al., 2017).

Beam Search

The computational efﬁciency of the transition-based approach discussed earlier de-
rives from the fact that it makes a single pass through the sentence, greedily making
decisions without considering alternatives. Of course, this is also the source of its
greatest weakness – once a decision has been made it can not be undone, even in
the face of overwhelming evidence arriving later in a sentence. Another approach
is to systematically explore alternative decision sequences, selecting the best among
those alternatives. The key problem for such a search is to manage the large number
of potential sequences. Beam search accomplishes this by combining a breadth-ﬁrst
search strategy with a heuristic ﬁlter that prunes the search frontier to stay within a
ﬁxed-size beam width.
In applying beam search to transition-based parsing, we’ll elaborate on the al-
gorithm given in Fig. 13.6. Instead of choosing the single best transition operator
at each iteration, we’ll apply all applicable operators to each state on an agenda and
then score the resulting conﬁgurations. We then add each of these new conﬁgura-
tions to the frontier, subject to the constraint that there has to be room within the
beam. As long as the size of the agenda is within the speciﬁed beam width, we can
add new conﬁgurations to the agenda. Once the agenda reaches the limit, we only
add new conﬁgurations that are better than the worst conﬁguration on the agenda
(removing the worst element so that we stay within the limit). Finally, to insure that
we retrieve the best possible state on the agenda, the while loop continues as long as
there are non-ﬁnal states on the agenda.
The beam search approach requires a more elaborate notion of scoring than we
used with the greedy algorithm. There, we assumed that a classiﬁer trained using
supervised machine learning would serve as an oracle, selecting the best transition
operator based on features extracted from the current conﬁguration. Regardless of
the speciﬁc learning approach, this choice can be viewed as assigning a score to all
the possible transitions and picking the best one.
ˆT (c) = argmaxScore(t , c)

286 CHA PTER 13

• D E P END ENCY PAR S ING

With a beam search we are now searching through the space of decision se-
quences, so it makes sense to base the score for a conﬁguration on its entire history.
More speciﬁcally, we can deﬁne the score for a new conﬁguration as the score of its
predecessor plus the score of the operator used to produce it.

ConﬁgScore(c0 ) = 0.0
ConﬁgScore(ci ) = ConﬁgScore(ci−1 ) + Score(ti , ci−1 )
This score is used both in ﬁltering the agenda and in selecting the ﬁnal answer.
The new beam search version of transition-based parsing is given in Fig. 13.11.

function D E P END ENCYB EAMPAR SE(words, width) returns dependency tree
state ← {[root], [words], [], 0.0}
;initial conﬁguration
agenda ← (cid:104)state(cid:105);
initial agenda
while agenda contains non-ﬁnal states
newagenda ← (cid:104)(cid:105)
for each state ∈ agenda do
for all {t | t ∈ VA L IDO PERATOR S(state)} do
child ← A P PLY(t, state)
newagenda ← ADDTOB EAM(child, newagenda, width)
agenda ← newagenda
return B E S TO F(agenda)

function ADDTOB EAM(state, agenda, width) returns updated agenda

if L ENG TH(agenda) < width then
agenda ← IN SERT(state, agenda)
else if SCORE(state) > SCOR E(WOR S TO F(agenda))
agenda ← R EMOV E(WOR STO F(agenda))
agenda ← IN SERT(state, agenda)

return agenda

Figure 13.11 Beam search applied to transition-based dependency parsing.

13.5 Graph-Based Dependency Parsing

Graph-based approaches to dependency parsing search through the space of possible
trees for a given sentence for a tree (or trees) that maximize some score. These
methods encode the search space as directed graphs and employ methods drawn
from graph theory to search the space for optimal solutions. More formally, given a
sentence S we’re looking for the best dependency tree in Gs , the space of all possible
trees for that sentence, that maximizes some score.
ˆT (S) = argmax

score(t , S)

t ∈GS

As with the probabilistic approaches to context-free parsing discussed in Chap-
ter 12, the overall score for a tree can be viewed as a function of the scores of the
parts of the tree. The focus of this section is on edge-factored approaches where the

edge-factored

maximum
spanning tree

13 .5

• GRA PH -BA SED D E PEND ENCY PAR S ING

287

score(e)

score for a tree is based on the scores of the edges that comprise the tree.
score(t , S) = (cid:88)e∈t
There are several motivations for the use of graph-based methods. First, unlike
transition-based approaches, these methods are capable of producing non-projective
trees. Although projectivity is not a signiﬁcant issue for English, it is deﬁnitely a
problem for many of the world’s languages. A second motivation concerns parsing
accuracy, particularly with respect to longer dependencies. Empirically, transition-
based methods have high accuracy on shorter dependency relations but accuracy de-
clines signiﬁcantly as the distance between the head and dependent increases (Mc-
Donald and Nivre, 2011). Graph-based methods avoid this difﬁculty by scoring
entire trees, rather than relying on greedy local decisions.
The following section examines a widely-studied approach based on the use of a
maximum spanning tree (MST) algorithm for weighted, directed graphs. We then
discuss features that are typically used to score trees, as well as the methods used to
train the scoring models.

13.5.1 Parsing

The approach described here uses an efﬁcient greedy algorithm to search for optimal
spanning trees in directed graphs. Given an input sentence, it begins by constructing
a fully-connected, weighted, directed graph where the vertices are the input words
and the directed edges represent all possible head-dependent assignments. An addi-
tional ROOT node is included with outgoing edges directed at all of the other vertices.
The weights in the graph reﬂect the score for each possible head-dependent relation
as provided by a model generated from training data. Given these weights, a maxi-
mum spanning tree of this graph emanating from the ROOT represents the preferred
dependency parse for the sentence. A directed graph for the example Book that
ﬂight is shown in Fig. 13.12, with the maximum spanning tree corresponding to the
desired parse shown in blue. For ease of exposition, we’ll focus here on unlabeled
dependency parsing. Graph-based approaches to labeled parsing are discussed in
Section 13.5.3.
Before describing the algorithm it’s useful to consider two intuitions about di-
rected graphs and their spanning trees. The ﬁrst intuition begins with the fact that
every vertex in a spanning tree has exactly one incoming edge. It follows from this
that every connected component of a spanning tree will also have one incoming edge.
The second intuition is that the absolute values of the edge scores are not critical to
determining its maximum spanning tree. Instead, it is the relative weights of the
edges entering each vertex that matters. If we were to subtract a constant amount
from each edge entering a given vertex it would have no impact on the choice of
the maximum spanning tree since every possible spanning tree would decrease by
exactly the same amount.
The ﬁrst step of the algorithm itself is quite straightforward. For each vertex
in the graph, an incoming edge (representing a possible head assignment) with the
highest score is chosen. If the resulting set of edges produces a spanning tree then
we’re done. More formally, given the original fully-connected graph G = (V, E ), a
subgraph T = (V, F ) is a spanning tree if it has no cycles and each vertex (other than
the root) has exactly one edge entering it. If the greedy selection process produces
such a tree then it is the best possible one.

288 CHA PTER 13

• D E P END ENCY PAR S ING

Figure 13.12

Initial rooted, directed graph for Book that ﬂight.

Unfortunately, this approach doesn’t always lead to a tree since the set of edges
selected may contain cycles. Fortunately, in yet another case of multiple discovery,
there is a straightforward way to eliminate cycles generated during the greedy se-
lection phase. Chu and Liu (1965) and Edmonds (1967) independently developed
an approach that begins with greedy selection and follows with an elegant recursive
cleanup phase that eliminates cycles.
The cleanup phase begins by adjusting all the weights in the graph by subtracting
the score of the maximum edge entering each vertex from the score of all the edges
entering that vertex. This is where the intuitions mentioned earlier come into play.
We have scaled the values of the edges so that the weight of the edges in the cycle
have no bearing on the weight of any of the possible spanning trees. Subtracting the
value of the edge with maximum weight from each edge entering a vertex results
in a weight of zero for all of the edges selected during the greedy selection phase,
including all of the edges involved in the cycle.
Having adjusted the weights, the algorithm creates a new graph by selecting a
cycle and collapsing it into a single new node. Edges that enter or leave the cycle
are altered so that they now enter or leave the newly collapsed node. Edges that do
not touch the cycle are included and edges within the cycle are dropped.
Now, if we knew the maximum spanning tree of this new graph, we would have
what we need to eliminate the cycle. The edge of the maximum spanning tree di-
rected towards the vertex representing the collapsed cycle tells us which edge to
delete to eliminate the cycle. How do we ﬁnd the maximum spanning tree of this
new graph? We recursively apply the algorithm to the new graph. This will either
result in a spanning tree or a graph with a cycle. The recursions can continue as long
as cycles are encountered. When each recursion completes we expand the collapsed
vertex, restoring all the vertices and edges from the cycle with the exception of the
single edge to be deleted.
Putting all this together, the maximum spanning tree algorithm consists of greedy
edge selection, re-scoring of edge costs and a recursive cleanup phase when needed.
The full algorithm is shown in Fig. 13.13.
Fig. 13.14 steps through the algorithm with our Book that ﬂight example. The
ﬁrst row of the ﬁgure illustrates greedy edge selection with the edges chosen shown

13 .5

• GRA PH -BA SED D E PEND ENCY PAR S ING

289

for each v ∈ V do

function MAXS PANN INGTR EE(G=(V,E), root, score) returns spanning tree
F ← []
T’ ← []
score’ ← []
bestInEdge ← argmaxe=(u,v)∈ E score[e]
F ← F ∪ bestInEdge
score’[e] ← score[e] − score[bestInEdge]
if T=(V,F) is a spanning tree then return it

for each e=(u,v) ∈ E do

else

C ← a cycle in F
G’ ← CONTRAC T(G, C)
T’ ← MAXS PANN INGTR EE(G’, root, score’)
T ← EX PAND(T’, C)

return T

function CONTRAC T(G, C) returns contracted graph

function EX PAND(T, C) returns expanded graph

Figure 13.13 The Chu-Liu Edmonds algorithm for ﬁnding a maximum spanning tree in a
weighted directed graph.

in blue (corresponding to the set F in the algorithm). This results in a cycle between
that and ﬂight. The scaled weights using the maximum value entering each node are
shown in the graph to the right.
Collapsing the cycle between that and ﬂight to a single node (labelled tf) and
recursing with the newly scaled costs is shown in the second row. The greedy selec-
tion step in this recursion yields a spanning tree that links root to book, as well as an
edge that links book to the contracted node. Expanding the contracted node, we can
see that this edge corresponds to the edge from book to ﬂight in the original graph.
This in turn tells us which edge to drop to eliminate the cycle
On arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)
time, where m is the number of edges and n is the number of nodes. Since this par-
ticular application of the algorithm begins by constructing a fully connected graph
m = n2 yielding a running time of O(n3 ). Gabow et al. (1986) present a more efﬁ-
cient implementation with a running time of O(m + nl ogn).

13.5.2 Features and Training

Given a sentence, S, and a candidate tree, T , edge-factored parsing models reduce
the score for the tree to a sum of the scores of the edges that comprise the tree.
score(S, T ) = (cid:88)e∈T
Each edge score can, in turn, be reduced to a weighted sum of features extracted
from it.

score(S, e)

score(S, e) =

wi f i (S, e)

N(cid:88)i=1

290 CHA PTER 13

• D E P END ENCY PAR S ING

Figure 13.14 Chu-Liu-Edmonds graph-based example for Book that ﬂight

Or more succinctly.

score(S, e) = w · f
Given this formulation, we are faced with two problems in training our parser:
identifying relevant features and ﬁnding the weights used to score those features.
The features used to train edge-factored models mirror those used in training
transition-based parsers (as shown in Fig. 13.9). This is hardly surprising since in
both cases we’re trying to capture information about the relationship between heads
and their dependents in the context of a single relation. To summarize this earlier
discussion, commonly used features include:
• Wordforms, lemmas, and parts of speech of the headword and its dependent.
• Corresponding features derived from the contexts before, after and between
the words.
• Word embeddings.
• The dependency relation itself.
• The direction of the relation (to the right or left).
• The distance from the head to the dependent.
As with transition-based approaches, pre-selected combinations of these features are
often used as well.
Given a set of features, our next problem is to learn a set of weights correspond-
ing to each. Unlike many of the learning problems discussed in earlier chapters,

inference-based
learning

13 .6

• EVA LUAT ION

291

here we are not training a model to associate training items with class labels, or
parser actions. Instead, we seek to train a model that assigns higher scores to cor-
rect trees than to incorrect ones. An effective framework for problems like this is to
use inference-based learning combined with the perceptron learning rule. In this
framework, we parse a sentence (i.e, perform inference) from the training set using
some initially random set of initial weights. If the resulting parse matches the cor-
responding tree in the training data, we do nothing to the weights. Otherwise, we
ﬁnd those features in the incorrect parse that are not present in the reference parse
and we lower their weights by a small amount based on the learning rate. We do this
incrementally for each sentence in our training data until the weights converge.
More recently, recurrent neural network (RNN) models have demonstrated state-
of-the-art performance in shared tasks on multilingual parsing (Zeman et al. 2017,Dozat
et al. 2017). These neural approaches rely solely on lexical information in the form
of word embeddings, eschewing the use of hand-crafted features such as those de-
scribed earlier.

13.5.3 Advanced Issues in Graph-Based Parsing

13.6 Evaluation

As with phrase structure-based parsing, the evaluation of dependency parsers pro-
ceeds by measuring how well they work on a test-set. An obvious metric would be
exact match (EM) — how many sentences are parsed correctly. This metric is quite
pessimistic, with most sentences being marked wrong. Such measures are not ﬁne-
grained enough to guide the development process. Our metrics need to be sensitive
enough to tell if actual improvements are being made.
For these reasons, the most common method for evaluating dependency parsers
are labeled and unlabeled attachment accuracy. Labeled attachment refers to the
proper assignment of a word to its head along with the correct dependency relation.
Unlabeled attachment simply looks at the correctness of the assigned head, ignor-
ing the dependency relation. Given a system output and a corresponding reference
parse, accuracy is simply the percentage of words in an input that are assigned the
correct head with the correct relation. This metrics are usually referred to as the
labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we
can make use of a label accuracy score (LS), the percentage of tokens with correct
labels, ignoring where the relations are coming from.
As an example, consider the reference parse and system parse for the following
example shown in Fig. 13.15.
(13.11) Book me the ﬂight through Houston.
The system correctly ﬁnds 4 of the 6 dependency relations present in the refer-
ence parse and therefore receives an LAS of 2/3. However, one of the 2 incorrect
relations found by the system holds between book and ﬂight, which are in a head-
dependent relation in the reference parse; therefore the system therefore achieves an
UAS of 5/6.
Beyond attachment scores, we may also be interested in how well a system is
performing on a particular kind of dependency relation, for example N SUB J, across
a development corpus. Here we can make use of the notions of precision and recall
introduced in Chapter 8, measuring the percentage of relations labeled N SUB J by
the system that were correct (precision), and the percentage of the N SUB J relations

292 CHA PTER 13

• D E P END ENCY PAR S ING

root

iobj

obj

det

nmod

case

root

x-comp

nsubj

det

nmod

case

Book me the

ﬂight
Reference

through Houston

Book me the ﬂight
System

through Houston

Figure 13.15 Reference and system parses for Book me the ﬂight through Houston, resulting in an LAS of
4/6 and an UAS of 5/6.

present in the development set that were in fact discovered by the system (recall).
We can employ a confusion matrix to keep track of how often each dependency type
was confused for another.

13.7 Summary

This chapter has introduced the concept of dependency grammars and dependency
parsing. Here’s a summary of the main points that we covered:
• In dependency-based approaches to syntax, the structure of a sentence is de-
scribed in terms of a set of binary relations that hold between the words in a
sentence. Larger notions of constituency are not directly encoded in depen-
dency analyses.
• The relations in a dependency structure capture the head-dependent relation-
ship among the words in a sentence.
• Dependency-based analyses provides information directly useful in further
language processing tasks including information extraction, semantic parsing
and question answering
• Transition-based parsing systems employ a greedy stack-based algorithm to
create dependency structures.
• Graph-based methods for creating dependency structures are based on the use
of maximum spanning tree methods from graph theory.
• Both transition-based and graph-based approaches are developed using super-
vised machine learning techniques.
• Treebanks provide the data needed to train these systems. Dependency tree-
banks can be created directly by human annotators or via automatic transfor-
mation from phrase-structure treebanks.
• Evaluation of dependency parsers is based on labeled and unlabeled accuracy
scores as measured against withheld development and test corpora.

Bibliographical and Historical Notes

The dependency-based approach to grammar is much older than the relatively re-
cent phrase-structure or constituency grammars that have been the primary focus of
both theoretical and computational linguistics for years. It has its roots in the an-
cient Greek and Indian linguistic traditions. Contemporary theories of dependency

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

293

grammar all draw heavily on the work of Tesni `ere (1959). The most inﬂuential
dependency grammar frameworks include Meaning-Text Theory (MTT) (Mel’ ˘cuk,
1988), Word Grammar (Hudson, 1984), Functional Generative Description (FDG)
(Sgall et al., 1986). These frameworks differ along a number of dimensions in-
cluding the degree and manner in which they deal with morphological, syntactic,
semantic and pragmatic factors, their use of multiple layers of representation, and
the set of relations used to categorize dependency relations.
Automatic parsing using dependency grammars was ﬁrst introduced into compu-
tational linguistics by early work on machine translation at the RAND Corporation
led by David Hays. This work on dependency parsing closely paralleled work on
constituent parsing and made explicit use of grammars to guide the parsing process.
After this early period, computational work on dependency parsing remained inter-
mittent over the following decades. Notable implementations of dependency parsers
for English during this period include Link Grammar (Sleator and Temperley, 1993),
Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).
Dependency parsing saw a major resurgence in the late 1990’s with the appear-
ance of large dependency-based treebanks and the associated advent of data driven
approaches described in this chapter. Eisner (1996) developed an efﬁcient dynamic
programming approach to dependency parsing based on bilexical grammars derived
from the Penn Treebank. Covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. Yamada and Mat-
sumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce
paradigm and the use of supervised machine learning in the form of support vector
machines to dependency parsing.
Nivre (2003) deﬁned the modern, deterministic, transition-based approach to de-
pendency parsing. Subsequent work by Nivre and his colleagues formalized and an-
alyzed the performance of numerous transition systems, training methods, and meth-
ods for dealing with non-projective language Nivre and Scholz 2004,Nivre 2006,Nivre
and Nilsson 2005,Nivre et al. 2007,Nivre 2007.
The graph-based maximum spanning tree approach to dependency parsing was
introduced by McDonald et al. 2005,McDonald et al. 2005.
The earliest source of data for training and evaluating dependency English parsers
came from the WSJ Penn Treebank (Marcus et al., 1993) described in Chapter 10.
The use of head-ﬁnding rules developed for use with probabilistic parsing facili-
tated the automatic extraction of dependency parses from phrase-based ones (Xia
and Palmer, 2001).
The long-running Prague Dependency Treebank project (Haji ˇc, 1998) is the most
signiﬁcant effort to directly annotate a corpus with multiple layers of morphological,
syntactic and semantic information. The current PDT 3.0 now contains over 1.5 M
tokens (Bej ˇcek et al., 2013).
Universal Dependencies (UD) (Nivre et al., 2016b) is a project directed at cre-
ating a consistent framework for dependency treebank annotation across languages
with the goal of advancing parser development across the worlds languages. Under
the auspices of this effort, treebanks for over 30 languages have been annotated and
made available in a single consistent format. The UD annotation scheme evolved out
of several distinct efforts including Stanford dependencies (de Marneffe et al. 2006,
de Marneffe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-
of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic
tagsets (Zeman, 2008). Driven in part by the UD framework, dependency treebanks
of a signiﬁcant size and quality are now available in over 30 languages (Nivre et al.,

294 CHA PTER 13

• D E P END ENCY PAR S ING

2016b).
The Conference on Natural Language Learning (CoNLL) has conducted an in-
ﬂuential series of shared tasks related to dependency parsing over the years (Buch-
holz and Marsi 2006, Nilsson et al. 2007, Surdeanu et al. 2008a, Haji ˇc et al. 2009).
More recent evaluations have focused on parser robustness with respect to morpho-
logically rich languages (Seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (Petrov and McDonald, 2012).
Choi et al. (2015) presents a detailed performance analysis of 10 state-of-the-art de-
pendency parsers across an impressive range of metrics, as well as D E P ENDAB LE, a
robust parser evaluation tool.

Exercises

CHAPTER

14 The Representation of Sen-
tence Meaning

I SHMAE L : Surely all this is not without meaning.

Herman Melville, Moby Dick

meaning
representations

meaning
representation
languages

The approach to semantics introduced here, and elaborated on in the next two chap-
ters, is based on the idea that the meaning of linguistic expressions can be cap-
tured in formal structures called meaning representations. Correspondingly, the
frameworks that specify the syntax and semantics of these representations are called
meaning representation languages. These meaning representations play a role
analogous to that of the syntactic representations introduced in earlier chapters—
they abstract away from surface forms and facilitate downstream processing.
The need for meaning representations arises when neither the raw linguistic in-
puts nor any of the syntactic structures derivable from them facilitate the kind of se-
mantic processing that is required. We need representations that bridge the gap from
linguistic inputs to the knowledge of the world needed to perform tasks. Consider
the following ordinary language tasks that require some form of semantic processing
of natural language:
• Deciding what to order at a restaurant by reading a menu
• Learning to use a new piece of software by reading the manual
• Answering essay questions on an exam
• Realizing that you’ve been insulted
• Following recipes
Grammatical representations aren’t sufﬁcient to accomplish these tasks. What
is required are representations that link the linguistic elements to the non-linguistic
knowledge of the world needed to successfully accomplish the tasks:
• Reading a menu and deciding what to order, giving advice about where to go
to dinner, following a recipe, and generating new recipes all require knowl-
edge about food and its preparation, what people like to eat, and what restau-
rants are like.
• Answering and grading essay questions requires background knowledge about
the topic of the question, the desired knowledge level of the students, and how
such questions are normally answered.
• Learning to use a piece of software by reading a manual, or giving advice
about how to use the software, requires knowledge about current computers,
the speciﬁc software in question, similar software applications, and knowl-
edge about users in general.
In this chapter, we assume that linguistic expressions have meaning representa-
tions that are made up of the same kind of stuff that is used to represent this kind
of everyday common-sense knowledge of the world. The process whereby such

295

296 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

∃e, y H aving(e) ∧ H aver(e, S peaker) ∧ H adT hing(e, y) ∧ Car(y)

Having:
Haver:
Speaker
HadThing:
Car

Figure 14.1 A list of symbols, two directed graphs, and a record structure: a sampler of
meaning representations for I have a car.

representations are created and assigned to linguistic inputs is called semantic anal-
ysis, and the entire enterprise of designing meaning representations and associated
semantic analyzers is referred to as computational semantics.
To make these notions a bit more concrete, consider Fig. 14.1, which shows
example meaning representations for the sentence I have a car using four com-
monly used meaning representation languages. The top row illustrates a sentence
in First-Order Logic, covered in detail in Section 14.3; the directed graph and its
corresponding textual form is an example of an Abstract Meaning Representa-
tion (AMR) form, to be discussed in Chapter 18, and ﬁnally a Frame-Based or
Slot-Filler representation, discussed in Section 14.5 and again in Chapter 17.
While there are non-trivial differences among these approaches, they all share
the notion that a meaning representation consists of structures composed from a
set of symbols, or representational vocabulary. When appropriately arranged, these
symbol structures are taken to correspond to objects, properties of objects, and rela-
tions among objects in some state of affairs being represented or reasoned about. In
this case, all four representations make use of symbols corresponding to the speaker,
a car, and a relation denoting the possession of one by the other.
Importantly, these representations can be viewed from at least two distinct per-
spectives in all of these approaches: as representations of the meaning of the par-
ticular linguistic input I have a car, and as representations of the state of affairs in
some world. It is this dual perspective that allows these representations to be used
to link linguistic inputs to the world and to our knowledge of it.
This chapter introduces the basics of what is needed in a meaning representa-
tion. A number of extremely important issues are therefore deferred to later chap-
ters. The focus of this chapter is on representing the literal meaning of individual
sentences. By this, we mean representations that are derived from the core conven-
tional meanings of words and that do not reﬂect much of the context in which they
occur. Chapter 15 and Chapter 16 introduce techniques for generating these formal
meaning representations for linguistic inputs. Chapter 17 focuses on the extraction
of entities, relations events and times, Chapter 18 and Chapter 19 on semantic struc-
ture of verbs and their arguments, while the task of producing representations for

computational
semantics

literal meaning

14 . 1

• COM PU TAT IONA L D E S ID ERATA FOR R E PR E S EN TAT ION S

297

larger stretches of discourse is deferred to Chapter 20 and Chapter 21.
There are four major parts to this chapter. Section 14.1 explores some of the key
computational requirements for what we need in a meaning representation language.
Section 14.2 discusses how we can provide some guarantees that these representa-
tions will actually do what we need them to do—provide a correspondence to the
state of affairs being represented. Section 14.3 then introduces First-Order Logic,
which has historically been the primary technique for investigating issues in natural
language semantics. Section 14.4 then describes how FO L can be used to capture the
semantics of events and states in English.

14.1 Computational Desiderata for Representations

We begin by considering the issue of why meaning representations are needed and
what they should do for us. To focus this discussion, we use the task of giving advice
about restaurants to tourists. Assume that we have a computer system that accepts
spoken language inputs from tourists and constructs appropriate responses using a
knowledge base of relevant domain knowledge. A series of examples will serve to
introduce some of the basic requirements that a meaning representation must fulﬁll
and some of the complications that inevitably arise in the process of designing such
meaning representations.

14.1.1 Veriﬁability

Consider the following simple question:

(14.1) Does Maharani serve vegetarian food?

This example illustrates the most basic requirement for a meaning representation:
it must be possible to use the representation to determine the relationship between
the meaning of a sentence and the state of the world as we know it. In other words,
we need to be able to determine the truth of our representations. Section 14.2 ex-
plores the standard approach to this topic in some detail. For now, let’s assume that
computational semantic systems require the ability to compare, or match, meaning
representations associated with linguistic expressions with formal representations in
a knowledge base, its store of information about its world.
In this example, assume that the meaning of this question involves the proposi-
tion Maharani serves vegetarian food. For now, we will simply gloss this represen-
tation as

Serves(Maharani,Veget arianFood )

(14.2)

This representation of the input can be matched against our knowledge base of facts
about a set of restaurants. If the system ﬁnds a representation matching this propo-
sition in its knowledge base, it can return an afﬁrmative answer. Otherwise, it must
either say No if its knowledge of local restaurants is complete, or say that it doesn’t
know if there is reason to believe that its knowledge is incomplete.
This notion, known as veriﬁability, describes a system’s ability to compare the
state of affairs described by a representation to the state of affairs in some world as
modeled in a knowledge base.

knowledge base

veriﬁability

298 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

vagueness

14.1.2 Unambiguous Representations

Semantics, like all the other domains we have studied, is subject to ambiguity.
Speciﬁcally, individual linguistic expressions can have different meaning represen-
tations assigned to them based on the circumstances in which they occur. Consider
the following example from the B ER P corpus:
(14.3) I wanna eat someplace that’s close to IC S I.
Given the allowable argument structures for the verb eat, this sentence can either
mean that the speaker wants to eat at some nearby location, or under a Godzilla-as-
speaker interpretation, the speaker may want to devour some nearby location. The
answer generated by the system for this request will depend on which interpretation
is chosen as the correct one.
Since ambiguities such as this abound in all genres of all languages, some means
of determining that certain interpretations are preferable (or alternatively, not as
preferable) to others is needed. The various linguistic phenomena that give rise
to such ambiguities and the techniques that can be employed to deal with them are
discussed in detail in the next four chapters.
Our concern in this chapter, however, is with the status of our meaning repre-
sentations with respect to ambiguity, and not with the means by which we might
arrive at correct interpretations. Since we reason about, and act upon, the semantic
content of linguistic inputs, the ﬁnal representation of an input’s meaning should be
free from any ambiguity.1
A concept closely related to ambiguity is vagueness. Like ambiguity, vagueness
can make it difﬁcult to determine what to do with a particular input on the basis
of its meaning representation. Vagueness, however, does not give rise to multiple
representations. Consider the following request as an example:
(14.4) I want to eat Italian food.
While the use of the phrase Italian food may provide enough information for a
restaurant advisor to provide reasonable recommendations, it is nevertheless quite
vague as to what the user really wants to eat. Therefore, a vague representation
of the meaning of this phrase may be appropriate for some purposes, while a more
speciﬁc representation may be needed for other purposes. It will, therefore, be ad-
vantageous for a meaning representation language to support representations that
maintain a certain level of vagueness. Note that it is not always easy to distinguish
ambiguity from vagueness. Zwicky and Sadock (1975) provide a useful set of tests
that can be used as diagnostics.

14.1.3 Canonical Form

The notion that single sentences can be assigned multiple meanings leads to the
related phenomenon of distinct inputs that should be assigned the same meaning
representation. Consider the following alternative ways of expressing (14.1):
(14.5) Does Maharani have vegetarian dishes?
(14.6) Do they have vegetarian food at Maharani?
(14.7) Are vegetarian dishes served at Maharani?
(14.8) Does Maharani serve vegetarian fare?

1 This does not preclude the use of intermediate semantic representations that maintain some level of
ambiguity on the way to a single unambiguous form. Examples of such representations are discussed in
Chapter 15.

canonical form

14 . 1

• COM PU TAT IONA L D E S ID ERATA FOR R E PR E S EN TAT ION S

299

Given that these alternatives use different words and have widely varying syn-
tactic analyses, it would not be unreasonable to expect them to have quite different
meaning representations. Such a situation would, however, have undesirable con-
sequences for how we determine the truth of our representations. If the system’s
knowledge base contains only a single representation of the fact in question, then
the representations underlying all but one of our alternatives will fail to produce
a match. We could, of course, store all possible alternative representations of the
same fact in the knowledge base, but doing so would lead to an enormous number
of problems related to keeping such a knowledge base consistent.
The way out of this dilemma is motivated by the fact that since the answers given
for each of these alternatives should be the same in all situations, we might say that
they all mean the same thing, at least for the purposes of giving restaurant recom-
mendations. In other words, at least in this domain, we can legitimately consider
assigning the same meaning representation to the propositions underlying each of
these requests. Taking such an approach would guarantee that our simple scheme
for answering yes-no questions will still work.
The notion that distinct inputs that mean the same thing should have the same
meaning representation is known as the doctrine of canonical form. This approach
greatly simpliﬁes various reasoning tasks since systems need only deal with a single
meaning representation for a potentially wide range of expressions.
Canonical form does complicate the task of semantic analysis. To see this, note
that the alternatives given above use completely different words and syntax to refer
to vegetarian fare and to what restaurants do with it. To assign the same representa-
tion to all of these requests, our system would have to conclude that vegetarian fare,
vegetarian dishes, and vegetarian food refer to the same thing in this context, that
the use here of having and serving are similarly equivalent, and that the different
syntactic parses underlying these requests are all compatible with the same meaning
representation.
Being able to assign the same representation to such diverse inputs is a tall or-
der. Fortunately, systematic meaning relationships among word senses and among
grammatical constructions can be exploited to make this task tractable. Consider the
issue of the meanings of the words food, dish, and fare in these examples. A little
introspection or a glance at a dictionary reveals that these words have a fair number
of distinct uses. However, it also reveals that at least one sense is shared among them
all. If a system has the ability to choose that shared sense, then an identical meaning
representation can be assigned to the phrases containing these words.
Just as there are systematic relationships among the meanings of different words,
there are similar relationships related to the role that syntactic analyses play in as-
signing meanings to sentences. Speciﬁcally, alternative syntactic analyses often have
meanings that are, if not identical, at least systematically related to one another.
Consider the following pair of examples:
(14.9) Maharani serves vegetarian dishes.
(14.10) Vegetarian dishes are served by Maharani.
Despite the different placement of the arguments to serve in these examples, we
can still assign Maharani and vegetarian dishes to the same roles in both of these
examples because of our knowledge of the relationship between active and passive
sentence constructions. In particular, we can use knowledge of where grammatical
subjects and direct objects appear in these constructions to assign Maharani to the
role of the server, and vegetarian dishes to the role of thing being served in both
of these examples, despite the fact that they appear in different surface locations.

300 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

inference

The precise role of the grammar in the construction of meaning representations is
covered in Chapter 15.

14.1.4

Inference and Variables

Continuing with the topic of the computational purposes that meaning representa-
tions should serve, we should consider more complex requests such as the following:
(14.11) Can vegetarians eat at Maharani?
Here, it would be a mistake to invoke canonical form to force our system to as-
sign the same representation to this request as for the previous examples. That this
request results in the same answer as the others arises, not because they mean the
same thing, but because there is a common-sense connection between what vegetar-
ians eat and what vegetarian restaurants serve. This is a fact about the world and
not a fact about any particular kind of linguistic regularity. This implies that no
approach based on canonical form and simple matching will give us an appropriate
answer to this request. What is needed is a systematic way to connect the meaning
representation of this request with the facts about the world as they are represented
in a knowledge base.
We use the term inference to refer generically to a system’s ability to draw valid
conclusions based on the meaning representation of inputs and its store of back-
ground knowledge. It must be possible for the system to draw conclusions about the
truth of propositions that are not explicitly represented in the knowledge base but
that are nevertheless logically derivable from the propositions that are present.
Now consider the following somewhat more complex request:
(14.12) I’d like to ﬁnd a restaurant where I can get vegetarian food.
Unlike our previous examples, this request does not make reference to any particular
restaurant. The user is expressing a desire for information about an unknown and
unnamed entity that is a restaurant that serves vegetarian food. Since this request
does not mention any particular restaurant, the kind of simple matching-based ap-
proach we have been advocating is not going to work. Rather, answering this request
requires a more complex kind of matching that involves the use of variables. We can
gloss a representation containing such variables as follows:

Serves(x,Veget arianFood )

(14.13)

Matching such a proposition succeeds only if the variable x can be replaced by some
known object in the knowledge base in such a way that the entire proposition will
then match. The concept that is substituted for the variable can then be used to fulﬁll
the user’s request. Of course, this simple example only hints at the issues involved
in the use of such variables. Sufﬁce it to say that linguistic inputs contain many
instances of all kinds of indeﬁnite references, and it is, therefore, critical for any
meaning representation language to be able to handle this kind of expression.

14.1.5 Expressiveness

Finally, to be useful, a meaning representation scheme must be expressive enough
to handle a wide range of subject matter. The ideal situation would be to have a sin-
gle meaning representation language that could adequately represent the meaning
of any sensible natural language utterance. Although this is probably too much to
expect from any single representational system, First-Order Logic, as described in

14 .2

• MOD EL -TH EOR ET IC S EMAN T IC S

301

Section 14.3, is expressive enough to handle quite a lot of what needs to be repre-
sented.

14.2 Model-Theoretic Semantics

The last two sections focused on various desiderata for meaning representations and
on some of the ways in which natural languages convey meaning. We haven’t said
much formally about what it is about meaning representation languages that allows
them to do all the things we want them to.
In particular, we might like to have
some kind of guarantee that these representations can do the work that we require of
them: bridge the gap from merely formal representations to representations that tell
us something about some state of affairs in the world.
To see how we might provide such a guarantee, let’s start with the basic notions
shared by most meaning representation schemes. What they all have in common
is the ability to represent objects, properties of objects, and relations among ob-
jects. This ability can be formalized by the notion of a model. A model is a formal
construct that stands for the particular state of affairs in the world. Expressions in
a meaning representation language can be mapped in a systematic way to the ele-
ments of the model. If the model accurately captures the facts we’re interested in
concerning some state of affairs, then a consistent mapping between the meaning
representation and the model provides the bridge between the meaning representa-
tion and world being considered. As we show, models provide a surprisingly simple
and powerful way to ground the expressions in meaning representation languages.
First, some terminology. The vocabulary of a meaning representation consists of
two parts: the non-logical vocabulary and the logical vocabulary. The non-logical
vocabulary consists of the open-ended set of names for the objects, properties, and
relations that make up the world we’re trying to represent. These appear in various
schemes as predicates, nodes, labels on links, or labels in slots in frames, The log-
ical vocabulary consists of the closed set of symbols, operators, quantiﬁers, links,
etc., that provide the formal means for composing expressions in a given meaning
representation language.
We’ll start by requiring that each element of the non-logical vocabulary have a
denotation in the model. By denotation, we simply mean that every element of the
non-logical vocabulary corresponds to a ﬁxed, well-deﬁned part of the model. Let’s
start with objects, the most basic notion in most representational schemes. The do-
main of a model is simply the set of objects that are part of the application, or state
of affairs, being represented. Each distinct concept, category, or individual in an ap-
plication denotes a unique element in the domain. A domain is therefore formally a
set. Note that it isn’t mandatory that every element of the domain have a correspond-
ing concept in our meaning representation; it’s perfectly acceptable to have domain
elements that aren’t mentioned or conceived of in the meaning representation. Nor
do we require that elements of the domain have a single denoting concept in the
meaning representation; a given element in the domain might have several distinct
representations denoting it, such as Mary, WifeOf(Abe), or MotherOf(Robert).
We can capture properties of objects in a model by denoting those domain ele-
ments that have the property in question; that is, properties denote sets. Similarly,
relations among objects denote sets of ordered lists, or tuples, of domain elements
that take part in the corresponding relations. This approach to properties and rela-
tions is thus an extensional one: the denotation of properties like red is the set of

model

non-logical
vocabulary

logical
vocabulary

denotation

domain

extensional

302 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

interpretation

things we think are red, the denotation of a relation like Married is simply set of
pairs of domain elements that are married. To summarize:
• Objects denote elements of the domain
• Properties denote sets of elements of the domain
• Relations denote sets of tuples of elements of the domain
There is one additional element that we need to make this scheme work. We
need a mapping that systematically gets us from our meaning representation to the
corresponding denotations. More formally, we need a function that maps from the
non-logical vocabulary of our meaning representation to the proper denotations in
the model. We’ll call such a mapping an interpretation.
To make these notions more concrete, let’s return to our restaurant advice appli-
cation. Assume that our application domain consists of sets of restaurants, patrons,
and various facts about the likes and dislikes of the patrons, and facts about the
restaurants such as their cuisine, typical cost, and noise level.
To begin populating our domain, D , let’s assume that we’re dealing with four pa-
trons designated by the non-logical symbols Matthew, Franco, Katie, and Caroline.
These four symbols will denote four unique domain elements. We’ll use the con-
stants a, b, c and, d to stand for these domain elements. Note that we’re deliberately
using meaningless, non-mnemonic names for our domain elements to emphasize the
fact that whatever it is that we know about these entities has to come from the formal
properties of the model and not from the names of the symbols. Continuing, let’s
assume that our application includes three restaurants, designated as Frasca, Med,
and Rio in our meaning representation, that denote the domain elements e, f , and g.
Finally, let’s assume that we’re dealing with the three cuisines Italian, Mexican, and
Eclectic, denoted by h, i, and j in our model.
Having populated the domain, let’s move on to the properties and relations we
believe to be true in this particular state of affairs. For our application, we need to
represent various properties of restaurants such as the fact that some are noisy or
expensive. Properties like N oisy denote the subset of restaurants from our domain
that are known to be noisy. Two-place relational notions, such as which restaurants
individual patrons Like, denote ordered pairs, or tuples, of the objects from the do-
main. And, since we decided to represent cuisines as objects in our model, we can
capture which restaurants Serve which cuisines as a set of tuples. One possible state
of affairs using this scheme is given in Fig. 14.2.
Given this simple scheme, we can ground our meaning representations by con-
sulting the appropriate denotations in the corresponding model. For example, we can
evaluate a representation claiming that Matthew likes the Rio, or that The Med serves
Italian by mapping the objects in the meaning representations to their corresponding
domain elements and mapping any links, predicates, or slots in the meaning repre-
sentation to the appropriate relations in the model. More concretely, we can verify
a representation asserting that Matthew likes Frasca by ﬁrst using our interpretation
function to map the symbol Mat t hew to its denotation a, Frasca to e, and the Likes
relation to the appropriate set of tuples. We then check that set of tuples for the
presence of the tuple (cid:104)a, e(cid:105). If, as it is in this case, the tuple is present in the model,
then we can conclude that Matthew likes Frasca is true; if it isn’t then we can’t.
This is all pretty straightforward—we’re using sets and operations on sets to
ground the expressions in our meaning representations. Of course, the more inter-
esting part comes when we consider more complex examples such as the following:
(14.14) Katie likes the Rio and Matthew likes the Med.

14 .2

• MOD EL -TH EOR ET IC S EMAN T IC S

303

Domain

Matthew, Franco, Katie and Caroline
Frasca, Med, Rio
Italian, Mexican, Eclectic

D = {a, b, c, d , e, f , g, h, i, j}
a, b, c, d
e, f , g
h, i, j

Properties

Noisy
Frasca, Med, and Rio are noisy

Relations

Likes
Matthew likes the Med
Katie likes the Med and Rio
Franco likes Frasca
Caroline likes the Med and Rio
Serves
Med serves eclectic
Rio serves Mexican
Frasca serves Italian

Figure 14.2 A model of the restaurant world.

N oisy = {e, f , g}

Likes = {(cid:104)a, f (cid:105), (cid:104)c, f (cid:105), (cid:104)c, g(cid:105), (cid:104)b, e(cid:105), (cid:104)d , f (cid:105), (cid:104)d , g(cid:105)}

Serves = {(cid:104) f , j(cid:105), (cid:104)g, i(cid:105), (cid:104)e, h(cid:105)}

(14.15) Katie and Caroline like the same restaurants.
(14.16) Franco likes noisy, expensive restaurants.
(14.17) Not everybody likes Frasca.

Our simple scheme for grounding the meaning of representations is not adequate
for examples such as these. Plausible meaning representations for these examples
will not map directly to individual entities, properties, or relations. Instead, they
involve complications such as conjunctions, equality, quantiﬁed variables, and nega-
tions. To assess whether these statements are consistent with our model, we’ll have
to tear them apart, assess the parts, and then determine the meaning of the whole
from the meaning of the parts according to the details of how the whole is assem-
bled.
Consider the ﬁrst example given above. A meaning representation for an exam-
ple like this will include two distinct propositions expressing the individual patron’s
preferences, conjoined with some kind of implicit or explicit conjunction operator.
Our model doesn’t have a relation that encodes pairwise preferences for all of the
patrons and restaurants in our model, nor does it need to. We know from our model
that Matthew likes the Med and separately that Katie likes the Rio (that is, the tuples
(cid:104)a, f (cid:105) and (cid:104)c, g(cid:105) are members of the set denoted by the Likes relation). All we really
need to know is how to deal with the semantics of the conjunction operator. If we
assume the simplest possible semantics for the English word and, the whole state-
ment is true if it is the case that each of the components is true in our model. In this
case, both components are true since the appropriate tuples are present and therefore
the sentence as a whole is true.
What we’ve done with this example is provide a truth-conditional semantics
for the assumed conjunction operator in some meaning representation. That is,
we’ve provided a method for determining the truth of a complex expression from
the meanings of the parts (by consulting a model) and the meaning of an operator by
consulting a truth table. Meaning representation languages are truth-conditional to
the extent that they give a formal speciﬁcation as to how we can determine the mean-

truth-
conditional
semantics

304 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

|

Formula → AtomicFormula
| Formula Connective Formula
| Quantiﬁer Variable, . . . Formula
| ¬ Formula
(Formula)
AtomicFormula → Predicate(Term, . . .)
Term → Function(Term, . . .)
| Constant
| Variable
Connective → ∧ | ∨ | =⇒
Quantiﬁer → ∀ | ∃
Constant → A | Veget arianFood | Maharani · · ·
Variable → x | y | · · ·
Predicate → Serves | N ear | · · ·
Function → LocationOf | CuisineOf | · · ·

Figure 14.3 A context-free grammar speciﬁcation of the syntax of First-Order Logic rep-
resentations. Adapted from Russell and Norvig (2002)

.

ing of complex sentences from the meaning of their parts. In particular, we need to
know the semantics of the entire logical vocabulary of the meaning representation
scheme being used.
Note that although the details of how this happens depends on details of the
particular meaning representation being used, it should be clear that assessing the
truth conditions of examples like these involves nothing beyond the simple set op-
erations we’ve been discussing. We return to these issues in the next section, where
we discuss them in the context of the semantics of First-Order Logic.

14.3 First-Order Logic

First-Order Logic (FO L) is a ﬂexible, well-understood, and computationally tractable
meaning representation language that satisﬁes many of the desiderata given in Sec-
tion 14.1. It provides a sound computational basis for the veriﬁability, inference,
and expressiveness requirements, as well as a sound model-theoretic semantics.
An additional attractive feature of FO L is that it makes very few speciﬁc com-
mitments as to how things ought to be represented. And, the speciﬁc commitments
it does make are ones that are fairly easy to live with and that are shared by many of
the schemes mentioned earlier; the represented world consists of objects, properties
of objects, and relations among objects.
The remainder of this section introduces the basic syntax and semantics of FO L
and then describes the application of FO L to the representation of events.

14.3.1 Basic Elements of First-Order Logic

Let’s explore FOL by ﬁrst examining its various atomic elements and then showing
how they can be composed to create larger meaning representations. Figure 14.3,
which provides a complete context-free grammar for the particular syntax of FO L
that we will use, is our roadmap for this section.
Let’s begin by examining the notion of a term, the FO L device for representing

term

14 .3

• F IR ST-ORDER LOG IC

305

objects. As can be seen from Fig. 14.3, FOL provides three ways to represent these
basic building blocks: constants, functions, and variables. Each of these devices can
be thought of as designating an object in the world under consideration.
Constants in FOL refer to speciﬁc objects in the world being described. Such
constants are conventionally depicted as either single capitalized letters such as A
and B or single capitalized words that are often reminiscent of proper nouns such as
Maharani and H arry. Like programming language constants, FO L constants refer
to exactly one object. Objects can, however, have multiple constants that refer to
them.
Functions in FOL correspond to concepts that are often expressed in English as
genitives such as Frasca’s location. A FO L translation of such an expression might
look like the following.

LocationOf (Frasca)

(14.18)

FOL functions are syntactically the same as single argument predicates. It is im-
portant to remember, however, that while they have the appearance of predicates,
they are in fact t erms in that they refer to unique objects. Functions provide a con-
venient way to refer to speciﬁc objects without having to associate a named constant
with them. This is particularly convenient in cases in which many named objects,
like restaurants, have a unique concept such as a location associated with them.
Variables are variable our ﬁnal FO L mechanism for referring to objects. Vari-
ables, depicted as single lower-case letters, let us make assertions and draw infer-
ences about objects without having to make reference to any particular named ob-
ject. This ability to make statements about anonymous objects comes in two ﬂavors:
making statements about a particular unknown object and making statements about
all the objects in some arbitrary world of objects. We return to the topic of variables
after we have presented quantiﬁers, the elements of FOL that make variables useful.
Now that we have the means to refer to objects, we can move on to the FO L
mechanisms that are used to state relations that hold among objects. Predicates are
symbols that refer to, or name, the relations that hold among some ﬁxed number
of objects in a given domain. Returning to the example introduced informally in
Section 14.1, a reasonable FOL representation for Maharani serves vegetarian food
might look like the following formula:

Serves(Maharani,Veget arianFood )

(14.19)

This FO L sentence asserts that Serves, a two-place predicate, holds between the
objects denoted by the constants Maharani and Veget arianFood .
A somewhat different use of predicates is illustrated by the following fairly typ-
ical representation for a sentence like Maharani is a restaurant:

Rest aurant (Maharani)

(14.20)

This is an example of a one-place predicate that is used, not to relate multiple objects,
but rather to assert a property of a single object. In this case, it encodes the category
membership of Maharani.
With the ability to refer to objects, to assert facts about objects, and to relate
objects to one another, we can create rudimentary composite representations. These
representations correspond to the atomic formula level in Fig. 14.3. This ability
to compose complex representations is, however, not limited to the use of single
predicates. Larger composite representations can also be put together through the
use of logical connectives. As can be seen from Fig. 14.3, logical connectives let

Constants

Functions

variable

logical
connectives

306 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

quantiﬁers

us create larger representations by conjoining logical formulas using one of three
operators. Consider, for example, the following B ER P sentence and one possible
representation for it:
(14.21) I only have ﬁve dollars and I don’t have a lot of time.
H ave(S peaker, F iveDol l ars) ∧ ¬H ave(S peaker, LotOfTime)
The semantic representation for this example is built up in a straightforward way
from semantics of the individual clauses through the use of the ∧ and ¬ operators.
Note that the recursive nature of the grammar in Fig. 14.3 allows an inﬁnite number
of logical formulas to be created through the use of these connectives. Thus, as with
syntax, we can use a ﬁnite device to create an inﬁnite number of representations.

(14.22)

14.3.2 Variables and Quantiﬁers

We now have all the machinery necessary to return to our earlier discussion of vari-
ables. As noted above, variables are used in two ways in FOL: to refer to particular
anonymous objects and to refer generically to all objects in a collection. These two
uses are made possible through the use of operators known as quantiﬁers. The two
operators that are basic to FO L are the existential quantiﬁer, which is denoted ∃ and
is pronounced as “there exists”, and the universal quantiﬁer, which is denoted ∀ and
is pronounced as “for all”.
The need for an existentially quantiﬁed variable is often signaled by the presence
of an indeﬁnite noun phrase in English. Consider the following example:
(14.23) a restaurant that serves Mexican food near ICSI.
Here, reference is being made to an anonymous object of a speciﬁed category with
particular properties. The following would be a reasonable representation of the
meaning of such a phrase:

∃xRest aurant (x) ∧ Serves(x, MexicanFood )
∧ N ear((LocationOf (x), LocationOf (ICSI ))
The existential quantiﬁer at the head of this sentence instructs us on how to
interpret the variable x in the context of this sentence. Informally, it says that for
this sentence to be true there must be at least one object such that if we were to
substitute it for the variable x, the resulting sentence would be true. For example,
if AyCaramba is a Mexican restaurant near ICSI, then substituting AyCaramba for x
results in the following logical formula:

(14.24)

(14.25)

Rest aurant (AyCaramba) ∧ Serves(AyCaramba, MexicanFood )
∧N ear((LocationOf (AyCaramba), LocationOf (ICSI ))
Based on the semantics of the ∧ operator, this sentence will be true if all of its
three component atomic formulas are true. These in turn will be true if they are
either present in the system’s knowledge base or can be inferred from other facts in
the knowledge base.
The use of the universal quantiﬁer also has an interpretation based on substi-
tution of known objects for variables. The substitution semantics for the universal
quantiﬁer takes the expression for all quite literally; the ∀ operator states that for the
logical formula in question to be true, the substitution of any object in the knowledge
base for the universally quantiﬁed variable should result in a true formula. This is in

14 .3

• F IR ST-ORDER LOG IC

307

marked contrast to the ∃ operator, which only insists on a single valid substitution
for the sentence to be true.
Consider the following example:

(14.26) All vegetarian restaurants serve vegetarian food.

A reasonable representation for this sentence would be something like the following:

∀xVeget arianRest aurant (x) =⇒ Serves(x,Veget arianFood )
For this sentence to be true, it must be the case that every substitution of a known
object for x must result in a sentence that is true. We can divide the set of all possible
substitutions into the set of objects consisting of vegetarian restaurants and the set
consisting of everything else. Let us ﬁrst consider the case in which the substituted
object actually is a vegetarian restaurant; one such substitution would result in the
following sentence:

(14.27)

Veget arianRest aurant (Maharani) =⇒ Serves(Maharani,Veget arianFood )
(14.28)
If we assume that we know that the consequent clause

Serves(Maharani,Veget arianFood )

(14.29)

is true, then this sentence as a whole must be true. Both the antecedent and the
consequent have the value T rue and, therefore, according to the ﬁrst two rows of
Fig. 14.4 on page 309 the sentence itself can have the value T rue. This result will be
the same for all possible substitutions of Terms representing vegetarian restaurants
for x.
Remember, however, that for this sentence to be true, it must be true for all
possible substitutions. What happens when we consider a substitution from the set
of objects that are not vegetarian restaurants? Consider the substitution of a non-
vegetarian restaurant such as Ay Caramba’s for the variable x:

Veget arianRest aurant (AyCaramba) =⇒ Serves(AyCaramba,Veget arianFood )
Since the antecedent of the implication is Fal se, we can determine from Fig. 14.4
that the sentence is always T rue, again satisfying the ∀ constraint.
Note that it may still be the case that Ay Caramba serves vegetarian food with-
out actually being a vegetarian restaurant. Note also, that despite our choice of
examples, there are no implied categorical restrictions on the objects that can be
substituted for x by this kind of reasoning. In other words, there is no restriction of
x to restaurants or concepts related to them. Consider the following substitution:

Veget arianRest aurant (Carburet or) =⇒ Serves(Carburet or,Veget arianFood )
Here the antecedent is still false, and hence, the rule remains true under this kind of
irrelevant substitution.
To review, variables in logical formulas must be either existentially (∃) or uni-
versally (∀) quantiﬁed. To satisfy an existentially quantiﬁed variable, at least one
substitution must result in a true sentence. Sentences with universally quantiﬁed
variables must be true under all possible substitutions.

308 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

lambda
notation

λ -reduction

14.3.3 Lambda Notation

The ﬁnal element we need to complete our discussion of FO L is called the lambda
notation (Church, 1940). This notation provides a way to abstract from fully speci-
ﬁed FO L formula in a way that will be particularly useful for semantic analysis. The
lambda notation extends the syntax of FOL to include expressions of the following
form:

λ x.P(x)
(14.30)
Such expressions consist of the Greek symbol λ , followed by one or more variables,
followed by a FOL formula that makes use of those variables.
The usefulness of these λ -expressions is based on the ability to apply them to
logical terms to yield new FO L expressions where the formal parameter variables are
bound to the speciﬁed terms. This process is known as λ -reduction and consists of
a simple textual replacement of the λ variables with the speciﬁed FO L terms, accom-
panied by the subsequent removal of the λ . The following expressions illustrate the
application of a λ -expression to the constant A, followed by the result of performing
a λ -reduction on this expression:

λ x.P(x)(A)
P(A)

(14.31)

An important and useful variation of this technique is the use of one λ -expression
as the body of another as in the following expression:

λ x.λ y.N ear(x, y)

(14.32)

This fairly abstract expression can be glossed as the state of something being
near something else. The following expressions illustrate a single λ -application and
subsequent reduction with this kind of embedded λ -expression:

λ x.λ y.N ear(x, y)(Bacaro)
λ y.N ear(Bacaro, y)

(14.33)

The important point here is that the resulting expression is still a λ -expression;
the ﬁrst reduction bound the variable x and removed the outer λ , thus revealing the
inner expression. As might be expected, this resulting λ -expression can, in turn,
be applied to another term to arrive at a fully speciﬁed logical formula, as in the
following:

λ y.N ear(Bacaro, y)(Cent ro)
N ear(Bacaro,Cent ro)

(14.34)

currying

This general technique, called currying2 (Sch ¨onkﬁnkel, 1924) is a way of con-
verting a predicate with multiple arguments into a sequence of single-argument pred-
icates.
As we show in Chapter 15, the λ -notation provides a way to incrementally gather
arguments to a predicate when they do not all appear together as daughters of the
predicate in a parse tree.

2 Currying is the standard term, although Heim and Kratzer (1998) present an interesting argument for
the term Sch ¨onkﬁnkelization over currying, since Curry later built on Sch ¨onﬁnkel’s work.

14 .3

• F IR ST-ORDER LOG IC

309

14.3.4 The Semantics of First-Order Logic

The various objects, properties, and relations represented in a FO L knowledge base
acquire their meanings by virtue of their correspondence to objects, properties, and
relations out in the external world being modeled. We can accomplish this by em-
ploying the model-theoretic approach introduced in Section 14.2. Recall that this
approach employs simple set-theoretic notions to provide a truth-conditional map-
ping from the expressions in a meaning representation to the state of affairs being
modeled. We can apply this approach to FO L by going through all the elements in
Fig. 14.3 on page 304 and specifying how each should be accounted for.
We can start by asserting that the objects in our world, FOL terms, denote ele-
ments in a domain, and asserting that atomic formulas are captured either as sets of
domain elements for properties, or as sets of tuples of elements for relations. As an
example, consider the following:
(14.35) Centro is near Bacaro.
Capturing the meaning of this example in FOL involves identifying the Terms
and Pred icat es that correspond to the various grammatical elements in the sentence
and creating logical formulas that capture the relations implied by the words and
syntax of the sentence. For this example, such an effort might yield something like
the following:

N ear(Cent ro, Bacaro)
(14.36)
The meaning of this logical formula is based on whether the domain elements de-
noted by the terms Cent ro and Bacaro are contained among the tuples denoted by
the relation denoted by the predicate N ear in the current model.
The interpretations of formulas involving logical connectives is based on the
meaning of the components in the formulas combined with the meanings of the
connectives they contain. Figure 14.4 gives interpretations for each of the logical
operators shown in Fig. 14.3.

P

Q

Fal se
Fal se
T rue
T rue

Fal se
T rue
Fal se
T rue

¬ P

T rue
T rue
Fal se
Fal se

P ∧ Q

Fal se
Fal se
Fal se
T rue

P ∨ Q

Fal se
T rue
T rue
T rue

P =⇒Q

T rue
T rue
Fal se
T rue

Figure 14.4 Truth table giving the semantics of the various logical connectives.

The semantics of the ∧ (and) and ¬ (not) operators are fairly straightforward,
and are correlated with at least some of the senses of the corresponding English
terms. However, it is worth pointing out that the ∨ (or) operator is not disjunctive
in the same way that the corresponding English word is, and that the =⇒ (im-
plies) operator is only loosely based on any common-sense notions of implication
or causation.
The ﬁnal bit we need to address involves variables and quantiﬁers. Recall that
there are no variables in our set-based models, only elements of the domain and
relations that hold among them. We can provide a model-based account for formulas
with variables by employing the notion of a substitution introduced earlier on page
306. Formulas involving ∃ are true if a substitution of terms for variables results
in a formula that is true in the model. Formulas involving ∀ must be true under all
possible substitutions.

310 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

14.3.5

Inference

One of the most important desiderata given in Section 14.1 for a meaning rep-
resentation language is that it should support inference, or deduction. That is, the
ability to add valid new propositions to a knowledge base or to determine the truth of
propositions not explicitly contained within a knowledge base. This section brieﬂy
discusses modus ponens, the most widely implemented inference method provided
by FOL. Applications of modus ponens to inference in discourse is discussed in
Chapter 21.
Modus ponens is a familiar form of inference that corresponds to what is in-
formally known as if-then reasoning. We can abstractly deﬁne modus ponens as
follows, where α and β should be taken as FO L formulas:

Modus ponens

α
α =⇒ β
β

(14.37)

A schema like this indicates that the formula below the line can be inferred from
the formulas above the line by some form of inference. Modus ponens simply states
that if the left-hand side of an implication rule is true, then the right-hand side of the
rule can be inferred. In the following discussions, we will refer to the left-hand side
of an implication as the antecedent and the right-hand side as the consequent.
For a typical use of modus ponens, consider the following example, which uses
a rule from the last section:

Veget arianRest aurant (Leaf )
∀xVeget arianRest aurant (x) =⇒ Serves(x,Veget arianFood )
Serves(Leaf ,Veget arianFood )

(14.38)

Here, the formula Veget arianRest aurant (Leaf ) matches the antecedent of the rule,
thus allowing us to use modus ponens to conclude Serves(Leaf ,Veget arianFood ).
Modus ponens can be put to practical use in one of two ways: forward chaining
and backward chaining. In forward chaining systems, modus ponens is used in
precisely the manner just described. As individual facts are added to the knowledge
base, modus ponens is used to ﬁre all applicable implication rules. In this kind of
arrangement, as soon as a new fact is added to the knowledge base, all applicable
implication rules are found and applied, each resulting in the addition of new facts to
the knowledge base. These new propositions in turn can be used to ﬁre implication
rules applicable to them. The process continues until no further facts can be deduced.
The forward chaining approach has the advantage that facts will be present in
the knowledge base when needed, because, in a sense all inference is performed in
advance. This can substantially reduce the time needed to answer subsequent queries
since they should all amount to simple lookups. The disadvantage of this approach
is that facts that will never be needed may be inferred and stored.
In backward chaining, modus ponens is run in reverse to prove speciﬁc propo-
sitions called queries. The ﬁrst step is to see if the query formula is true by determin-
ing if it is present in the knowledge base. If it is not, then the next step is to search
for applicable implication rules present in the knowledge base. An applicable rule is
one whereby the consequent of the rule matches the query formula. If there are any
such rules, then the query can be proved if the antecedent of any one them can be
shown to be true. Not surprisingly, this can be performed recursively by backward

forward
chaining

backward
chaining

14 .4

• EV EN T AND S TAT E R E PR E S EN TAT ION S

311

chaining on the antecedent as a new query. The Prolog programming language is a
backward chaining system that implements this strategy.
To see how this works, let’s assume that we have been asked to verify the truth of
the proposition Serves(Leaf ,Veget arianFood ), assuming the facts given above the
line in (14.38). Since this proposition is not present in the knowledge base, a search
for an applicable rule is initiated resulting in the rule given above. After substituting
the constant Leaf for the variable x, our next task is to prove the antecedent of the
rule, Veget arianRest aurant (Leaf ), which, of course, is one of the facts we are given.
Note that it is critical to distinguish between reasoning by backward chaining
from queries to known facts and reasoning backwards from known consequents to
unknown antecedents. To be speciﬁc, by reasoning backwards we mean that if the
consequent of a rule is known to be true, we assume that the antecedent will be as
well. For example, let’s assume that we know that Serves(Leaf ,Veget arianFood ) is
true. Since this fact matches the consequent of our rule, we might reason backwards
to the conclusion that Veget arianRest aurant (Leaf ).
While backward chaining is a sound method of reasoning, reasoning backwards
is an invalid, though frequently useful, form of plausible reasoning. Plausible rea-
soning from consequents to antecedents is known as abduction, and as we show in
Chapter 21, is often useful in accounting for many of the inferences people make
while analyzing extended discourses.
While forward and backward reasoning are sound, neither is complete. This
means that there are valid inferences that cannot be found by systems using these
methods alone. Fortunately, there is an alternative inference technique called reso-
lution that is sound and complete. Unfortunately, inference systems based on res-
olution are far more computationally expensive than forward or backward chaining
systems. In practice, therefore, most systems use some form of chaining and place
a burden on knowledge-base developers to encode the knowledge in a fashion that
permits the necessary inferences to be drawn.

abduction

complete

resolution

14.4 Event and State Representations

Much of the semantics that we wish to capture consists of representations of states
and events. States are conditions, or properties, that remain unchanged over an
extended period of time, and events denote changes in some state of affairs. The
representation of both states and events may involve a host of participants, props,
times and locations.
The representations for events and states that we have used thus far have con-
sisted of single predicates with as many arguments as are needed to incorporate all
the roles associated with a given example. For example, the representation for Leaf
serves vegetarian fare consists of a single predicate with arguments for the entity
doing the serving and the thing served.

Serves(Leaf ,Veget arianFare)

(14.39)

This approach assumes that the predicate used to represent an event verb has the
same number of arguments as are present in the verb’s syntactic subcategorization
frame. Unfortunately, this is clearly not always the case. Consider the following
examples of the verb eat:
(14.40) I ate.

312 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

arity

event variable

neo-
Davidsonian

(14.41) I ate a turkey sandwich.
(14.42) I ate a turkey sandwich at my desk.
(14.43) I ate at my desk.
(14.44) I ate lunch.
(14.45) I ate a turkey sandwich for lunch.
(14.46) I ate a turkey sandwich for lunch at my desk.
Clearly, choosing the correct number of arguments for the predicate represent-
ing the meaning of eat is a tricky problem. These examples introduce ﬁve distinct
arguments, or roles, in an array of different syntactic forms, locations, and combina-
tions. Unfortunately, predicates in FO L have ﬁxed arity – they take a ﬁxed number
of arguments.
To address this problem, we introduce the notion of an event variable to allow
us to make assertions about particular events. To do this, we can refactor our event
predicates to have an existentially quantiﬁed variable as their ﬁrst, and only, argu-
ment. Using this event variable, we can introduce additional predicates to represent
the other information we have about the event. These predicates take an event vari-
able as their ﬁrst argument and related FO L terms as their second argument. The
following formula illustrates this scheme with the meaning representation of 14.41
from our earlier discussion.
∃e E at ing(e) ∧ E at er(e, S peaker) ∧ E at en(e, T urkeySandwich)
Here, the quantiﬁed variable e stands for the eating event and is used to bind the
event predicate with the core information provided via the named roles E at er and
E at en. To handle the more complex examples, we simply add additional relations
to capture the provided information, as in the following for 14.46.
∃e E at ing(e) ∧ E at er(e, S peaker) ∧ E at en(e, T urkeySandwich)
∧ Meal (e, Lunch) ∧ Locat ion(e, Desk)
Event representations of this sort are referred to as neo-Davidsonian event repre-
sentations (Davidson, 1967; Parsons, 1990) after the philosopher Donald Davidson
who introduced the notion of an event variable (Davidson, 1967). To summarize, in
the neo-Davidsonian approach to event representations:
• Events are captured with predicates that take a single event variable as an
argument.
• There is no need to specify a ﬁxed number of arguments for a given FO L
predicate; rather, as many roles and ﬁllers can be glued on as are provided in
the input.
• No more roles are postulated than are mentioned in the input.
• The logical connections among closely related inputs that share the same pred-
icate are satisﬁed without the need for additional inference.
This approach still leaves us with the problem of determining the set of predi-
cates needed to represent roles associated with speciﬁc events like E at er and E at en,
as well as more general concepts like Locat ion and T ime. We’ll return to this prob-
lem in more detail in Chapter 18.

(14.47)

14.4.1 Representing Time

In our discussion of events, we did not seriously address the issue of capturing the
time when the represented events are supposed to have occurred. The representation

temporal logic

tense logic

14 .4

• EV EN T AND S TAT E R E PR E S EN TAT ION S

313

of such information in a useful form is the domain of temporal logic. This dis-
cussion introduces the most basic concerns of temporal logic and brieﬂy discusses
the means by which human languages convey temporal information, which, among
other things, includes tense logic, the ways that verb tenses convey temporal infor-
mation. A more detailed discussion of robust approaches to the representation and
analysis of temporal expressions is presented in Chapter 17.
The most straightforward theory of time holds that it ﬂows inexorably forward
and that events are associated with either points or intervals in time, as on a timeline.
Given these notions, we can order distinct events by situating them on the timeline.
More speciﬁcally, we can say that one event precedes another if the ﬂow of time
leads from the ﬁrst event to the second. Accompanying these notions in most theo-
ries is the idea of the current moment in time. Combining this notion with the idea
of a temporal ordering relationship yields the familiar notions of past, present, and
future.
Not surprisingly, a large number of schemes can represent this kind of temporal
information. The one presented here is a fairly simple one that stays within the FO L
framework of reiﬁed events that we have been pursuing. Consider the following
examples:
(14.48) I arrived in New York.
(14.49) I am arriving in New York.
(14.50) I will arrive in New York.
These sentences all refer to the same kind of event and differ solely in the tense of
the verb. In our current scheme for representing events, all three would share the
following kind of representation, which lacks any temporal information:
∃eArriving(e) ∧ Arriver(e, S peaker) ∧ Dest inat ion(e, N ewYork)
The temporal information provided by the tense of the verbs can be exploited
by predicating additional information about the event variable e. Speciﬁcally, we
can add temporal variables representing the interval corresponding to the event, the
end point of the event, and temporal predicates relating this end point to the current
time as indicated by the tense of the verb. Such an approach yields the following
representations for our arriving examples:
∃e, i, n Arriving(e) ∧ Arriver(e, S peaker) ∧ Dest inat ion(e, N ewYork)
∧ IntervalOf (e, i) ∧ E ndPoint (i, n) ∧ Preced es(n, N ow)
∃e, i, n Arriving(e) ∧ Arriver(e, S peaker) ∧ Dest inat ion(e, N ewYork)
∧ IntervalOf (e, i) ∧ MemberOf (i, Now)
∃e, i, n Arriving(e) ∧ Arriver(e, S peaker) ∧ Dest inat ion(e, N ewYork)
∧ IntervalOf (e, i) ∧ E ndPoint (i, n) ∧ Preced es(N ow, n)
This representation introduces a variable to stand for the interval of time as-
sociated with the event and a variable that stands for the end of that interval. The
two-place predicate Preced es represents the notion that the ﬁrst time-point argument
precedes the second in time; the constant N ow refers to the current time. For past
events, the end point of the interval must precede the current time. Similarly, for fu-
ture events the current time must precede the end of the event. For events happening
in the present, the current time is contained within the event interval.
Unfortunately, the relation between simple verb tenses and points in time is by
no means straightforward. Consider the following examples:

(14.51)

314 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

reference point

(14.52) Ok, we ﬂy from San Francisco to Boston at 10.
(14.53) Flight 1390 will be at the gate an hour now.
In the ﬁrst example, the present tense of the verb ﬂy is used to refer to a future event,
while in the second the future tense is used to refer to a past event.
More complications occur when we consider some of the other verb tenses. Con-
sider the following examples:
(14.54) Flight 1902 arrived late.
(14.55) Flight 1902 had arrived late.
Although both refer to events in the past, representing them in the same way seems
wrong. The second example seems to have another unnamed event lurking in the
background (e.g., Flight 1902 had already arrived late when something else hap-
pened). To account for this phenomena, Reichenbach (1947) introduced the notion
of a reference point. In our simple temporal scheme, the current moment in time
is equated with the time of the utterance and is used as a reference point for when
the event occurred (before, at, or after). In Reichenbach’s approach, the notion of
the reference point is separated from the utterance time and the event time. The
following examples illustrate the basics of this approach:
(14.56) When Mary’s ﬂight departed, I ate lunch.
(14.57) When Mary’s ﬂight departed, I had eaten lunch.
In both of these examples, the eating event has happened in the past, that is, prior
to the utterance. However, the verb tense in the ﬁrst example indicates that the eating
event began when the ﬂight departed, while the second example indicates that the
eating was accomplished prior to the ﬂight’s departure. Therefore, in Reichenbach’s
terms the departure event speciﬁes the reference point. These facts can be accom-
modated by additional constraints relating the eating and departure events. In the
ﬁrst example, the reference point precedes the eating event, and in the second exam-
ple, the eating precedes the reference point. Figure 14.5 illustrates Reichenbach’s
approach with the primary English tenses. Exercise 14.6 asks you to represent these
examples in FO L.

Figure 14.5 Reichenbach’s approach applied to various English tenses. In these diagrams,
time ﬂows from left to right, an E denotes the time of the event, an R denotes the reference
time, and an U denotes the time of the utterance.

14 .4

• EV EN T AND S TAT E R E PR E S EN TAT ION S

315

This discussion has focused narrowly on the broad notions of past, present, and
future and how they are signaled by various English verb tenses. Of course, lan-
guages also have many other more direct and more speciﬁc ways to convey temporal
information, including the use of a wide variety of temporal expressions, as in the
following ATIS examples:
(14.58) I’d like to go at 6:45, in the morning.
(14.59) Somewhere around noon, please.
As we show in Chapter 17, grammars for such temporal expressions are of consid-
erable practical importance to information extraction and question-answering appli-
cations.
Finally, we should note that a systematic conceptual organization is reﬂected in
examples like these. In particular, temporal expressions in English are frequently ex-
pressed in spatial terms, as is illustrated by the various uses of at, in, somewhere, and
near in these examples (Lakoff and Johnson, 1980; Jackendoff, 1983). Metaphori-
cal organizations such as these, in which one domain is systematically expressed in
terms of another, are very common in languages of the world.

14.4.2 Aspect

In the last section, we discussed ways to represent the time of an event with respect
to the time of an utterance describing it. In this section, we address the notion of
aspect, which concerns a cluster of related topics, including whether an event has
ended or is ongoing, whether it is conceptualized as happening at a point in time or
over some interval, and whether any particular state in the world comes about be-
cause of it. Based on these and related notions, event expressions have traditionally
been divided into four general classes illustrated in the following examples:
Stative: I know my departure gate.
Activity: John is ﬂying.
Accomplishment: Sally booked her ﬂight.
Achievement: She found her gate.
Although the earliest versions of this classiﬁcation were discussed by Aristotle,
the one presented here is due to Vendler (1967).
Stative expressions represent the notion of an event participant having a partic-
ular property, or being in a state, at a given point in time. As such, these expressions
can be thought of as capturing an aspect of a world at a single point in time. Consider
the following ATIS examples.
(14.60) I like Flight 840 arriving at 10:06.
(14.61) I need the cheapest fare.
(14.62) I want to go ﬁrst class.
In examples like these, the event participant denoted by the subject can be seen as
experiencing something at a speciﬁc point in time. Whether or not the experiencer
was in the same state earlier or will be in the future is left unspeciﬁed.
Activity expressions describe events undertaken by a participant and have no
particular end point. Unlike statives, activities are seen as occurring over some span
of time and are therefore not associated with single points in time. Consider the
following examples:
(14.63) She drove a Mazda.

aspect

Stative
expressions

Activity
expressions

316 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

(14.64) I live in Brooklyn.
These examples both specify that the subject is engaged in, or has engaged in, the
activity speciﬁed by the verb for some period of time.
The ﬁnal aspectual class, achievement expressions, is similar to accomplish-
ments in that these expressions result in a state. Consider the following:
(14.65) She found her gate.
(14.66) I reached New York.
Unlike accomplishments, achievement events are thought of as happening in an in-
stant and are not equated with any particular activity leading up to the state. To be
more speciﬁc, the events in these examples may have been preceded by extended
searching or traveling events, but the events corresponding directly to found and
reach are conceived of as points, not intervals.
Note that since both accomplishments and achievements are events that result
in a state, they are sometimes characterized as subtypes of a single aspectual class.
Members of this combined class are known as telic eventualities.

achievement
expressions

telic
eventualities

14.5 Description Logics

As noted at the beginning of this chapter, a fair number of representational schemes
have been invented to capture the meaning of linguistic utterances. It is now widely
accepted that meanings represented in these various approaches can, in principle, be
translated into equivalent statements in FO L with relative ease. The difﬁculty is that
in many of these approaches the semantics of a statement are deﬁned procedurally.
That is, the meaning arises from whatever the system that interprets it does with it.
Description logics are an effort to better specify the semantics of these earlier
structured network representations and to provide a conceptual framework that is
especially well suited to certain kinds of domain modeling. Formally, the term De-
scription Logics refers to a family of logical approaches that correspond to varying
subsets of FO L. The restrictions placed on the expressiveness of Description Logics
serve to guarantee the tractability of various critical kinds of inference. Our focus
here, however, will be on the modeling aspects of DLs rather than on computational
complexity issues.
When using Description Logics to model an application domain, the emphasis
is on the representation of knowledge about categories, individuals that belong to
those categories, and the relationships that can hold among these individuals. The
set of categories, or concepts, that make up a particular application domain is called
its terminology. The portion of a knowledge base that contains the terminology is
traditionally called the TBox; this is in contrast to the ABox that contains facts about
individuals. The terminology is typically arranged into a hierarchical organization
called an ontology that captures the subset/superset relations among the categories.
Returning to our earlier culinary domain, we represented domain concepts like
using unary predicates such as Rest aurant (x); the DL equivalent simply omits the
variable, so the restaurant category is simply written as Restaurant.3 To capture
the fact that a particular domain element, such as Frasca, is a restaurant, we assert
Restaurant(Frasca) in much the same way we would in FOL. The semantics of

3 DL statements are conventionally typeset with a sans serif font. We’ll follow that convention here,
reverting to our standard mathematical notation when giving FO L equivalents of DL statements.

terminology
TBox
ABox
ontology

subsumption

14 .5

• D E SCR I P T ION LOG IC S

317

these categories are speciﬁed in precisely the same way that was introduced earlier in
Section 14.2: a category like Restaurant simply denotes the set of domain elements
that are restaurants.
Once we’ve speciﬁed the categories of interest in a particular domain, the next
step is to arrange them into a hierarchical structure. There are two ways to cap-
ture the hierarchical relationships present in a terminology: we can directly assert
relations between categories that are related hierarchically, or we can provide com-
plete deﬁnitions for our concepts and then rely on inference to provide hierarchical
relationships. The choice between these methods hinges on the use to which the re-
sulting categories will be put and the feasibility of formulating precise deﬁnitions for
many naturally occurring categories. We’ll discuss the ﬁrst option here and return to
the notion of deﬁnitions later in this section.
To directly specify a hierarchical structure, we can assert subsumption relations
between the appropriate concepts in a terminology. The subsumption relation is
conventionally written as C (cid:118) D and is read as C is subsumed by D; that is, all
members of the category C are also members of the category D. Not surprisingly, the
formal semantics of this relation are provided by a simple set relation; any domain
element that is in the set denoted by C is also in the set denoted by D.
Adding the following statements to the TBox asserts that all restaurants are com-
mercial establishments and, moreover, that there are various subtypes of restaurants.

Restaurant (cid:118) CommercialEstablishment
ItalianRestaurant (cid:118) Restaurant
ChineseRestaurant (cid:118) Restaurant
MexicanRestaurant (cid:118) Restaurant

(14.67)
(14.68)
(14.69)
(14.70)

Ontologies such as this are conventionally illustrated with diagrams such as the one
shown in Fig. 14.6, where subsumption relations are denoted by links between the
nodes representing the categories.

Figure 14.6 A graphical network representation of a set of subsumption relations in the
restaurant domain.

Note, that it was precisely the vague nature of semantic network diagrams like
this that motivated the development of Description Logics. For example, from this

318 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

diagram we can’t tell whether the given set of categories is exhaustive or disjoint.
That is, we can’t tell if these are all the kinds of restaurants that we’ll be dealing with
in our domain or whether there might be others. We also can’t tell if an individual
restaurant must fall into only one of these categories, or if it is possible, for example,
for a restaurant to be both Italian and Chinese. The DL statements given above are
more transparent in their meaning; they simply assert a set of subsumption relations
between categories and make no claims about coverage or mutual exclusion.
If an application requires coverage and disjointness information, then such in-
formation must be made explicitly. The simplest ways to capture this kind of in-
formation is through the use of negation and disjunction operators. For example,
the following assertion would tell us that Chinese restaurants can’t also be Italian
restaurants.

ChineseRestaurant (cid:118) not ItalianRestaurant

(14.71)

Specifying that a set of subconcepts covers a category can be achieved with disjunc-
tion, as in the following:

Restaurant (cid:118)
(or ItalianRestaurant ChineseRestaurant MexicanRestaurant)

(14.72)

Having a hierarchy such as the one given in Fig. 14.6 tells us next to nothing
about the concepts in it. We certainly don’t know anything about what makes a
restaurant a restaurant, much less Italian, Chinese, or expensive. What is needed are
additional assertions about what it means to be a member of any of these categories.
In Description Logics such statements come in the form of relations between the
concepts being described and other concepts in the domain.
In keeping with its
origins in structured network representations, relations in Description Logics are
typically binary and are often referred to as roles, or role-relations.
To see how such relations work, let’s consider some of the facts about restaurants
discussed earlier in the chapter. We’ll use the hasCuisine relation to capture infor-
mation as to what kinds of food restaurants serve and the hasPriceRange relation
to capture how pricey particular restaurants tend to be. We can use these relations
to say something more concrete about our various classes of restaurants. Let’s start
with our ItalianRestaurant concept. As a ﬁrst approximation, we might say some-
thing uncontroversial like Italian restaurants serve Italian cuisine. To capture these
notions, let’s ﬁrst add some new concepts to our terminology to represent various
kinds of cuisine.

MexicanCuisine (cid:118) Cuisine
ItalianCuisine (cid:118) Cuisine
ChineseCuisine (cid:118) Cuisine
VegetarianCuisine (cid:118) Cuisine

ExpensiveRestaurant (cid:118) Restaurant
ModerateRestaurant (cid:118) Restaurant
CheapRestaurant (cid:118) Restaurant

Next, let’s revise our earlier version of ItalianRestaurant to capture cuisine
information.

ItalianRestaurant (cid:118) Restaurant (cid:117) ∃hasCuisine.ItalianCuisine

(14.73)

The correct way to read this expression is that individuals in the category Italian-
Restaurant are subsumed both by the category Restaurant and by an unnamed

14 .5

• D E SCR I P T ION LOG IC S

319

class deﬁned by the existential clause—the set of entities that serve Italian cuisine.
An equivalent statement in FO L would be

(14.74)

∀xIt al ianRest aurant (x) → Rest aurant (x)
∧(∃yServes(x, y) ∧ It al ianCuisine(y))
This FO L translation should make it clear what the DL assertions given above do
and do not entail. In particular, they don’t say that domain entities classiﬁed as Ital-
ian restaurants can’t engage in other relations like being expensive or even serving
Chinese cuisine. And critically, they don’t say much about domain entities that we
know do serve Italian cuisine. In fact, inspection of the FO L translation makes it
clear that we cannot infer that any new entities belong to this category based on their
characteristics. The best we can do is infer new facts about restaurants that we’re
explicitly told are members of this category.
Of course, inferring the category membership of individuals given certain char-
acteristics is a common and critical reasoning task that we need to support. This
brings us back to the alternative approach to creating hierarchical structures in a
terminology: actually providing a deﬁnition of the categories we’re creating in the
form of necessary and sufﬁcient conditions for category membership. In this case,
we might explicitly provide a deﬁnition for ItalianRestaurant as being those restau-
rants that serve Italian cuisine, and ModerateRestaurant as being those whose
price range is moderate.

ItalianRestaurant ≡ Restaurant (cid:117) ∃hasCuisine.ItalianCuisine
ModerateRestaurant ≡ Restaurant (cid:117) hasPriceRange.ModeratePrices (14.76)

(14.75)

While our earlier statements provided necessary conditions for membership in these
categories, these statements provide both necessary and sufﬁcient conditions.
Finally, let’s now consider the superﬁcially similar case of vegetarian restaurants.
Clearly, vegetarian restaurants are those that serve vegetarian cuisine. But they don’t
merely serve vegetarian fare, that’s all they serve. We can accommodate this kind of
constraint by adding an additional restriction in the form of a universal quantiﬁer to
our earlier description of VegetarianRestaurants, as follows:

VegetarianRestaurant ≡ Restaurant
(cid:117)∃hasCuisine.VegetarianCuisine
(cid:117)∀hasCuisine.VegetarianCuisine

(14.77)

Inference

Paralleling the focus of Description Logics on categories, relations, and individuals
is a processing focus on a restricted subset of logical inference. Rather than employ-
ing the full range of reasoning permitted by FOL, DL reasoning systems emphasize
the closely coupled problems of subsumption and instance checking.
Subsumption, as a form of inference, is the task of determining, based on the
facts asserted in a terminology, whether a superset/subset relationship exists between
two concepts. Correspondingly, instance checking asks if an individual can be a
member of a particular category given the facts we know about both the individual
and the terminology. The inference mechanisms underlying subsumption and in-
stance checking go beyond simply checking for explicitly stated subsumption rela-
tions in a terminology. They must explicitly reason using the relational information

Subsumption

instance
checking

320 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

Figure 14.7 A graphical network representation of the complete set of subsumption rela-
tions in the restaurant domain given the current set of assertions in the TBox.

asserted about the terminology to infer appropriate subsumption and membership
relations.
Returning to our restaurant domain, let’s add a new kind of restaurant using the
following statement:

IlFornaio (cid:118) ModerateRestaurant (cid:117) ∃hasCuisine.ItalianCuisine

(14.78)

Given this assertion, we might ask whether the IlFornaio chain of restaurants might
be classiﬁed as an Italian restaurant or a vegetarian restaurant. More precisely, we
can pose the following questions to our reasoning system:

IlFornaio (cid:118) ItalianRestaurant
IlFornaio (cid:118) VegetarianRestaurant

(14.79)
(14.80)

The answer to the ﬁrst question is positive since IlFornaio meets the criteria we
speciﬁed for the category ItalianRestaurant: it’s a Restaurant since we explicitly
classiﬁed it as a ModerateRestaurant, which is a subtype of Restaurant, and it
meets the has.Cuisine class restriction since we’ve asserted that directly.
The answer to the second question is negative. Recall, that our criteria for veg-
etarian restaurants contains two requirements: it has to serve vegetarian fare, and
that’s all it can serve. Our current deﬁnition for IlFornaio fails on both counts since
we have not asserted any relations that state that IlFornaio serves vegetarian fare,
and the relation we have asserted, hasCuisine.ItalianCuisine, contradicts the sec-
ond criteria.
A related reasoning task, based on the basic subsumption inference, is to derive
the implied hierarchy for a terminology given facts about the categories in the ter-
minology. This task roughly corresponds to a repeated application of the subsump-
tion operator to pairs of concepts in the terminology. Given our current collection of
statements, the expanded hierarchy shown in Fig. 14.7 can be inferred. You should
convince yourself that this diagram contains all and only the subsumption links that
should be present given our current knowledge.
Instance checking is the task of determining whether a particular individual can
be classiﬁed as a member of a particular category. This process takes what is known

implied
hierarchy

14 .6

• SUMMARY

321

about a given individual, in the form of relations and explicit categorical statements,
and then compares that information with what is known about the current terminol-
ogy. It then returns a list of the most speciﬁc categories to which the individual can
belong.
As an example of a categorization problem, consider an establishment that we’re
told is a restaurant and serves Italian cuisine.

Restaurant(Gondolier)
hasCuisine(Gondolier, ItalianCuisine)

Here, we’re being told that the entity denoted by the term Gondolier is a restau-
rant and serves Italian food. Given this new information and the contents of our
current TBox, we might reasonably like to ask if this is an Italian restaurant, if it is
a vegetarian restaurant, or if it has moderate prices.
Assuming the deﬁnitional statements given earlier, we can indeed categorize
the Gondolier as an Italian restaurant. That is, the information we’ve been given
about it meets the necessary and sufﬁcient conditions required for membership in
this category. And as with the IlFornaio category, this individual fails to match the
stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also
turn out to be a moderately priced restaurant, but we can’t tell at this point since
we don’t know anything about its prices. What this means is that given our current
knowledge the answer to the query ModerateRestaurant(Gondolier) would be false
since it lacks the required hasPriceRange relation.
The implementation of subsumption, instance checking, as well as other kinds of
inferences needed for practical applications, varies according to the expressivity of
the Description Logic being used. However, for a Description Logic of even modest
power, the primary implementation techniques are based on satisﬁability methods
that in turn rely on the underlying model-based semantics introduced earlier in this
chapter.

OWL and the Semantic Web

The highest-proﬁle role for Description Logics, to date, has been as a part of the
development of the Semantic Web. The Semantic Web is an ongoing effort to pro-
vide a way to formally specify the semantics of the contents of the Web (Fensel
et al., 2003). A key component of this effort involves the creation and deployment
of ontologies for various application areas of interest. The meaning representation
language used to represent this knowledge is the Web Ontology Language (OWL)
(McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that
corresponds roughly to the one we’ve been describing here.

Web Ontology
Language

14.6 Summary

This chapter has introduced the representational approach to meaning. The follow-
ing are some of the highlights of this chapter:
• A major approach to meaning in computational linguistics involves the cre-
ation of formal meaning representations that capture the meaning-related
content of linguistic inputs. These representations are intended to bridge the
gap from language to common-sense knowledge of the world.

322 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

• The frameworks that specify the syntax and semantics of these representa-
tions are called meaning representation languages. A wide variety of such
languages are used in natural language processing and artiﬁcial intelligence.
• Such representations need to be able to support the practical computational
requirements of semantic processing. Among these are the need to determine

the truth of propositions, to support unambiguous representations, to rep-

resent variables, to support inference, and to be sufﬁciently expressive.
• Human languages have a wide variety of features that are used to convey
meaning. Among the most important of these is the ability to convey a predicate-

argument structure.

• First-Order Logic is a well-understood, computationally tractable meaning
representation language that offers much of what is needed in a meaning rep-
resentation language.
• Important elements of semantic representation including states and events
can be captured in FOL.
• Semantic networks and frames can be captured within the FO L framework.
• Modern Description Logics consist of useful and computationally tractable
subsets of full First-Order Logic. The most prominent use of a description
logic is the Web Ontology Language (OWL), used in the speciﬁcation of the
Semantic Web.

Bibliographical and Historical Notes

The earliest computational use of declarative meaning representations in natural lan-
guage processing was in the context of question-answering systems (Green et al.,
1961; Raphael, 1968; Lindsey, 1963). These systems employed ad hoc representa-
tions for the facts needed to answer questions. Questions were then translated into
a form that could be matched against facts in the knowledge base. Simmons (1965)
provides an overview of these early efforts.
Woods (1967) investigated the use of FO L-like representations in question an-
swering as a replacement for the ad hoc representations in use at the time. Woods
(1973) further developed and extended these ideas in the landmark Lunar system.
Interestingly, the representations used in Lunar had both truth-conditional and pro-
cedural semantics. Winograd (1972) employed a similar representation based on the
Micro-Planner language in his SHRD LU system.
During this same period, researchers interested in the cognitive modeling of lan-
guage and memory had been working with various forms of associative network
representations. Masterman (1957) was the ﬁrst to make computational use of a
semantic network-like knowledge representation, although semantic networks are
generally credited to Quillian (1968). A considerable amount of work in the se-
mantic network framework was carried out during this era (Norman and Rumelhart,
1975; Schank, 1972; Wilks, 1975c, 1975b; Kintsch, 1974). It was during this pe-
riod that a number of researchers began to incorporate Fillmore’s notion of case roles
(Fillmore, 1968) into their representations. Simmons (1973) was the earliest adopter
of case roles as part of representations for natural language processing.
Detailed analyses by Woods (1975) and Brachman (1979) aimed at ﬁguring out
what semantic networks actually mean led to the development of a number of more

EX ERC I SE S

323

sophisticated network-like languages including KRL (Bobrow and Winograd, 1977)
and K L -ON E (Brachman and Schmolze, 1985). As these frameworks became more
sophisticated and well deﬁned, it became clear that they were restricted variants of
FOL coupled with specialized indexing inference procedures. A useful collection of
papers covering much of this work can be found in Brachman and Levesque (1985).
Russell and Norvig (2002) describe a modern perspective on these representational
efforts.
Linguistic efforts to assign semantic structures to natural language sentences in
the generative era began with the work of Katz and Fodor (1963). The limitations
of their simple feature-based representations and the natural ﬁt of logic to many
of the linguistic problems of the day quickly led to the adoption of a variety of
predicate-argument structures as preferred semantic representations (Lakoff, 1972;
McCawley, 1968). The subsequent introduction by Montague (1973) of the truth-
conditional model-theoretic framework into linguistic theory led to a much tighter
integration between theories of formal syntax and a wide range of formal semantic
frameworks. Good introductions to Montague semantics and its role in linguistic
theory can be found in Dowty et al. (1981) and Partee (1976).
The representation of events as reiﬁed objects is due to Davidson (1967). The
approach presented here, which explicitly reiﬁes event participants, is due to Parsons
(1990).
Most current computational approaches to temporal reasoning are based on Allen’s
notion of temporal intervals (Allen, 1984); see Chapter 17. ter Meulen (1995) pro-
vides a modern treatment of tense and aspect. Davis (1990) describes the use of FO L
to represent knowledge across a wide range of common-sense domains including
quantities, space, time, and beliefs.
A recent comprehensive treatment of logic and language can be found in van
Benthem and ter Meulen (1997). A classic semantics text is Lyons (1977). McCaw-
ley (1993) is an indispensable textbook covering a wide range of topics concerning
logic and language. Chierchia and McConnell-Ginet (1991) also broadly covers
semantic issues from a linguistic perspective. Heim and Kratzer (1998) is a more
recent text written from the perspective of current generative theory.

Exercises

14.1 Peruse your daily newspaper for three examples of ambiguous sentences or
headlines. Describe the various sources of the ambiguities.
14.2 Consider a domain in which the word coffee can refer to the following con-
cepts in a knowledge-based system: a caffeinated or decaffeinated beverage,
ground coffee used to make either kind of beverage, and the beans themselves.
Give arguments as to which of the following uses of coffee are ambiguous and
which are vague.
1. I’ve had my coffee for today.
2. Buy some coffee on your way home.
3. Please grind some more coffee.
14.3 The following rule, which we gave as a translation for Example 14.26, is not
a reasonable deﬁnition of what it means to be a vegetarian restaurant.

∀xVeget arianRest aurant (x) =⇒ Serves(x,Veget arianFood )

324 CHA PTER 14

• TH E R E PR E S EN TAT ION O F S EN T ENC E M EAN ING

Give a FOL rule that better deﬁnes vegetarian restaurants in terms of what they
serve.
14.4 Give FOL translations for the following sentences:
1. Vegetarians do not eat meat.
2. Not all vegetarians eat eggs.
14.5 Give a set of facts and inferences necessary to prove the following assertions:
1. McDonald’s is not a vegetarian restaurant.
2. Some vegetarians can eat at McDonald’s.
Don’t just place these facts in your knowledge base. Show that they can be
inferred from some more general facts about vegetarians and McDonald’s.
14.6 For the following sentences, give FO L translations that capture the temporal
relationships between the events.
1. When Mary’s ﬂight departed, I ate lunch.
2. When Mary’s ﬂight departed, I had eaten lunch.
14.7 On page 309, we gave the representation N ear(Cent ro, Bacaro) as a transla-
tion for the sentence Centro is near Bacaro. In a truth-conditional semantics,
this formula is either true or false given some model. Critique this truth-
conditional approach with respect to the meaning of words like near.

CHAPTER

15 Computational Semantics

Placeholder

325

CHAPTER

16 Semantic Parsing

Placeholder

326

CHAPTER

17 Information Extraction

I am the very model of a modern Major-General,
I’ve information vegetable, animal, and mineral,
I know the kings of England, and I quote the ﬁghts historical
From Marathon to Waterloo, in order categorical...
Gilbert and Sullivan, Pirates of Penzance

Imagine that you are an analyst with an investment ﬁrm that tracks airline stocks.
You’re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. His-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? You will need to know at least the name of the airline, the nature of
the proposed fare hike, the dates of the announcement, and possibly the response of
other airlines. Fortunately, these can be all found in news articles like this one:
Citing high fuel prices, United Airlines said Friday it has increased fares
by $6 per round trip on ﬂights to some cities also served by lower-
cost carriers. American Airlines, a unit of AMR Corp., immediately
matched the move, spokesman Tim Wagner said. United, a unit of UAL
Corp., said the increase took effect Thursday and applies to most routes
where it competes against discount carriers, such as Chicago to Dallas
and Denver to San Francisco.
This chapter presents techniques for extracting limited kinds of semantic con-
tent from text. This process of information extraction (IE), turns the unstructured
information embedded in texts into structured data, for example for populating a
relational database to enable further processing.
We begin with the ﬁrst step in most IE tasks, ﬁnding the proper names or named
entities in a text. The task of named entity recognition (NER) is to ﬁnd each
mention of a named entity in the text and label its type. What constitutes a named
entity type is task speciﬁc; people, places, and organizations are common, but gene
or protein names (Cohen and Demner-Fushman, 2014) or ﬁnancial asset classes
might be relevant for some tasks. Once all the named entities in a text have been
extracted, they can be linked together in sets corresponding to real-world entities,
inferring, for example, that mentions of United Airlines and United refer to the same
company. This is the joint task of coreference resolution and entity linking which
we defer til Chapter 20.
Next, we turn to the task of relation extraction: ﬁnding and classifying semantic
relations among the text entities. These are often binary relations like child-of, em-
ployment, part-whole, and geospatial relations. Relation extraction has close links
to populating a relational database.
Finally, we discuss three tasks related to events. Event extraction is ﬁnding
events in which these entities participate, like, in our sample text, the fare increases

information
extraction

named entity
recognition

relation
extraction

event
extraction

328 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

temporal
expression

temporal
normalization

template ﬁlling

by United and American and the reporting events said and cite. Event coreference
(Chapter 20) is needed to ﬁgure out which event mentions in a text refer to the same
event; in our running example the two instances of increase and the phrase the move
all refer to the same event.
To ﬁgure out when the events in a text happened we extract temporal expres-
sions like days of the week (Friday and Thursday), relative expressions like two
days from now or next year and times such as 3:30 P.M.. These expressions must be
normalized onto speciﬁc calendar dates or times of day to situate events in time. In
our sample task, this will allow us to link Friday to the time of United’s announce-
ment, and Thursday to the previous day’s fare increase, and produce a timeline in
which United’s announcement follows the fare increase and American’s announce-
ment follows both of those events.
Finally, many texts describe recurring stereotypical events or situations. The task
of template ﬁlling is to ﬁnd such situations in documents and ﬁll in the template
slots. These slot-ﬁllers may consist of text segments extracted directly from the text,
or concepts like times, amounts, or ontology entities that have been inferred from
text elements through additional processing.
Our airline text is an example of this kind of stereotypical situation since airlines
often raise fares and then wait to see if competitors follow along.
In this situa-
tion, we can identify United as a lead airline that initially raised its fares, $6 as the
amount, Thursday as the increase date, and American as an airline that followed
along, leading to a ﬁlled template like the following.

FAR E -RA I SE ATT EM PT: 

L EAD A IR L IN E :
AMOUNT:
E FFEC T IV E DATE :
FO LLOW ER :

UN I TED A IR L IN E S

$6
2 006 -10 -2 6

AM ER ICAN A IR L IN E S



17.1 Named Entity Recognition

named entity

temporal
expressions

The ﬁrst step in information extraction is to detect the entities in the text. A named
entity is, roughly speaking, anything that can be referred to with a proper name:
a person, a location, an organization. The term is commonly extended to include
things that aren’t entities per se, including dates, times, and other kinds of temporal
expressions, and even numerical expressions like prices. Here’s the sample text
introduced earlier with the named entities marked:
Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
has increased fares by [MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ORG American Airlines], a
unit of [ORG AMR Corp.], immediately matched the move, spokesman
[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
said the increase took effect [TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [LOC Chicago]

to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].

The text contains 13 mentions of named entities including 5 organizations, 4 loca-
tions, 2 times, 1 person, and 1 mention of money.
In addition to their use in extracting events and the relationship between par-
ticipants, named entities are useful for many other language processing tasks. In

17 .1

• NAM ED EN T I TY R ECOGN I T ION

329

sentiment analysis we might want to know a consumer’s sentiment toward a partic-
ular entity. Entities are a useful ﬁrst stage in question answering, or for linking text
to information in structured knowledge sources like Wikipedia.
Figure 17.1 shows typical generic named entity types. Many applications will
also need to use speciﬁc entity types like proteins, genes, commercial products, or
works of art.

Type

Tag Sample Categories

Example sentences

People
P ER people, characters
Organization ORG companies, sports teams
Location
LOC regions, mountains, seas
Geo-Political
G PE countries, states, provinces Palo Alto is raising the fees for parking.
Entity
Facility
Vehicles

FAC bridges, buildings, airports Consider the Golden Gate Bridge.
V EH planes, trains, automobiles
It was a classic Ford Falcon.

Turing is a giant of computer science.
The IPCC warned about the cyclone.

The Mt. Sanitas loop is in Sunshine Canyon.

Figure 17.1 A list of generic named entity types with the kinds of entities they refer to.

Named entity recognition means ﬁnding spans of text that constitute proper
names and then classifying the type of the entity. Recognition is difﬁcult partly be-
cause of the ambiguity of segmentation; we need to decide what’s an entity and what
isn’t, and where the boundaries are. Another difﬁculty is caused by type ambiguity.
The mention JFK can refer to a person, the airport in New York, or any number of
schools, bridges, and streets around the United States. Some examples of this kind
of cross-type confusion are given in Figures 17.2 and 17.3.

Name

Washington
Downing St.
IRA
Louis Vuitton

Possible Categories

Person, Location, Political Entity, Organization, Vehicle
Location, Organization
Person, Organization, Monetary Instrument
Person, Organization, Commercial Product

Figure 17.2 Common categorical ambiguities associated with various proper names.

[PER Washington] was born into slavery on the farm of James Burroughs.
[ORG Washington] went up 2 games to 1 in the four-game series.
Blair arrived in [LOC Washington] for what may well be his last state visit.
In June, [GPE Washington] passed a primary seatbelt law.
The [VEH Washington] had proved to be a leaky ship, every passage I made...

Figure 17.3 Examples of type ambiguities in the use of the name Washington.

17.1.1 NER as Sequence Labeling

The standard algorithm for named entity recognition is as a word-by-word sequence
labeling task, in which the assigned tags capture both the boundary and the type. A
sequence classiﬁer like an MEMM/CRF or a bi-LSTM is trained to label the tokens
in a text with tags that indicate the presence of particular kinds of named entities.
Consider the following simpliﬁed excerpt from our running example.
[ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched
the move, spokesman [PER Tim Wagner] said.

330 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

IOB

Figure 17.4 shows the same excerpt represented with IOB tagging. In IOB tag-
ging we introduce a tag for the beginning (B) and inside (I) of each entity type,
and one for tokens outside (O) any entity. The number of tags is thus 2n + 1 tags,
where n is the number of entity types. IOB tagging can represent exactly the same
information as the bracketed notation.

Words

American
Airlines
,
a
unit
of
AMR
Corp.
,
immediately
matched
the
move
,
spokesman
Tim
Wagner
said
.

IOB Label

B-ORG
I-ORG
O
O
O
O
B-ORG
I-ORG
O
O
O
O
O
O
O
B-PER
I-PER
O
O

IO Label

I-ORG
I-ORG
O
O
O
O
I-ORG
I-ORG
O
O
O
O
O
O
O
I-PER
I-PER
O
O

Figure 17.4 Named entity tagging as a sequence model, showing IOB and IO encodings.

We’ve also shown IO tagging, which loses some information by eliminating the
B tag. Without the B tag IO tagging is unable to distinguish between two entities of
the same type that are right next to each other. Since this situation doesn’t arise very
often (usually there is at least some punctuation or other deliminator), IO tagging
may be sufﬁcient, and has the advantage of using only n + 1 tags.
In the following three sections we introduce the three standard families of al-
gorithms for NER tagging: feature based (MEMM/CRF), neural (bi-LSTM), and
rule-based.

17.1.2 A feature-based algorithm for NER

identity of wi , identity of neighboring words
embeddings for wi , embeddings for neighboring words
part of speech of wi , part of speech of neighboring words
base-phrase syntactic chunk label of wi and neighboring words
presence of wi in a gazetteer
wi contains a particular preﬁx (from all preﬁxes of length ≤ 4)
wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4)
wi is all upper case
word shape of wi , word shape of neighboring words
short word shape of wi , short word shape of neighboring words
presence of hyphen

Figure 17.5 Typical features for a feature-based NER system.

word shape

gazetteer

17 .1

• NAM ED EN T I TY R ECOGN I T ION

331

The ﬁrst approach is to extract features and train an MEMM or CRF sequence
model of the type we saw for part-of-speech tagging in Chapter 8. Figure 17.5 lists
standard features used in such feature-based systems. We’ve seen many of these
features before in the context of part-of-speech tagging, particularly for tagging un-
known words. This is not surprising, as many unknown words are in fact named
entities. Word shape features are thus particularly important in the context of NER.
Recall that word shape features are used to represent the abstract letter pattern of
the word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to ’d’, and
retaining punctuation. Thus for example I.M.F would map to X.X.X. and DC10-30
would map to XXdd-dd. A second class of shorter word shape features is also used.
In these features consecutive character types are removed, so DC10-30 would be
mapped to Xd-d but I.M.F would still map to X.X.X. This feature by itself accounts
for a considerable part of the success of feature-based NER systems for English
news text. Shape features are also particularly important in recognizing names of
proteins and genes in biological texts.
For example the named entity token L’Occitane would generate the following
non-zero valued feature values:

preﬁx(wi ) = L
preﬁx(wi ) = L’
preﬁx(wi ) = L’O
preﬁx(wi ) = L’Oc
word-shape(wi ) = X’Xxxxxxxx

sufﬁx(wi ) = tane
sufﬁx(wi ) = ane
sufﬁx(wi ) = ne
sufﬁx(wi ) = e
short-word-shape(wi ) = X’Xx

A gazetteer is a list of place names, often providing millions of entries for lo-
cations with detailed geographical and political information.1 A related resource
is name-lists; the United States Census Bureau also provides extensive lists of ﬁrst
names and surnames derived from its decadal census in the U.S.2 Similar lists of cor-
porations, commercial products, and all manner of things biological and mineral are
also available from a variety of sources. Gazetteer and name features are typically
implemented as a binary feature for each name list. Unfortunately, such lists can
be difﬁcult to create and maintain, and their usefulness varies considerably. While
gazetteers can be quite effective, lists of persons and organizations are not always
helpful (Mikheev et al., 1999).
Feature effectiveness depends on the application, genre, media, and language.
For example, shape features, critical for English newswire texts, are of little use
with automatic speech recognition transcripts, or other non-edited or informally-
edited sources, or for languages like Chinese that don’t use orthographic case. The
features in Fig. 17.5 should therefore be thought of as only a starting point.
Figure 17.6 illustrates the result of adding part-of-speech tags, syntactic base-
phrase chunk tags, and some shape information to our earlier example.
Given such a training set, a sequence classiﬁer like an MEMM can be trained to
label new sentences. Figure 17.7 illustrates the operation of such a sequence labeler
at the point where the token Corp. is next to be labeled. If we assume a context win-
dow that includes the two preceding and following words, then the features available
to the classiﬁer are those shown in the boxed area.

1 www.geonames.org
2 www.census.gov

332 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

POS Chunk

Short shape

Word

American
NNP B-NP
Xx
Airlines
NNPS I-NP
Xx
,
,
O
,
a
DT
B-NP
x
unit
NN
I-NP
x
of
IN
B-PP
x
AMR
NNP B-NP
X
Corp.
NNP
I-NP
Xx.
,
,
O
,
immediately RB
B-ADVP x
matched
VBD B-VP
x
the
DT
B-NP
x
move
NN
I-NP
x
,
,
O
,
spokesman NN
B-NP
x
Tim
NNP
I-NP
Xx
Wagner
NNP
I-NP
Xx
said
VBD B-VP
x
.
,
O
.

Label

B-ORG
I-ORG
O
O
O
O
B-ORG
I-ORG
O
O
O
O
O
O
O
B-PER
I-PER
O
O

Figure 17.6 Word-by-word feature encoding for NER.

Figure 17.7 Named entity recognition as sequence labeling. The features available to the classiﬁer during
training and classiﬁcation are those in the boxed area.

17.1.3 A neural algorithm for NER

The standard neural algorithm for NER is based on the bi-LSTM introduced in Chap-
ter 9. Recall that in that model, word and character embeddings are computed for
input word wi . These are passed through a left-to-right LSTM and a right-to-left
LSTM, whose outputs are concatenated (or otherwise combined) to produce a sin-
gle output layer at position i. In the simplest method, this layer can then be directly
passed onto a softmax that creates a probability distribution over all NER tags, and
the most likely tag is chosen as ti .
For named entity tagging this greedy approach to decoding is insufﬁcient, since
it doesn’t allow us to impose the strong constraints neighboring tokens have on each
other (e.g., the tag I-PER must follow another I-PER or B-PER). Instead a CRF layer
is normally used on top of the bi-LSTM output, and the Viterbi decoding algorithm
is used to decode. Fig. 17.8 shows a sketch of the algorithm

17 .1

• NAM ED EN T I TY R ECOGN I T ION

333

Figure 17.8 Putting it all together: character embeddings and words together a bi-LSTM
sequence model. After (Lample et al., 2016)

17.1.4 Rule-based NER

While machine learned (neural or MEMM/CRF) sequence models are the norm in
academic research, commercial approaches to NER are often based on pragmatic
combinations of lists and rules, with some smaller amount of supervised machine
learning (Chiticariu et al., 2013). For example IBM System T is a text understand-
ing architecture in which a user speciﬁes complex declarative constraints for tagging
tasks in a formal query language that includes regular expressions, dictionaries, se-
mantic constraints, NLP operators, and table structures, all of which the system
compiles into an efﬁcient extractor (Chiticariu et al., 2018)
One common approach is to make repeated rule-based passes over a text, allow-
ing the results of one pass to inﬂuence the next. The stages typically ﬁrst involve
the use of rules that have extremely high precision but low recall. Subsequent stages
employ more error-prone statistical methods that take the output of the ﬁrst pass into
account.
1. First, use high-precision rules to tag unambiguous entity mentions.
2. Then, search for substring matches of the previously detected names.
3. Consult application-speciﬁc name lists to identify likely name entity mentions
from the given domain.
4. Finally, apply probabilistic sequence labeling techniques that make use of the
tags from previous stages as additional features.
The intuition behind this staged approach is twofold. First, some of the entity
mentions in a text will be more clearly indicative of a given entity’s class than others.
Second, once an unambiguous entity mention is introduced into a text, it is likely that
subsequent shortened versions will refer to the same entity (and thus the same type
of entity).

17.1.5 Evaluation of Named Entity Recognition

The familiar metrics of recall, precision, and F1 measure are used to evaluate NER
systems. Remember that recall is the ratio of the number of correctly labeled re-
sponses to the total that should have been labeled; precision is the ratio of the num-

334 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

Figure 17.9 The 17 relations used in the ACE relation extraction task.

ber of correctly labeled responses to the total labeled; and F-measure is the harmonic
mean of the two. For named entities, the entity rather than the word is the unit of
response. Thus in the example in Fig. 17.6, the two entities Tim Wagner and AMR
Corp. and the non-entity said would each count as a single response.
The fact that named entity tagging has a segmentation component which is not
present in tasks like text categorization or part-of-speech tagging causes some prob-
lems with evaluation. For example, a system that labeled American but not American
Airlines as an organization would cause two errors, a false positive for O and a false
negative for I-ORG. In addition, using entities as the unit of response but words as
the unit of training means that there is a mismatch between the training and test
conditions.

17.2 Relation Extraction

Next on our list of tasks is to discern the relationships that exist among the detected
entities. Let’s return to our sample airline text:

Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
has increased fares by [MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ORG American Airlines], a
unit of [ORG AMR Corp.], immediately matched the move, spokesman
[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
said the increase took effect [TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [LOC Chicago]

to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].

The text tells us, for example, that Tim Wagner is a spokesman for American
Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR.
These binary relations are instances of more generic relations such as part-of or
employs that are fairly frequent in news-style texts. Figure 17.9 lists the 17 relations
used in the ACE relation extraction evaluations and Fig. 17.10 shows some sample
relations. We might also extract more domain-speciﬁc relation such as the notion of
an airline route. For example from this text we can conclude that United has routes
to Chicago, Dallas, Denver, and San Francisco.

17 .2

• R ELAT ION EX TRACT ION

335

Relations

Physical-Located
Part-Whole-Subsidiary
Person-Social-Family
Org-AFF-Founder

Types

PER-GPE
ORG-ORG
PER-PER
PER-ORG

Examples
He was in Tennessee

XYZ, the parent company of ABC
Yoko’s husband John
Steve Jobs, co-founder of Apple...

Figure 17.10 Semantic relations with examples and the named entity types they involve.

Domain

United, UAL, American Airlines, AMR
Tim Wagner
Chicago, Dallas, Denver, and San Francisco

Classes

United, UAL, American, and AMR are organizations
Tim Wagner is a person
Chicago, Dallas, Denver, and San Francisco are places

Relations

United is a unit of UAL
American is a unit of AMR
Tim Wagner works for American Airlines
United serves Chicago, Dallas, Denver, and San Francisco

Figure 17.11 A model-based view of the relations and entities in our sample text.

D = {a, b, c, d , e, f , g, h, i}
a, b, c, d
e
f , g, h, i

Org = {a, b, c, d }
Pers = {e}
Loc = { f , g, h, i}

PartOf = {(cid:104)a, b(cid:105), (cid:104)c, d (cid:105)}
OrgAff = {(cid:104)c, e(cid:105)}
Serves = {(cid:104)a, f (cid:105), (cid:104)a, g(cid:105), (cid:104)a, h(cid:105), (cid:104)a, i(cid:105)}

These relations correspond nicely to the model-theoretic notions we introduced
in Chapter 14 to ground the meanings of the logical forms. That is, a relation consists
of a set of ordered tuples over elements of a domain. In most standard information-
extraction applications, the domain elements correspond to the named entities that
occur in the text, to the underlying entities that result from co-reference resolution, or
to entities selected from a domain ontology. Figure 17.11 shows a model-based view
of the set of entities and relations that can be extracted from our running example.
Notice how this model-theoretic view subsumes the NER task as well; named entity
recognition corresponds to the identiﬁcation of a class of unary relations.
Sets of relations have been deﬁned for many other domains as well. For example
UMLS, the Uniﬁed Medical Language System from the US National Library of
Medicine has a network that deﬁnes 134 broad subject categories, entity types, and
54 relations between the entities, such as the following:

Entity

Relation

Entity

Injury
disrupts
Physiological Function
Bodily Location
location-of Biologic Function
Anatomical Structure
part-of
Organism
Pharmacologic Substance causes
Pathological Function
Pharmacologic Substance treats
Pathologic Function
Given a medical sentence like this one:
(17.1) Doppler echocardiography can be used to diagnose left anterior descending
artery stenosis in patients with type 2 diabetes
We could thus extract the UMLS relation:
Echocardiography, Doppler Diagnoses Acquired stenosis
Wikipedia also offers a large supply of relations, drawn from infoboxes, struc-
tured tables associated with certain Wikipedia articles. For example, the Wikipedia

infoboxes

336 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

RDF
RDF triple

infobox for Stanford includes structured facts like state = "California" or
president = "Mark Tessier-Lavigne". These facts can be turned into rela-
tions like president-of or located-in. or into relations in a metalanguage called RDF
(Resource Description Framework). An RDF triple is a tuple of entity-relation-
entity, called a subject-predicate-object expression. Here’s a sample RDF triple:

subject

predicate object

Golden Gate Park location

San Francisco

Freebase

is-a
hypernym

For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-
rived from Wikipedia containing over 2 billion RDF triples. Another dataset from
Wikipedia infoboxes, Freebase (Bollacker et al., 2008), has relations like
people/person/nationality
location/location/contains
WordNet or other ontologies offer useful ontological relations that express hier-
archical relations between words or concepts. For example WordNet has the is-a or
hypernym relation between classes,
Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...
WordNet also has Instance-of relation between individuals and classes, so that for
example San Francisco is in the Instance-of relation with city. Extracting these
relations is an important step in extending or building ontologies.
There are ﬁve main classes of algorithms for relation extraction: hand-written

patterns, supervised machine learning, semi-supervised (via bootstrapping and

via distant supervision), and unsupervised. We’ll introduce each of these in the
next sections.

17.2.1 Using Patterns to Extract Relations

The earliest and still common algorithm for relation extraction is lexico-syntactic
patterns, ﬁrst developed by Hearst (1992a). Consider the following sentence:
Agar is a substance prepared from a mixture of red algae, such as Ge-
lidium, for laboratory or industrial use.
Hearst points out that most human readers will not know what Gelidium is, but that
they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is.
She suggests that the following lexico-syntactic pattern

NP0 such as NP1 {, NP2 . . . , (and|or)NPi }, i ≥ 1
implies the following semantics

allowing us to infer

∀NPi , i ≥ 1, hyponym(NPi , NP0 )

hyponym(Gelidium, red algae)

(17.2)

(17.3)

(17.4)

Figure 17.12 shows ﬁve patterns Hearst (1992a, 1998) suggested for inferring
the hyponym relation; we’ve shown NPH as the parent/hyponym. Modern versions
of the pattern-based approach extend it by adding named entity constraints. For
example if our goal is to answer questions about “Who holds what ofﬁce in which
organization?”, we can use patterns like the following:

17 .2

• R ELAT ION EX TRACT ION

337

NP {, NP}* {,} (and|or) other NPH
NPH such as {NP,}* {(or|and)} NP
such NPH as {NP,}* {(or|and)} NP
NPH {,} including {NP,}* {(or|and)} NP
NPH {,} especially {NP}* {(or|and)} NP

temples, treasuries, and other important civic buildings
red algae such as Gelidium
such authors as Herrick, Goldsmith, and Shakespeare
common-law countries, including Canada and England
European countries, especially France, England, and Spain
Figure 17.12 Hand-built lexico-syntactic patterns for ﬁnding hypernyms, using {} to mark optionality
(Hearst 1992a, Hearst 1998).

PER, POSITION of ORG:
George Marshall, Secretary of State of the United States

PER (named|appointed|chose|etc.) PER Prep? POSITION
Truman appointed Marshall Secretary of State

PER [be]? (named|appointed|etc.) Prep? ORG POSITION
George Marshall was named US Secretary of State

Hand-built patterns have the advantage of high-precision and they can be tailored
to speciﬁc domains. On the other hand, they are often low-recall, and it’s a lot of
work to create them for all possible patterns.

17.2.2 Relation Extraction via Supervised Learning

Supervised machine learning approaches to relation extraction follow a scheme that
should be familiar by now. A ﬁxed set of relations and entities is chosen, a training
corpus is hand-annotated with the relations and entities, and the annotated texts are
then used to train classiﬁers to annotate an unseen test set.
The most straightforward approach has three steps, illustrated in Fig. 17.13. Step
one is to ﬁnd pairs of named entities (usually in the same sentence). In step two, a
ﬁltering classiﬁer is trained to make a binary decision as to whether a given pair of
named entities are related (by any relation). Positive examples are extracted directly
from all relations in the annotated corpus, and negative examples are generated from
within-sentence entity pairs that are not annotated with a relation. In step 3, a classi-
ﬁer is trained to assign a label to the relations that were found by step 2. The use of
the ﬁltering classiﬁer can speed up the ﬁnal classiﬁcation and also allows the use of
distinct feature-sets appropriate for each task. For each of the two classiﬁers, we can
use any of the standard classiﬁcation techniques (logistic regression, neural network,
SVM, etc.)

function F INDR ELAT ION S(words) returns relations
relations ← nil
entities ← F INDEN T I T I E S(words)
if R ELAT ED ?(e1, e2)
relations ← relations+C LA S S I FYR ELAT ION(e1, e2)
Figure 17.13 Finding and classifying the relations among entities in a text.

forall entity pairs (cid:104)e1, e2(cid:105) in entities do

For the feature-based classiﬁers like logistic regression or random forests the
most important step is to identify useful features. Let’s consider features for clas-

338 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

sifying the relationship between American Airlines (Mention 1, or M1) and Tim
Wagner (Mention 2, M2) from this sentence:
(17.5) American Airlines, a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
Useful word features include
• The headwords of M1 and M2 and their concatenation
Airlines Wagner
Airlines-Wagner
• Bag-of-words and bigrams in M1 and M2
American, Airlines, Tim, Wagner, American Airlines, Tim Wagner
• Words or bigrams in particular positions
M2: -1 spokesman
M2: +1 said
• Bag of words or bigrams between M1 and M2:
a, AMR, of, immediately, matched, move, spokesman, the, unit
• Stemmed versions of the same
Embeddings can be used to represent words in any of these features. Useful named
entity features include
• Named-entity types and their concatenation
(M1: ORG, M2: PER, M1M2: ORG-PER)
• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)
M1: NAME [it or he would be PRONOUN]
M2: NAME [the company would be NOMINAL]
• Number of entities between the arguments (in this case 1, for AMR)
The syntactic structure of a sentence can also signal relationships among its
entities. Syntax is often featured by using strings representing syntactic paths: the
(dependency or constituency) path traversed through the tree in getting from one
entity to the other.
• Base syntactic chunk sequence from M1 to M2
NP NP PP VP NP NP
• Constituent paths between M1 and M2
NP ↑ NP ↑ S ↑ S ↓ NP
• Dependency-tree paths
Airlines ←sub j matched ←com p said →sub j Wagner
Figure 17.14 summarizes many of the features we have discussed that could be
used for classifying the relationship between American Airlines and Tim Wagner
from our example text.
Neural models for relation extraction similarly treat the task as supervised clas-
siﬁcation. One option is to use a similar architecture as we saw for named entity
tagging: a bi-LSTM model with word embeddings as inputs and a single softmax
classiﬁcation of the sentence output as a 1-of-N relation label. Because relations
often hold between entities that are far part in a sentence (or across sentences), it
may be possible to get higher performance from algorithms like convolutional nets
(dos Santos et al., 2015) or chain or tree LSTMS (Miwa and Bansal 2016, Peng
et al. 2017).
In general, if the test set is similar enough to the training set, and if there is
enough hand-labeled data, supervised relation extraction systems can get high ac-
curacies. But labeling a large training set is extremely expensive and supervised

17 .2

• R ELAT ION EX TRACT ION

339

airlines (as a word token or an embedding)
Wagner

NONE

said
{a, unit, of, AMR, Inc., immediately, matched, the, move, spokesman }

ORG
P ER S
ORG - P ER S

M1 headword
M2 headword
Word(s) before M1
Word(s) after M2
Bag of words between
M1 type
M2 type
Concatenated types
Constituent path
Base phrase path

NP ↑ NP ↑ S ↑ S ↓ NP
NP → NP → PP → NP → V P → NP → NP
Typed-dependency path Airlines ←sub j matched ←com p said →sub j Wagner

Figure 17.14 Sample of features extracted during classiﬁcation of the <American Airlines, Tim Wagner>
tuple; M1 is the ﬁrst mention, M2 the second.

models are brittle: they don’t generalize well to different text genres. For this rea-
son, much research in relation extraction has focused on the semi-supervised and
unsupervised approaches we turn to next.

17.2.3 Semisupervised Relation Extraction via Bootstrapping

Supervised machine learning assumes that we have lots of labeled data. Unfortu-
nately, this is expensive. But suppose we just have a few high-precision seed pat-
terns, like those in Section 17.2.1, or perhaps a few seed tuples. That’s enough
to bootstrap a classiﬁer! Bootstrapping proceeds by taking the entities in the seed
pair, and then ﬁnding sentences (on the web, or whatever dataset we are using) that
contain both entities. From all such sentences, we extract and generalize the context
around the entities to learn new patterns. Fig. 17.15 sketches a basic algorithm.

seed patterns
seed tuples
bootstrapping

iterate

function BOOT STRA P(Relation R) returns new relation tuples
tuples ← Gather a set of seed tuples that have relation R
sentences ← ﬁnd sentences that contain entities in tuples
patterns ← generalize the context between and around entities in sentences
newpairs ← use patterns to grep for more tuples
newpairs ← newpairs with high conﬁdence
tuples ← tuples + newpairs

return tuples

Figure 17.15 Bootstrapping from seed entity pairs to learn relations.

Suppose, for example, that we need to create a list of airline/hub pairs, and we
know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover
new patterns by ﬁnding other mentions of this relation in our corpus. We search
for the terms Ryanair, Charleroi and hub in some proximity. Perhaps we ﬁnd the
following set of sentences:
(17.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all
weekend ﬂights out of the airport.
(17.7) All ﬂights in and out of Ryanair’s Belgian hub at Charleroi airport were
grounded on Friday...

340 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

conﬁdence
values
semantic drift

noisy-or

(17.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000
passengers had already been affected.
From these results, we can use the context of words between the entity mentions,
the words before mention one, the word after mention two, and the named entity
types of the two mentions, and perhaps other features, to extract general patterns
such as the following:

/ [ORG], which uses [LOC] as a hub /
/ [ORG]’s hub at [LOC] /
/ [LOC] a main hub for [ORG] /

These new patterns can then be used to search for additional tuples.
Bootstrapping systems also assign conﬁdence values to new tuples to avoid se-
mantic drift. In semantic drift, an erroneous pattern leads to the introduction of
erroneous tuples, which, in turn, lead to the creation of problematic patterns and the
meaning of the extracted relations ‘drifts’. Consider the following example:
(17.9) Sydney has a ferry hub at Circular Quay.
If accepted as a positive example, this expression could lead to the incorrect in-
troduction of the tuple (cid:104)Syd ney,Circul arQuay(cid:105). Patterns based on this tuple could
propagate further errors into the database.
Conﬁdence values for patterns are based on balancing two factors: the pattern’s
performance with respect to the current set of tuples and the pattern’s productivity
in terms of the number of matches it produces in the document collection. More
formally, given a document collection D , a current set of tuples T , and a proposed
pattern p, we need to track two factors:
• hit s: the set of tuples in T that p matches while looking in D
• f ind s: The total set of tuples that p ﬁnds in D
The following equation balances these considerations (Riloff and Jones, 1999).

Conf Rl ogF ( p) =

(17.10)

hits p
ﬁnds p × l og(ﬁnds p )
This metric is generally normalized to produce a probability.
We can assess the conﬁdence in a proposed new tuple by combining the evidence
supporting it from all the patterns P(cid:48) that match that tuple in D (Agichtein and
Gravano, 2000). One way to combine such evidence is the noisy-or technique.
Assume that a given tuple is supported by a subset of the patterns in P, each with
its own conﬁdence assessed as above. In the noisy-or model, we make two basic
assumptions. First, that for a proposed tuple to be false, all of its supporting patterns
must have been in error, and second, that the sources of their individual failures are
all independent. If we loosely treat our conﬁdence measures as probabilities, then
the probability of any individual pattern p failing is 1 − Conf ( p); the probability of
all of the supporting patterns for a tuple being wrong is the product of their individual
failure probabilities, leaving us with the following equation for our conﬁdence in a
new tuple.
Conf (t ) = 1 − (cid:89)p∈P(cid:48)
Setting conservative conﬁdence thresholds for the acceptance of new patterns
and tuples during the bootstrapping process helps prevent the system from drifting
away from the targeted relation.

(1 − Conf ( p))

(17.11)

distant
supervision

17.2.4 Distant Supervision for Relation Extraction

17 .2

• R ELAT ION EX TRACT ION

341

Although text that has been hand-labeled with relation labels is extremely expensive
to produce, there are ways to ﬁnd indirect sources of training data. The distant
supervision method of Mintz et al. (2009) combines the advantages of bootstrapping
with supervised learning. Instead of just a handful of seeds, distant supervision uses
a large database to acquire a huge number of seed examples, creates lots of noisy
pattern features from all these examples and then combines them in a supervised
classiﬁer.
For example suppose we are trying to learn the place-of-birth relationship be-
tween people and their birth cities. In the seed-based approach, we might have only
5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase
have tens of thousands of examples of many relations; including over 100,000 ex-

amples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein,

Ulm>, etc.,). The next step is to run named entity taggers on large amounts of text—
Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences
that have two named entities that match the tuple, like the following:
...Hubble was born in Marshﬁeld...
...Einstein, born (1879), Ulm...
...Hubble’s birthplace in Marshﬁeld...
Training instances can now be extracted from this data, one training instance
for each identical tuple <relation, entity1, entity2>. Thus there will be one
training instance for each of:

<born-in, Edwin Hubble, Marshfield>
<born-in, Albert Einstein, Ulm>
<born-year, Albert Einstein, 1879>

and so on.
We can then apply feature-based or neural classiﬁcation. For feature-based clas-
siﬁcation, standard supervised relation extraction features like the named entity la-
bels of the two mentions, the words and dependency paths in between the mentions,
and neighboring words. Each tuple will have features collected from many training
instances; the feature vector for a single training instance like (<born-in,Albert
Einstein, Ulm> will have lexical and syntactic features from many different sen-
tences that mention Einstein and Ulm.
Because distant supervision has very large training sets, it is also able to use very
rich features that are conjunctions of these individual features. So we will extract
thousands of patterns that conjoin the entity types with the intervening words or
dependency paths like these:
PER was born in LOC
PER, born (XXXX), LOC
PER’s birthplace in LOC
To return to our running example, for this sentence:
(17.12) American Airlines, a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
we would learn rich conjunction features like this one:
M1 = ORG & M2 = PER & nextword=“said”& path= NP ↑ NP ↑ S ↑ S ↓ NP
The result is a supervised classiﬁer that has a huge rich set of features to use
in detecting relations. Since not every test sentence will have one of the training

342 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

relations, the classiﬁer will also need to be able to label an example as no-relation.
This label is trained by randomly selecting entity pairs that do not appear in any
Freebase relation, extracting features for them, and building a feature vector for
each such tuple. The ﬁnal algorithm is sketched in Fig. 17.16.

function D I STAN T SU PERV I S ION(Database D, Text T) returns relation classiﬁer C

foreach relation R
foreach tuple (e1,e2) of entities with relation R in D
sentences ← Sentences in T that contain e1 and e2
f ← Frequent features in sentences
observations ← observations + new training tuple (e1, e2, f, R)
C ← Train supervised classiﬁer on observations

return C

Figure 17.16 The distant supervision algorithm for relation extraction. A neural classiﬁer
might not need to use the feature set f .

Distant supervision shares advantages with each of the methods we’ve exam-
ined. Like supervised classiﬁcation, distant supervision uses a classiﬁer with lots
of features, and supervised by detailed hand-created knowledge. Like pattern-based
classiﬁers, it can make use of high-precision evidence for the relation between en-
tities. Indeed, distance supervision systems learn patterns just like the hand-built
patterns of early relation extractors. For example the is-a or hypernym extraction
system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as
distant supervision, and then learned new patterns from large amounts of text. Their
system induced exactly the original 5 template patterns of Hearst (1992a), but also
70,000 additional patterns including these four:
NPH like NP Many hormones like leptin...
NPH called NP ...using a markup language called XHTML
NP is a NPH
Ruby is a programming language...
NP, a NPH
IBM, a company with a long...
This ability to use a large number of features simultaneously means that, un-
like the iterative expansion of patterns in seed-based systems, there’s no semantic
drift. Like unsupervised classiﬁcation, it doesn’t use a labeled training corpus of
texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very
large amounts of unlabeled data. Distant supervision also has the advantage that it
can create training tuples to be used with neural classiﬁers, where features are not
required.
But distant supervision can only help in extracting relations for which a large
enough database already exists. To extract new relations without datasets, or rela-
tions for new domains, purely unsupervised methods must be used.

17.2.5 Unsupervised Relation Extraction

open
information
extraction

The goal of unsupervised relation extraction is to extract relations from the web
when we have no labeled training data, and not even any list of relations. This task
is often called open information extraction or Open IE. In Open IE, the relations
are simply strings of words (usually beginning with a verb).
For example, the ReVerb system (Fader et al., 2011) extracts a relation from a
sentence s in 4 steps:

17 .2

• R ELAT ION EX TRACT ION

343

1. Run a part-of-speech tagger and entity chunker over s
2. For each verb in s, ﬁnd the longest sequence of words w that start with a verb
and satisfy syntactic and lexical constraints, merging adjacent matches.
3. For each phrase w, ﬁnd the nearest noun phrase x to the left which is not a
relative pronoun, wh-word or existential “there”. Find the nearest noun phrase
y to the right.
4. Assign conﬁdence c to the relation r = (x, w, y) using a conﬁdence classiﬁer
and return it.
A relation is only accepted if it meets syntactic and lexical constraints. The
syntactic constraints ensure that it is a verb-initial sequence that might also include
nouns (relations that begin with light verbs like make, have, or do often express the
core of the relation with a noun, like have a hub in):
V | VP | VW*P
V = verb particle? adv?
W = (noun | adj | adv | pron | det )
P = (prep | particle | inf. marker)
The lexical constraints are based on a dictionary D that is used to prune very rare,
long relation strings. The intuition is to eliminate candidate relations that don’t oc-
cur with sufﬁcient number of distinct argument types and so are likely to be bad
examples. The system ﬁrst runs the above relation extraction algorithm ofﬂine on
500 million web sentences and extracts a list of all the relations that occur after nor-
malizing them (removing inﬂection, auxiliary verbs, adjectives, and adverbs). Each
relation r is added to the dictionary if it occurs with at least 20 different arguments.
Fader et al. (2011) used a dictionary of 1.7 million normalized relations.
Finally, a conﬁdence value is computed for each relation using a logistic re-
gression classiﬁer. The classiﬁer is trained by taking 1000 random web sentences,
running the extractor, and hand labelling each extracted relation as correct or incor-
rect. A conﬁdence classiﬁer is then trained on this hand-labeled data, using features
of the relation and the surrounding words. Fig. 17.17 shows some sample features
used in the classiﬁcation.

(x,r,y) covers all words in s
the last preposition in r is for
the last preposition in r is on
len(s) ≤ 10
there is a coordinating conjunction to the left of r in s
r matches a lone V in the syntactic constraints
there is preposition to the left of x in s
there is an NP to the right of y in s

Figure 17.17 Features for the classiﬁer that assigns conﬁdence to relations extracted by the
Open Information Extraction system R EV ERB (Fader et al., 2011).

For example the following sentence:
(17.13) United has a hub in Chicago, which is the headquarters of United
Continental Holdings.
has the relation phrases has a hub in and is the headquarters of (it also has has and
is, but longer phrases are preferred). Step 3 ﬁnds United to the left and Chicago to
the right of has a hub in, and skips over which to ﬁnd Chicago to the left of is the
headquarters of. The ﬁnal output is:

344 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

r1:
r2:

<United, has a hub in, Chicago>
<Chicago, is the headquarters of, United Continental Holdings>

The great advantage of unsupervised relation extraction is its ability to handle
a huge number of relations without having to specify them in advance. The disad-
vantage is the need to map these large sets of strings into some canonical form for
adding to databases or other knowledge sources. Current methods focus heavily on
relations expressed with verbs, and so will miss many relations that are expressed
nominally.

17.2.6 Evaluation of Relation Extraction

Supervised relation extraction systems are evaluated by using test sets with human-
annotated, gold-standard relations and computing precision, recall, and F-measure.
Labeled precision and recall require the system to classify the relation correctly,
whereas unlabeled methods simply measure a system’s ability to detect entities that
are related.
Semi-supervised and unsupervised methods are much more difﬁcult to evalu-
ate, since they extract totally new relations from the web or a large text. Because
these methods use very large amounts of text, it is generally not possible to run them
solely on a small labeled test set, and as a result it’s not possible to pre-annotate a
gold set of correct instances of relations.
For these methods it’s possible to approximate (only) precision by drawing a
random sample of relations from the output, and having a human check the accuracy
of each of these relations. Usually this approach focuses on the tuples to be extracted
from a body of text rather than on the relation mentions; systems need not detect
every mention of a relation to be scored correctly. Instead, the evaluation is based
on the set of tuples occupying the database when the system is ﬁnished. That is,
we want to know if the system can discover that Ryanair has a hub at Charleroi; we
don’t really care how many times it discovers it. The estimated precision ˆP is then

ˆP =

# of correctly extracted relation tuples in the sample
total # of extracted relation tuples in the sample.

(17.14)

Another approach that gives us a little bit of information about recall is to com-
pute precision at different levels of recall. Assuming that our system is able to
rank the relations it produces (by probability, or conﬁdence) we can separately com-
pute precision for the top 1000 new relations, the top 10,000 new relations, the top
100,000, and so on. In each case we take a random sample of that set. This will
show us how the precision curve behaves as we extract more and more tuples. But
there is no way to directly evaluate recall.

17.3 Extracting Times

Times and dates are a particularly important kind of named entity that play a role
in question answering, in calendar and personal assistant applications. In order to
reason about times and dates, after we extract these temporal expressions they must
be normalized—converted to a standard format so we can reason about them. In this
section we consider both the extraction and normalization of temporal expressions.

17 .3

• EX TRAC T ING T IM E S

345

17.3.1 Temporal Expression Extraction

absolute
relative

duration

Temporal expressions are those that refer to absolute points in time, relative times,
durations, and sets of these. Absolute temporal expressions are those that can be
mapped directly to calendar dates, times of day, or both. Relative temporal expres-
sions map to particular times through some other reference point (as in a week from
last Tuesday). Finally, durations denote spans of time at varying levels of granular-
ity (seconds, minutes, days, weeks, centuries, etc.). Figure 17.18 lists some sample
temporal expressions in each of these categories.

Absolute

April 24, 1916
The summer of ’77
10:15 AM
The 3rd quarter of 2006

Relative

yesterday
next semester
two weeks from yesterday
last quarter

Durations

four hours
three weeks
six days
the last three quarters

Figure 17.18 Examples of absolute, relational and durational temporal expressions.

lexical triggers

Temporal expressions are grammatical constructions that have temporal lexical
triggers as their heads. Lexical triggers might be nouns, proper nouns, adjectives,
and adverbs; full temporal expressions consist of their phrasal projections: noun
phrases, adjective phrases, and adverbial phrases. Figure 17.19 provides examples.

Category

Examples

Noun
morning, noon, night, winter, dusk, dawn
Proper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet
Adjective
recent, past, annual, former
Adverb
hourly, daily, monthly, yearly

Figure 17.19 Examples of temporal lexical triggers.

Let’s look at the TimeML annotation scheme, in which temporal expressions are
annotated with an XML tag, TIMEX3, and various attributes to that tag (Pustejovsky
et al. 2005, Ferro et al. 2005). The following example illustrates the basic use of this
scheme (we defer discussion of the attributes until Section 17.3.2).

A fare increase initiated <TIMEX3>last week</TIMEX3> by UAL
Corp’s United Airlines was matched by competitors over <TIMEX3>the
weekend</TIMEX3>, marking the second successful fare increase in
<TIMEX3>two weeks</TIMEX3>.

The temporal expression recognition task consists of ﬁnding the start and end of
all of the text spans that correspond to such temporal expressions. Rule-based ap-
proaches to temporal expression recognition use cascades of automata to recognize
patterns at increasing levels of complexity. Tokens are ﬁrst part-of-speech tagged,
and then larger and larger chunks are recognized from the results from previous
stages, based on patterns containing trigger words (e.g., February) or classes (e.g.,
MONTH). Figure 17.20 gives a fragment from a rule-based system.
Sequence-labeling approaches follow the same IOB scheme used for named-
entity tags, marking words that are either inside, outside or at the beginning of a
TIMEX3-delimited temporal expression with the I, O, and B tags as follows:

A
O

fare
O

increase
O

initiated
O

last
B

week
I

by
O

UAL
O

Corp’s...
O

346 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

# yesterday/today/tomorrow
$string =˜ s/((($OT+the$CT+\s+)?$OT+day$CT+\s+$OT+(before|after)$CT+\s+)?$OT+$TERelDayExpr$CT+
(\s+$OT+(morning|afternoon|evening|night)$CT+)?)/<TIMEX$tever TYPE=\"DATE\">$1
<\/TIMEX$tever>/gio;

$string =˜ s/($OT+\w+$CT+\s+)<TIMEX$tever TYPE=\"DATE\"[ˆ>]*>($OT+(Today|Tonight)$CT+)
<\/TIMEX$tever>/$1$4/gso;

# this (morning/afternoon/evening)
$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+this$CT+\s*$OT+(morning|afternoon|evening)$CT+)/
<TIMEX$tever TYPE=\"DATE\">$1<\/TIMEX$tever>/gosi;
$string =˜ s/(($OT+(early|late)$CT+\s+)?$OT+last$CT+\s*$OT+night$CT+)/<TIMEX$tever
TYPE=\"DATE\">$1<\/TIMEX$tever>/gsio;

Figure 17.20 Perl fragment from the GUTime temporal tagging system in Tarsqi (Verhagen et al., 2005).

Features are extracted from the token and its context, and a statistical sequence
labeler is trained (any sequence model can be used). Figure 17.21 lists standard
features used in temporal tagging.

Feature

Explanation

Token
The target token to be labeled
Tokens in window Bag of tokens in the window around a target
Shape
Character shape features
POS
Parts of speech of target and window words
Chunk tags
Base-phrase chunk tag for target and words in a window
Lexical triggers
Presence in a list of temporal terms

Figure 17.21 Typical features used to train IOB-style temporal expression taggers.

Temporal expression recognizers are evaluated with the usual recall, precision,
and F-measures. A major difﬁculty for all of these very lexicalized approaches is
avoiding expressions that trigger false positives:
(17.15) 1984 tells the story of Winston Smith...
(17.16) ...U2’s classic Sunday Bloody Sunday

17.3.2 Temporal Normalization

Temporal normalization is the process of mapping a temporal expression to either
a speciﬁc point in time or to a duration. Points in time correspond to calendar dates,
to times of day, or both. Durations primarily consist of lengths of time but may also
include information about start and end points. Normalized times are represented
with the VA LU E attribute from the ISO 8601 standard for encoding temporal values
(ISO8601, 2004). Fig. 17.22 reproduces our earlier example with the value attributes
added in.

temporal
normalization

<TIMEX3 i d = ’ ’ t 1 ’ ’
t y p e = ”DATE” v a l u e = ” 2007−07−02 ” f u n c t i o n I n D o c u m e n t = ” CREATION TIME ”
> J u l y 2 , 2 0 0 7 < / TIMEX3> A f a r e
i n c r e a s e
i n i t i a t e d <TIMEX3 i d = ” t 2 ” t y p e = ”DATE”
v a l u e = ” 2007−W26” a n c h o r T i m e I D = ” t 1 ”> l a s t w e e k< / TIMEX3> b y U n i t e d A i r l i n e s w a s
m a t c h e d b y c o m p e t i t o r s o v e r <TIMEX3 i d = ” t 3 ” t y p e = ”DURATION” v a l u e = ”P1WE”
a n c h o r T i m e I D = ” t 1 ”> t h e w e e k e n d < / TIMEX3> , m a r k i n g t h e s e c o n d s u c c e s s f u l
f a r e
i n c r e a s e
i n <TIMEX3 i d = ” t 4 ” t y p e = ”DURATION” v a l u e = ”P2W” a n c h o r T i m e I D = ” t 1 ”> t w o
w e e k s < / TIMEX3> .

Figure 17.22 TimeML markup including normalized values for temporal expressions.

The dateline, or document date, for this text was July 2, 2007. The ISO repre-
sentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.

17 .3

• EX TRAC T ING T IM E S

347

The encodings for the temporal expressions in our sample text all follow from this
date, and are shown here as values for the VALU E attribute.
The ﬁrst temporal expression in the text proper refers to a particular week of the
year. In the ISO standard, weeks are numbered from 01 to 53, with the ﬁrst week
of the year being the one that has the ﬁrst Thursday of the year. These weeks are
represented with the template YYYY-Wnn. The ISO week for our document date is
week 27; thus the value for last week is represented as “2007-W26”.
The next temporal expression is the weekend. ISO weeks begin on Monday;
thus, weekends occur at the end of a week and are fully contained within a single
week. Weekends are treated as durations, so the value of the VA LU E attribute has
to be a length. Durations are represented according to the pattern Pnx, where n is
an integer denoting the length and x represents the unit, as in P3Y for three years
or P2D for two days. In this example, one weekend is captured as P1WE. In this
case, there is also sufﬁcient information to anchor this particular weekend as part of
a particular week. Such information is encoded in the ANCHORT IM E ID attribute.
Finally, the phrase two weeks also denotes a duration captured as P2W. There is a
lot more to the various temporal annotation standards—far too much to cover here.
Figure 17.23 describes some of the basic ways that other times and durations are
represented. Consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al.
(2005) for more details.

Unit

Fully speciﬁed dates
Weeks
Weekends
24-hour clock times
Dates and times
Financial quarters

Pattern

YYYY-MM-DD
YYYY-Wnn
PnWE
HH:MM:SS
YYYY-MM-DDTHH:MM:SS
Qn

Sample Value

1991-09-28
2007-W27
P1WE
11:13:45
1991-09-28T11:00:00
1999-Q3

Figure 17.23 Sample ISO patterns for representing various times and durations.

Most current approaches to temporal normalization are rule-based (Chang and
Manning 2012, Str ¨otgen and Gertz 2013). Patterns that match temporal expres-
sions are associated with semantic analysis procedures. As in the compositional
rule-to-rule approach introduced in Chapter 15, the meaning of a constituent is com-
puted from the meaning of its parts using a method speciﬁc to the constituent, al-
though here the semantic composition rules involve temporal arithmetic rather than
λ -calculus attachments.
Fully qualiﬁed date expressions contain a year, month, and day in some con-
ventional form. The units in the expression must be detected and then placed in the
correct place in the corresponding ISO pattern. The following pattern normalizes
expressions like April 24, 1916.

FQTE → Mont h Dat e , Year
{Year.val − Mont h.val − Dat e.val }
The non-terminals Month, Date, and Year represent constituents that have already
been recognized and assigned semantic values, accessed through the *.val notation.
The value of this FQE constituent can, in turn, be accessed as FQTE.val during
further processing.
Fully qualiﬁed temporal expressions are fairly rare in real texts. Most temporal
expressions in news articles are incomplete and are only implicitly anchored, of-
ten with respect to the dateline of the article, which we refer to as the document’s

fully qualiﬁed

348 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

temporal
anchor

temporal anchor. The values of temporal expressions such as today, yesterday, or
tomorrow can all be computed with respect to this temporal anchor. The semantic
procedure for today simply assigns the anchor, and the attachments for tomorrow
and yesterday add a day and subtract a day from the anchor, respectively. Of course,
given the cyclic nature of our representations for months, weeks, days, and times of
day, our temporal arithmetic procedures must use modulo arithmetic appropriate to
the time unit being used.
Unfortunately, even simple expressions such as the weekend or Wednesday in-
troduce a fair amount of complexity. In our current example, the weekend clearly
refers to the weekend of the week that immediately precedes the document date. But
this won’t always be the case, as is illustrated in the following example.
(17.17) Random security checks that began yesterday at Sky Harbor will continue
at least through the weekend.
In this case, the expression the weekend refers to the weekend of the week that the
anchoring date is part of (i.e., the coming weekend). The information that signals
this meaning comes from the tense of continue, the verb governing the weekend.
Relative temporal expressions are handled with temporal arithmetic similar to
that used for today and yesterday. The document date indicates that our example
article is ISO week 27, so the expression last week normalizes to the current week
minus 1. To resolve ambiguous next and last expressions we consider the distance
from the anchoring date to the nearest unit. Next Friday can refer either to the
immediately next Friday or to the Friday following that, but the closer the document
date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such
ambiguities are handled by encoding language and domain-speciﬁc heuristics into
the temporal attachments.

17.4 Extracting Events and their Times

event
extraction

The task of event extraction is to identify mentions of events in texts. For the
purposes of this task, an event mention is any expression denoting an event or state
that can be assigned to a particular point, or interval, in time. The following markup
of the sample text on page 345 shows all the events in this text.
[EVENT Citing] high fuel prices, United Airlines [EVENT said] Fri-
day it has [EVENT increased] fares by $6 per round trip on ﬂights to
some cities also served by lower-cost carriers. American Airlines, a unit
of AMR Corp., immediately [EVENT matched]
[EVENT the move],
spokesman Tim Wagner [EVENT said]. United, a unit of UAL Corp.,
[EVENT said] [EVENT the increase] took effect Thursday and [EVENT
applies] to most routes where it [EVENT competes] against discount
carriers, such as Chicago to Dallas and Denver to San Francisco.
In English, most event mentions correspond to verbs, and most verbs introduce
events. However, as we can see from our example, this is not always the case. Events
can be introduced by noun phrases, as in the move and the increase, and some verbs
fail to introduce events, as in the phrasal verb took effect, which refers to when the
event began rather than to the event itself. Similarly, light verbs such as make, take,
and have often fail to denote events; for light verbs the event is often expressed by
the nominal direct object (took a ﬂight), and these light verbs just provide a syntactic
structure for the noun’s arguments.

reporting
events

17 .4

• EX TRAC T ING EV EN T S AND THE IR T IM E S

349

Various versions of the event extraction task exist, depending on the goal. For
example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract
events and aspects like their aspectual and temporal properties. Events are to be
classiﬁed as actions, states, reporting events (say, report, tell, explain), perception
events, and so on. The aspect, tense, and modality of each event also needs to be
extracted. Thus for example the various said events in the sample text would be
annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).
Event extraction is generally modeled via supervised learning, detecting events
via sequence models with IOB tagging, and assigning event classes and attributes
with multi-class classiﬁers. Common features include surface information like parts
of speech, lexical items, and verb tense information; see Fig. 17.24.

Feature

Explanation

Character afﬁxes
Character-level preﬁxes and sufﬁxes of target word
Nominalization sufﬁx
Character level sufﬁxes for nominalizations (e.g., -tion)
Part of speech
Part of speech of the target word
Light verb
Binary feature indicating that the target is governed by a light verb
Subject syntactic category Syntactic category of the subject of the sentence
Morphological stem
Stemmed version of the target word
Verb root
Root form of the verb basis for a nominalization
WordNet hypernyms
Hypernym set for the target

Figure 17.24 Features commonly used in both rule-based and machine learning approaches to event detec-
tion.

17.4.1 Temporal Ordering of Events

With both the events and the temporal expressions in a text having been detected, the
next logical task is to use this information to ﬁt the events into a complete timeline.
Such a timeline would be useful for applications such as question answering and
summarization. This ambitious task is the subject of considerable current research
but is beyond the capabilities of current systems.
A somewhat simpler, but still useful, task is to impose a partial ordering on the
events and temporal expressions mentioned in a text. Such an ordering can provide
many of the same beneﬁts as a true timeline. An example of such a partial ordering
is the determination that the fare increase by American Airlines came after the fare
increase by United in our sample text. Determining such an ordering can be viewed
as a binary relation detection and classiﬁcation task similar to those described earlier
in Section 17.2. The temporal relation between events is classiﬁed into one of the
standard set of Allen relations shown in Fig. 17.25 (Allen, 1984), using feature-
based classiﬁers as in Section 17.2, trained on the TimeBank corpus with features
like words/embeddings, parse paths, tense and aspect.
The TimeBank corpus consists of text annotated with much of the information
we’ve been discussing throughout this section (Pustejovsky et al., 2003b). Time-
Bank 1.2 consists of 183 news articles selected from a variety of sources, including
the Penn TreeBank and PropBank collections.
Each article in the TimeBank corpus has had the temporal expressions and event
mentions in them explicitly annotated in the TimeML annotation (Pustejovsky et al.,
2003a).
In addition to temporal expressions and events, the TimeML annotation
provides temporal links between events and temporal expressions that specify the
nature of the relation between them. Consider the following sample sentence and

Allen relations

TimeBank

350 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

Figure 17.25 The 13 temporal relations from Allen (1984).

<TIMEX3 tid="t57" type="DATE" value="1989-10-26"
10/26/89 </TIMEX3>

functionInDocument="CREATION_TIME">

Delta Air Lines earnings <EVENT eid="e1" class="OCCURRENCE"> soared </EVENT> 33% to a
record in
<TIMEX3 tid="t58" type="DATE" value="1989-Q1" anchorTimeID="t57"> the
fiscal first quarter </TIMEX3>, <EVENT eid="e3"
class="OCCURRENCE">bucking</EVENT>
the industry trend toward <EVENT eid="e4" class="OCCURRENCE">declining</EVENT>
profits.

Figure 17.26 Example from the TimeBank corpus.

its corresponding markup shown in Fig. 17.26, selected from one of the TimeBank
documents.
(17.18) Delta Air Lines earnings soared 33% to a record in the ﬁscal ﬁrst quarter,
bucking the industry trend toward declining proﬁts.
As annotated, this text includes three events and two temporal expressions. The
events are all in the occurrence class and are given unique identiﬁers for use in fur-
ther annotations. The temporal expressions include the creation time of the article,
which serves as the document time, and a single temporal expression within the text.
In addition to these annotations, TimeBank provides four links that capture the
temporal relations between the events and times in the text, using the Allen relations
from Fig. 17.25. The following are the within-sentence temporal relations annotated
for this example.

17 .5

• T EM PLAT E F I LL ING

351

• Soaringe1 is included in the ﬁscal ﬁrst quartert 58
• Soaringe1 is before 1989-10-26t 57
• Soaringe1 is simultaneous with the buckinge3
• Declininge4 includes soaringe1

17.5 Template Filling

scripts

templates

template ﬁlling

Many texts contain reports of events, and possibly sequences of events, that often
correspond to fairly common, stereotypical situations in the world. These abstract
situations or stories, related to what have been called scripts (Schank and Abel-
son, 1977), consist of prototypical sequences of sub-events, participants, and their
roles. The strong expectations provided by these scripts can facilitate the proper
classiﬁcation of entities, the assignment of entities into roles and relations, and most
critically, the drawing of inferences that ﬁll in things that have been left unsaid. In
their simplest form, such scripts can be represented as templates consisting of ﬁxed
sets of slots that take as values slot-ﬁllers belonging to particular classes. The task
of template ﬁlling is to ﬁnd documents that invoke particular scripts and then ﬁll the
slots in the associated templates with ﬁllers extracted from the text. These slot-ﬁllers
may consist of text segments extracted directly from the text, or they may consist of
concepts that have been inferred from text elements through some additional pro-
cessing.
A ﬁlled template from our original airline story might look like the following.

FAR E -RA I SE ATT EM PT: 
This template has four slots (LEAD A IR L IN E, AMOUN T, E FFECT IVE DATE, FOL -

L EAD A IR L IN E :
AMOUNT:
E FFEC T IV E DATE :
FO LLOW ER :

AM ER ICAN A IR L IN E S

LOW ER). The next section describes a standard sequence-labeling approach to ﬁlling
slots. Section 17.5.2 then describes an older system based on the use of cascades of
ﬁnite-state transducers and designed to address a more complex template-ﬁlling task
that current learning-based systems don’t yet address.

UN I TED A IR L IN E S

$6
2 006 -10 -2 6



17.5.1 Machine Learning Approaches to Template Filling

In the standard paradigm for template ﬁlling, we are trying to ﬁll ﬁxed known tem-
plates with known slots, and also assumes training documents labeled with examples
of each template, and the ﬁllers of each slot marked in the text. The is to create one
template for each event in the input documents, with the slots ﬁlled with text from
the document.
The task is generally modeled by training two separate supervised systems. The
ﬁrst system decides whether the template is present in a particular sentence. This
task is called template recognition or sometimes, in a perhaps confusing bit of
terminology, event recognition. Template recognition can be treated as a text classi-
ﬁcation task, with features extracted from every sequence of words that was labeled
in training documents as ﬁlling any slot from the template being detected. The usual
set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,
syntactic chunk tags, and named entity tags.

template
recognition

352 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

role-ﬁller
extraction

The second system has the job of role-ﬁller extraction. A separate classiﬁer is
trained to detect each role (LEAD -A IR L IN E, AMOUN T, and so on). This can be a
binary classiﬁer that is run on every noun-phrase in the parsed input sentence, or a
sequence model run over sequences of words. Each role classiﬁer is trained on the
labeled data in the training set. Again, the usual set of features can be used, but now
trained only on an individual noun phrase or the ﬁllers of a single slot.
Multiple non-identical text segments might be labeled with the same slot la-
bel. For example in our sample text, the strings United or United Airlines might be
labeled as the L EAD A IR L IN E. These are not incompatible choices and the corefer-
ence resolution techniques introduced in Chapter 20 can provide a path to a solution.
A variety of annotated collections have been used to evaluate this style of ap-
proach to template ﬁlling, including sets of job announcements, conference calls for
papers, restaurant guides, and biological texts. Recent work focuses on extracting
templates in cases where there is no training data or even predeﬁned templates, by
inducing templates as sets of linked events (Chambers and Jurafsky, 2011).

17.5.2 Earlier Finite-State Template-Filling Systems

The templates above are relatively simple. But consider the task of producing a
template that contained all the information in a text like this one (Grishman and
Sundheim, 1995):
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan
with a local concern and a Japanese trading house to produce golf clubs to be
shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-
ized at 20 million new Taiwan dollars, will start production in January 1990
with production of 20,000 iron and “metal wood” clubs a month.
The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were
a series of U.S. government-organized information-extraction evaluations) was to
produce hierarchically linked templates describing joint ventures. Figure 17.27
shows a structure produced by the FA S TU S system (Hobbs et al., 1997). Note how
the ﬁller of the AC T IV I TY slot of the T I E -U P template is itself a template with slots.

Tie-up-1

R E LAT ION SH I P
EN T I T I E S

tie-up
Bridgestone Sports Co.
a local concern
a Japanese trading house
JO INT V ENTUR E Bridgestone Sports Taiwan Co.
Activity-1
NT$20000000

AC T IV I TY
AMOUNT

Activity-1:

COM PANY
PRODUC T

Bridgestone Sports Taiwan Co.
iron and “metal wood” clubs
S TART DATE DUR ING : January 1990

Figure 17.27 The templates produced by FA S TU S given the input text on page 352.

Early systems for dealing with these complex templates were based on cascades
of transducers based on hand-written rules, as sketched in Fig. 17.28.
The ﬁrst four stages use hand-written regular expression and grammar rules to
do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and
events with a FST-based recognizer and inserts the recognized objects into the ap-
propriate slots in templates. This FST recognizer is based on hand-built regular
expressions like the following (NG indicates Noun-Group and VG Verb-Group),
which matches the ﬁrst sentence of the news story above.

17 .6

• SUMMARY

353

No. Step
Tokens
Complex Words
Basic phrases
Complex phrases
Semantic Patterns
Merging

1
2
3
4
5
6

Description

Tokenize input stream of characters
Multiword phrases, numbers, and proper names.
Segment sentences into noun and verb groups
Identify complex noun groups and verb groups
Identify entities and events, insert into templates.
Merge references to the same entity or event

Figure 17.28 Levels of processing in FA S TU S (Hobbs et al., 1997). Each level extracts a
speciﬁc type of information which is then passed on to the next higher level.

NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)

VG(Produce) NG(Product)

The result of processing these two sentences is the ﬁve draft templates (Fig. 17.29)
that must then be merged into the single hierarchical structure shown in Fig. 17.27.
The merging algorithm, after performing coreference resolution, merges two activi-
ties that are likely to be describing the same events.

Value

T I E -U P

Bridgestone Co., a local concern, a Japanese trading house

PRODUC T ION

“golf clubs”

# Template/Slot

1 R E LAT ION SH I P :
EN T I T I E S :
2 AC T IV I TY:
PRODUC T:
3 R E LAT ION SH I P :

AMOUN T:
4 AC T IV I TY:
COM PANY:
S TARTDAT E :
5 AC T IV I TY:
PRODUC T:

T I E -U P

JO INT V ENTUR E : “Bridgestone Sports Taiwan Co.”
NT$20000000

PRODUC T ION

“Bridgestone Sports Taiwan Co.”
DUR ING : January 1990

PRODUC T ION

“iron and “metal wood” clubs”

Figure 17.29 The ﬁve partial templates produced by stage 5 of FA S TU S. These templates
are merged in stage 6 to produce the ﬁnal template shown in Fig. 17.27 on page 352.

17.6 Summary

This chapter has explored techniques for extracting limited forms of semantic con-
tent from texts.
• Named entities can be recognized and classiﬁed by featured-based or neural
sequence labeling techniques.
• Relations among entities can be extracted by pattern-based approaches, su-
pervised learning methods when annotated training data is available, lightly
supervised bootstrapping methods when small numbers of seed tuples or
seed patterns are available, distant supervision when a database of relations
is available, and unsupervised or Open IE methods.
• Reasoning about time can be facilitated by detection and normalization of
temporal expressions through a combination of statistical learning and rule-

354 CHA PTER 17

•

IN FORMAT ION EX TRACT ION

based methods.
• Events can be detected and ordered in time using sequence models and classi-
ﬁers trained on temporally- and event-labeled data like the TimeBank corpus.
• Template-ﬁlling applications can recognize stereotypical situations in texts
and assign elements from the text to roles represented as ﬁxed sets of slots.

Bibliographical and Historical Notes

The earliest work on information extraction addressed the template-ﬁlling task in the
context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.
government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-
heim 1993, Sundheim 1995). Early MUC systems like C IRCU S system (Lehnert
et al., 1991) and SC I SOR (Jacobs and Rau, 1990) were quite inﬂuential and inspired
later systems like FA S TU S (Hobbs et al., 1997). Chinchor et al. (1993) describe the
MUC evaluation techniques.
Due to the difﬁculty of porting systems from one domain to another, attention
shifted to machine learning approaches.
Early supervised learning approaches to IE ( Cardie 1993, Cardie 1994, Riloff 1993,
Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acqui-
sition process, mainly for ﬁnite-state rule-based systems. Their success, and the
earlier success of HMM-based speech recognition, led to the use of sequence la-
beling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Laf-
ferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural
approaches to NER mainly follow from the pioneering results of Collobert et al.
(2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and
character-based embeddings as input followed shortly and became a standard neural
algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016).
Neural algorithms for relation extraction often explore architectures that can
handle entities far apart in the sentence: recursive networks (Socher et al., 2012),
convolutional nets (dos Santos et al., 2015), or chain or tree LSTMS (Miwa and
Bansal 2016, Peng et al. 2017).
Progress in this area continues to be stimulated by formal evaluations with shared
benchmark datasets, including the Automatic Content Extraction (ACE) evaluations
of 2000-2007 on named entity recognition, relation extraction, and temporal ex-
pressions3 , the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Sur-
deanu 2013) of relation extraction tasks like slot ﬁlling (extracting attributes (‘slots’)
like age, birthplace, and spouse for a given entity) and a series of SemEval work-
shops (Hendrickx et al., 2009).
Semisupervised relation extraction was ﬁrst proposed by Hearst (1992b), and
extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-
BALL (Agichtein and Gravano, 2000), and (Jones et al., 1999). The distant super-
vision algorithm we describe was drawn from Mintz et al. (2009), who coined the
term ‘distant supervision’, but similar ideas occurred in earlier systems like Craven
and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data,
as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many exten-
sions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open

3 www.nist.gov/speech/tests/ace/

KBP
slot ﬁlling

EX ERC I SE S

355

IE systems include KNOW I TA L L Etzioni et al. (2005), TextRunner (Banko et al.,
2007), and R EV ERB (Fader et al., 2011). See Riedel et al. (2013) for a universal
schema that combines the advantages of distant supervision and Open IE.
HeidelTime (Str ¨otgen and Gertz, 2013) and SUTime (Chang and Manning, 2012)
are downloadable temporal extraction and normalization systems. The 2013 TempE-
val challenge is described in UzZaman et al. (2013); Chambers (2013) and Bethard
(2013) give typical approaches.

Exercises

17.1 Develop a set of regular expressions to recognize the character shape features
described on page 331.
17.2 The IOB labeling scheme given in this chapter isn’t the only possible one. For
example, an E tag might be added to mark the end of entities, or the B tag
can be reserved only for those situations where an ambiguity exists between
adjacent entities. Propose a new set of IOB tags for use with your NER system.
Experiment with it and compare its performance with the scheme presented
in this chapter.
17.3 Names of works of art (books, movies, video games, etc.) are quite different
from the kinds of named entities we’ve discussed in this chapter. Collect a
list of names of works of art from a particular category from a Web-based
source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list
and give examples of ways that the names in it are likely to be problematic for
the techniques described in this chapter.
17.4 Develop an NER system speciﬁc to the category of names that you collected in
the last exercise. Evaluate your system on a collection of text likely to contain
instances of these named entities.
17.5 Acronym expansion, the process of associating a phrase with an acronym, can
be accomplished by a simple form of relational analysis. Develop a system
based on the relation analysis approaches described in this chapter to populate
a database of acronym expansions.
If you focus on English Three Letter
Acronyms (TLAs) you can evaluate your system’s performance by comparing
it to Wikipedia’s TLA page.
17.6 A useful functionality in newer email and calendar applications is the ability
to associate temporal expressions connected with events in email (doctor’s
appointments, meeting planning, party invitations, etc.) with speciﬁc calendar
entries. Collect a corpus of email containing temporal expressions related to
event planning. How do these expressions compare to the kinds of expressions
commonly found in news text that we’ve been discussing in this chapter?
17.7 Acquire the CMU seminar corpus and develop a template-ﬁlling system by
using any of the techniques mentioned in Section 17.5. Analyze how well
your system performs as compared with state-of-the-art results on this corpus.

356 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

CHAPTER

18 Semantic Role Labeling

Sometime between the 7th and 4th centuries BCE, the Indian grammarian P ¯an. ini1
wrote a famous treatise on Sanskrit grammar, the As. t. ¯adhy ¯ay¯ı (‘8 books’), a treatise
that has been called “one of the greatest monuments of
human intelligence” (Bloomﬁeld, 1933b, 11). The work
describes the linguistics of the Sanskrit language in the
form of 3959 sutras, each very efﬁciently (since it had to
be memorized!) expressing part of a formal rule system
that brilliantly preﬁgured modern mechanisms of formal
language theory (Penn and Kiparsky, 2012). One set of
rules, relevant to our discussion in this chapter, describes
the k ¯arakas, semantic relationships between a verb and
noun arguments, roles like agent, instrument, or destina-
tion. P ¯an. ini’s work was the earliest we know of that tried
to understand the linguistic realization of events and their participants. This task
of understanding participants and their relationship to events—being able to answer
the question “Who did what to whom” (and perhaps also “when and where”)—is a
central question of natural language understanding.
Let’s move forward 2.5 millenia to the present and consider the very mundane
goal of understanding text about a purchase of stock by XYZ Corporation. This
purchasing event could take on a wide variety of surface forms. In the following
sentences we see that it could be described by a verb (sold, bought) or a noun (pur-
chase), and that XYZ Corp can be the syntactic subject (of bought), the indirect ob-
ject (of sold), or in a genitive or noun compound relation (with the noun purchase)
despite having notationally the same role in all of them:
• XYZ corporation bought the stock.
• They sold the stock to XYZ corporation.
• The stock was bought by XYZ corporation.
• The purchase of the stock by XYZ corporation...
• The stock purchase by XYZ corporation...
In this chapter we introduce a level of representation that lets us capture the
commonality between these sentences. We will be able to represent the fact that
there was a purchase event, that the participants in this event were XYZ Corp and
some stock, and that XYZ Corp played a speciﬁc role, the role of acquiring the stock.
We call this shallow semantic representation level semantic roles. Semantic
roles are representations that express the abstract role that arguments of a predicate
can take in the event; these can be very speciﬁc, like the BUY ER, abstract like the
AG EN T, or super-abstract (the PROTO -AG EN T). These roles can both represent gen-
eral semantic properties of the arguments and also express their likely relationship to
the syntactic role of the argument in the sentence. AG EN T S tend to be the subject of

1 Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based
on the Sanskrit grammar of Panini. Image from the Wellcome Collection.

18 .1

• S EMANT IC RO LE S

357

an active sentence, THEM E S the direct object, and so on. These relations are codiﬁed
in databases like PropBank and FrameNet. We’ll introduce semantic role labeling,
the task of assigning roles to the constituents or phrases in sentences. We’ll also
discuss selectional restrictions, the semantic sortal restrictions or preferences that
each individual predicate can express about its potential arguments, such as the fact
that the theme of the verb eat is generally something edible. Along the way, we’ll
describe the various ways these representations can help in language understanding
tasks like question answering and machine translation.

18.1 Semantic Roles

Consider how in Chapter 14 we represented the meaning of arguments for sentences
like these:

(18.1) Sasha broke the window.
(18.2) Pat opened the door.

A neo-Davidsonian event representation of these two sentences would be

∃e, x, y Breaking(e) ∧ Breaker(e, Sasha)
∧BrokenT hing(e, y) ∧ W ind ow(y)
∃e, x, y O pening(e) ∧ O pener(e, Pat )
∧O penedT hing(e, y) ∧ Door(y)
In this representation, the roles of the subjects of the verbs break and open are
Breaker and Opener respectively. These deep roles are speciﬁc to each event; Break-
ing events have Breakers, Opening events have Openers, and so on.
If we are going to be able to answer questions, perform inferences, or do any
further kinds of natural language understanding of these events, we’ll need to know
a little more about the semantics of these arguments. Breakers and Openers have
something in common. They are both volitional actors, often animate, and they have
direct causal responsibility for their events.
Thematic roles are a way to capture this semantic commonality between Break-
ers and Eaters. We say that the subjects of both these verbs are agents. Thus, AGEN T
is the thematic role that represents an abstract idea such as volitional causation. Sim-
ilarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are
both prototypically inanimate objects that are affected in some way by the action.
The semantic role for these participants is theme.
Although thematic roles are one of the oldest linguistic models, as we saw above,
their modern formulation is due to Fillmore (1968) and Gruber (1965). Although
there is no universally agreed-upon set of roles, Figs. 18.1 and 18.2 list some the-
matic roles that have been used in various computational papers, together with rough
deﬁnitions and examples. Most thematic role sets have about a dozen roles, but we’ll
see sets with smaller numbers of roles with even more abstract meanings, and sets
with very large numbers of roles that are speciﬁc to situations. We’ll use the general
term semantic roles for all sets of roles, whether small or large.

deep roles

thematic roles
agents

theme

semantic roles

358 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

Thematic Role

AGEN T
EX P ER I ENC ER
FORC E
TH EM E
R E SULT
CON T EN T
IN STRUM EN T
B EN E FIC IARY
SOURCE
GOAL

Deﬁnition

The volitional causer of an event
The experiencer of an event
The non-volitional causer of the event
The participant most directly affected by an event
The end product of an event
The proposition or content of a propositional event
An instrument used in an event
The beneﬁciary of an event
The origin of the object of a transfer event
The destination of an object of a transfer event

Figure 18.1 Some commonly used thematic roles with their deﬁnitions.

Thematic Role

AGEN T
EX P ER I ENC ER
FORC E
TH EM E
R E SULT
CON T EN T
IN STRUM EN T
B EN E FIC IARY
SOURCE
GOAL

Example

The waiter spilled the soup.
John has a headache.
The wind blows debris from the mall into our yards.
Only after Benjamin Franklin broke the ice...
The city built a regulation-size baseball diamond...
Mona asked “You met Mary Ann at a supermarket?”
He poached catﬁsh, stunning them with a shocking device...
Whenever Ann Callahan makes hotel reservations for her boss...
I ﬂew in from Boston.
I drove to Portland.

Figure 18.2 Some prototypical examples of various thematic roles.

18.2 Diathesis Alternations

The main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple inferences that aren’t possible
from the pure surface string of words, or even from the parse tree. To extend the
earlier examples, if a document says that Company A acquired Company B, we’d
like to know that this answers the query Was Company B acquired? despite the fact
that the two sentences have very different surface syntax. Similarly, this shallow
semantics might act as a useful intermediate language in machine translation.
Semantic roles thus help generalize over different surface realizations of pred-
icate arguments. For example, while the AGEN T is often realized as the subject of
the sentence, in other cases the TH EM E can be the subject. Consider these possible
realizations of the thematic arguments of the verb break:
(18.3) John
broke the window.

AG EN T

TH EM E

(18.4) John

broke the window

with a rock.

AG EN T

TH EM E

IN STRUM EN T

(18.5) The rock

broke the window.

IN STRUM EN T

TH EM E

(18.6) The window

broke.

THEM E

(18.7) The window

was broken by John.

THEM E

AG EN T

18 . 3

• S EMANT IC RO LE S : PROB L EM S W I TH TH EMAT IC RO LE S

359

thematic grid
case frame

These examples suggest that break has (at least) the possible arguments AGEN T,
THEM E, and IN STRUM EN T. The set of thematic role arguments taken by a verb is
often called the thematic grid, θ -grid, or case frame. We can see that there are
(among others) the following possibilities for the realization of these arguments of
break:

IN STRUM EN T/PPwith

AGEN T/Subject, THEM E/Object
AGEN T/Subject, TH EM E/Object,
IN STRUM EN T/Subject, TH EM E/Object
TH EM E/Subject
It turns out that many verbs allow their thematic roles to be realized in various
syntactic positions. For example, verbs like give can realize the TH EM E and GOA L
arguments in two different ways:
(18.8)
a. Doris
gave the book

to Cary.

AG EN T

THEM E

GOA L

b. Doris

gave Cary

the book.

AG EN T

GOAL

TH EM E

verb
alternation
dative
alternation

These multiple argument structure realizations (the fact that break can take AG EN T,
IN STRUM EN T, or THEM E as subject, and give can realize its THEM E and GOAL in
either order) are called verb alternations or diathesis alternations. The alternation
we showed above for give, the dative alternation, seems to occur with particular se-
mantic classes of verbs, including “verbs of future having” (advance, allocate, offer,
owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw),
and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which
they belong (47 high-level classes, divided into 193 more speciﬁc classes) and the
various alternations in which they participate. These lists of verb classes have been
incorporated into the online resource VerbNet (Kipper et al., 2000), which links each
verb to both WordNet and FrameNet entries.

18.3 Semantic Roles: Problems with Thematic Roles

Representing meaning at the thematic role level seems like it should be useful in
dealing with complications like diathesis alternations. Yet it has proved quite difﬁ-
cult to come up with a standard set of roles, and equally difﬁcult to produce a formal
deﬁnition of roles like AG EN T, TH EM E, or IN STRUM EN T.
For example, researchers attempting to deﬁne role sets often ﬁnd they need to
fragment a role like AGEN T or TH EM E into many speciﬁc roles. Levin and Rappa-
port Hovav (2005) summarize a number of such cases, such as the fact there seem
to be at least two kinds of IN STRUM EN T S, intermediary instruments that can appear
as subjects and enabling instruments that cannot:
(18.9)
a. The cook opened the jar with the new gadget.
b. The new gadget opened the jar.
a. Shelly ate the sliced banana with a fork.
b. *The fork ate the sliced banana.
In addition to the fragmentation problem, there are cases in which we’d like to
reason about and generalize across semantic roles, but the ﬁnite discrete lists of roles
don’t let us do this.

(18.10)

360 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

semantic role

proto-agent
proto-patient

Finally, it has proved difﬁcult to formally deﬁne the thematic roles. Consider the
AG EN T role; most cases of AGEN T S are animate, volitional, sentient, causal, but any
individual noun phrase might not exhibit all of these properties.
These problems have led to alternative semantic role models that use either
many fewer or many more roles.
The ﬁrst of these options is to deﬁne generalized semantic roles that abstract
over the speciﬁc thematic roles. For example, PROTO -AG EN T and PROTO - PAT I EN T
are generalized roles that express roughly agent-like and roughly patient-like mean-
ings. These roles are deﬁned, not by necessary and sufﬁcient conditions, but rather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. Thus, the more an argument displays agent-like properties (being voli-
tionally involved in the event, causing an event or a change of state in another par-
ticipant, being sentient or intentionally involved, moving) the greater the likelihood
that the argument can be labeled a PROTO -AGEN T. The more patient-like the proper-
ties (undergoing change of state, causally affected by another participant, stationary
relative to other participants, etc.), the greater the likelihood that the argument can
be labeled a PROTO - PAT I ENT.
The second direction is instead to deﬁne semantic roles that are speciﬁc to a
particular verb or a particular group of semantically related verbs or nouns.
In the next two sections we describe two commonly used lexical resources that
make use of these alternative versions of semantic roles. PropBank uses both proto-
roles and verb-speciﬁc semantic roles. FrameNet uses semantic roles that are spe-
ciﬁc to a general semantic idea called a frame.

18.4 The Proposition Bank

PropBank

The Proposition Bank, generally referred to as PropBank, is a resource of sen-
tences annotated with semantic roles. The English PropBank labels all the sentences
in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese
TreeBank. Because of the difﬁculty of deﬁning a universal set of thematic roles,
the semantic roles in PropBank are deﬁned with respect to an individual verb sense.
Each sense of each verb thus has a speciﬁc set of roles, which are given only numbers
rather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the
PROTO -AG EN T, and Arg1, the PROTO - PAT I ENT. The semantics of the other roles
are less consistent, often being deﬁned speciﬁcally for each verb. Nonetheless there
are some generalization; the Arg2 is often the benefactive, instrument, attribute, or
end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4
the end point.
Here are some slightly simpliﬁed PropBank entries for one sense each of the
verbs agree and fall. Such PropBank entries are called frame ﬁles; note that the
deﬁnitions in the frame ﬁle for each role (“Other entity agreeing”, “Extent, amount
fallen”) are informal glosses intended to be read by humans, rather than being formal
deﬁnitions.

(18.11) agree.01

18 .4

• TH E PRO PO S I T ION BANK

361

Arg0: Agreer
Arg1: Proposition
Arg2: Other entity agreeing

Ex1:
Ex2:

[Arg0 The group] agreed [Arg1 it wouldn’t make an offer].
[ArgM-TMP Usually] [Arg0 John] agrees [Arg2 with Mary]
[Arg1 on everything].

(18.12) fall.01

Arg1: Logical subject, patient, thing falling
Arg2: Extent, amount fallen
Arg3: start point
Arg4: end point, end state of arg1
Ex1:
[Arg1 Sales] fell [Arg4 to $25 million] [Arg3 from $27 million].
Ex2:
[Arg1 The average junk bond] fell [Arg2 by 4.2%].
Note that there is no Arg0 role for fall, because the normal subject of fall is a

PROTO - PAT I ENT.

The PropBank semantic roles can be useful in recovering shallow semantic in-
formation about verbal arguments. Consider the verb increase:
(18.13) increase.01 “go up incrementally”
Arg0: causer of increase
Arg1:
thing increasing
Arg2: amount increased by, EXT, or MNR
Arg3: start point
Arg4: end point
A PropBank semantic role labeling would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case Big
Fruit Co. is the AG EN T and the price of bananas is the THEM E, despite the differing
surface forms.
(18.14)
[Arg0 Big Fruit Co. ] increased [Arg1 the price of bananas].
(18.15)
[Arg1 The price of bananas] was increased again [Arg0 by Big Fruit Co. ]
(18.16)
[Arg1 The price of bananas] increased [Arg2 5%].
PropBank also has a number of non-numbered arguments called ArgMs, (ArgM-
TMP, ArgM-LOC, etc) which represent modiﬁcation or adjunct meanings. These are
relatively stable across predicates, so aren’t listed with each frame ﬁle. Data labeled
with these modiﬁers can be helpful in training systems to detect temporal, location,
or directional modiﬁcation across predicates. Some of the ArgM’s include:

when?
where?
where to/from?
how?

TMP
LOC
DIR
MNR
PRP/CAU why?
REC
ADV
PRD

yesterday evening, now
at the museum, in San Francisco
down, to Bangkok
clearly, with much enthusiasm
because ... , in response to the ruling
themselves, each other

NomBank

miscellaneous
secondary predication
...ate the meat raw
While PropBank focuses on verbs, a related project, NomBank (Meyers et al.,
2004) adds annotations to noun predicates. For example the noun agreement in
Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as

362 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

the Arg2. This allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.

18.5 FrameNet

While making inferences about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
inferences in many more situations, across different verbs, and also between verbs
and nouns. For example, we’d like to extract the similarity among these three sen-
tences:
(18.17)
[Arg1 The price of bananas] increased [Arg2 5%].
(18.18)
[Arg1 The price of bananas] rose [Arg2 5%].
(18.19) There has been a [Arg2 5%] rise [Arg1 in the price of bananas].
Note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. We’d like a system to recognize that the
price of bananas is what went up, and that 5% is the amount it went up, no matter
whether the 5% appears as the object of the verb increased or as a nominal modiﬁer
of the noun rise.
The FrameNet project is another semantic-role-labeling project that attempts
to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,
Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank
project are speciﬁc to an individual verb, roles in the FrameNet project are speciﬁc

to a frame.

What is a frame? Consider the following set of words:
reservation, ﬂight, travel, buy, price, cost, fare, rates, meal, plane
There are many individual lexical relations of hyponymy, synonymy, and so on
between many of the words in this list. The resulting set of relations does not,
however, add up to a complete account of how these words are related. They are
clearly all deﬁned with respect to a coherent chunk of common-sense background
information concerning air travel.
We call the holistic background knowledge that unites these words a frame (Fill-
more, 1985). The idea that groups of words are deﬁned with respect to some back-
ground information is widespread in artiﬁcial intelligence and cognitive science,
where besides frame we see related works like a model (Johnson-Laird, 1983), or
even script (Schank and Abelson, 1977).
A frame in FrameNet is a background knowledge structure that deﬁnes a set of
frame-speciﬁc semantic roles, called frame elements, and includes a set of predi-
cates that use these roles. Each word evokes a frame and proﬁles some aspect of the
frame and its elements. The FrameNet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. For example, the change position on a scale frame is deﬁned as
follows:
This frame consists of words that indicate the change of an Item’s posi-
tion on a scale (the Attribute) from a starting point (Initial value) to an
end point (Final value).
Some of the semantic roles (frame elements) in the frame are deﬁned as in
Fig. 18.3. Note that these are separated into core roles, which are frame speciﬁc, and

FrameNet

frame

model
script

frame elements

core roles

non-core roles

non-core roles, which are more like the Arg-M arguments in PropBank, expressed
more general properties of time, location, and so on.

18 .5

• FRAM EN ET

363

Core Roles

F INA L VA LU E

ATTR IBU T E
D I FFER ENC E
F INA L S TAT E

The ATTR IBU T E is a scalar property that the I TEM possesses.
The distance by which an I TEM changes its position on the scale.
A description that presents the I T EM’s state after the change in the AT TR IBUT E’s
value as an independent predication.
The position on the scale where the I TEM ends up.
IN I T IA L STATE A description that presents the I TEM’s state before the change in the AT-
TR IBU TE’s value as an independent predication.
IN I T IA L VALU E The initial position on the scale from which the I TEM moves away.
The entity that has a position on the scale.
VA LU E RANG E A portion of the scale, typically identiﬁed by its end points, along which the
values of the AT TR IBU T E ﬂuctuate.

I T EM

DURAT ION
S PE ED
GROU P

Some Non-Core Roles

The length of time over which the change takes place.
The rate of change of the VA LU E.
The GROU P in which an I TEM changes the value of an
ATTR IBU T E in a speciﬁed way.

Figure 18.3 The frame elements in the change position on a scale frame from the FrameNet Labelers
Guide (Ruppenhofer et al., 2016).

Here are some example sentences:
(18.20)
[I T EM Oil] rose [ATTR IBU T E in price] [D I FFER ENC E by 2%].
(18.21)
[I TEM It] has increased [F INA L STATE to having them 1 day a month].
(18.22)
[I T EM Microsoft shares] fell [F INA L VA LU E to 7 5/8].
(18.23)
[I T EM Colon cancer incidence] fell [D I FFER ENC E by 50%] [GROU P among
men].
(18.24) a steady increase [IN I T IA L VALU E from 9.5] [F INA L VA LU E to 14.3] [I TEM
in dividends]
(18.25) a [D I FFER ENC E 5%] [I TEM dividend] increase...
Note from these example sentences that the frame includes target words like rise,
fall, and increase. In fact, the complete frame consists of the following words:

VERBS: dwindle move
soar
advance
edge
mushroom swell
climb
explode plummet
swing
decline
fall
reach
triple
decrease ﬂuctuate rise
tumble
diminish gain
rocket
dip
grow
shift
double
increase skyrocket
drop
jump
slide

decline
decrease

NOUNS: hike

increase
rise

escalation shift
explosion
tumble
fall
ﬂuctuation ADVERBS:
gain
increasingly
growth

FrameNet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be representing by inher-
itance as well). Thus, there is a Cause change of position on a scale frame that is
linked to the Change of position on a scale frame by the cause relation, but that
adds an AG EN T role and is used for causative examples such as the following:

364 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

(18.26)

[AG EN T They] raised [I T EM the price of their soda] [D I FFER ENC E by 2%].
Together, these two frames would allow an understanding system to extract the
common event semantics of all the verbal and nominal causative and non-causative
usages.
FrameNets have also been developed for many other languages including Span-
ish, German, Japanese, Portuguese, Italian, and Chinese.

18.6 Semantic Role Labeling

semantic role
labeling

Semantic role labeling (sometimes shortened as SRL) is the task of automatically
ﬁnding the semantic roles of each argument of each predicate in a sentence. Cur-
rent approaches to semantic role labeling are based on supervised machine learning,
often using the FrameNet and PropBank resources to specify what counts as a pred-
icate, deﬁne the set of roles used in the task, and provide training and test sets.
Recall that the difference between these two models of semantic roles is that
FrameNet (18.27) employs many frame-speciﬁc frame elements as roles, while Prop-
Bank (18.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speciﬁc labels, along with the more general ARGM labels. Some
examples:

(18.27)

(18.28)

[You]

can’t

[blame]

[the program] [for being unable to identify it]

COGN I ZER

TARG ET EVA LUE E

R EA SON

[The San Francisco Examiner]

issued

[a special edition]

ARG0

TARG E T ARG1

[yesterday]

ARGM - TM P

18.6.1 A Feature-based Algorithm for Semantic Role Labeling

A simpliﬁed feature-based semantic role labeling algorithm is sketched in Fig. 18.4.
Feature-based algorithms—from the very earliest systems like (Simmons, 1973)—
begin by parsing, using broad-coverage parsers to assign a parse to the input string.
Figure 18.5 shows a parse of (18.28) above. The parse is then traversed to ﬁnd all
words that are predicates.
For each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classiﬁcation to decide the semantic role (if any) it plays
for this predicate. Given a labeled training set such as PropBank or FrameNet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. A 1-of-N classiﬁer is then trained to predict a semantic role for
each constituent given these features, where N is the number of potential semantic
roles plus an extra NONE role for non-role constituents. Any standard classiﬁcation
algorithms can be used. Finally, for each test sentence to be labeled, the classiﬁer is
run on each relevant constituent.
Instead of training a single-stage classiﬁer as in Fig. 18.5, the node-level classi-
ﬁcation task can be broken down into multiple steps:
1. Pruning: Since only a small number of the constituents in a sentence are
arguments of any given predicate, many systems use simple heuristics to prune
unlikely constituents.
2. Identiﬁcation: a binary classiﬁcation of each node as an argument to be la-
beled or a NON E.

18 .6

• S EMAN T IC RO LE LABE L ING

365

function S EMANT ICRO LELABE L(words) returns labeled tree
parse ← PAR S E(words)
for each predicate in parse do

for each node in parse do

featurevector ← EX TRACTF EATUR E S(node, predicate, parse)
C LA S S I FYNODE(node, featurevector, parse)

Figure 18.4 A generic semantic-role-labeling algorithm. C LA S S I FYNODE is a 1-of-N clas-
siﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labeled data
such as FrameNet or PropBank.

Figure 18.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line
shows the path feature NP↑S↓VP↓VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.

3. Classiﬁcation: a 1-of-N classiﬁcation of all the constituents that were labeled
as arguments by the previous stage
The separation of identiﬁcation and classiﬁcation may lead to better use of fea-
tures (different features may be useful for the two tasks) or to computational efﬁ-
ciency.

Global Optimization

The classiﬁcation algorithm of Fig. 18.5 classiﬁes each argument separately (‘lo-
cally’), making the simplifying assumption that each argument of a predicate can be
labeled independently. This assumption is false; there are interactions between argu-
ments that require a more ‘global’ assignment of labels to constituents. For example,
constituents in FrameNet and PropBank are required to be non-overlapping. More
signiﬁcantly, the semantic roles of constituents are not independent. For example
PropBank does not allow multiple identical arguments; two constituents of the same
verb cannot both be labeled ARG0 .
Role labeling systems thus often add a fourth step to deal with global consistency
across the labels in a sentence. For example, the local classiﬁers can return a list of
possible labels associated with probabilities for each constituent, and a second-pass
Viterbi decoding or re-ranking approach can be used to choose the best consensus
label. Integer linear programming (ILP) is another common way to choose a solution
that conforms best to multiple constraints.

366 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

Features for Semantic Role Labeling

Most systems use some generalization of the core set of features introduced by
Gildea and Jurafsky (2000). Common basic features templates (demonstrated on
the NP-SBJ constituent The San Francisco Examiner in Fig. 18.5) include:
• The governing predicate, in this case the verb issued. The predicate is a cru-
cial feature since labels are deﬁned only with respect to a particular predicate.
• The phrase type of the constituent, in this case, NP (or NP-SBJ). Some se-
mantic roles tend to appear as NPs, others as S or PP, and so on.
• The headword of the constituent, Examiner. The headword of a constituent
can be computed with standard head rules, such as those given in Chapter 10
in Fig. 10.12. Certain headwords (e.g., pronouns) place strong constraints on
the possible semantic roles they are likely to ﬁll.
• The headword part of speech of the constituent, NNP.
• The path in the parse tree from the constituent to the predicate. This path is
marked by the dotted line in Fig. 18.5. Following Gildea and Jurafsky (2000),
we can use a simple linear representation of the path, NP↑S↓VP↓VBD. ↑ and
↓ represent upward and downward movement in the tree, respectively. The
path is very useful as a compact representation of many kinds of grammatical
function relationships between the constituent and the predicate.
• The voice of the clause in which the constituent appears, in this case, active
(as contrasted with passive). Passive sentences tend to have strongly different
linkings of semantic roles to surface form than do active ones.
• The binary linear position of the constituent with respect to the predicate,
• The subcategorization of the predicate, the set of expected arguments that
appear in the verb phrase. We can extract this information by using the phrase-
structure rule that expands the immediate parent of the predicate; VP → VBD
NP PP for the predicate in Fig. 18.5.
• The named entity type of the constituent.
• The ﬁrst words and the last word of the constituent.
The following feature vector thus represents the ﬁrst NP in our example (recall
that most observations will have the value NONE rather than, for example, ARG0 ,
since most constituents in the parse tree will not bear a semantic role):

either before or after.

ARG0: [issued, NP, Examiner, NNP, NP↑S↓VP↓VBD, active, before, VP → NP PP,
ORG, The, Examiner]

Other features are often used in addition, such as sets of n-grams inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).
It’s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.

18.6.2 A Neural Algorithm for Semantic Role Labeling

The standard neural algorithm for semantic role labeling is based on the bi-LSTM
IOB tagger introduced in Chapter 9, which we’ve seen applied to part-of-speech
tagging and named entity tagging, among other tasks. Recall that with IOB tagging,

18 .6

• S EMAN T IC RO LE LABE L ING

367

Figure 18.6 A bi-LSTM approach to semantic role labeling. Most actual networks are
much deeper than shown in this ﬁgure; 3 to 4 bi-LSTM layers (6 to 8 total LSTMs) are
common. The input is a concatenation of an embedding for the input word and an embedding
of a binary variable which is 1 for the predicate to 0 for all other words. After He et al. (2017).

we have a begin and end tag for each possible role (B -ARG0 , I -ARG0 ; B -ARG1 ,
I -ARG1, and so on), plus an outside tag O.
As with all the taggers, the goal is to compute the highest probability tag se-
quence ˆy, given the input sequence of words w:

ˆy = argmax

y∈T

P(y|w)

In algorithms like He et al. (2017), each input word is mapped to pre-trained em-
beddings, and also associated with an embedding for a ﬂag (0/1) variable indicating
whether that input word is the predicate. These concatenated embeddings are passed
through multiple layers of bi-directional LSTM. State-of-the-art algorithms tend to
be deeper than for POS or NER tagging, using 3 to 4 layers (6 to 8 total LSTMs).
Highway layers can be used to connect these layers as well.
Output from the last bi-LSTM can then be turned into an IOB sequence as for
POS or NER tagging. Tags can be locally optimized by taking the bi-LSTM output,
passing it through a single layer into a softmax for each word that creates a proba-
bility distribution over all SRL tags and the most likely tag for word xi is chosen as
ti , computing for each word essentially:

ˆyi = argmax

t ∈t ags

P(t |wi )

However, just as feature-based SRL tagging, this local approach to decoding doesn’t
exploit the global constraints between tags; a tag I-ARG0, for example, must follow
another I-ARG0 or B-ARG0.
As we saw for POS and NER tagging, there are many ways to take advantage of
these global constraints. A CRF layer can be used instead of a softmax layer on top
of the bi-LSTM output, and the Viterbi decoding algorithm can be used to decode
from the CRF.
An even simpler Viterbi decoding algorithm that may perform equally well and
doesn’t require adding CRF complexity to the training process is to start with the
simple softmax. The softmax output (the entire probability distribution over tags)
for each word is then treated it as a lattice and we can do Viterbi decoding through
the lattice. The hard IOB constraints can act as the transition probabilities in the

368 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

Viterbi decoding (Thus the transition from state I-ARG0 to I-ARG1 would have
probability 0). Alternatively, the training data can be used to learn bigram or trigram
tag transition probabilities as if doing HMM decoding. Fig. 18.6 shows a sketch of
the algorithm.

18.6.3 Evaluation of Semantic Role Labeling

The standard evaluation for semantic role labeling is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and F-measure. Identiﬁcation and classiﬁcation can
also be evaluated separately. Two common datasets used for evaluation are CoNLL-
2005 (Carreras and M `arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).

18.7 Selectional Restrictions

selectional
restriction

We turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. A selectional restriction is a semantic type con-
straint that a verb imposes on the kind of concepts that are allowed to ﬁll its argument
roles. Consider the two meanings associated with the following example:
(18.29) I want to eat someplace nearby.
There are two possible parses and semantic interpretations for this sentence.
In
the sensible interpretation, eat is intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. In the nonsensical speaker-as-
Godzilla interpretation, eat is transitive and the phrase someplace nearby is the direct
object and the THEM E of the eating, like the NP Malaysian food in the following
sentences:
(18.30)
I want to eat Malaysian food.
How do we know that someplace nearby isn’t the direct object in this sentence?
One useful cue is the semantic fact that the THEM E of EAT ING events tends to be
something that is edible. This restriction placed by the verb eat on the ﬁller of its
THEM E argument is a selectional restriction.
Selectional restrictions are associated with senses, not entire lexemes. We can
see this in the following examples of the lexeme serve:
(18.31) The restaurant serves green-lipped mussels.
(18.32) Which airlines serve Denver?
Example (18.31) illustrates the offering-food sense of serve, which ordinarily re-
stricts its TH EM E to be some kind of food Example (18.32) illustrates the provides a
commercial service to sense of serve, which constrains its TH EM E to be some type
of appropriate location.
Selectional restrictions vary widely in their speciﬁcity. The verb imagine, for
example, imposes strict requirements on its AGEN T role (restricting it to humans
and other animate entities) but places very few semantic requirements on its TH EM E
role. A verb like diagonalize, on the other hand, places a very speciﬁc constraint
on the ﬁller of its TH EM E role: it has to be a matrix, while the arguments of the
adjectives odorless are restricted to concepts that could possess an odor:
(18.33) In rehearsal, I often ask the musicians to imagine a tennis game.

18 .7

• S E LEC T IONA L R E S TR IC T ION S

369

(18.34) Radon is an odorless gas that can’t be detected by human senses.
(18.35) To diagonalize a matrix is to ﬁnd its eigenvalues.
These examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
This distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.

18.7.1 Representing Selectional Restrictions

One way to capture the semantics of selectional restrictions is to use and extend the
event representation of Chapter 14. Recall that the neo-Davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. Ignoring the issue of
the λ -structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eat might look like the following:

∃e, x, y E at ing(e) ∧ Agent (e, x) ∧ T heme(e, y)
With this representation, all we know about y, the ﬁller of the THEM E role, is that
it is associated with an E at ing event through the Theme relation. To stipulate the
selectional restriction that y must be something edible, we simply add a new term to
that effect:
∃e, x, y E at ing(e) ∧ Agent (e, x) ∧ T heme(e, y) ∧ E d ibl eT hing(y)
When a phrase like ate a hamburger is encountered, a semantic analyzer can
form the following kind of representation:

∃e, x, y E at ing(e) ∧ E at er(e, x) ∧ T heme(e, y) ∧ E d ibl eT hing(y) ∧ H amburger(y)
This representation is perfectly reasonable since the membership of y in the category
Hamburger is consistent with its membership in the category EdibleThing, assuming
a reasonable set of facts in the knowledge base. Correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as Takeoff would be inconsistent with membership in the
category EdibleThing.
While this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. First, using FOL to perform the simple
task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms
can do the job with far less computational cost. The second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. Unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.
A more practical approach is to state selectional restrictions in terms of WordNet
synsets rather than as logical concepts. Each predicate simply speciﬁes a WordNet
synset as the selectional restriction on each of its arguments. A meaning representa-
tion is well-formed if the role ﬁller word is a hyponym (subordinate) of this synset.
For our ate a hamburger example, for instance, we could set the selectional
restriction on the TH EM E role of the verb eat to the synset {food, nutrient}, glossed
as any substance that can be metabolized by an animal to give energy and build

370 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

Sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich
=> snack food
=> dish
=> nutriment, nourishment, nutrition...
=> food, nutrient
=> substance
=> matter
=> physical entity
=> entity

Figure 18.7 Evidence from WordNet that hamburgers are edible.

tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 18.7 reveals
that hamburgers are indeed food. Again, the ﬁller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
We can apply this approach to the THEM E roles of the verbs imagine, lift, and di-
agonalize, discussed earlier. Let us restrict imagine’s TH EM E to the synset {entity},
lift’s TH EM E to {physical entity}, and diagonalize to {matrix}. This arrangement
correctly permits imagine a hamburger and lift a hamburger, while also correctly
ruling out diagonalize a hamburger.

18.7.2 Selectional Preferences

In the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (Katz and Fodor 1963,
Hirst 1987). For example, the verb eat might require that its TH EM E argument be
[+FOOD]. Early word sense disambiguation systems used this idea to rule out senses
that violated the selectional restrictions of their governing predicates.
Very quickly, however, it became clear that these selectional restrictions were
better represented as preferences rather than strict constraints (Wilks 1975c, Wilks 1975b).
For example, selectional restriction violations (like inedible arguments of eat) often
occur in well-formed sentences, for example because they are negated (18.36), or
because selectional restrictions are overstated (18.37):
(18.36) But it fell apart in 1931, perhaps because people realized you can’t eat
gold for lunch if you’re hungry.
(18.37) In his two championship trials, Mr. Kulkarni ate glass on an empty
stomach, accompanied only by water and tea.
Modern systems for selectional preferences therefore specify the relation be-
tween a predicate and its possible arguments with soft constraints of some kind.

Selectional Association

selectional
preference
strength

One of the most inﬂuential has been the selectional association model of Resnik
(1993). Resnik deﬁnes the idea of selectional preference strength as the general
amount of information that a predicate tells us about the semantic class of its argu-
ments. For example, the verb eat tells us a lot about the semantic class of its direct
objects, since they tend to be edible. The verb be, by contrast, tells us less about
its direct objects. The selectional preference strength can be deﬁned by the differ-
ence in information between two distributions: the distribution of expected semantic

relative entropy
KL divergence

selectional
association

18 .7

• S E LEC T IONA L R E S TR IC T ION S

371

classes P(c) (how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb P(c|v) (how likely is
it that the direct object of the speciﬁc verb v will fall into semantic class c). The
greater the difference between these distributions, the more information the verb is
giving us about possible objects. The difference between these two distributions can
be quantiﬁed by relative entropy, or the Kullback-Leibler divergence (Kullback and
Leibler, 1951). The Kullback-Leibler or KL divergence D(P||Q) expresses the dif-
ference between two probability distributions P and Q (we’ll return to this when we
discuss distributional models of meaning in Chapter 6).
D(P||Q) = (cid:88)x
The selectional preference SR (v) uses the KL divergence to express how much in-
formation, in bits, the verb v expresses about the possible semantic class of its argu-
ment.

P(x)
Q(x)

P(x) log

(18.38)

SR (v) = D(P(c|v)||P(c))
P(c|v) log
P(c|v)
P(c)

= (cid:88)c

(18.39)

1

SR (v)

AR (v, c) =

Resnik then deﬁnes the selectional association of a particular class and verb as the
relative contribution of that class to the general selectional preference of the verb:
P(c|v)
P(c)

P(c|v) log
The selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
Resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the WordNet concepts containing the
word. The following table from Resnik (1996) shows some sample high and low
selectional associations for verbs and some WordNet semantic classes of their direct
objects.

(18.40)

Direct Object
Semantic Class Assoc

Direct Object
Semantic Class Assoc

Verb

read
write
see

WR I T ING
WR I T ING
ENT I TY

6.80
7.26
5.79

AC T IV I TY
COMM ERC E
M E THOD

-.20
0
-0.01

Selectional Preference via Conditional Probability

An alternative to using selectional association between a verb and the WordNet class
of its arguments, is to simply use the conditional probability of an argument word
given a predicate verb. This simple model of selectional preferences can be used
to directly model the strength of association of one verb (predicate) with one noun
(argument).
The conditional probability model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. The conditional probability of an

372 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

argument noun given a verb for a particular relation P(n|v, r) can then be used as a
selectional preference metric for that pair of words (Brockmann and Lapata, 2003):

P(n|v, r) = (cid:40) C(n,v,r)

C(v,r)

if C(n, v, r) > 0
0 otherwise

The inverse probability P(v|n, r) was found to have better performance in some cases
(Brockmann and Lapata, 2003):

P(v|n, r) = (cid:40) C(n,v,r)

C(n,r)

if C(n, v, r) > 0
0 otherwise

pseudowords

In cases where it’s not possible to get large amounts of parsed data, another option,
at least for direct objects, is to get the counts from simple part-of-speech based
approximations. For example pairs can be extracted using the pattern ”V Det N”,
where V is any form of the verb, Det is the—a— and N is the singular or plural
form of the noun (Keller and Lapata, 2003).
An even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument log count (v, n, r) instead of conditional probability;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (Brockmann and Lapata, 2003).

Evaluating Selectional Preferences

One way to evaluate models of selectional preferences is to use pseudowords (Gale
et al. 1992c, Sch ¨utze 1992a). A pseudoword is an artiﬁcial word created by concate-
nating a test word in some context (say banana) with a confounder word (say door)
to create banana-door). The task of the system is to identify which of the two words
is the original word. To evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. For each verb token (say drive) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor, the noun with the
frequency closest to the original (say house), to make car/house). We then use the
selectional preference model to choose which of car and house are more preferred
objects of drive, and compute how often the model chooses the correct original ob-
ject (e.g., (car) (Chambers and Jurafsky, 2010).
Another evaluation metric is to get human preferences for a test set of verb-
argument pairs, and have them rate their degree of plausibility. This is usually done
by using magnitude estimation, a technique from psychophysics, in which subjects
rate the plausibility of an argument proportional to a modulus item. A selectional
preference model can then be evaluated by its correlation with the human prefer-
ences (Keller and Lapata, 2003).

18.8 Primitive Decomposition of Predicates

One way of thinking about the semantic roles we have discussed through the chapter
is that they help us deﬁne the roles that arguments play in a decompositional way,
based on ﬁnite lists of thematic roles (agent, patient, instrument, proto-agent, proto-
patient, etc.) This idea of decomposing meaning into sets of primitive semantics

elements or features, called primitive decomposition or componential analysis,

componential
analysis

18 . 8

• PR IM I T IV E D ECOM PO S I T ION O F PR ED ICAT E S

373

has been taken even further, and focused particularly on predicates.
Consider these examples of the verb kill:
(18.41) Jim killed his philodendron.
(18.42) Jim did something to cause his philodendron to become not alive.
There is a truth-conditional (‘propositional semantics’) perspective from which these
two sentences have the same meaning. Assuming this equivalence, we could repre-
sent the meaning of kill as:
(18.43) K I LL(x,y) ⇔ CAU S E(x, B ECOM E(NOT(A L IV E(y))))
thus using semantic primitives like do, cause, become not, and alive.
Indeed, one such set of potential semantic primitives has been used to account for
some of the verbal alternations discussed in Section 18.2 (Lakoff 1965, Dowty 1979).
Consider the following examples.
(18.44) John opened the door. ⇒ CAU S E(John, B ECOM E(O PEN(door)))
(18.45) The door opened. ⇒ B ECOM E(O PEN(door))
(18.46) The door is open. ⇒ O PEN(door)
The decompositional approach asserts that a single state-like predicate associ-
ated with open underlies all of these examples. The differences among the meanings
of these examples arises from the combination of this single predicate with the prim-

itives CAU S E and B ECOM E.

While this approach to primitive decomposition can explain the similarity be-
tween states and actions or causative and non-causative predicates, it still relies on
having a large number of predicates like open. More radical approaches choose to
break down these predicates as well. One such approach to verbal predicate de-
composition that played a role in early natural language understanding systems is
conceptual dependency (CD), a set of ten primitive predicates, shown in Fig. 18.8.

Primitive

ATRAN S

P TRAN S
M TRAN S

MBU I LD
PRO P EL
MOVE
ING E S T
EX PE L
S P EAK
ATT END

Deﬁnition
The abstract transfer of possession or control from one entity to
another
The physical transfer of an object from one location to another
The transfer of mental concepts between entities or within an
entity
The creation of new information within an entity
The application of physical force to move an object
The integral movement of a body part by an animal
The taking in of a substance by an animal
The expulsion of something from an animal
The action of producing a sound
The action of focusing a sense organ

Figure 18.8 A set of conceptual dependency primitives.

Below is an example sentence along with its CD representation. The verb brought
is translated into the two primitives ATRAN S and PTRAN S to indicate that the waiter
both physically conveyed the check to Mary and passed control of it to her. Note
that CD also associates a ﬁxed set of thematic roles with each primitive to represent
the various participants in the action.
(18.47) The waiter brought Mary the check.

conceptual
dependency

374 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

∃x, y At rans(x) ∧ Act or(x,Wait er) ∧ Ob ject (x,Check) ∧ To(x, Mary)
∧Pt rans(y) ∧ Act or(y,Wait er) ∧ Ob ject (y,Check) ∧ To(y, Mary)

18.9 Summary

• Semantic roles are abstract models of the role an argument plays in the event
described by the predicate.
• Thematic roles are a model of semantic roles based on a single ﬁnite list of
roles. Other semantic role models include per-verb semantic role lists and

proto-agent/proto-patient, both of which are implemented in PropBank,

and per-frame role lists, implemented in FrameNet.
• Semantic role labeling is the task of assigning semantic role labels to the con-
stituents of a sentence. The task is generally treated as a supervised machine
learning task, with models trained on PropBank or FrameNet. Algorithms
generally start by parsing a sentence and then automatically tag each parse
tree node with a semantic role.
• Semantic selectional restrictions allow words (particularly predicates) to post
constraints on the semantic properties of their argument words. Selectional
preference models (like selectional association or simple conditional proba-
bility) allow a weight or probability to be assigned to the association between
a predicate and an argument word or class.

Bibliographical and Historical Notes

Although the idea of semantic roles dates back to P ¯an. ini, they were re-introduced
into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968)).
Fillmore, interestingly, had become interested in argument structure by studying
Lucien Tesni `ere’s groundbreaking ´El ´ements de Syntaxe Structurale (Tesni `ere, 1959)
in which the term ‘dependency’ was introduced and the foundations were laid for
dependency grammar. Following Tesni `ere’s terminology, Fillmore ﬁrst referred to
argument roles as actants (Fillmore, 1966) but quickly switched to the term case,
(see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent,
Patient, Instrument, etc.), that could be taken on by the arguments of predicates.
Verbs would be listed in the lexicon with their case frame, the list of obligatory (or
optional) case arguments.
The idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-speciﬁed representations of meaning was quickly adopted in natural language
processing, and systems for extracting case frames were created for machine trans-
lation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language
understanding (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977).
General-purpose semantic role labelers were developed. The earliest ones (Sim-
mons, 1973) ﬁrst parsed a sentence by means of an ATN (Augmented Transition

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

375

Network) parser. Each verb then had a set of rules specifying how the parse should
be mapped to semantic roles. These rules mainly made reference to grammatical
functions (subject, object, complement of speciﬁc prepositions) but also checked
constituent internal features such as the animacy of head nouns. Later systems as-
signed roles from pre-built parse trees, again by using dictionaries with verb-speciﬁc
case frames (Levin 1977, Marcus 1980).
By 1977 case representation was widely used and taught in AI and NLP courses,
and was described as a standard of natural language understanding in the ﬁrst edition
of Winston’s (1977) textbook Artiﬁcial Intelligence.
In the 1980s Fillmore proposed his model of frame semantics, later describing
the intuition as follows:
“The idea behind frame semantics is that speakers are aware of possi-
bly quite complex situation types, packages of connected expectations,
that go by various names—frames, schemas, scenarios, scripts, cultural
narratives, memes—and the words in our language are understood with
such frames as their presupposed background.” (Fillmore, 2012, p. 712)
The word frame seemed to be in the air for a suite of related notions proposed at
about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as
well as related notions with other names like scripts (Schank and Abelson, 1975)
and schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).
Fillmore was also inﬂuenced by the semantic ﬁeld theorists and by a visit to the Yale
AI lab where he took notice of the lists of slots and ﬁllers used by early information
extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s
Fillmore drew on these insights to begin the FrameNet corpus annotation project.
At the same time, Beth Levin drew on her early case frame dictionaries (Levin,
1977) to develop her book which summarized sets of verb classes deﬁned by shared
argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper
et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus
created by Martha Palmer and colleagues (Palmer et al., 2005).
The combination of rich linguistic annotation and corpus-based approach in-
stantiated in FrameNet and PropBank led to a revival of automatic approaches to
semantic role labeling, ﬁrst on FrameNet (Gildea and Jurafsky, 2000) and then on
PropBank data (Gildea and Palmer, 2002, inter alia). The problem ﬁrst addressed in
the 1970s by hand-written rules was thus now generally recast as one of supervised
machine learning enabled by large and consistent databases. Many popular features
used for role labeling are deﬁned in Gildea and Jurafsky (2002), Surdeanu et al.
(2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao
et al. (2009). The use of dependency rather than constituency parses was introduced
in the CoNLL-2008 shared task (Surdeanu et al., 2008b). For surveys see Palmer
et al. (2010) and M `arquez et al. (2008).
The use of neural approachess to semantic role labeling was pioneered by Col-
lobert et al. (2011), who applied a CRF on top of a convolutional net. Early work
like Foland, Jr. and Martin (2015) focused on using dependency features. Later
work eschewed syntactic features altogether; (Zhou and Xu, 2015) introduced the
use of a stacked (6-8 layer) bi-LSTM architecture, and (He et al., 2017) showed
how to augment the bi-LSTM architecture with highway networks and also replace
the CRF with A* decoding that make it possible to apply a wide variety of global
constraints in SRL decoding.
Most semantic role labeling schemes only work within a single sentence, fo-
cusing on the object of the verbal (or nominal, in the case of NomBank) predicate.

376 CHA PTER 18

• S EMAN T IC RO LE LAB EL ING

implicit
argument

iSRL

However, in many cases, a verbal or nominal predicate may have an implicit argu-
ment: one that appears only in a contextual sentence, or perhaps not at all and must
be inferred. In the two sentences This house has a new owner. The sale was ﬁnalized
10 days ago. the sale in the second sentence has no ARG1, but a reasonable reader
would infer that the Arg1 should be the house mentioned in the prior sentence. Find-
ing these arguments, implicit argument detection (sometimes shortened as iSRL)
was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do
et al. (2017) for more recent neural models.
To avoid the need for huge labeled training sets, unsupervised approaches for
semantic role labeling attempt to induce the set of semantic roles by clustering over
arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier
and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev
(2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khod-
dam (2014).
Recent innovations in frame labeling include connotation frames, which mark
richer information about the argument of predicates. Connotation frames mark the
sentiment of the writer or reader toward the arguements (for example using the verb
survive in he survived a bombing expresses the writer’s sympathy toward the subject
he and negative sentiment toward the bombing. Connotation frames also mark effect
(something bad happened to x), value: (x is valuable), and mental state: (x is dis-
tressed by the event) (Rashkin et al. 2016, Rashkin et al. 2017). Connotation frames
can also mark the power differential between the arguments (using the verb implore
means that the theme argument has greater power than the agent), and the agency
of each argument (waited is low agency). Fig. 18.9 shows a visualization from Sap
et al. (2017).

Figure 18.9 The connotation frames of Sap et al. (2017), showing that the verb implore
implies the agent has lower power than the theme (in contrast, say, with a verb like demanded,
and showing the low level of agency of the subject of waited. Figure from Sap et al. (2017).

Selectional preference has been widely studied beyond the selectional associa-
tion models of Resnik (1993) and Resnik (1996). Methods have included cluster-
ing (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008), and topic
models (S ´eaghdha 2010, Ritter et al. 2010), and constraints can be expressed at the
level of words or classes (Agirre and Martinez, 2001). Selectional preferences have
also been successfully integrated into semantic role labeling (Erk 2007, Zapirain
et al. 2013, Do et al. 2017).

Exercises

EX ERC I SE S

377

378 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

CHAPTER

19 Lexicons for Sentiment, Affect,
and Connotation

“[W]e write, not with the ﬁngers, but with the whole person. The nerve which
controls the pen winds itself about every ﬁbre of our being, threads the heart,
pierces the liver.”

Virginia Woolf, Orlando

“She runs the gamut of emotions from A to B.”

Dorothy Parker, reviewing Hepburn’s performance in Little Women

affective

subjectivity

In this chapter we turn to tools for interpreting affective meaning, extending our
study of sentiment analysis in Chapter 4. We use the word ‘affective’, following
the tradition in affective computing (Picard, 1995) to mean emotion, sentiment, per-
sonality, mood, and attitudes. Affective meaning is closely related to subjectivity,
the study of a speaker or writer’s evaluations, opinions, emotions, and speculations
(Wiebe et al., 1999).
How should affective meaning be deﬁned? One inﬂuential typology of affec-
tive states comes from Scherer (2000), who deﬁnes each class of affective states by
factors like its cognition realization and time course:

Emotion: Relatively brief episode of response to the evaluation of an external
or internal event as being of major signiﬁcance.
(angry, sad, joyful, fearful, ashamed, proud, elated, desperate)
Mood: Diffuse affect state, most pronounced as change in subjective feeling, of
low intensity but relatively long duration, often without apparent cause.
(cheerful, gloomy, irritable, listless, depressed, buoyant)
Interpersonal stance: Affective stance taken toward another person in a spe-
ciﬁc interaction, colouring the interpersonal exchange in that situation.
(distant, cold, warm, supportive, contemptuous, friendly)
Attitude: Relatively enduring, affectively colored beliefs, preferences, and pre-
dispositions towards objects or persons.
(liking, loving, hating, valuing, desiring)
Personality traits: Emotionally laden, stable personality dispositions and be-
havior tendencies, typical for a person.
(nervous, anxious, reckless, morose, hostile, jealous)

Figure 19.1 The Scherer typology of affective states (Scherer, 2000).

We can design extractors for each of these kinds of affective states. Chapter 4
already introduced sentiment analysis, the task of extracting the positive or negative

19 .1

• D E FIN ING EMOT ION

379

orientation that a writer expresses in a text. This corresponds in Scherer’s typology
to the extraction of attitudes: ﬁguring out what people like or dislike, from affect-
rish texts like consumer reviews of books or movies, newspaper editorials, or public
sentiment in blogs or tweets.
Detecting emotion and moods is useful for detecting whether a student is con-
fused, engaged, or certain when interacting with a tutorial system, whether a caller
to a help line is frustrated, whether someone’s blog posts or tweets indicated depres-
sion. Detecting emotions like fear in novels, for example, could help us trace what
groups or situations are feared and how that changes over time.
Detecting different interpersonal stances can be useful when extracting infor-
mation from human-human conversations. The goal here is to detect stances like
friendliness or awkwardness in interviews or friendly conversations, or even to de-
tect ﬂirtation in dating. For the task of automatically summarizing meetings, we’d
like to be able to automatically understand the social relations between people, who
is friendly or antagonistic to whom. A related task is ﬁnding parts of a conversation
where people are especially excited or engaged, conversational hot spots that can
help a summarizer focus on the correct region.
Detecting the personality of a user—such as whether the user is an extrovert
or the extent to which they are open to experience— can help improve conversa-
tional agents, which seem to work better if they match users’ personality expecta-
tions (Mairesse and Walker, 2008).
Affect is important for generation as well as recognition; synthesizing affect
is important for conversational agents in various domains, including literacy tutors
such as children’s storybooks, or computer games.
In Chapter 4 we introduced the use of Naive Bayes classiﬁcation to classify a
document’s sentiment. Various classiﬁers have been successfully applied to many of
these tasks, using all the words in the training set as input to a classiﬁer which then
determines the affect status of the text.
In this chapter we focus on an alternative model, in which instead of using every
word as a feature, we focus only on certain words, ones that carry particularly strong
cues to affect or sentiment. We call these lists of words affective lexicons or senti-
ment lexicons. These lexicons presuppose a fact about semantics: that words have
affective meanings or connotations. The word connotation has different meanings
in different ﬁelds, but here we use it to mean the aspects of a word’s meaning that
are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. In
addition to their ability to help determine the affective status of a text, connotation
lexicons can be useful features for other kinds of affective tasks, and for computa-
tional social science analysis.
In the next sections we introduce basic theories of emotion, show how sentiment
lexicons can be viewed as a special case of emotion lexicons, and then summarize
some publicly available lexicons. We then introduce three ways for building new
lexicons: human labeling, semi-supervised, and supervised.
Finally, we turn to some other kinds of affective meaning, including interper-
sonal stance, personality, and connotation frames.

connotations

19.1 Deﬁning Emotion

emotion

One of the most important affective classes is emotion, which Scherer (2000) deﬁnes
as a “relatively brief episode of response to the evaluation of an external or internal

380 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

event as being of major signiﬁcance”.
Detecting emotion has the potential to improve a number of language processing
tasks. Automatically detecting emotions in reviews or customer responses (anger,
dissatisfaction, trust) could help businesses recognize speciﬁc problem areas or ones
that are going well. Emotion recognition could help dialog systems like tutoring
systems detect that a student was unhappy, bored, hesitant, conﬁdent, and so on.
Emotion can play a role in medical informatics tasks like detecting depression or
suicidal intent. Detecting emotions expressed toward characters in novels might
play a role in understanding how different social groups were viewed by society at
different times.
There are two widely-held families of theories of emotion. In one family, emo-
tions are viewed as ﬁxed atomic units, limited in number, and from which others
are generated, often called basic emotions (Tomkins 1962, Plutchik 1962). Per-
haps most well-known of this family of theories are the 6 emotions proposed by
(Ekman, 1999) as a set of emotions that is likely to be universally present in all
cultures: surprise, happiness, anger, fear, disgust, sadness. Another atomic theory
is the (Plutchik, 1980) wheel of emotion, consisting of 8 basic emotions in four
opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise,
together with the emotions derived from them, shown in Fig. 19.2.

basic emotions

Figure 19.2 Plutchik wheel of emotion.

The second class of emotion theories views emotion as a space in 2 or 3 di-
mensions (Russell, 1980). Most models include the two dimensions valence and
arousal, and many add a third, dominance. These can be deﬁned as:
valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus
In the next sections we’ll see lexicons for both kinds of theories of emotion.

19 . 2

• AVA I LABL E S ENT IM EN T AND A FFEC T L EX ICON S

381

Sentiment can be viewed as a special case of this second view of emotions as
points in space. In particular, the valence dimension, measuring how pleasant or
unpleasant a word is, is often used directly as a measure of sentiment.

19.2 Available Sentiment and Affect Lexicons

General
Inquirer

A wide variety of affect lexicons have been created and released. The most basic
lexicons label words along one dimension of semantic variability, generally called
”sentiment” or ”valence”.
In the simplest lexicons this dimension is represented in a binary fashion, with
a wordlist for positive words and a wordlist for negative words. The oldest is the
General Inquirer (Stone et al., 1966), which drew on early work in the cognition
psychology of word meaning (Osgood et al., 1957) and on work in content analysis.
The General Inquirer has a lexicon of 1915 positive words an done of 2291 negative
words (and also includes other lexicons discussed below).
The MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and
4912 negative words drawn from prior lexicons plus a bootstrapped list of subjec-
tive words and phrases (Riloff and Wiebe, 2003) Each entry in the lexicon is hand-
labeled for sentiment and also labeled for reliability (strongly subjective or weakly
subjective).
The polarity lexicon of Hu and Liu (2004b) gives 2006 positive and 4783 nega-
tive words, drawn from product reviews, labeled using a bootstrapping method from
WordNet.

Positive

admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-
tastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,
rejoice, relief, respect, satisfactorily, sensational, super, terriﬁc, thank, vivid, wise, won-
derful, zest
Negative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,
defective, disappointment, embarrass, fake, fear, ﬁlthy, fool, guilt, hate, idiot, inﬂict, lazy,
miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,
vile, wicked

Figure 19.3 Some samples of words with consistent sentiment across three sentiment lexicons: the General
Inquirer (Stone et al., 1966), the MPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of
Hu and Liu (2004b).

EmoLex

Slightly more general than these sentiment lexicons are lexicons that assign each
word a value on all three emotional dimension The lexicon of Warriner et al. (2013)
assigns valence, arousal, and dominance scores to 14,000 words. Some examples
are shown in Fig. 19.4
The NRC Word-Emotion Association Lexicon, also called EmoLex (Moham-
mad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions deﬁned above.
The lexicon includes around 14,000 words including words from prior lexicons as
well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for
some sample words:

382 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

Valence

Arousal

Dominance

vacation
happy
whistle
conscious
torture

8.53
8.47
5.7
5.53
1.4

rampage
tornado
zucchini
dressy
dull

7.56
7.45
4.18
4.15
1.67

self
incredible
skillet
concur
earthquake

7.74
7.74
5.33
5.29
2.14

Figure 19.4 Samples of the values of selected words on the three emotional dimensions
from Warriner et al. (2013).

r
e

g

Word a

n

n

o

i

t

a

p

i

c

i

t

n

a

t

s

u

g

s

i

d

s
s

e

n
d

a

s

e

s

i

r

p

r

u

s

r

a

e

f

y
o

j

e

v

i

t

s

u

r

t

t

i

s

o

p

e

v

i

t

a
g

e

n

reward
0 1 0 0 1 0 1 1 1 0
worry
0 1 0 1 0 1 0 0 0 1
tenderness 0 0 0 0 1 0 0 0 1 0
sweetheart 0 1 0 0 1 1 0 1 1 0
suddenly
0 0 0 0 0 0 1 0 0 0
thirst
0 1 0 0 0 1 1 0 0 0
garbage
0 0 1 0 0 0 0 0 0 1

concrete
abstract

LIWC

There are various other hand-built affective lexicons. The General Inquirer in-
cludes additional lexicons for dimensions like strong vs. weak, active vs. passive,
overstated vs. understated, as well as lexicons for categories like pleasure, pain,
virtue, vice, motivation, and cognitive orientation.
Another useful feature for various tasks is the distinction between concrete
words like banana or bathrobe and abstract words like belief and although. The
lexicon in (Brysbaert et al., 2014) used crowdsourcing to assign a rating from 1 to 5
of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5,
belief 1.19, although 1.07, and in between words like brisk a 2.5.

LIWC, Linguistic Inquiry and Word Count, is another set of 73 lexicons con-

taining over 2300 words (Pennebaker et al., 2007), designed to capture aspects of
lexical meaning relevant for social psychological tasks. In addition to sentiment-
related lexicons like ones for negative emotion (bad, weird, hate, problem, tough)
and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like
anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown
in Fig. 19.5.

19.3 Creating affect lexicons by human labeling

crowdsourcing

The earliest method used to build affect lexicons, and still in common use, is to have
humans label each word. This is now most commonly done via crowdsourcing:
breaking the task into small pieces and distributing them to a large number of anno-
tators. Let’s take a look at some of the methodological choices for two crowdsourced
emotion lexicons.
The NRC Word-Emotion Association Lexicon (EmoLex) (Mohammad and Tur-
ney, 2013), labeled emotions in two steps. In order to ensure that the annotators
were judging the correct sense of the word, they ﬁrst answered a multiple-choice

19 . 3

• CR EAT ING A FFEC T LEX ICON S BY HUMAN LAB E L ING

383

Positive
Emotion

appreciat*
comfort*
great
happy
interest
joy*
perfect*
please*
safe*
terriﬁc
value
wow*

Negative
Emotion

anger*
bore*
cry
despair*
fail*
fear
griev*
hate*
panic*
suffers
terrify
violent*

Insight

aware*
believe
decid*
feel
ﬁgur*
know
knew
means
notice*
recogni*
sense
think

Inhibition

avoid*
careful*
hesitat*
limit*
oppos*
prevent*
reluctan*
safe*
stop
stubborn*
wait
wary

Family

brother*
cousin*
daughter*
family
father*
grandf*
grandm*
husband
mom
mother
niece*
wife

Negate

aren’t
cannot
didn’t
neither
never
no
nobod*
none
nor
nothing
nowhere
without

Figure 19.5 Samples from 5 of the 73 lexical categories in LIWC (Pennebaker et al., 2007).
The * means the previous letters are a word preﬁx and all words with that preﬁx are included
in the category.

synonym question that primed the correct sense of the word (without requiring the
annotator to read a potentially confusing sense deﬁnition). These were created au-
tomatically using the headwords associated with the thesaurus category of the sense
in question in the Macquarie dictionary and the headwords of 3 random distractor
categories. An example:

Which word is closest in meaning (most related) to startle?
• automobile
• shake
• honesty
• entertain

For each word (e.g. startle), the annotator was then asked to rate how associated
that word is with each of the 8 emotions (joy, fear, anger, etc.). The associations
were rated on a scale of not, weakly, moderately, and strongly associated. Outlier
ratings were removed, and then each term was assigned the class chosen by the ma-
jority of the annotators, with ties broken by choosing the stronger intensity, and then
the 4 levels were mapped into a binary label for each word (no and weak mapped to
0, moderate and strong mapped to 1).
For the Warriner et al. (2013) lexicon of valence, arousal, and dominance, crowd-
workers marked each word with a value from 1-9 on each of the dimensions, with
the scale deﬁned for them as follows:
• valence (the pleasantness of the stimulus)
9: happy, pleased, satisﬁed, contented, hopeful
1: unhappy, annoyed, unsatisﬁed, melancholic, despaired, or bored
• arousal (the intensity of emotion provoked by the stimulus)
9: stimulated, excited, frenzied, jittery, wide-awake, or aroused
1: relaxed, calm, sluggish, dull, sleepy, or unaroused;
• dominance (the degree of control exerted by the stimulus)
9: in control, inﬂuential, important, dominant, autonomous, or controlling
1: controlled, inﬂuenced, cared-for, awed, submissive, or guided

384 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

19.4 Semi-supervised induction of affect lexicons

Another common way to learn sentiment lexicons is to start from a set of seed words
that deﬁne two poles of a semantic axis (words like good or bad), and then ﬁnd ways
to label each word w by its similarity to the two seed sets. Here we summarize two
families of seed-based semi-supervised lexicon induction algorithms, axis-based and
graph-based.

19.4.1 Semantic axis methods

One of the most well-known lexicon induction methods, the Turney and Littman
(2003) algorithm, is given seed words like good or bad, and then for each word w to
be labeled, measures both how similar it is to good and how different it is from bad.
Here we describe a slight extension of the algorithm due to An et al. (2018), which
is based on computing a semantic axis.
In the ﬁrst step, we choose seed words by hand. Because the sentiment or affect
of a word is different in different contexts, it’s common to choose different seed
words for different genres, and most algorithms are quite sensitive to the choice of
seeds. For example, for inducing sentiment lexicons, Hamilton et al. (2016a) deﬁnes
one set of seed words for general sentiment analyis, a different set for Twitter, and
yet another set for learning a lexicon for sentiment in ﬁnancial text:

Domain
General

Twitter

Finance

Positive seeds

Negative seeds

good, lovely, excellent, fortunate, pleas-
ant, delightful, perfect,
loved,
love,
happy
love,
loved,
loves, awesome, nice,
amazing, best, fantastic, correct, happy
successful, excellent, proﬁt, beneﬁcial,
improving,
improved, success, gains,
positive

bad, horrible, poor, unfortunate, un-
pleasant, disgusting, evil, hated, hate,
unhappy
hate, hated, hates, terrible, nasty, awful,
worst, horrible, wrong, sad
negligent, loss, volatile, wrong, losses,
damages, bad, litigation, failure, down,
negative

In the second step, we compute embeddings for each of the pole words. These
embeddings can be off-the-shelf word2vec embeddings, or can be computed directly
on a speciﬁc corpus (for example using a ﬁnancial corpus if a ﬁnance lexicon is the
goal), or we can ﬁne-tune off-the-shelf embeddings to a corpus. Fine-tuning is espe-
cially important if we have a very speciﬁc genre of text but don’t have enough data
to train good embeddings. In ﬁne-tuning, we begin with off-the-shelf embeddings
like word2vec, and continue training them on the small target corpus.
Once we have embeddings for each pole word, we we create an embedding that
represents each pole by taking the centroid of the embeddings of each of the seed
words; recall that the centroid is the multidimensional version of the mean. Given
a set of embeddings for the positive seed words S+ = {E (w+
n )},
and embeddings for the negative seed words S− = {E (w−1 ), E (w−2 ), ..., E (w−m )}, the

1 ), E (w+
2 ), ..., E (w+

19 . 4

• S EM I - SU P ERV I SED INDUC T ION O F A FFEC T LEX ICON S

385

pole centroids are:

V+ =

V− =

1
n

1
n

n(cid:88)1
m(cid:88)1

E (w+

i )

E (w−i )

(19.1)

The semantic axis deﬁned by the poles is computed just by subtracting the two vec-
tors:

Vaxis = V+ − V−

(19.2)

Vaxis , the semantic axis, is a vector in the direction of sentiment. Finally, we compute
how close each word w is to this sentiment axis, by taking the cosine between w’s
embedding and the axis vector. A higher cosine means that w is more aligned with
S+ than S− .
score(w) = (cid:0)cos(E (w), Vaxis (cid:1)
E (w) · Vaxis
(cid:107)E (w)(cid:107)(cid:107)Vaxis (cid:107)
If a dictionary of words with sentiment scores is sufﬁcient, we’re done! Or if we
need to group words into a positive and a negative lexicon, we can use a threshold
or other method to give us discrete lexicons.

(19.3)

=

19.4.2 Label propagation

An alternative family of methods deﬁnes lexicons by propagating sentiment labels
on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown
(1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of
Hamilton et al. (2016a), which has four steps:
1. Deﬁne a graph: Given word embeddings, build a weighted lexical graph
by connecting each word with its k nearest neighbors (according to cosine-
similarity). The weights of the edge between words wi and w j are set as:
Ei, j = arccos (cid:18)−
2. Deﬁne a seed set: By hand, choose positive and negative seed words.

(cid:107)wi(cid:107)(cid:107)wj(cid:107) (cid:19) .

wi(cid:62)wj

(19.4)

3. Propagate polarities from the seed set: Now we perform a random walk on

this graph, starting at the seed set. In a random walk, we start at a node and
then choose a node to move to with probability proportional to the edge prob-
ability. A word’s polarity score for a seed set is proportional to the probability
of a random walk from the seed set landing on that word, (Fig. 19.6).
4. Create word scores: We walk from both positive and negative seed sets,
resulting in positive (score+ (wi )) and negative (score− (wi )) label scores. We
then combine these values into a positive-polarity score as:
score+ (wi )
score+ (wi ) + score− (wi )
It’s often helpful to standardize the scores to have zero mean and unit variance
within a corpus.

score+ (wi ) =

(19.5)

386 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

5. Assign conﬁdence to each score: Because sentiment scores are inﬂuenced by
the seed set, we’d like to know how much the score of a word would change if
a different seed set is used. We can use bootstrap-sampling to get conﬁdence
regions, by computing the propagation B times over random subsets of the
positive and negative seed sets (for example using B = 50 and choosing 7 of
the 10 seed words each time). The standard deviation of the bootstrap-sampled
polarity scores gives a conﬁdence measure.

(a)

(b)

Figure 19.6

Intuition of the S EN TPRO P algorithm. (a) Run random walks from the seed words. (b) Assign
polarity scores (shown here as colors green or red) based on the frequency of random walk visits.

19.4.3 Other methods

The core of semisupervised algorithms is the metric for measuring similarity with
the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) ap-
proaches above used embedding cosine as the distance metric: words were labeled
as positive basically if their embeddings had high cosines with positive seeds and
low cosines with negative seeds. Other methods have chosen other kinds of distance
metrics besides embedding cosine.
For example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic
cues; two adjectives are considered similar if they were frequently conjoined by and
and rarely conjoined by but. This is based on the intuition that adjectives conjoined
by the words and tend to have the same polarity; positive adjectives are generally
coordinated with positive, negative with negative:
fair and legitimate, corrupt and brutal
but less often positive adjectives coordinated with negative:
*fair and brutal, *corrupt and legitimate
By contrast, adjectives conjoined by but are likely to be of opposite polarity:
fair but brutal
Another cue to opposite polarity comes from morphological negation (un-, im-,
-less). Adjectives with the same root but differing in a morphological negative (ad-
equate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity.
Yet another method for ﬁnding words that have a similar polarity to seed words is
to make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b).
A word’s synonyms presumably share its polarity while a word’s antonyms probably
have the opposite polarity. After a seed lexicon is built, each lexicon is updated as
follows, possibly iterated.
Lex+ : Add synonyms of positive words (well) and antonyms (like ﬁne) of negative
words

19 . 5

• SU PERV I S ED L EARN ING O F WORD S EN T IM EN T

387

Lex− : Add synonyms of negative words (awful) and antonyms ( like evil) of positive
words
An extension of this algorithm assigns polarity to WordNet senses, called Senti-
WordNet (Baccianella et al., 2010). Fig. 19.7 shows some examples.

SentiWordNet

Synset

good#6
‘agreeable or pleasing’
respectable#2 honorable#4 good#4 estimable#2
‘deserving of esteem’
estimable#3 computable#1
‘may be computed or estimated’
sting#1 burn#4 bite#2
‘cause a sharp or stinging pain’
acute#6
‘of critical importance and consequence’
acute#4
‘of an angle; less than 90 degrees’
acute#1
‘having or experiencing a rapid onset and short but severe course’

Pos Neg Obj

1
0
0
0.75
0
0.25
0
0
1
0
0.875 .125
0.625 0.125 .250
0
0
1
0
0.5
0.5

Figure 19.7 Examples from SentiWordNet 3.0 (Baccianella et al., 2010). Note the differences between senses
of homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive
(acute#6), negative (acute#1), or neutral (acute #4)

.

In this algorithm, polarity is assigned to entire synsets rather than words. A
positive lexicon is built from all the synsets associated with 7 positive words, and a
negative lexicon from synsets associated with 7 negative words. A classiﬁer is then
trained from this data to take a WordNet gloss and decide if the sense being deﬁned
is positive, negative or neutral. A further step (involving a random-walk algorithm)
assigns a score to each WordNet synset for its degree of positivity, negativity, and
neutrality.
In summary, semisupervised algorithms use a human-deﬁned set of seed words
for the two poles of a dimension, and use similarity metrics like embedding cosine,
coordination, morphology, or thesaurus structure to score words by how similar they
are to the positive seeds and how dissimilar to the negative seeds.

19.5 Supervised learning of word sentiment

Semi-supervised methods require only minimal human supervision (in the form of
seed sets). But sometimes a supervision signal exists in the world and can be made
use of. One such signal is the scores associated with online reviews.
The web contains an enormous number of online reviews for restaurants, movies,
books, or other products, each of which have the text of the review along with an
associated review score: a value that may range from 1 star to 5 stars, or scoring 1
to 10. Fig. 19.8 shows samples extracted from restaurant, book, and movie reviews.
We can use this review score as supervision: positive words are more likely to
appear in 5-star reviews; negative words in 1-star reviews. And instead of just a
binary polarity, this kind of supervision allows us to assign a word a more complex
representation of its polarity: its distribution over stars (or other scores).
Thus in a ten-star system we could represent the sentiment of each word as a
10-tuple, each number a score representing the word’s association with that polarity
level. This association can be a raw count, or a likelihood P(w|c), or some other
function of the count, for each class c from 1 to 10.
For example, we could compute the IMDB likelihood of a word like disap-
point(ed/ing) occurring in a 1 star review by dividing the number of times disap-
point(ed/ing) occurs in 1-star reviews in the IMDB dataset (8,557) by the total num-

388 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

Movie review excerpts (IMDB)

10 A great movie. This ﬁlm is just a wonderful experience. It’s surreal, zany, witty and slapstick
all at the same time. And terriﬁc performances too.
1 This was probably the worst movie I have ever seen. The story went nowhere even though they
could have done some interesting stuff with it.

Restaurant review excerpts (Yelp)

5 The service was impeccable. The food was cooked and seasoned perfectly... The watermelon
was perfectly square ... The grilled octopus was ... mouthwatering...
...it took a while to get our waters, we got our entree before our starter, and we never received
silverware or napkins until we requested them...

2

Book review excerpts (GoodReads)

1

I am going to try and stop being deceived by eye-catching titles. I so wanted to like this book
and was so disappointed by it.
5 This book is hilarious. I would recommend it to anyone looking for a satirical read with a
romantic twist and a narrator that keeps butting in

Product review excerpts (Amazon)

5 The lid on this blender though is probably what I like the best about it... enables you to pour
into something without even taking the lid off! ... the perfect pitcher! ... works fantastic.
I hate this blender... It is nearly impossible to get frozen fruit and ice to turn into a smoothie...
You have to add a TON of liquid. I also wish it had a spout ...

1

Figure 19.8 Excerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except
IMDB, which is on a scale of 1 to 10 stars.

ber of words occurring in 1-star reviews (25,395,214), so the IMDB estimate of
P(disappointing|1) is .0003.
A slight modiﬁcation of this weighting, the normalized likelihood, can be used
as an illuminating visualization (Potts, 2011)1 :

Potts diagram

(19.6)

P(w|c) =

Pot t sScore(w) =

count (w, c)
(cid:80)w∈C count (w, c)
(cid:80)c P(w|c)
P(w|c)
Dividing the IMDB estimate P(disappointing|1) of .0003 by the sum of the like-
lihood P(w|c) over all categories gives a Potts score of 0.10. The word disappointing
thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The
Potts diagram (Potts, 2011) is a visualization of these word scores, representing the
prior sentiment of a word as a distribution over the rating categories.
Fig. 19.9 shows the Potts diagrams for 3 positive and 3 negative scalar adjectives.
Note that the curve for strongly positive scalars have the shape of the letter J, while
strongly negative scalars look like a reverse J. By contrast, weakly positive and neg-
ative scalars have a hump-shape, with the maximum either below the mean (weakly
negative words like disappointing) or above the mean (weakly positive words like
good). These shapes offer an illuminating typology of affective word meaning.
Fig. 19.10 shows the Potts diagrams for emphasizing and attenuating adverbs.
Again we see generalizations in the characteristic curves associated with words of
particular meanings. Note that emphatics tend to have a J-shape (most likely to occur

1 Potts shows that the normalized likelihood is an estimate of the posterior P(c|w) if we make the
incorrect but simplifying assumption that all categories c have equal probability.

19 . 5

• SU PERV I S ED L EARN ING O F WORD S EN T IM EN T

389

Figure 19.9 Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, show-
ing the J-shape and reverse J-shape for strongly positive and negative adjectives, and the
hump-shape for more weakly polarized adjectives.

in the most positive reviews) or a U-shape (most likely to occur in the strongly posi-
tive and negative). Attenuators all have the hump-shape, emphasizing the middle of
the scale and downplaying both extremes.

Figure 19.10 Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.

The diagrams can be used both as a typology of lexical sentiment, and also play
a role in modeling sentiment compositionality.
In addition to functions like posterior P(c|w), likelihood P(w|c), or normalized
likelihood (Eq. 19.6) many other functions of the count of a word occurring with a
sentiment label have been used. We’ll introduce some of these on page 394, includ-
ing ideas like normalizing the counts per writer in Eq. 19.14.

390 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

19.5.1 Log odds ratio informative Dirichlet prior

One thing we often want to do with word polarity is to distinguish between words
that are more likely to be used in one category of texts than in another. We may, for
example, want to know the words most associated with 1 star reviews versus those
associated with 5 star reviews. These differences may not be just related to senti-
ment. We might want to ﬁnd words used more often by Democratic than Republican
members of Congress, or words used more often in menus of expensive restaurants
than cheap restaurants.
Given two classes of documents, to ﬁnd words more associated with one cate-
gory than another, we might choose to just compute the difference in frequencies
(is a word w more frequent in class A or class B?). Or instead of the difference in
frequencies we might want to compute the ratio of frequencies, or the log odds ratio
(the log of the ratio between the odds of the two words). Then we can sort words
by whichever of these associations with the category we use, (sorting from words
overrepresented in category A to words overrepresented in category B).
The problem with simple log-likelihood or log odds methods is that they don’t
work well for very rare words or very frequent words; for words that are very fre-
quent, all differences seem large, and for words that are very rare, no differences
seem large.
In this section we walk through the details of one solution to this problem: the
“log odds ratio informative Dirichlet prior” method of Monroe et al. (2008) that is a
particularly useful method for ﬁnding words that are statistically overrepresented in
one particular category of texts compared to another. It’s based on the idea of using
another large corpus to get a prior estimate of what we expect the frequency of each
word to be.
Let’s start with the goal: assume we want to know whether the word horrible
occurs more in corpus i or corpus j . We could compute the log likelihood ratio,
using f i (w) to mean the frequency of word w in corpus i, and ni to mean the total
number of words in corpus i:

(19.7)

Pi (horribl e)
llr(horribl e) = log
P j (horribl e)
= log Pi (horribl e) − log P j (horribl e)
fi (horribl e)
f j (horribl e)
= log
− log
ni
n j
Instead, let’s compute the log odds ratio: does horrible have higher odds in i or in
j:
lor(horribl e) = log (cid:18) Pi (horribl e)
1 − Pi (horribl e) (cid:19) − log (cid:18) P j (horribl e)
1 − P j (horribl e) (cid:19)
= log 
fi (horribl e)
f j (horribl e)
ni
n j
fi (horribl e)
f j (horribl e)
ni
n j
= log (cid:18) fi (horribl e)
ni − fi (horribl e) (cid:19) − log (cid:18) f j (horribl e)
n j − f j (horribl e) (cid:19) (19.8)
The Dirichlet intuition is to use a large background corpus to get a prior estimate of
what we expect the frequency of each word w to be. We’ll do this very simply by

 − log 



1 −

1 −

log likelihood
ratio

log odds ratio

19 . 6

• U S ING L EX ICON S FOR S EN T IM EN T R ECOGN I T ION

391

δ (i− j)
w

(19.9)

w

1

w

f j

f i

w + αw
ni + α0 − ( f i

(cid:17) ≈

w + αw ) (cid:33)
w + αw
n j + α0 − ( f j

adding the counts from that corpus to the numerator and denominator, so that we’re
essentially shrinking the counts toward that prior. It’s like asking how large are the
differences between i and j given what we would expect given their frequencies in
a well-estimated large background corpus.
The method estimates the difference between the frequency of word w in two
corpora i and j via the prior-modiﬁed log odds ratio for w, δ (i− j)
, which is estimated
as:
= log (cid:18)
w + αw ) (cid:19) − log (cid:32)
(where ni is the size of corpus i, n j is the size of corpus j , f i
w is the count of word w
in corpus i, f j
w is the count of word w in corpus j, α0 is the size of the background
corpus, and αw is the count of word w in the background corpus.)
In addition, Monroe et al. (2008) make use of an estimate for the variance of the
log–odds–ratio:
σ 2 (cid:16) ˆδ (i− j)
f j
The ﬁnal statistic for a word is then the z–score of its log–odds–ratio:
w(cid:114)σ 2 (cid:16) ˆδ (i− j)
The Monroe et al. (2008) method thus modiﬁes the commonly used log odds ratio
in two ways: it uses the z-scores of the log odds ratio, which controls for the amount
of variance in a words frequency, and it uses counts from a background corpus to
provide a prior count for words.
Fig. 19.11 shows the method applied to a dataset of restaurant reviews from
Yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews
(Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the
1-star reviews using negative sentiment words like worse, bad, awful and the 5-star
reviews using positive sentiment words like great, best, amazing. But there are other
illuminating differences. 1-star reviews use logical negation (no, not), while 5-star
reviews use emphatics and emphasize universality (very, highly, every, always). 1-
star reviews use ﬁrst person plurals (we, us, our) while 5 star reviews use the second
person. 1-star reviews talk about people (manager, waiter, customer) while 5-star
reviews talk about dessert and properties of expensive restaurants like courses and
atmosphere. See Jurafsky et al. (2014) for more details.

w + αw

+

f i

w + αw

1

w

(19.10)

(19.11)

ˆδ (i− j)

(cid:17)

19.6 Using Lexicons for Sentiment Recognition

In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The
lexicons we have focused on throughout the chapter so far can be used in a number
of ways to improve sentiment detection.
In the simplest case, lexicons can be used when we don’t have sufﬁcient training
data to build a supervised sentiment analyzer; it can often be expensive to have a
human assign sentiment to each document to train the supervised classiﬁer.

392 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

Class
Positive

Emphatics/
universals
2 pro
Articles
Advice

Conjunct
Nouns

Words in 5-star reviews

great,
best,
love(d),
delicious,
amazing,
favorite, perfect, excel-
lent, awesome,
friendly,
fantastic,
fresh, wonderful, incredible, sweet,
yum(my)
very, highly, perfectly, deﬁnitely, ab-
solutely, everything, every, always
you
a, the
try, recommend

also, as, well, with, and
atmosphere,
dessert,
wine, course, menu

chocolate,

Auxiliaries

is/’s, can, ’ve, are

Class
Negative

Words in 1-star reviews

worst,
rude,
terrible, horrible,
bad, awful, disgusting, bland,
tasteless, gross, mediocre, over-
priced, worse, poor

Negation

no, not

1Pl pro
3 pro

we, us, our
she, he, her, him
Past verb was, were, asked, told, said, did,
charged, waited, left, took

Sequencers after, then
Nouns

manager, waitress, waiter, cus-
tomer, customers, attitude, waste,
poisoning, money, bill, minutes
would, should

Irrealis
modals
Comp

to, that
in, of, die, city, mouth
Figure 19.11 The top 50 words associated with one–star and ﬁve-star restaurant reviews in a Yelp dataset of
900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).

Prep, other

In such situations, lexicons can be used in a simple rule-based algorithm for
classiﬁcation. The simplest version is just to use the ratio of positive to negative
words: if a document has more positive than negative words (using the lexicon to
decide the polarity of each word in the document), it is classiﬁed as positive. Often
a threshold λ is used, in which a document is classiﬁed as positive only if the ratio
is greater than λ . If the sentiment lexicon includes positive and negative weights for
each word, θ +
w and θ −w , these can be used as well. Here’s a simple such sentiment
algorithm:

f + =

θ +

w count (w)

(cid:88)w s.t. w∈ posit ivel exicon
(cid:88)w s.t. w∈negat ivel exicon

θ −w count (w)

f − =
sent iment = 
If supervised training data is available, these counts computed from sentiment lex-
icons, sometimes weighted or normalized in various ways, can also be used as fea-
tures in a classiﬁer along with other lexical or non-lexical features. We return to
such algorithms in Section 19.8.

+ if f +
f − > λ
f + > λ

− if f −
0
otherwise.

(19.12)

19.7 Other tasks: Personality

personality

Many other kinds of affective meaning can be extracted from text and speech. For
example detecting a person’s personality from their language can be useful for di-
alog systems (users tend to prefer agents that match their personality), and can play

19 .8

• A FFEC T R ECOGN I T ION

393

a useful role in computational social science questions like understanding how per-
sonality is related to other kinds of behavior.
Many theories of human personality are based around a small number of dimen-
sions, such as various versions of the “Big Five” dimensions (Digman, 1990):
Extroversion vs. Introversion: sociable, assertive, playful vs. aloof, reserved,
shy
Emotional stability vs. Neuroticism: calm, unemotional vs. insecure, anxious
Agreeableness vs. Disagreeableness: friendly, cooperative vs. antagonistic, fault-
ﬁnding

Conscientiousness vs. Unconscientiousness: self-disciplined, organized vs. in-

efﬁcient, careless
Openness to experience: intellectual, insightful vs. shallow, unimaginative
A few corpora of text and speech have been labeled for the personality of their
author by having the authors take a standard personality test. The essay corpus of
Pennebaker and King (1999) consists of 2,479 essays (1.9 million words) from psy-
chology students who were asked to “write whatever comes into your mind” for 20
minutes. The EAR (Electronically Activated Recorder) corpus of Mehl et al. (2006)
was created by having volunteers wear a recorder throughout the day, which ran-
domly recorded short snippets of conversation throughout the day, which were then
transcribed. The Facebook corpus of (Schwartz et al., 2013) includes 309 million
words of Facebook posts from 75,000 volunteers.
For example, here are samples from Pennebaker and King (1999) from an essay
written by someone on the neurotic end of the neurotic/emotionally stable scale,
One of my friends just barged in, and I jumped in my seat. This is crazy.
I should tell him not to do that again. I’m not that fastidious actually.
But certain things annoy me. The things that would annoy me would
actually annoy any normal human being, so I know I’m not a freak.
and someone on the emotionally stable end of the scale:
I should excel in this sport because I know how to push my body harder
than anyone I know, no matter what the test I always push my body
harder than everyone else. I want to be the best no matter what the sport
or event. I should also be good at this because I love to ride my bike.
Another kind of affective meaning is what Scherer (2000) calls interpersonal
stance, the ‘affective stance taken toward another person in a speciﬁc interaction
coloring the interpersonal exchange’. Extracting this kind of meaning means au-
tomatically labeling participants for whether they are friendly, supportive, distant.
For example Ranganath et al. (2013) studied a corpus of speed-dates, in which par-
ticipants went on a series of 4-minute romantic dates, wearing microphones. Each
participant labeled each other for how ﬂirtatious, friendly, awkward, or assertive
they were. Ranganath et al. (2013) then used a combination of lexicons and other
features to detect these interpersonal stances from text.

interpersonal
stance

19.8 Affect Recognition

Detection of emotion, personality, interactional stance, and the other kinds of af-
fective meaning described by Scherer (2000) can be done by generalizing the algo-
rithms described above for detecting sentiment.

394 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

The most common algorithms involve supervised classiﬁcation: a training set is
labeled for the affective meaning to be detected, and a classiﬁer is built using features
extracted from the training set. As with sentiment analysis, if the training set is large
enough, and the test set is sufﬁciently similar to the training set, simply using all
the words or all the bigrams as features in a powerful classiﬁer like SVM or logistic
regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose
performance is hard to beat. Thus we can treat affective meaning classiﬁcation of a
text sample as simple document classiﬁcation.
Some modiﬁcations are nonetheless often necessary for very large datasets. For
example, the Schwartz et al. (2013) study of personality, gender, and age using 700
million words of Facebook posts used only a subset of the n-grams of lengths 1-
3. Only words and phrases used by at least 1% of the subjects were included as
features, and 2-grams and 3-grams were only kept if they had sufﬁciently high PMI
(PMI greater than 2 ∗ length, where length is the number of words):
p(phrase)
pmi(phrase) = log
p(w)

(19.13)

(cid:89)w∈phrase

Various weights can be used for the features, including the raw count in the training
set, or some normalized probability or log probability. Schwartz et al. (2013), for
example, turn feature counts into phrase likelihoods by normalizing them by each
subject’s total word use.

p(phrase|subject) =

(cid:88)

freq(phrase, subject)
freq(phrase(cid:48) , subject)

(19.14)

phrase(cid:48) ∈vocab(subject)
If the training data is sparser, or not as similar to the test set, any of the lexicons
we’ve discussed can play a helpful role, either alone or in combination with all the
words and n-grams.
Many possible values can be used for lexicon features. The simplest is just an
indicator function, in which the value of a feature fL takes the value 1 if a particular
text has any word from the relevant lexicon L. Using the notation of Chapter 4, in
which a feature value is deﬁned for a particular output class c and document x.
fL (c, x) = (cid:26) 1 if ∃w : w ∈ L & w ∈ x & cl ass = c
0 otherwise
Alternatively the value of a feature fL for a particular lexicon L can be the total
number of word tokens in the document that occur in L:

fL = (cid:88)w∈L

count (w)

For lexica in which each word is associated with a score or weight, the count can be
multiplied by a weight θ L
w :

fL = (cid:88)w∈L

θ L

w count (w)

Counts can alternatively be logged or normalized per writer as in Eq. 19.14.

19 .9

• CONNOTAT ION FRAM E S

395

However they are deﬁned, these lexicon features are then used in a supervised
classiﬁer to predict the desired affective category for the text or document. Once
a classiﬁer is trained, we can examine which lexicon features are associated with
which classes. For a classiﬁer like logistic regression the feature weight gives an
indication of how associated the feature is with the class.
Thus, for example, Mairesse and Walker (2008) found that for classifying per-
sonality, for the dimension Agreeable, the LIWC lexicons Family and Home were
positively associated while the LIWC lexicons anger and swear were negatively
associated. By contrast, Extroversion was positively associated with the Friend,
Religion and Self lexicons, and Emotional Stability was positively associated with
Sports and negatively associated with Negative Emotion.

(a)

(b)

Figure 19.12 Word clouds from Schwartz et al. (2013), showing words highly associated
with introversion (left) or extroversion (right). The size of the word represents the association
strength (the regression coefﬁcient), while the color (ranging from cold to hot) represents the
relative frequency of the word/phrase (from low to high).

In the situation in which we use all the words and phrases in the document as
potential features, we can use the resulting weights from the learned regression clas-
siﬁer as the basis of an affective lexicon. In the Extroversion/Introversion classiﬁer
of Schwartz et al. (2013), ordinary least-squares regression is used to predict the
value of a personality dimension from all the words and phrases. The resulting re-
gression coefﬁcient for each word or phrase can be used as an association value with
the predicted dimension. The word clouds in Fig. 19.12 show an example of words
associated with introversion (a) and extroversion (b).

19.9 Connotation Frames

connotation
frame

The lexicons we’ve described so far deﬁne a word as a point in affective space. A
connotation frame, by contrast, is lexicon that incorporates a richer kind of gram-
matical structure, by combining affective lexicons with the frame semantic lexicons
of Chapter 18. The basic insight of connotation frame lexicons is that a predicate
like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016,
Rashkin et al. 2017).
Consider sentences like:

(19.15) Country A violated the sovereignty of Country B
(19.16) the teenager ... survived the Boston Marathon bombing”

396 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

By using the verb violate in (19.15), the author is expressing their sympathies with
Country B, portraying Country B as a victim, and expressing antagonism toward
the agent Country A. By contrast, in using the verb survive, the author of (19.16) is
expressing that the bombing is a negative experience, and the subject of the sentence
the teenager, is a sympathetic character. These aspects of connotation are inherent
in the meaning of the verbs violate and survive, as shown in Fig. 19.13.

(a)

(b)

Figure 19.13 Connotation frames for survive and violate. (a) For survive, the writer and reader have positive
sentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate, the
writer and reader have positive sentiment instead toward Role2, the direct object.

The connotation frame lexicons of Rashkin et al. (2016) and Rashkin et al.
(2017) also express other connotative aspects of the predicate toward each argument,
including the effect (something bad happened to x) value: (x is valuable), and mental
state: (x is distressed by the event). Connotation frames can also mark aspects of
power and agency; see Chapter 18 (Sap et al., 2017).
Connotation frames can be built by hand (Sap et al., 2017), or they can be learned
by supervised learning (Rashkin et al., 2016), for example using hand-labeled train-
ing data to supervise classiﬁers for each of the individual relations, e.g., whether
S(writer → Role1) is + or -, and then improving accuracy via global constraints
across all relations.

19.10 Summary

• Many kinds of affective states can be distinguished, including emotions, moods,
attitudes (which include sentiment), interpersonal stance, and personality.
• Emotion can be represented by ﬁxed atomic units often called basic emo-
tions, or as points in space deﬁned by dimensions like valence and arousal.
• Words have connotational aspects related to these affective states, and this
connotational aspect of word meaning can be represented in lexicons.
• Affective lexicons can be built by hand, using crowd sourcing to label the
affective content of each word.
• Lexicons can be built with semi-supervised, bootstrapping from seed words
using similarity metrics like embedding cosine.
• Lexicons can be learned in a fully supervised manner, when a convenient
training signal can be found in the world, such as ratings assigned by users on
a review site.

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

397

• Words can be assigned weights in a lexicon by using various functions of word
counts in training texts, and ratio metrics like log odds ratio informative

Dirichlet prior.

• Personality is often represented as a point in 5-dimensional space.
• Affect can be detected, just like sentiment, by using standard supervised text
classiﬁcation techniques, using all the words or bigrams in a text as features.
Additional features can be drawn from counts of words in lexicons.
• Lexicons can also be used to detect affect in a rule-based classiﬁer by picking
the simple majority sentiment based on counts of words in each lexicon.
• Connotation frames express richer relations of affective meaning that a pred-
icate encodes about its arguments.

Bibliographical and Historical Notes

subjectivity

The idea of formally representing the subjective meaning of words began with Os-
good et al. (1957), the same pioneering study that ﬁrst proposed the vector space
model of meaning described in Chapter 6. Osgood et al. (1957) had participants rate
words on various scales, and ran factor analysis on the ratings. The most signiﬁcant
factor they uncovered was the evaluative dimension, which distinguished between
pairs like good/bad, valuable/worthless, pleasant/unpleasant. This work inﬂuenced
the development of early dictionaries of sentiment and affective meaning in the ﬁeld
of content analysis (Stone et al., 1966).
Wiebe (1994) began an inﬂuential line of work on detecting subjectivity in text,
beginning with the task of identifying subjective sentences and the subjective char-
acters who are described in the text as holding private states, beliefs or attitudes.
Learned sentiment lexicons such as the polarity lexicons of (Hatzivassiloglou and
McKeown, 1997) were shown to be a useful feature in subjectivity detection (Hatzi-
vassiloglou and Wiebe 2000, Wiebe 2000).
The term sentiment seems to have been introduced in 2001 by Das and Chen
(2001), to describe the task of measuring market sentiment by looking at the words in
stock trading message boards. In the same paper Das and Chen (2001) also proposed
the use of a sentiment lexicon. The list of words in the lexicon was created by
hand, but each word was assigned weights according to how much it discriminated
a particular class (say buy versus sell) by maximizing across-class variation and
minimizing within-class variation. The term sentiment, and the use of lexicons,
caught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) ﬁrst showed
the power of using all the words without a sentiment lexicon; see also Wang and
Manning (2012).
Most of the semi-supervised methods we describe for extending sentiment dic-
tionaries drew on the early idea that synonyms and antonyms tend to co-occur in the
same sentence. (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shep-
herd 1997). Other semi-supervized methods for learning cues to affective mean-
ing rely on information extraction techniques, like the AutoSlog pattern extractors
(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were ﬁrst sug-
gested by Hatzivassiloglou and McKeown (1997), and graph propagation became
a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004,
Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by

398 CHA PTER 19

• L EX ICON S FOR S ENT IM EN T, A FFEC T, AND CONNOTAT ION

ﬁltering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,
Fast et al. 2016).
Much recent work focuses on ways to learn embeddings that directly encode
sentiment or other properties, such as the D EN S I FIER algorithm of (Rothe et al.,
2016) that learns to transform the embedding space to focus on sentiment (or other)
information.

CHAPTER

20 Coreference Resolution and
Entity Linking

Placeholder

399

CHAPTER

21 Discourse Coherence

Placeholder

400

CHAPTER

22 Machine Translation

Placeholder

401

402 CHA PTER 23

• QU E S T ION AN SW ER ING

CHAPTER

23 Question Answering

The quest for knowledge is deeply human, and so it is not surprising that practi-
cally as soon as there were computers we were asking them questions. By the early
1960s, systems used the two major paradigms of question answering—information-
retrieval-based and knowledge-based—to answer questions about baseball statis-
tics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought,
the computer that Douglas Adams invented in The Hitchhiker’s Guide to the Galaxy,
managed to answer “the Great Question Of Life The Universe and Everything”.1 In
2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!
using a hybrid architecture that surpassed humans at answering questions like
WILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPAL-
ITIES OF WALLACHIA AND MOLDOVIA” INSPIRED THIS AU-
THOR’S MOST FAMOUS NOVEL2
Most question answering systems focus on factoid questions, questions that can
be answered with simple facts expressed in short texts. The answers to the questions
below can expressed by a personal name, temporal expression, or location:
(23.1) Who founded Virgin Airlines?
(23.2) What is the average age of the onset of autism?
(23.3) Where is Apple Computer based?
In this chapter we describe the two major paradigms for factoid question an-
swering. Information-retrieval or IR-based question answering relies on the vast
quantities of textual information on the web or in collections like PubMed. Given
a user question, information retrieval techniques ﬁrst ﬁnd relevant documents and
passages. Then systems (feature-based, neural, or both) use reading comprehen-
sion algorithms to read these retrieved documents or passages and draw an answer
directly from spans of text.

In the second paradigm, knowledge-based question answering, a system in-

stead builds a semantic representation of the query, mapping What states border
Texas? to the logical representation: λ x.st at e(x) ∧ bord ers(x, t exas), or When was
Ada Lovelace born? to the gapped relation: birth-year (Ada Lovelace, ?x).
These meaning representations are then used to query databases of facts.
Finally, large industrial systems like the DeepQA system in IBM’s Watson are
often hybrids, using both text datasets and structured knowledge bases to answer
questions. DeepQA ﬁnds many candidate answers in both knowledge bases and in
textual sources, and then scores each candidate answer using knowledge sources like
geospatial databases, taxonomical classiﬁcation, or other textual sources.
We describe IR-based approaches (including neural reading comprehension sys-
tems) in the next section, followed by sections on knowledge-based systems, on
Watson Deep QA, and a discussion of evaluation.

1 The answer was 42, but unfortunately the details of the question were never revealed
2 The answer, of course, is Bram Stoker, and the novel was the fantastically Gothic Dracula.

23 . 1

•

IR -BA SED FACTO ID QU E S T ION AN SW ER ING

403

23.1

IR-based Factoid Question Answering

The goal of information retrieval based question answering is to answer a user’s
question by ﬁnding short text segments on the web or some other collection of doc-
uments. Figure 23.1 shows some sample factoid questions and their answers.

Question
Where is the Louvre Museum located?
What’s the abbreviation for limited partnership?
What are the names of Odin’s ravens?
What currency is used in China?
What kind of nuts are used in marzipan?
What instrument does Max Roach play?
What’s the ofﬁcial language of Algeria?
How many pounds are there in a stone?

Figure 23.1 Some sample factoid questions and their answers.

Answer
in Paris, France
L.P.
Huginn and Muninn
the yuan
almonds
drums
Arabic
14

Figure 23.2 shows the three phases of an IR-based factoid question-answering
system: question processing, passage retrieval and ranking, and answer extraction.

Figure 23.2

IR-based factoid question answering has three stages: question processing, passage retrieval, and
answer processing.

23.1.1 Question Processing

The main goal of the question-processing phase is to extract the query: the keywords
passed to the IR system to match potential documents. Some systems additionally
extract further information such as:
• answer type: the entity type (person, location, time, etc.). of the answer
• focus: the string of words in the question that are likely to be replaced by the
answer in any answer string found.
• question type: is this a deﬁnition question, a math question, a list question?
For example, for the question Which US state capital has the largest population?
the query processing might produce:
query: “US state capital has the largest population”

answer type: city

focus: state capital
In the next two sections we summarize the two most commonly used tasks, query
formulation and answer type detection.

404 CHA PTER 23

• QU E S T ION AN SW ER ING

23.1.2 Query Formulation

Query formulation is the task of creating a query—a list of tokens— to send to an
information retrieval system to retrieve documents that might contain answer strings.
For question answering from the web, we can simply pass the entire question
to the web search engine, at most perhaps leaving out the question word (where,
when, etc.). For question answering from smaller sets of documents like corporate
information pages or Wikipedia, we still use an IR engine to index and search our
documents, generally using standard tf-idf cosine matching, but we might need to do
more processing. For example, for searching Wikipedia, it helps to compute tf-idf
over bigrams rather than unigrams in the query and document (Chen et al., 2017).
Or we might need to do query expansion, since while on the web the answer to a
question might appear in many different forms, one of which will probably match
ther question, in smaller document sets an answer might appear only once. Query
expansion methods can add query terms in hopes of matching the particular form of
the answer as it appears, like adding morphological variants of the content words in
the question, or synonyms from a thesaurus.
A query formulation approach that is sometimes used for questioning the web is
to apply query reformulation rules to the query. The rules rephrase the question to
make it look like a substring of possible declarative answers. The question “when
was the laser invented?” might be reformulated as “the laser was invented”; the
question “where is the Valley of the Kings?” as “the Valley of the Kings is located
in”. Here are some sample hand-written reformulation rules from Lin (2007):
(23.4) wh-word did A verb B → . . . A verb+ed B
(23.5) Where is A → A is located in

23.1.3 Answer Types

Some systems make use of question classiﬁcation, the task of ﬁnding the answer
type, the named-entity categorizing the answer. A question like “Who founded Vir-
gin Airlines?” expects an answer of type P ER SON. A question like “What Canadian
city has the largest population?” expects an answer of type C I TY. If we know that
the answer type for a question is a person, we can avoid examining every sentence
in the document collection, instead focusing on sentences mentioning people.
While answer types might just be the named entities like P ER SON, LOCAT ION,
and ORGAN I ZAT ION described in Chapter 17, we can also use a larger hierarchical
set of answer types called an answer type taxonomy. Such taxonomies can be built
automatically, from resources like WordNet (Harabagiu et al. 2000, Pasca 2003), or
they can be designed by hand. Figure 23.4 shows one such hand-built ontology, the
Li and Roth (2005) tagset; a subset is also shown in Fig. 23.3. In this hierarchical
tagset, each question can be labeled with a coarse-grained tag like HUMAN or a ﬁne-

grained tag like HUMAN :D E SCR I P T ION, HUMAN :GROU P, HUMAN : IND, and so on.

The HUMAN :D E SCR I P T ION type is often called a B IOGRA PHY question because the
answer is required to give a brief biography of the person rather than just a name.
Question classiﬁers can be built by hand-writing rules like the following rule
from (Hovy et al., 2002) for detecting the answer type B IOGRA PHY:
(23.6) who {is | was | are | were} PER SON
Most question classiﬁers, however, are based on supervised learning, trained on
databases of questions that have been hand-labeled with an answer type (Li and
Roth, 2002). Either feature-based or neural methods can be used. Feature based

query
reformulation

question
classiﬁcation
answer type

answer type
taxonomy

23 . 1

•

IR -BA SED FACTO ID QU E S T ION AN SW ER ING

405

Figure 23.3 A subset of the Li and Roth (2005) answer types.

methods rely on words in the questions and their embeddings, the part-of-speech of
each word, and named entities in the questions. Often, a single word in the question
gives extra information about the answer type, and its identity is used as a feature.
This word is sometimes called the answer type word or question headword, and
may be deﬁned as the headword of the ﬁrst NP after the question’s wh-word; head-
words are indicated in boldface in the following examples:
(23.7) Which city in China has the largest number of foreign ﬁnancial companies?
(23.8) What is the state ﬂower of California?
In general, question classiﬁcation accuracies are relatively high on easy ques-
tion types like PER SON, LOCAT ION, and T IM E questions; detecting R EA SON and
D E SCR I P T ION questions can be much harder.

23.1.4 Document and Passage Retrieval

The IR query produced from the question processing stage is sent to an IR engine,
resulting in a set of documents ranked by their relevance to the query. Because
most answer-extraction methods are designed to apply to smaller regions such as
paragraphs, QA systems next divide the top n documents into smaller passages such
as sections, paragraphs, or sentences. These might be already segmented in the
source document or we might need to run a paragraph segmentation algorithm.
The simplest form of passage retrieval is then to simply pass along every pas-
sages to the answer extraction stage. A more sophisticated variant is to ﬁlter the
passages by running a named entity or answer type classiﬁcation on the retrieved
passages. Passages that don’t contain the answer type that was assigned to the ques-
tion are discarded.
It’s also possible to use supervised learning to fully rank the remaining passages,
using features like:
• The number of named entities of the right type in the passage
• The number of question keywords in the passage
• The longest exact sequence of question keywords that occurs in the passage
• The rank of the document from which the passage was extracted
• The proximity of the keywords from the original query to each other (Pasca 2003,
Monz 2004).
• The number of n-grams that overlap between the passage and the question
(Brill et al., 2002).

passages

passage
retrieval

406 CHA PTER 23

• QU E S T ION AN SW ER ING

Tag

ABBREVIATION
abb
exp
DESCRIPTION
deﬁnition
description
manner
reason
ENTITY
animal
body
color
creative
currency
disease/medicine
event
food
instrument
lang
letter
other
plant
product
religion
sport
substance
symbol
technique
term
vehicle
word
HUMAN
description
group
ind
title
LOCATION
city
country
mountain
other
state
NUMERIC
code
count
date
distance
money
order
other
period
percent
temp
speed
size
weight

Example

What’s the abbreviation for limited partnership?
What does the “c” stand for in the equation E=mc2?

What are tannins?
What are the words to the Canadian National anthem?
How can you get rust stains out of clothing?
What caused the Titanic to sink?

What are the names of Odin’s ravens?
What part of your body contains the corpus callosum?
What colors make up a rainbow?
In what book can I ﬁnd the story of Aladdin?
What currency is used in China?
What does Salk vaccine prevent?
What war involved the battle of Chapultepec?
What kind of nuts are used in marzipan?
What instrument does Max Roach play?
What’s the ofﬁcial language of Algeria?
What letter appears on the cold-water tap in Spain?
What is the name of King Arthur’s sword?
What are some fragrant white climbing roses?
What is the fastest computer?
What religion has the most members?
What was the name of the ball game played by the Mayans?
What fuel do airplanes use?
What is the chemical symbol for nitrogen?
What is the best way to remove wallpaper?
How do you say “ Grandma” in Irish?
What was the name of Captain Bligh’s ship?
What’s the singular of dice?

Who was Confucius?
What are the major companies that are part of Dow Jones?
Who was the ﬁrst Russian astronaut to do a spacewalk?
What was Queen Victoria’s title regarding India?

What’s the oldest capital city in the Americas?
What country borders the most others?
What is the highest peak in Africa?
What river runs through Liverpool?
What states do not have state income tax?

What is the telephone number for the University of Colorado?
About how many soldiers died in World War II?
What is the date of Boxing Day?
How long was Mao’s 1930s Long March?
How much did a McDonald’s hamburger cost in 1963?
Where does Shanghai rank among world cities in population?
What is the population of Mexico?
What was the average life expectancy during the Stone Age?
What fraction of a beaver’s life is spent swimming?
How hot should the oven be when making Peachy Oat Mufﬁns?
How fast must a spacecraft travel to escape Earth’s gravity?
What is the size of Argentina?
How many pounds are there in a stone?

Figure 23.4 Question typology from Li and Roth (2002), (2005). Example sentences are
from their corpus of 5500 labeled questions. A question can be labeled either with a coarse-
grained tag like HUMAN or NUM ER IC or with a ﬁne-grained tag like HUMAN :D E SCR I P T ION,

HUMAN :GROU P, HUMAN : IND, and so on.

23 . 1

•

IR -BA SED FACTO ID QU E S T ION AN SW ER ING

407

snippets

For question answering from the web we can instead take snippets from the Web
search engine (see Fig. 23.5) as the passages.

Figure 23.5 Five snippets from Google in response to the query When was movable type
metal printing invented in Korea?

23.1.5 Answer Extraction

span

The ﬁnal stage of question answering is to extract a speciﬁc answer from the passage,
for example responding 29,029 feet to a question like “How tall is Mt. Everest?”.
This task is commonly modeled by span labeling: given a passage, identifying the
span of text which constitutes an answer.
A simple baseline algorithm for answer extraction is to run a named entity tagger
on the candidate passage and return whatever span in the passage is the correct an-
swer type. Thus, in the following examples, the underlined named entities would be
extracted from the passages as the answer to the HUMAN and D I STANCE -QUAN T I TY
questions:

“Who is the prime minister of India?”
Manmohan Singh, Prime Minister of India, had told left leaders that the
deal would not be renegotiated.

“How tall is Mt. Everest?”
The ofﬁcial height of Mount Everest is 29029 feet

408 CHA PTER 23

• QU E S T ION AN SW ER ING

Unfortunately, the answers to many questions, such as D E FIN I T ION questions,
don’t tend to be of a particular named entity type. For this reason modern work on
answer extraction uses more sophisticated algorithms, generally based on supervised
learning. The next section introduces a simple feature-based classiﬁer, after which
we turn to modern neural algorithms.

23.1.6 Feature-based Answer Extraction

Supervised learning approaches to answer extraction train classiﬁers to decide if a
span or a sentence contains an answer. One obviously useful feature is the answer
type feature of the above baseline algorithm. Hand-written regular expression pat-
terns also play a role, such as the sample patterns for deﬁnition questions in Fig. 23.6.

Pattern

Question

Answer

<AP> such as <QP> What is autism?
“, developmental disorders such as autism”
<QP>, a <AP>
What is a caldera? “the Long Valley caldera, a volcanic crater 19
miles long”
Figure 23.6 Some answer-extraction patterns using the answer phrase (AP) and question
phrase (QP) for deﬁnition questions (Pasca, 2003).

Other features in such classiﬁers include:
Answer type match: True if the candidate answer contains a phrase with the cor-
rect answer type.
Pattern match: The identity of a pattern that matches the candidate answer.

Number of matched question keywords: How many question keywords are con-

tained in the candidate answer.
Keyword distance: The distance between the candidate answer and query key-
words
Novelty factor: True if at least one word in the candidate answer is novel, that is,
not in the query.
Apposition features: True if the candidate answer is an appositive to a phrase con-
taining many question terms. Can be approximated by the number of question
terms separated from the candidate answer through at most three words and
one comma (Pasca, 2003).
Punctuation location: True if the candidate answer is immediately followed by a
comma, period, quotation marks, semicolon, or exclamation mark.
Sequences of question terms: The length of the longest sequence of question
terms that occurs in the candidate answer.

23.1.7 N-gram tiling answer extraction

An alternative approach to answer extraction, used solely in Web search, is based
on n-gram tiling, an approach that relies on the redundancy of the web (Brill
et al. 2002, Lin 2007). This simpliﬁed method begins with the snippets returned
from the Web search engine, produced by a reformulated query. In the ﬁrst step,
n-gram mining, every unigram, bigram, and trigram occurring in the snippet is ex-
tracted and weighted. The weight is a function of the number of snippets in which
the n-gram occurred, and the weight of the query reformulation pattern that returned
it.
In the n-gram ﬁltering step, n-grams are scored by how well they match the
predicted answer type. These scores are computed by hand-written ﬁlters built for

n-gram tiling

23 . 1

•

IR -BA SED FACTO ID QU E S T ION AN SW ER ING

409

each answer type. Finally, an n-gram tiling algorithm concatenates overlapping n-
gram fragments into longer answers. A standard greedy method is to start with the
highest-scoring candidate and try to tile each other candidate with this candidate.
The best-scoring concatenation is added to the set of candidates, the lower-scoring
candidate is removed, and the process continues until a single answer is built.

23.1.8 Neural Answer Extraction

Neural network approaches to answer extraction draw on the intuition that a question
and its answer are semantically similar in some appropriate way. As we’ll see, this
intuition can be ﬂeshed out by computing an embedding for the question and an
embedding for each token of the passage, and then selecting passage spans whose
embeddings are closest to the question embedding.

Reading Comprehension Datasets. Because neural answer extractors are often
designed in the context of the reading comprehension task, let’s begin by talking
about that task. It was Hirschman et al. (1999) who ﬁrst proposed to take children’s
reading comprehension tests—pedagogical instruments in which a child is given
a passage to read and must answer questions about it—and use them to evaluate
machine text comprehension algorithm. They acquired a corpus of 120 passages
with 5 questions each designed for 3rd-6th grade children, built an answer extraction
system, and measured how well the answers given by their system corresponded to
the answer key from the test’s publisher.
Modern reading comprehension systems tend to use collections of questions that
are designed speciﬁcally for NLP, and so are large enough for training supervised
learning systems. For example the Stanford Question Answering Dataset (SQuAD)
consists of passages from Wikipedia and associated questions whose answers are
spans from the passage, as well as some questions that are designed to be unan-
swerable (Rajpurkar et al. 2016, Rajpurkar et al. 2018); a total of just over 150,000
questions. Fig. 23.7 shows a (shortened) excerpt from a SQUAD 2.0 passage to-
gether with three questions and their answer spans.

reading
comprehension

SQuAD

Beyonc ´e Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter,
record producer and actress. Born and raised in Houston, Texas, she performed in various
singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer
of R&B girl-group Destiny’s Child. Managed by her father, Mathew Knowles, the group became
one of the world’s best-selling girl groups of all time. Their hiatus saw the release of Beyonc ´e’s
debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned
ﬁve Grammy Awards and featured the Billboard Hot 100 number-one singles “Crazy in Love” and
“Baby Boy”.
Q: “In what city and state did Beyonc ´e grow up?”
A: “Houston, Texas”
Q: “What areas did Beyonc ´e compete in when she was growing up?”
A: “singing and dancing”
Q: “When did Beyonc ´e release Dangerously in Love?”
A: “2003”

Figure 23.7 A (Wikipedia) passage from the SQuAD 2.0 dataset (Rajpurkar et al., 2018) with 3 sample
questions and the labeled answer spans.

SQuAD was build by having humans write questions for a given Wikipedia
passage and choose the answer span. Other datasets used similar techniques; the

410 CHA PTER 23

• QU E S T ION AN SW ER ING

sentence
selection

NewsQA dataset consists of 100,000 question-answer pairs from CNN news arti-
cles, For other datasets like WikiQA the span is the entire sentence containing the
answer (Yang et al., 2015); the task of choosing a sentence rather than a smaller
answer span is sometimes called the sentence selection task.
These reading comprehension datasets are used both as a reading comprehension
task in themselves, and as a training set and evaluation set for the sentence extraction
component of open question answering algorithms.

Basic Reading Comprehension Algorithm. Neural algorithms for reading com-

prehension are given a question q of l tokens q1 , ..., ql ¡ and a passage p of m tokens
p1 , ..., pm . Their goal is to compute, for each token pi the probability pstart (i) that
pi is the start of the answer span, and the probability pend (i), that pi is the end of
the answer span.
Fig. 23.8 shows the architecture of the Document Reader component of the
DrQA system of Chen et al. (2017). Like most such systems, DrQA builds an
embedding for the question, builds an embedding for each token in the passage,
computes a similarity function between the question and each passage word in con-
text, and then uses the question-passage similarity scores to decide where the answer
span starts and ends.

Figure 23.8 The question answering system of Chen et al. (2017), considering part of the question When did
Beyonc ´e release Dangerously in Love? and the passage starting Beyonc ´e’s debut album, Dangerously in Love
(2003).

Let’s consider the algorithm in detail, following closely the description in Chen
et al. (2017). The question is represented by a single embedding q, which is a
weighted sum of representations for each question word qi .
It is computed by
passing the series of embeddings PE(q1 ), ..., E(ql ) of question words through an
RNN (such as a bi-LSTM shown in Fig. 23.8). The resulting hidden representations
{q1 , ..., ql } are combined by a weighted sum

b j q j

(23.9)

q = (cid:88)j

23 . 2

• KNOW LEDGE -BA SED QU E S T ION AN SW ER ING

411

The weight b j is a measure of the relevance of each question word, and relies on a
learned weight vector w:

b j =

ex p(w · q j )
(cid:80) j (cid:48) ex p(w · q(cid:48)j )

(23.10)

To compute the passage embedding {p1 , ..., pm} we ﬁrst form an input represen-
tation ˜p = {˜p1 , ..., ˜pm } by concatenating four components:
• An embedding for each word E( pi ) such as from GLoVE (Pennington et al.,
2014).
• Token features like the part of speech of pi , or the named entity tag of pi , from
running POS or NER taggers.
• Exact match features representing whether the passage word pi occurred in
the question: 1( pi ∈ q). Separate exact match features might be used for
lemmatized or lower-cased versions of the tokens.
• Aligned question embedding: In addition to the exact match features, many
QA systems use an attention mechanism to give a more sophisticated model of
similarity between the passage and question words, such as similar but non-
identical words like release and singles. For example a weighted similarity
(cid:80) j ai, j E(q j ) can be used, where the attention weight ai, j encodes the simi-
larity between pi and each question word q j . This attention weight can be
computed as the dot product between functions α of the word embeddings of
the question and passage:

qi, j =

exp(α (E( pi )) · α (E(q j )))
(cid:80) j (cid:48) exp(α (E( pi )) · α (E(q(cid:48)j )))

α (·) can be a simple feed forward network.
We then pass ˜p through a biLSTM:

{p1 , ..., pm }) = RNN ({˜p1 , ..., ˜pm })

(23.11)

(23.12)

The result of the previous two step is a single question embedding q and a rep-
resentations for each word in the passage {p1 , ..., pm }. In order to ﬁnd the answer
span, we can train two separate classiﬁers, one to compute for each pi the probability
pstart (i) that pi is the start of the answer span, and one to compute the probability
pend (i). While the classiﬁers could just take the dot product between the passage
and question embeddings as input, it turns out to work better to learn a more sophis-
ticated similarity function, like a bilinear attention layer W:

pstart (i) ∝ ex p(piWsq)
pend (i) ∝ ex p(piWeq)
These neural answer extractors can be trained end-to-end by using datasets like
SQuAD.

(23.13)

23.2 Knowledge-based Question Answering

While an enormous amount of information is encoded in the vast amount of text
on the web, information obviously also exists in more structured forms. We use

412 CHA PTER 23

• QU E S T ION AN SW ER ING

the term knowledge-based question answering for the idea of answering a natural
language question by mapping it to a query over a structured database. Like the text-
based paradigm for question answering, this approach dates back to the earliest days
of natural language processing, with systems like BASEBALL (Green et al., 1961)
that answered questions from a structured database of baseball games and stats.
Systems for mapping from a text string to any logical form are called seman-
tic parsers. Semantic parsers for question answering usually map either to some
version of predicate calculus or a query language like SQL or SPARQL, as in the
examples in Fig. 23.9.

Question

When was Ada Lovelace born?
What states border Texas?
What is the largest state
How many people survived the sinking of
the Titanic

Logical form

birth-year (Ada Lovelace, ?x)

λ x.state(x) ∧ borders(x,texas)
argmax(λ x.state(x), λ x.size(x))

(count (!fb:event.disaster.survivors
fb:en.sinking of the titanic))

Figure 23.9 Sample logical forms produced by a semantic parser for question answering. These range from
simple relations like birth-year, or relations normalized to databases like Freebase, to full predicate calculus.

The logical form of the question is thus either in the form of a query or can easily
be converted into one. The database can be a full relational database, or simpler
structured databases like sets of RDF triples. Recall from Chapter 17 that an RDF
triple is a 3-tuple, a predicate with two arguments, expressing some simple relation
or proposition. Popular ontologies like Freebase (Bollacker et al., 2008) or DBpedia
(Bizer et al., 2009) have large numbers of triples derived from Wikipedia infoboxes,
the structured tables associated with certain Wikipedia articles.
The simplest formation of the knowledge-based question answering task is to
answer factoid questions that ask about one of the missing arguments in a triple.
Consider an RDF triple like the following:

subject

predicate object

Ada Lovelace birth-year 1815
This triple can be used to answer text questions like ‘When was Ada Lovelace
born?’ or ‘Who was born in 1815?’. Question answering in this paradigm requires
mapping from textual strings like ”When was ... born” to canonical relations in the
knowledge base like birth-year. We might sketch this task as:
“When was Ada Lovelace born?” → birth-year (Ada Lovelace, ?x)
“What is the capital of England?” → capital-city(?x, England)

23.2.1 Rule-based Methods

For relations that are very frequent, it may be worthwhile to write hand-written rules
to extract relations from the question, just as we saw in Section 17.2. For example,
to extract the birth-year relation, we could write patterns that search for the question
word When, a main verb like born, and that extract the named entity argument of the
verb.

23.2.2 Supervised Methods

In some cases we have supervised data, consisting of a set of questions paired with
their correct logical form like the examples in Fig. 23.9. The task is then to take

23 . 2

• KNOW LEDGE -BA SED QU E S T ION AN SW ER ING

413

those pairs of training tuples and produce a system that maps from new questions to
their logical forms.
Most supervised algorithms for learning to answer these simple questions about
relations ﬁrst parse the questions and then align the parse trees to the logical form.
Generally these systems bootstrap by having a small set of rules for building this
mapping, and an initial lexicon as well. For example, a system might have built-
in strings for each of the entities in the system (Texas, Ada Lovelace), and then
have simple default rules mapping fragments of the question parse tree to particular
relations:

nsubj

dobj

Who V ENTITY → relation( ?x, entity)

tmod

nsubj

When V ENTITY → relation( ?x, entity)

Then given these rules and the lexicon, a training tuple like the following:
“When was Ada Lovelace born?” → birth-year (Ada Lovelace, ?x)
would ﬁrst be parsed, resulting in the following mapping.

tmod

nsubj

When was Ada Lovelace born → birth-year(Ada Lovelace, ?x)

From many pairs like this, we could induce mappings between pieces of parse
fragment, such as the mapping between the parse fragment on the left and the rela-
tion on the right:

tmod

nsubj

When was · born → birth-year( , ?x)

A supervised system would thus parse each tuple in the training set and induce a
bigger set of such speciﬁc rules, allowing it to map unseen examples of “When was
X born?” questions to the birth-year relation. Rules can furthermore be associ-
ated with counts based on the number of times the rule is used to parse the training
data. Like rule counts for probabilistic grammars, these can be normalized into prob-
abilities. The probabilities can then be used to choose the highest probability parse
for sentences with multiple semantic interpretations.
The supervised approach can be extended to deal with more complex questions
that are not just about single relations. Consider the question What is the biggest
state bordering Texas? —taken from the GeoQuery database of questions on U.S.
Geography (Zelle and Mooney, 1996)—with the semantic form: argmax(λ x.st at e(x) ∧
bord ers(x, t exas), λ x.size(x)) This question has much more complex structures than
the simple single-relation questions we considered above, such as the argmax func-
tion, the mapping of the word biggest to size and so on. Zettlemoyer and Collins
(2005) shows how more complex default rules (along with richer syntactic struc-
tures) can be used to learn to map from text sentences to more complex logical
forms. The rules take the training set’s pairings of sentence and meaning as above

414 CHA PTER 23

• QU E S T ION AN SW ER ING

and use the complex rules to break each training example down into smaller tuples
that can then be recombined to parse new sentences.

23.2.3 Dealing with Variation: Semi-Supervised Methods

Because it is difﬁcult to create training sets with questions labeled with their mean-
ing representation, supervised datasets can’t cover the wide variety of forms that
even simple factoid questions can take. For this reason most techniques for mapping
factoid questions to the canonical relations or other structures in knowledge bases
ﬁnd some way to make use of textual redundancy.
The most common source of redundancy, of course, is the web, which contains
vast number of textual variants expressing any relation. For this reason, most meth-
ods make some use of web text, either via semi-supervised methods like distant

supervision or unsupervised methods like open information extraction, both intro-

duced in Chapter 17. For example the REVERB open information extractor (Fader
et al., 2011) extracts billions of (subject, relation, object) triples of strings from the
web, such as (“Ada Lovelace”,“was born in”, “1815”). By aligning these strings
with a canonical knowledge source like Wikipedia, we create new relations that can
be queried while simultaneously learning to map between the words in question and
canonical relations.
To align a REVERB triple with a canonical knowledge source we ﬁrst align
the arguments and then the predicate. Recall from Chapter 20 that linking a string
like “Ada Lovelace” with a Wikipedia page is called entity linking; we thus rep-
resent the concept ‘Ada Lovelace’ by a unique identiﬁer of a Wikipedia page. If
this subject string is not associated with a unique page on Wikipedia, we can dis-
ambiguate which page is being sought, for example by using the cosine distance
between the triple string (‘Ada Lovelace was born in 1815’) and each candidate
Wikipedia page. Date strings like ‘1815’ can be turned into a normalized form us-
ing standard tools for temporal normalization like SUTime (Chang and Manning,
2012). Once we’ve aligned the arguments, we align the predicates. Given the Free-

base relation people.person.birthdate(ada lovelace,1815) and the string

‘Ada Lovelace was born in 1815’, having linked Ada Lovelace and normalized
1815, we learn the mapping between the string ‘was born in’ and the relation peo-
ple.person.birthdate. In the simplest case, this can be done by aligning the relation
with the string of words in between the arguments; more complex alignment algo-
rithms like IBM Model 1 (Chapter 22) can be used. Then if a phrase aligns with a
predicate across many entities, it can be extracted into a lexicon for mapping ques-
tions to relations.
Here are some examples from such a resulting lexicon, produced by Berant
et al. (2013), giving many variants of phrases that align with the Freebase relation
country.capital between a country and its capital city:

capital of
capital city of
become capital of
capitol of
national capital of
ofﬁcial capital of
political capital of
administrative capital of
beautiful capital of
capitol city of
remain capital of
make capital of
political center of
bustling capital of
capital city in
cosmopolitan capital of
move its capital to
modern capital of
federal capital of
beautiful capital city of
administrative capital city of
Figure 23.10 Some phrases that align with the Freebase relation country.capital from
Berant et al. (2013).

23 . 3

• U S ING MULT I P LE IN FORMAT ION SOURC E S : IBM ’ S WAT SON

415

Another useful source of linguistic redundancy are paraphrase databases. For ex-
ample the site wikianswers.com contains millions of pairs of questions that users
have tagged as having the same meaning, 18 million of which have been collected
in the PARALEX corpus (Fader et al., 2013). Here’s an example:

Q: What are the green blobs in plant cells?

Lemmatized synonyms from PARALEX:
what be the green blob in plant cell?
what be green part in plant cell?
what be the green part of a plant cell?
what be the green substance in plant cell?
what be the part of plant cell that give it green color?
what cell part do plant have that enable the plant to be give a green color?
what part of the plant cell turn it green?
part of the plant cell where the cell get it green color?
the green part in a plant be call?
the part of the plant cell that make the plant green be call?
The resulting millions of pairs of question paraphrases can be aligned to each
other using MT alignment approaches to create an MT-style phrase table for trans-
lating from question phrases to synonymous phrases. These can be used by question
answering algorithms to generate all paraphrases of a question as part of the process
of ﬁnding an answer (Fader et al. 2013, Berant and Liang 2014).

23.3 Using multiple information sources: IBM’s Watson

Of course there is no reason to limit ourselves to just text-based or knowledge-based
resources for question answering. The Watson system from IBM that won the Jeop-
ardy! challenge in 2011 is an example of a system that relies on a wide variety of
resources to answer questions.

Figure 23.11 The 4 broad stages of Watson QA: (1) Question Processing, (2) Candidate Answer Generation,
(3) Candidate Answer Scoring, and (4) Answer Merging and Conﬁdence Scoring.

Figure 23.11 shows the 4 stages of the DeepQA system that is the question an-

416 CHA PTER 23

• QU E S T ION AN SW ER ING

swering component of Watson.
The ﬁrst stage is question processing. The DeepQA system runs parsing, named
entity tagging, and relation extraction on the question. Then, like the text-based
systems in Section 23.1, the DeepQA system extracts the focus, the answer type

(also called the lexical answer type or LAT), and performs question classiﬁcation
and question sectioning.

Consider these Jeopardy! examples, with a category followed by a question:
Poets and Poetry: He was a bank clerk in the Yukon before he published
“Songs of a Sourdough” in 1907.

THEATRE: A new play based on this Sir Arthur Conan Doyle canine

classic opened on the London stage in 2007.
The questions are parsed, named entities are extracted (Sir Arthur Conan Doyle
identiﬁed as a P ER SON, Yukon as a G EO PO L I T ICA L EN T I TY, “Songs of a Sour-
dough” as a COM PO S IT ION), coreference is run (he is linked with clerk) and rela-
tions like the following are extracted:

focus

lexical answer
type

authorof(focus,“Songs of a sourdough”)
publish (e1, he, “Songs of a sourdough”)
in (e2, e1, 1907)
temporallink(publish(...), 1907)
Next DeepQA extracts the question focus, shown in bold in both examples. The
focus is the part of the question that co-refers with the answer, used for example to
align with a supporting passage. The focus is extracted by hand-written rules—made
possible by the relatively stylized syntax of Jeopardy! questions—such as a rule
extracting any noun phrase with determiner “this” as in the Conan Doyle example,
and rules extracting pronouns like she, he, hers, him, as in the poet example.
The lexical answer type (shown in blue above) is a word or words which tell
us something about the semantic type of the answer. Because of the wide variety
of questions in Jeopardy!, Jeopardy! uses a far larger set of answer types than the
sets for standard factoid algorithms like the one shown in Fig. 23.4. Even a large
set of named entity tags is insufﬁcient to deﬁne a set of answer types. The DeepQA
team investigated a set of 20,000 questions and found that a named entity tagger
with over 100 named entity types covered less than half the types in these questions.
Thus DeepQA extracts a wide variety of words to be answer types; roughly 5,000
lexical answer types occurred in the 20,000 questions they investigated, often with
multiple answer types in each question.
These lexical answer types are again extracted by rules: the default rule is to
choose the syntactic headword of the focus. Other rules improve this default choice.
For example additional lexical answer types can be words in the question that are
coreferent with or have a particular syntactic relation with the focus, such as head-
words of appositives or predicative nominatives of the focus. In some cases even the
Jeopardy! category can act as a lexical answer type, if it refers to a type of entity
that is compatible with the other lexical answer types. Thus in the ﬁrst case above,
he, poet, and clerk are all lexical answer types. In addition to using the rules directly
as a classiﬁer, they can instead be used as features in a logistic regression classiﬁer
that can return a probability as well as a lexical answer type.
Note that answer types function quite differently in DeepQA than the purely IR-
based factoid question answerers. In the algorithm described in Section 23.1, we
determine the answer type, and then use a strict ﬁltering algorithm only considering
text strings that have exactly that type. In DeepQA, by contrast, we extract lots of

23 . 3

• U S ING MULT I P LE IN FORMAT ION SOURC E S : IBM ’ S WAT SON

417

answers, unconstrained by answer type, and a set of answer types, and then in the
later ‘candidate answer scoring’ phase, we simply score how well each answer ﬁts
the answer types as one of many sources of evidence.
Finally the question is classiﬁed by type (deﬁnition question, multiple-choice,
puzzle, ﬁll-in-the-blank). This is generally done by writing pattern-matching regular
expressions over words or parse trees.
In the second candidate answer generation stage, we combine the processed
question with external documents and other knowledge sources to suggest many
candidate answers. These candidate answers can either be extracted from text docu-
ments or from structured knowledge bases.
For structured resources like DBpedia, IMDB, or the triples produced by Open
Information Extraction, we can just query these stores with the relation and the
known entity, just as we saw in Section 23.2. Thus if we have extracted the rela-

tion authorof(focus,"Songs of a sourdough"), we can query a triple store
with authorof(?x,"Songs of a sourdough") to return the correct author.

The method for extracting answers from text depends on the type of text docu-
ments. To extract answers from normal text documents we can do passage search
just as we did in Section 23.1. As we did in that section, we need to generate a query
from the question; for DeepQA this is generally done by eliminating stop words, and
then upweighting any terms which occur in any relation with the focus. For example
from this query:
MOVIE-“ING”: Robert Redford and Paul Newman starred in this depression-
era grifter ﬂick. (Answer: “The Sting”)
the following weighted query might be extracted:
(2.0 Robert Redford) (2.0 Paul Newman) star depression era grifter (1.5 ﬂick)
The query can now be passed to a standard IR system. DeepQA also makes
use of the convenient fact that the vast majority of Jeopardy! answers are the title
of a Wikipedia document. To ﬁnd these titles, we can do a second text retrieval
pass speciﬁcally on Wikipedia documents. Then instead of extracting passages from
the retrieved Wikipedia document, we directly return the titles of the highly ranked
retrieved documents as the possible answers.
Once we have a set of passages, we need to extract candidate answers. If the
document happens to be a Wikipedia page, we can just take the title, but for other
texts, like news documents, we need other approaches. Two common approaches
are to extract all anchor texts in the document (anchor text is the text between <a>
and <\a> used to point to a URL in an HTML page), or to extract all noun phrases
in the passage that are Wikipedia document titles.
The third candidate answer scoring stage uses many sources of evidence to
score the candidates. One of the most important is the lexical answer type. DeepQA
includes a system that takes a candidate answer and a lexical answer type and returns
a score indicating whether the candidate answer can be interpreted as a subclass or
instance of the answer type. Consider the candidate “difﬁculty swallowing” and
the lexical answer type “manifestation”. DeepQA ﬁrst matches each of these words
with possible entities in ontologies like DBpedia and WordNet. Thus the candidate
“difﬁculty swallowing” is matched with the DBpedia entity “Dysphagia”, and then
that instance is mapped to the WordNet type “Symptom”. The answer type “man-
ifestation” is mapped to the WordNet type “Condition”. The system looks for a
link of hyponymy, instance-of or synonymy between these two types; in this case a
hyponymy relation is found between “Symptom” and “Condition”.

anchor texts

418 CHA PTER 23

• QU E S T ION AN SW ER ING

Other scorers are based on using time and space relations extracted from DBpe-
dia or other structured databases. For example, we can extract temporal properties
of the entity (when was a person born, when died) and then compare to time expres-
sions in the question. If a time expression in the question occurs chronologically
before a person was born, that would be evidence against this person being the an-
swer to the question.
Finally, we can use text retrieval to help retrieve evidence supporting a candidate
answer. We can retrieve passages with terms matching the question, then replace the
focus in the question with the candidate answer and measure the overlapping words
or ordering of the passage with the modiﬁed question.
The output of this stage is a set of candidate answers, each with a vector of
scoring features.
The ﬁnal answer merging and scoring step ﬁrst merges candidate answers that
are equivalent. Thus if we had extracted two candidate answers J.F.K. and John
F. Kennedy, this stage would merge the two into a single candidate. One useful
kind of resource are synonym dictionaries that are created by listing all anchor text
strings that point to the same Wikipedia page; such dictionaries give large num-
bers of synonyms for each Wikipedia title — e.g., JFK, John F. Kennedy, John
Fitzgerald Kennedy, Senator John F. Kennedy, President Kennedy, Jack Kennedy,
etc. (Spitkovsky and Chang, 2012). For common nouns, we can use morphological
parsing to merge candidates which are morphological variants.
We then merge the evidence for each variant, combining the scoring feature
vectors for the merged candidates into a single vector.
Now we have a set of candidates, each with a feature vector. A classiﬁer takes
each feature vector and assigns a conﬁdence value to this candidate answer. The
classiﬁer is trained on thousands of candidate answers, each labeled for whether it
is correct or incorrect, together with their feature vectors, and learns to predict a
probability of being a correct answer. Since, in training, there are far more incorrect
answers than correct answers, we need to use one of the standard techniques for
dealing with very imbalanced data. DeepQA uses instance weighting, assigning an
instance weight of .5 for each incorrect answer example in training. The candidate
answers are then sorted by this conﬁdence value, resulting in a single best answer.3
In summary, we’ve seen in the four stages of DeepQA that it draws on the intu-
itions of both the IR-based and knowledge-based paradigms. Indeed, Watson’s ar-
chitectural innovation is its reliance on proposing a very large number of candidate
answers from both text-based and knowledge-based sources and then developing a
wide variety of evidence features for scoring these candidates —again both text-
based and knowledge-based. See the papers mentioned at the end of the chapter for
more details.

23.4 Evaluation of Factoid Answers

mean
reciprocal rank
MRR

A common evaluation metric for factoid question answering, introduced in the TREC
Q/A track in 1999, is mean reciprocal rank, or MRR. MRR assumes a test set of
questions that have been human-labeled with correct answers. MRR also assumes

3 The merging and ranking is actually run iteratively; ﬁrst the candidates are ranked by the classiﬁer,
giving a rough ﬁrst value for each candidate answer, then that value is used to decide which of the variants
of a name to select as the merged answer, then the merged answers are re-ranked,.

23 .4

• EVALUAT ION O F FACTO ID AN SW ER S

419

that systems are returning a short ranked list of answers or passages containing an-
swers. Each question is then scored according to the reciprocal of the rank of the
ﬁrst correct answer. For example if the system returned ﬁve answers but the ﬁrst
three are wrong and hence the highest-ranked correct answer is ranked fourth, the
reciprocal rank score for that question would be 1
4 . Questions with return sets that
do not contain any correct answers are assigned a zero. The score of a system is
then the average of the score for each question in the set. More formally, for an
evaluation of a system returning a set of ranked answers for a test set consisting of
N questions, the MRR is deﬁned as

MRR =

1
N

N(cid:88)i=1 s.t. ranki (cid:54)=0

1
ranki

(23.14)

Reading comprehension systems on datasets like SQuAD are often evaluated
using two metrics, both ignoring punctuations and articles (a, an, the) (Rajpurkar
et al., 2016):
• Exact match: The percentage of predicted answers that match the gold answer
exactly.
• F1 score: The average overlap between predicted and gold answers. Treat the
prediction and gold as a bag of tokens, and compute F1, averaging the F1 over
all questions.

A number of test sets are available for question answering. Early systems used
the TREC QA dataset; questions and hand-written answers for TREC competitions
from 1999 to 2004 are publicly available. TriviaQA (Joshi et al., 2017) has 650K
question-answer evidence triples, from 95K hand-created question-answer pairs to-
gether with on average six supporting evidence documents collected retrospectively
from Wikipedia and the Web.
Another family of datasets starts from W EBQU E S T ION S (Berant et al., 2013),
which contains 5,810 questions asked by web users, each beginning with a wh-
word and containing exactly one entity. Questions are paired with hand-written an-
swers drawn from the Freebase page of the question’s entity. W EBQU E S T ION SSP
(Yih et al., 2016) augments W EBQU E S T ION S with human-created semantic parses
(SPARQL queries) for those questions answerable using Freebase. COM PL EXW E -
BQU E S T ION S augments the dataset with compositional and other kinds of complex
questions, resulting in 34,689 question questions, along with answers, web snippets,
and SPARQL queries. (Talmor and Berant, 2018).
There are a wide variety of datasets for training and testing reading comprehen-
sion/answer extraction in addition to the SQuAD (Rajpurkar et al., 2016) and Wik-
iQA (Yang et al., 2015) datasets discussed on page 410. The NarrativeQA (Ko ˇcisk `y
et al., 2018) dataset, for example, has questions based on entire long documents like
books or movie scripts, while the Question Answering in Context (QuAC) dataset
(Choi et al., 2018) has 100K questions created by two crowdworkers who are asking
and answering questions about a hidden Wikipedia text.
Others take their structure from the fact that reading comprehension tasks de-
signed for children tend to be multiple choice, with the task being to choose among
the given answers. The MCTest dataset uses this structure, with 500 ﬁctional short
stories created by crowd workers with questions and multiple choice answers (Richard-
son et al., 2013). The AI2 Reasoning Challenge (ARC) (Clark et al., 2018), has
questions that are designed to be hard to answer from simple lexical methods:

420 CHA PTER 23

• QU E S T ION AN SW ER ING

Which property of a mineral can be determined just by looking at it?
(A) luster [correct] (B) mass (C) weight (D) hardness
This ARC example is difﬁcult because the correct answer luster is unlikely to cooc-
cur frequently on the web with phrases like looking at it, while the word mineral is
highly associated with the incorrect answer hardness.

Bibliographical and Historical Notes

Question answering was one of the earliest NLP tasks, and early versions of the text-
based and knowledge-based paradigms were developed by the very early 1960s. The
text-based algorithms generally relied on simple parsing of the question and of the
sentences in the document, and then looking for matches. This approach was used
very early on (Phillips, 1960) but perhaps the most complete early system, and one
that strikingly preﬁgures modern relation-based systems, was the Protosynthex sys-
tem of Simmons et al. (1964). Given a question, Protosynthex ﬁrst formed a query
from the content words in the question, and then retrieved candidate answer sen-
tences in the document, ranked by their frequency-weighted term overlap with the
question. The query and each retrieved sentence were then parsed with dependency
parsers, and the sentence whose structure best matches the question structure se-
lected. Thus the question What do worms eat? would match worms eat grass: both
have the subject worms as a dependent of eat, in the version of dependency grammar
used at the time, while birds eat worms has birds as the subject:

What do worms eat

Worms eat grass

Birds eat worms

The alternative knowledge-based paradigm was implemented in the BASEBALL
system (Green et al., 1961). This system answered questions about baseball games
like “Where did the Red Sox play on July 7” by querying a structured database of
game information. The database was stored as a kind of attribute-value matrix with
values for attributes of each game:

Month = July
Place = Boston
Day = 7
Game Serial No.
= 96
(Team = Red Sox, Score = 5)
(Team = Yankees, Score = 3)

Each question was constituency-parsed using the algorithm of Zellig Harris’s
TDAP project at the University of Pennsylvania, essentially a cascade of ﬁnite-
state transducers (see the historical discussion in Joshi and Hopely 1999 and Kart-
tunen 1999). Then a content analysis phase each word or phrase was associated with
a program that computed parts of its meaning. Thus the phrase ‘Where’ had code to
assign the semantics Place = ?", with the result that the question “Where did the
Red Sox play on July 7” was assigned the meaning

Place = ?

LUNAR

EX ERC I SE S

421

Team = Red Sox
Month = July
Day = 7

The question is then matched against the database to return to the answer. Sim-
mons (1965) summarizes other early QA systems.
Another important progenitor of the knowledge-based paradigm for question-
answering is work that used predicate calculus as the meaning representation lan-
guage. The LUNAR system (Woods et al. 1972,Woods 1978) was designed to be
a natural language interface to a database of chemical facts about lunar geology. It
could answer questions like Do any samples have greater than 13 percent aluminum
by parsing them into a logical form
(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16
(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13PCT))))
The rise of the web brought the information-retrieval paradigm for question an-
swering to the forefront with the TREC QA track beginning in 1999, leading to a
wide variety of factoid and non-factoid systems competing in annual evaluations.
At the same time, Hirschman et al. (1999) introduced the idea of using chil-
dren’s reading comprehension tests to evaluate machine text comprehension algo-
rithm. They acquired a corpus of 120 passages with 5 questions each designed for
3rd-6th grade children, built an answer extraction system, and measured how well
the answers given by their system corresponded to the answer key from the test’s
publisher. Their algorithm focused on word overlap as a feature; later algorithms
added named entity features and more complex similarity between the question and
the answer span (Riloff and Thelen 2000, Ng et al. 2000).
Neural reading comprehension systems drew on the insight of these early sys-
tems that answer ﬁnding should focus on question-passage similarity. Many of the
architectural outlines of modern systems were laid out in the AttentiveReader (Her-
mann et al., 2015). The idea of using passage-aligned question embeddings in the
passage computation was introduced by (Lee et al., 2017). Seo et al. (2017) achieves
high-performance by introducing bi-directional attention ﬂow. Chen et al. (2017)
and Clark and Gardner (2018) show how to extract answers from entire documents.
The DeepQA component of the Watson system that won the Jeopardy! challenge
is described in a series of papers in volume 56 of the IBM Journal of Research and
Development; see for example Ferrucci (2012), Lally et al. (2012), Chu-Carroll et al.
(2012), Murdock et al. (2012b), Murdock et al. (2012a), Kalyanpur et al. (2012), and
Gondek et al. (2012).
Other question-answering tasks include Quiz Bowl, which has timing consid-
erations since the question can be interrupted (Boyd-Graber et al., 2018). Question
answering is also an important function of modern personal assistant dialog systems;
see Chapter 24 for more.

Exercises

422 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

CHAPTER

24 Dialog Systems and Chatbots

Les lois de la conversation sont en g ´en ´eral de ne s’y appesantir sur aucun ob-
jet, mais de passer l ´eg `erement, sans effort et sans affectation, d’un sujet `a un
autre ; de savoir y parler de choses frivoles comme de choses s ´erieuses

The rules of conversation are, in general, not to dwell on any one subject,
but to pass lightly from one to another without effort and without affectation;
to know how to speak about trivial topics as well as serious ones;

The 18th C. Encyclopedia of Diderot, start of the entry on conversation

conversation
dialog

conversational
agent
dialog system

The literature of the fantastic abounds in inanimate objects magically endowed with
sentience and the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s
Frankenstein, there is something deeply moving about creating something and then
having a chat with it. Legend has it that after ﬁnishing his
sculpture Moses, Michelangelo thought it so lifelike that
he tapped it on the knee and commanded it to speak. Per-
haps this shouldn’t be surprising. Language is the mark
of humanity and sentience, and conversation or dialog
is the most fundamental and specially privileged arena
of language. It is the ﬁrst kind of language we learn as
children, and for most of us, it is the kind of language
we most commonly indulge in, whether we are ordering
curry for lunch or buying spinach, participating in busi-
ness meetings or talking with our families, booking air-
line ﬂights or complaining about the weather.
This chapter introduces the fundamental algorithms of conversational agents,
or dialog systems. These programs communicate with users in natural language
(text, speech, or even both), and generally fall into two classes.
Task-oriented dialog agents are designed for a particular task and set up to
have short conversations (from as little as a single interaction to perhaps half-a-
dozen interactions) to get information from the user to help complete the task. These
include the digital assistants that are now on every cellphone or on home controllers
(Siri, Cortana, Alexa, Google Now/Home, etc.) whose dialog agents can give travel
directions, control home appliances, ﬁnd restaurants, or help make phone calls or
send texts. Companies deploy goal-based conversational agents on their websites to
help customers answer questions or address problems. Conversational agents play
an important role as an interface to robots. And they even have applications for
social good. DoNotPay is a “robot lawyer” that helps people challenge incorrect
parking ﬁnes, apply for emergency housing, or claim asylum if they are refugees.
Chatbots are systems designed for extended conversations, set up to mimic the

423

unstructured conversational or ‘chats’ characteristic of human-human interaction,
rather than focused on a particular task like booking plane ﬂights. These systems
often have an entertainment value, such as Microsoft’s XiaoIce (Little Bing 小冰)
system (Microsoft, 2014), which chats with people on text messaging platforms.
Chatbots are also often attempts to pass various forms of the Turing test (introduced
in Chapter 1). Yet starting from the very ﬁrst system, ELIZA (Weizenbaum, 1966),
chatbots have also been used for practical purposes, such as testing theories of psy-
chological counseling.
Note that the word ‘chatbot’ is often used in the media and in industry as a
synonym for conversational agent. In this chapter we will instead follow the usage
in the natural language processing community, limiting the designation chatbot to
this second subclass of systems designed for extended, casual conversation.
Let’s see some examples of dialog systems. One dimension of difference across
systems is how many turns they can deal with. A dialog consists of multiple turns,
each a single contribution to the dialog (the terminology is as if dialog is a game in
which I take a turn, then you take a turn, then me, and so on). A turn can consist
of a sentence, although it might be as short as a single word or as long as multiple
sentences. The simplest such systems generally handle a single turn from the user,
acting more like question-answering or command-and-control systems. This is espe-
cially common with digital assistants. For example Fig. 24.1 shows screen captures
from an early version of Apple’s Siri personal assistant from 2014, demonstrating
this kind of single-query behavior.

turn

(a)

(b)

Figure 24.1 Two sets of interactions with Siri in 2014. (a) A question (”Find restaurants near me”) returns
restaurants, but the system was unable to interpret a follow-up question (“Are any of them Italian?”). (b) An
alternative followup (“Tell me more about the second one”) similarly fails. This early system’s confusion at
follow-up questions suggests that it is mainly designed for a single interaction.

By contrast, Fig. 24.2 shows that a 2017 version of the Siri digital assistant can
handle slightly longer dialogs, handling a second turn with a follow-up question.
While spoken dialogs with mobile phone digital assistants tend to be short, some
tasks do require longer dialogs. One such task is travel planning and management,
a key concern of dialog systems since the very inﬂuential GUS system for planning
airline travel (Bobrow et al., 1977); we’ll see an example in the next section.
Dialog systems can even be used for much more complex domains like automatic
tutoring. Figure 24.3 shows part of a dialog from the adaptive ITSPOKE dialog
system (Forbes-Riley and Litman, 2011). In this example the system detects the
hesitancy of the student’s ﬁrst response (“Is it 19.6 m/s?”), and, even though the

424 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

(a)

(b)

Figure 24.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses
when posed to Siri in 2017.

answer is correct, decides to explain the answer and ask a follow-up question before
moving on.

Student:
Tutor:

Tutor: What will the velocity of the object be a second after that (where the initial
velocity is 9.8 m/s and the acceleration is 9.8 m/s2 )?
Is it 19.6 m/s?
Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is
because its acceleration tells us that every second, its velocity increases by 9.8
m/s. So what’ll its velocity be, a second after that?
Student: Um. I’m thinking. 29.4 m/s
Tutor:
Right. Now back to the man and his keys. We’ve shown that they both have an
acceleration of 9.8 m/ss . So, how do their velocities compare with each other, at
every second during the fall?
Figure 24.3 The tutoring domain: An excerpt from a tutorial interaction between a physics
student and the ITSPOKE system of Forbes-Riley and Litman (2011) based on the Why2-
Atlas text-based tutoring system (VanLehn et al., 2002).

Finally, conversational agents can be purely for fun, like the agents designed for
simple chit-chat like Cleverbot, an IR-based chatbot capable of carrying on the kinds
of conversations shown in Fig. 24.4.

User:
Do you like Doctor Who?
Cleverbot:
Yes, so funny.
User:
What’s your favorite show?
Cleverbot:
I don’t have one.
User:
What’s your name?
Cleverbot:
Jason I already told you.
User:
No your real name.
Cleverbot:
Stephanie IS my real name.
User:
I thought it was Jason?
Cleverbot:
How did you come to that conclusion.
User:
Because you said so.
Cleverbot:
I have to do my homework.
Figure 24.4 Chatbots: A conversation between a user and Cleverbot, an IR-based chatbot
(Carpenter, 2017).

24 .1

• CHATBOT S

425

We begin in the next section with chatbots, introducing the three major chatbot
architectures: rule-based systems, information retrieval systems, and transduction
models. We then turn to task-oriented dialog, introducing the frame-based architec-
ture (the GUS architecture) that underlies most modern task-based dialog systems,
the VoiceXML system that is a simple industrial instantiation of the architecture,
and information on designing and evaluating these systems.

24.1 Chatbots

chatbot

Chatbots are systems that can carry on extended conversations with the goal of
mimicking the unstructured conversational or ‘chats’ characteristic of human-human
interaction,
These systems often have an entertainment value, such as Microsoft’s XiaoIce
(Little Bing 小冰) system (Microsoft, 2014), which chats with people on text mes-
saging platforms. Yet starting from the very ﬁrst system, ELIZA (Weizenbaum,
1966), chatbots have also been used for practical purposes, such as testing theories
of psychological counseling.
Like practically everything else in language processing, chatbot architectures
fall into two classes: rule-based systems and corpus-based systems. Rule-based
systems include the early inﬂuential ELIZA and PARRY systems. Corpus-based
systems mine large datasets of human-human conversations, which can be done by
using information retrieval (IR-based systems simply copy a human’s response from
a previous conversation) or by using a machine translation paradigm such as neural
network sequence-to-sequence systems, to learn to map from a user utterance to a
system response.

24.1.1 Rule-based chatbots: ELIZA and PARRY

ELIZA (Weizenbaum, 1966) is the most important chatbot dialog system in the his-
tory of the ﬁeld, and so we take some time here to describe its algorithm in detail.
ELIZA was designed to simulate a Rogerian psychologist, based on a branch of
clinical psychology whose methods involve drawing the patient out by reﬂecting pa-
tient’s statements back at them. Rogerian psychology is the rare type of conversation
in which, as Weizenbaum points out, one can “assume the pose of knowing almost
nothing of the real world”. If a patient says “I went for a long boat ride” and the
psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what a
boat is, but rather assume she had some conversational goal. Most chatbots trying to
pass Turing test choose a domain with similar properties.
Let’s look at the most famous ELIZA conversation, excerpted (deleting some
material with ...) from a full interaction given in the original paper:

Men are all alike.
IN WHAT WAY
They’re always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well
my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
..

426 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

..
WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?
My father
YOUR FATHER
You are like my father in some ways.
WHAT RESEMBLANCE DO YOU SEE
You are not very aggressive but I think you don’t want me to notice that.
WHAT MAKES YOU THINK I AM NOT AGGRESSIVE
You don’t argue with me.
WHY DO YOU THINK I DON’T ARGUE WITH YOU
You are afraid of me.
DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU
My father is afraid of everybody.
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR
Bullies.
DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR

FATHER

BOYFRIEND MADE YOU COME HERE

As we summarized brieﬂy in Chapter 1, ELIZA worked by pattern/transform
rules like the following one:

(0 YOU 0 ME) [pattern]
->
(WHAT MAKES YOU THINK I 3 YOU) [transform]

In the ELIZA pattern, 0 means Kleene*, and in the transform rules the numbers
are the index of the constituent in the pattern. Thus the number 3 refers to the second
0 in the ﬁrst pattern. This rule would transfer

You hate me

into:

WHAT MAKES YOU THINK I HATE YOU

Each ELIZA pattern/rule is linked to a keyword that might occur in a user sen-
tence.
The architecture is summarized in Fig. 24.5.
Keywords are associated with a rank, with speciﬁc words being more highly
ranked, and more general words ranking lower. Consider the following user sen-
tence:

I know everybody laughed at me

Because it has the word “I”, this sentence could match the following rule whose
keyword is I:

(I *) -> (You say you 2)

producing:

YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU

But as Weizenbaum points out, “I” is a very general word and its keywords lead
to very general responses. Instead the keyword “everybody” is much more interest-
ing, since someone using universals like everybody or always is probably “referring
to some quite speciﬁc event or person”. Therefore, ELIZA prefers to respond with
the pattern associated with the more speciﬁc keyword everybody (implementing by
just assigning ”everybody” rank 5 and ”I” rank 0 in the lexicon), whose rule thus
transforms the sentence to:

24 .1

• CHATBOT S

427

function E L I ZA G EN ERATOR(user sentence) returns response

Find the word w in sentence that has the highest keyword rank
if w exists
Choose the highest ranked rule r for w that matches sentence
response ← Apply the transform in r to sentence
if w = ’my’
future ← Apply a transformation from the ‘memory’ rule list to sentence
Push future onto memory stack
else (no keyword applies)

either

or

response ← Apply the transform for the NONE keyword to sentence
response ← Pop the top response from the memory stack
return(response)

Figure 24.5 A simpliﬁed sketch of the ELIZA algorithm. The power of the algorithm
comes from the particular transforms associated with each keyword.

WHO IN PARTICULAR ARE YOU THINKING OF?

If no keyword matches, ELIZA chooses a non-commital response like “PLEASE
GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”.
Finally, ELIZA has a clever memory trick that accounts for the very last sen-
tence of the conversation above. Whenever the word “my” is the highest ranked
keyword, ELIZA will randomly select a transform on the MEMORY list, apply it to
the sentence, and store it on the stack:

(MEMORY MY
(0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)
(0 MY 0 = EARLIER YOU SAID YOUR 3)
(0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3

Later, if no keyword matches a sentence, ELIZA will return the top of the MEM-
ORY queue instead. 1
People became deeply emotionally involved with the program. Weizenbaum
tells the story of one of his staff who would ask Weizenbaum to leave the room
when she talked with ELIZA. When Weizenbaum suggested that he might want to
store all the ELIZA conversations for later analysis, people immediately pointed
out the privacy implications, which suggested that they were having quite private
conversations with ELIZA, despite knowing that it was just software.
Eliza’s framework is still used today; modern chatbot system tools like ALICE
are based on updated versions of ELIZA’s pattern/action architecture.
A few years after ELIZA, another chatbot with a clinical psychology focus,
PARRY (Colby et al., 1971), was used to study schizophrenia. In addition to ELIZA-
like regular expressions, the PARRY system including a model of its own mental
state, with affect variables for the agent’s levels of fear and anger; certain topics of
conversation might lead PARRY to become more angry or mistrustful. If PARRY’s
anger variable is high, he will choose from a set of “hostile” outputs. If the input
mentions his delusion topic, he will increase the value of his fear variable and then
begin to express the sequence of statements related to his delusion. Parry was the

1 Fun fact: because of its structure as a queue, this MEMORY trick is the earliest known hierarchical
model of discourse in natural language processing.

428 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

ﬁrst known system to pass the Turing test (in 1972!); psychiatrists couldn’t distin-
guish text transcripts of interviews with PARRY from transcripts of interviews with
real paranoids (Colby et al., 1972).

24.1.2 Corpus-based chatbots

Corpus-based chatbots, instead of using hand-built rules, mine conversations of
human-human conversations, or sometimes mine the human responses from human-
machine conversations. Serban et al. (2017) summarizes some such available cor-
pora, such as conversations on chat platforms, on Twitter, or in movie dialog, which
is available in great quantities and has been shown to resemble natural conversation
(Forchini, 2013). Chatbot responses can even be extracted from sentences in corpora
of non-dialog text.
There are two common architectures for corpus-based chatbots: information re-
trieval, and machine learned sequence transduction. Like rule-based chatbots (but
unlike frame-based dialog systems), most corpus-based chatbots do very little mod-
eling of the conversational context. Instead they focus on generating a single re-
sponse turn that is appropriate given the user’s immediately previous utterance. For
this reason they are often called response generation systems. Corpus-based chat-
bots thus have some similarity to question answering systems, which focus on single
responses while ignoring context or larger conversational goals.

IR-based chatbots

response
generation

The principle behind information retrieval based chatbots is to respond to a user’s
turn X by repeating some appropriate turn Y from a corpus of natural (human) text.
The differences across such systems lie in how they choose the corpus, and how they
decide what counts as an appropriate human turn to copy.
A common choice of corpus is to collect databases of human conversations.
These can come from microblogging platforms like Twitter or any Weibo (微博).
Another approach is to use corpora of movie dialog. Once a chatbot has been put
into practice, the turns that humans use to respond to the chatbot can be used as
additional conversational data for training.
Given the corpus and the user’s sentence, IR-based systems can use any retrieval
algorithm to choose an appropriate response from the corpus. The two simplest
methods are the following:

1. Return the response to the most similar turn: Given user query q and a con-

versational corpus C, ﬁnd the turn t in C that is most similar to q (for example has
the highest cosine with q) and return the following turn, i.e. the human response to t
in C:
r = res ponse (cid:18)argmax
The idea is that we should look for a turn that most resembles the user’s turn, and re-
turn the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011).
2. Return the most similar turn: Given user query q and a conversational corpus
C, return the turn t in C that is most similar to q (for example has the highest cosine
with q):

||q||t || (cid:19)

(24.1)

qT t

t ∈C

The idea here is to directly match the users query q with turns from C, since a good
response will often share words or semantics with the prior turn.

r = argmax

t ∈C

qT t

||q||t ||

(24.2)

24 .1

• CHATBOT S

429

In each case, any similarity function can be used, most commonly cosines com-
puted either over words (using tf-idf) or over embeddings.
Although returning the response to the most similar turn seems like a more in-
tuitive algorithm, returning the most similar turn seems to work better in practice,
perhaps because selecting the response adds another layer of indirection that can
allow for more noise (Ritter et al. 2011, Wang et al. 2013).
The IR-based approach can be extended by using more features than just the
words in the q (such as words in prior turns, or information about the user), and
using any full IR ranking approach. Commercial implementations of the IR-based
approach include Cleverbot (Carpenter, 2017) and Microsoft’s XiaoIce (Little Bing
小冰) system (Microsoft, 2014).
Instead of just using corpora of conversation, the IR-based approach can be used
to draw responses from narrative (non-dialog) text. For example, the pioneering
COBOT chatbot (Isbell et al., 2000) generated responses by selecting sentences from
a corpus that combined the Unabomber Manifesto by Theodore Kaczynski, articles
on alien abduction, the scripts of “The Big Lebowski” and “Planet of the Apes”.
Chatbots that want to generate informative turns such as answers to user questions
can use texts like Wikipedia to draw on sentences that might contain those answers
(Yan et al., 2016).

Sequence to sequence chatbots

An alternate way to use a corpus to generate dialog is to think of response generation
as a task of transducing from the user’s prior turn to the system’s turn. This is
basically the machine learning version of Eliza; the system learns from a corpus to
transduce a question to an answer.
This idea was ﬁrst developed by using phrase-based machine translation (Ritter
et al., 2011) to translate a user turn to a system response. It quickly became clear,
however, that the task of response generation was too different from machine trans-
lation. In machine translation words or phrases in the source and target sentences
tend to align well with each other; but in conversation, a user utterance may share
no words or phrases with a coherent response.
Instead, (roughly contemporaneously by Shang et al. 2015, Vinyals and Le 2015,
and Sordoni et al. 2015) transduction models for response generation were modeled
instead using encoder-decoder (seq2seq) models (Chapter 22), as shown in Fig. 24.6.

Figure 24.6 A sequence to sequence model for neural response generation in dialog.

A number of modiﬁcations are required to the basic seq2seq model to adapt it for
the task of response generation. For example basic seq2seq models have a tendency
to produce predictable but repetitive and therefore dull responses like “I’m OK” or
“I don’t know” that shut down the conversation. This can be addressed by changing
the objective function for seq2seq model training to a mutual information objective,
or by modifying a beam decoder to keep more diverse responses in the beam (Li

430 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

et al., 2016a).
Another problem with the simple S EQ2S EQresponse generation architecture is
its inability to model the longer prior context of the conversation. This can be done
by allowing the model to see prior turns, such as by using a hierarchical model that
summarizes information over multiple prior turns (Lowe et al., 2017b).
Finally, S EQ2S EQresponse generators focus on generating single responses, and
so don’t tend to do a good job of continuously generating responses that cohere
across multiple turns. This can be addressed by using reinforcement learning, as
well as techniques like adversarial networks, to learn to choose responses that make
the overall conversation more natural (Li et al. 2016b, Li et al. 2017).
Fig. 24.7 shows some sample responses generated by a vanilla S EQ2S EQmodel,
and from a model trained by an adversarial algorithm to produce responses that are
harder to distinguish from human responses (Li et al., 2017).

Input
Vanilla-SEQ2SEQ
Adversarial
Input
Vanilla-SEQ2SEQ
Adversarial
Input
Vanilla-SEQ2SEQ
Adversarial

tell me ... how long have you had this falling sickness ?
i’m not a doctor.
a few months, i guess .
so i had the doctors test sammy ’s response to conditioning .
sammy wrote the test sammy wrote the test .
so he took the pills .
they didn ’t have much success with this problem commander .
they ’re not the only ones who have been in the system .
can we ﬁnd someone else ?
Figure 24.7 Sample responses generated by a S EQ2S EQmodel trained either with a vanilla
maximum likelihood objective, or adversarially trained to produce sentences that are hard for
an adversary to distinguish from human sentences (Li et al., 2017).

Evaluating Chatbots

Chatbots are generally evaluated by humans. The slot-ﬁlling evaluations used for
task-based dialog (Section 24.2.3) aren’t appropriate for this task (Artstein et al.,
2009), and word-overlap metrics like BLEU for comparing a chatbot’s response to a
human response turn out to correlate very poorly with human judgments (Liu et al.,
2016). BLEU performs poorly because there are so many possible responses to any
given turn; word-overlap metrics work best when the space of responses is small and
lexically overlapping, as is the case in machine translation.
While human evaluation is therefore required for evaluating chatbots, there are
beginning to be models for automatic evaluation. The ADEM (Lowe et al., 2017a)
classiﬁer is trained on a set of responses labeled by humans with how appropriate
they are, and learns to predict this label from the dialog context and the words in the
system response.
Another paradigm is adversarial evaluation (Bowman et al. 2016, Kannan and
Vinyals 2016, Li et al. 2017), inspired by the Turing test. The idea is to train a
“Turing-like” evaluator classiﬁer to distinguish between human-generated responses
and machine-generated responses. The more successful a response generation sys-
tem is at fooling this evaluator, the better the system.

adversarial
evaluation

24.2 Frame Based Dialog Agents

domain
ontology

Modern task-based dialog systems are based on a domain ontology, a knowledge

24 .2

• FRAM E BA SED D IA LOG AG EN T S

431

frame
slot

structure representing the kinds of intentions the system can extract from user sen-
tences. The ontology deﬁnes one or more frames, each a collection of slots, and
deﬁnes the values that each slot can take. This frame-based architecture was ﬁrst
introduced in 1977 in the inﬂuential GUS system for travel planning (Bobrow et al.,
1977), and has been astonishingly long-lived, underlying most modern commercial
digital assistants. Figure 24.8 shows a conversation with the original GUS system.

GUS:

Hello. My name is GUS. I can help you plan a simple trip by air. Where do you
want to go ?
Client:
I want to go to San Diego on May 28
GUS:
What time do you want to leave?
Client:
I must be in San Diego before 10 am
GUS:
Would you like P.S.A. ﬂight 102 that arrives at 9.15 am ?
Client: What is the next ﬂight ?
GUS:
Air California ﬂight 310 that leaves at 8.30am and arrives at 10.00am
Client:
I’ll take the ﬁrst one
GUS:
In what name should I make a reservation, please?
Client:
Dan Bobrow
GUS:
I have conﬁrmed the following ﬂight: P.S.A. ﬂight 102 on Wednesday May 28
from San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am.
What date do you want to return on ?
On Friday in the evening.
Would you like the ﬂight that leaves at 7.45 pm ?
That’s ﬁne.
I have conﬁrmed the following ﬂight: P.S.A. ﬂight 307 on Friday May 30 from
San Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank you
for calling. Goodbye
Figure 24.8 The travel domain: A transcript of an actual dialog with the GUS system of
Bobrow et al. (1977). P.S.A. and Air California were airlines of that period.

GUS:
Client:
GUS:
Client:
GUS:

The set of slots in a GUS-style frame speciﬁes what the system needs to know,
and the ﬁller of each slot is constrained to values of a particular semantic type. In
the travel domain, for example, a slot might be of type city (hence take on values
like San Francisco, or Hong Kong) or of type date, airline, or time:

Slot

Type

ORIGIN CITY
city
DESTINATION CITY city
DEPARTURE TIME
time
DEPARTURE DATE date
ARRIVAL TIME
time
ARRIVAL DATE
date

Types in GUS, as in modern frame-based dialog agents, may have hierarchical
structure; for example the date type in GUS is itself a frame with slots with types
like integer or members of sets of weekday names:

DATE
MONTH NAME
DAY (BOUNDED-INTEGER 1 31)
YEAR INTEGER
WEEKDAY (MEMBER (SUNDAY MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY))

432 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

24.2.1 Control structure for frame-based dialog

The control architecture of frame-based dialog systems is designed around the frame.
The goal is to ﬁll the slots in the frame with the ﬁllers the user intends, and then per-
form the relevant action for the user (answering a question, or booking a ﬂight).
Most frame-based dialog systems are based on ﬁnite-state automata that are hand-
designed for the task by a dialog designer.

Figure 24.9 A simple ﬁnite-state automaton architecture for frame-based dialog.

Consider the very simple ﬁnite-state control architecture shown in Fig. 24.9,
implementing a trivial airline travel system whose job is to ask the user for the
information for 4 slots: departure city, a destination city, a time, and whether the trip
is one-way or round-trip. Let’s ﬁrst associate with each slot a question to ask the
user:

Slot

Question

ORIGIN CITY
“From what city are you leaving?”
DESTINATION CITY “Where are you going?”
DEPARTURE TIME
“When would you like to leave?”
ARRIVAL TIME
“When do you want to arrive?”
Figure 24.9 shows a sample dialog manager for such a system. The states of
the FSA correspond to the slot questions, user, and the arcs correspond to actions
to take depending on what the user responds. This system completely controls the
conversation with the user. It asks the user a series of questions, ignoring (or misin-
terpreting) anything that is not a direct answer to the question and then going on to
the next question.
The speaker in control of any conversation is said to have the initiative in the
conversation. Systems that completely control the conversation in this way are thus
called system-initiative. By contrast, in normal human-human dialog, initiative
shifts back and forth between the participants (Bobrow et al. 1977, Walker and Whit-
taker 1990).
The single-initiative ﬁnite-state dialog architecture has the advantage that the
system always knows what question the user is answering. This means the system
can prepare the speech recognizer with a language model tuned to answers for this

initiative

system-
initiative

universal

mixed initiative

24 .2

• FRAM E BA SED D IA LOG AG EN T S

433

question, and also makes natural language understanding easier. Most ﬁnite-state
systems also allow universal commands that can be said anywhere in the dialog,
like help, to give a help message, and start over (or main menu), which returns
the user to some speciﬁed main start state,. Nonetheless such a simplistic ﬁnite-state
architecture is generally applied only to simple tasks such as entering a credit card
number, or a name and password.
For most applications, users need a bit more ﬂexibility.
In a travel-planning
situation, for example, a user may say a sentence that ﬁlls multiple slots at once:
(24.3)
I want a ﬂight from San Francisco to Denver one way leaving after ﬁve
p.m. on Tuesday.
Or in cases where there are multiple frames, a user may say something to shift
frames, for example from airline reservations to reserving a rental car:
(24.4)
I’d like to book a rental car when I arrive at the airport.
The standard GUS architecture for frame-based dialog systems, used in various
forms in modern systems like Apple’s Siri, Amazon’s Alexa, and the Google Assis-
tant, therefore follows the frame in a more ﬂexible way. The system asks questions
of the user, ﬁlling any slot that the user speciﬁes, even if a user’s response ﬁlls mul-
tiple slots or doesn’t answer the question asked. The system simply skips questions
associated with slots that are already ﬁlled. Slots may thus be ﬁlled out of sequence.
The GUS architecture is thus a kind of mixed initiative, since the user can take at
least a bit of conversational initiative in choosing what to talk about.
The GUS architecture also has condition-action rules attached to slots. For ex-
ample, a rule attached to the D E S T INAT ION slot for the plane booking frame, once
the user has speciﬁed the destination, might automatically enter that city as the de-
fault StayLocation for the related hotel booking frame.
Once the system has enough information it performs the necessary action (like
querying a database of ﬂights) and returns the result to the user.
We mentioned in passing the linked airplane and travel frames. Many domains,
of which travel is one, require the ability to deal with multiple frames. Besides
frames for car or hotel reservations, we might need frames with general route in-
formation (for questions like Which airlines ﬂy from Boston to San Francisco?),
information about airfare practices (for questions like Do I have to stay a speciﬁc
number of days to get a decent airfare?).
In addition, once we have given the user options (such as a list of restaurants),
we can even have a special frame for ‘asking questions about this list’, whose slot is
the particular restaurant the user is asking for more information about, allowing the
user to say ‘the second one’ or ‘the Italian one’.
Since users may switch from frame to frame, the system must be able to disam-
biguate which slot of which frame a given input is supposed to ﬁll and then switch
dialog control to that frame.
Because of this need to dynamically switch control, the GUS architecture is a
production rule system. Different types of inputs cause different productions to
ﬁre, each of which can ﬂexibly ﬁll in different frames. The production rules can
then switch control according to factors such as the user’s input and some simple
dialog history like the last question that the system asked.
Commercial dialog systems provide convenient interfaces or libraries to make
it easy to build systems with these kinds of ﬁnite-state or production rule systems,
for example providing graphical interfaces to allow dialog modules to be chained
together.

434 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

domain
classiﬁcation

intent
determination

slot ﬁlling

24.2.2 Natural language understanding for ﬁlling slots

The goal of the natural language understanding component is to extract three things
from the user’s utterance. The ﬁrst task is domain classiﬁcation: is this user for
example talking about airlines, programming an alarm clock, or dealing with their
calendar? Of course this 1-of-n classiﬁcation tasks is unnecessary for single-domain
systems that are focused on, say, only calendar management, but multi-domain di-
alog systems are the modern standard. The second is user intent determination:
what general task or goal is the user trying to accomplish? For example the task
could be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment.
Finally, we need to do slot ﬁlling: extract the particular slots and ﬁllers that the user
intends the system to understand from their utterance with respect to their intent.
From a user utterance like this one:

Show me morning flights from Boston to San Francisco on Tuesday

a system might want to build a representation like:

DOMAIN:
AIR-TRAVEL
INTENT:
SHOW-FLIGHTS
ORIGIN-CITY: Boston
ORIGIN-DATE: Tuesday
ORIGIN-TIME: morning
DEST-CITY:
San Francisco

while an utterance like

Wake me tomorrow at 6

should give an intent like this:

DOMAIN: ALARM-CLOCK
INTENT: SET-ALARM
TIME:
2017-07-01 0600-0800

The task of slot-ﬁlling, and the simpler tasks of domain and intent classiﬁcation,
are special cases of the task of semantic parsing discussed in Chapter 16. Dialog
agents can thus extract slots, domains, and intents from user utterances by applying
any of the semantic parsing approaches discussed in that chapter.
The method used in the original GUS system, and still quite common in indus-
trial applications, is to use hand-written rules, often as part of the condition-action
rules attached to slots or concepts.
For example we might just deﬁne a regular expression consisting of a set strings
that map to the SET-ALARM intent:

wake me (up) | set (the|an) alarm | get me up

We can build more complex automata that instantiate sets of rules like those
discussed in Chapter 17, for example extracting a slot ﬁller by turning a string
like Monday at 2pm into an object of type date with parameters (DAY, MONTH,
YEAR, HOURS, MINUTES).
Rule-based systems can be even implemented with full grammars. Research sys-
tems like the Phoenix system (Ward and Issar, 1994) consists of large hand-designed
semantic grammars with thousands of rules. A semantic grammar is a context-free
grammar in which the left-hand side of each rule corresponds to the semantic entities
being expressed (i.e., the slot names) as in the following fragment:

semantic
grammar

24 .2

• FRAM E BA SED D IA LOG AG EN T S

435

SHOW
→ show me | i want | can i see|...
DEPART TIME RANGE → (after|around|before) HOUR |
morning | afternoon | evening
HOUR
→ one|two|three|four...|twelve (AMPM)
FLIGHTS
→ (a) ﬂight | ﬂights
AMPM
→ am | pm
ORIGIN
→ from CITY
DESTINATION
→ to CITY
CITY
→ Boston | San Francisco | Denver | Washington
Semantic grammars can be parsed by any CFG parsing algorithm (see Chap-
ter 11), resulting in a hierarchical labeling of the input string with semantic node
labels, as shown in Fig. 24.10.

S

SHOW

FLIGHTS

ORIGIN

DESTINATION

DEPARTDATE

DEPARTTIME

Show

me

ﬂights

from

Boston

to

San

Francisco

on

Tuesday

morning

Figure 24.10 A semantic grammar parse for a user sentence, using slot names as the internal parse tree nodes.

N-best list

Whether regular expressions or parsers are used, it remains only to put the ﬁllers
into some sort of canonical form, for example by normalizing dates as discussed in
Chapter 17.
A number of tricky issues have to be dealt with. One important issue is negation;
if a user speciﬁes that they “can’t ﬂy Tuesday morning”, or want a meeting ”any time
except Tuesday morning”, a simple system will often incorrectly extract “Tuesday
morning” as a user goal, rather than as a negative constraint.
Speech recognition errors must also be dealt with. One common trick is to make
use of the fact that speech recognizers often return a ranked N-best list of hypoth-
esized transcriptions rather than just a single candidate transcription. The regular
expressions or parsers can simply be run on every sentence in the N-best list, and
any patterns extracted from any hypothesis can be used.
As we saw earlier in discussing information extraction, the rule-based approach
is very common in industrial applications. It has the advantage of high precision,
and if the domain is narrow enough and experts are available, can provide sufﬁcient
coverage as well. On the other hand, the hand-written rules or grammars can be both
expensive and slow to create, and hand-written rules can suffer from recall problems.
A common alternative is to use supervised machine learning. Assuming a train-
ing set is available which associates each sentence with the correct semantics, we
can train a classiﬁer to map from sentences to intents and domains, and a sequence
model to map from sentences to slot ﬁllers.
For example given the sentence:

I want to fly to San

Francisco on Monday

afternoon

please

we might ﬁrst apply a simple 1-of-N classiﬁer (logistic regression, neural network,
etc.)
that uses features of the sentence like word N-grams to determine that the
domain is A IRL IN E and and the intent is SHOW FL IGHT.
Next to do slot ﬁlling we might ﬁrst apply a classiﬁer that uses similar features
of the sentence to predict which slot the user wants to ﬁll. Here in addition to

436 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

word unigram, bigram, and trigram features we might use named entity features or
features indicating that a word is in a particular lexicon (such as a list of cities, or
airports, or days of the week) and the classifer would return a slot name (in this case

D E S T INAT ION, D E PARTUR E -DAY, and D E PARTUR E - T IM E). A second classiﬁer can

then be used to determine the ﬁller of the named slot, for example a city classiﬁer that
uses N-grams and lexicon features to determine that the ﬁller of the D E S T INAT ION

slot is SAN FRANC I SCO.

An alternative is to use a sequence model (MEMMs, CRFs, RNNs) to directly
assign a slot label to each word in the sequence, following the method used for other
information extraction models in Chapter 17 (Pieraccini et al. 1991, Raymond and
Riccardi 2007, Mesnil et al. 2015, Hakkani-T ¨ur et al. 2016). Once again we would
need a supervised training test, with sentences paired with sequences of IOB labels
like the following:

IOB

O O
O
O
O B-DES I-DES
O
B-DEPTIME I-DEPTIME
I want to fly to San
Francisco on Monday
afternoon

O
please

Recall from Chapter 17 that in IOB tagging we introduce a tag for the beginning
(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.
The number of tags is thus 2n + 1 tags, where n is the number of slots.
Any IOB tagger sequence model can then be trained on a training set of such
labels. Feature-based sequence models (MEMM, CRF) make use of features like
word embeddings, word unigrams and bigrams, lexicons (for example lists of city
names), and slot transition features (perhaps D E S T INAT ION is more likely to follow
OR IG IN than the other way around) to map a user’s utterance to the slots. An MEMM
(Chapter 8) for example, combines these features of the input word wi , its neighbors
within l words wi+l
i−l , and the previous k slot tags si−1
i−k to compute the most likely slot
label sequence S from the word sequence W as follows:
ˆS = argmax
P(S|W )

S
S (cid:89)i
S (cid:89)i

= argmax

P(si |wi+l
i−l , si−1
i−k )

(24.5)

= argmax

i−k )(cid:33)
wi f i (si , wi+l
i−l , si−1
i−k )(cid:33)
wi f i (s(cid:48) , wi+l
i−l , t i−1

exp (cid:32)(cid:88)i
exp (cid:32)(cid:88)i
(cid:88)s(cid:48) ∈slotset
The Viterbi algorithm is used to decode the best slot sequence ˆS.
Neural network architectures mostly eschew the feature extraction step, instead
using the bi-LSTM architecture introduced in Chapter 9, and applied to IOB-style
named entity tagging in Chapter 17. A typical LSTM-style architecture is shown in
Fig. 24.11. Here the input is a series of words w1 ...wn , and the output is a series
of IOB tags s1 ...sn . In the architecture as introduced in Chapter 17, the input words
are converted into two embeddings: standard word2vec or GloVe embeddings, and
a character-based embedding, which are concatenated together and passed through a
bi-LSTM. The output of the bi-LSTM can be passed to a softmax choosing an IOB
tag for each input word, or to a CRF layer which uses Viterbi to ﬁnd the best series
of IOB tags. In addition, neural systems can combine the domain-classiﬁcation and
intent-extraction tasks with slot-ﬁlling simply by adding a domain concatenated with
an intent as the desired output for the ﬁnal EOS token.

24 .2

• FRAM E BA SED D IA LOG AG EN T S

437

Figure 24.11 An LSTM architecture for slot ﬁlling, mapping the words in the input to a
series of IOB tags plus a ﬁnal state consisting of a domain concatenated with an intent.

Once the sequence labeler has tagged the user utterance, a ﬁller string can be ex-
tracted for each slot from the tags (e.g., ”San Francisco”), and these word strings
can then be normalized to the correct form in the ontology (perhaps the airport
code‘SFO’). This normalization can take place by using homonym dictionaries (spec-
ifying, for example, that SF, SFO, and San Francisco are the same place).
In industrial contexts, machine learning-based systems for slot-ﬁlling are often
bootstrapped from rule-based systems in a semi-supervised learning manner. A rule-
based system is ﬁrst built for the domain, and a test-set is carefully labeled. As new
user utterances come in, they are paired with the labeling provided by the rule-based
system to create training tuples. A classiﬁer can then be trained on these tuples, us-
ing the test-set to test the performance of the classiﬁer against the rule-based system.
Some heuristics can be used to eliminate errorful training tuples, with the goal of in-
creasing precision. As sufﬁcient training samples become available the resulting
classiﬁer can often outperform the original rule-based system (Suendermann et al.,
2009), although rule-based systems may still remain higher-precision for dealing
with complex cases like negation.

24.2.3 Evaluating Slot Filling

Slot Error Rate for a Sentence =

An intrinsic error metric for natural language understanding systems for slot ﬁlling
is the Slot Error Rate for each sentence:
# of inserted/deleted/subsituted slots
# of total reference slots for sentence
Consider a system faced with the following sentence:
(24.7) Make an appointment with Chris at 10:30 in Gates 104
which extracted the following candidate slot structure:

(24.6)

Slot

Filler

PERSON Chris
TIME
11:30 a.m.
ROOM Gates 104
Here the slot error rate is 1/3, since the TIME is wrong. Instead of error rate, slot
precision, recall, and F-score can also be used.
A perhaps more important, although less ﬁne-grained, measure of success is an
extrinsic metric like task error rate. In this case, the task error rate would quantify
how often the correct meeting was added to the calendar at the end of the interaction.

438 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

24.2.4 Other components of frame-based dialog

We’ve focused on the natural language understanding component that is the core of
frame-based systems, but here we also brieﬂy mention other modules.
The ASR (automatic speech recognition) component takes audio input from a
phone or other device and outputs a transcribed string of words, as discussed in
Chapter 26. Various aspects of the ASR system may be optimized speciﬁcally for
use in conversational agents.
Because what the user says to the system is related to what the system has just
said, language models in conversational agent depend on the dialog state. For ex-
ample, if the system has just asked the user “What city are you departing from?”,
the ASR language model can be constrained to just model answers to that one ques-
tion. This can be done by training an N-gram language model on answers to this
question. Alternatively a ﬁnite-state or context-free grammar can be hand written
to recognize only answers to this question, perhaps consisting only of city names or
perhaps sentences of the form ‘I want to (leave|depart) from [CITYNAME]’. Indeed,
many simple commercial dialog systems use only non-probabilistic language mod-
els based on hand-written ﬁnite-state grammars that specify all possible responses
that the system understands. We give an example of such a hand-written grammar
for a VoiceXML system in Section 24.3.
A language model that is completely dependent on dialog state is called a re-
strictive grammar, and can be used to constrain the user to only respond to the
system’s last utterance. When the system wants to allow the user more options, it
might mix this state-speciﬁc language model with a more general language model.
The language generation module of any dialog system produces the utterances
that the system says to the user. Frame-based systems tend to use template-based
generation, in which all or most of the words in the sentence to be uttered to the
user are prespeciﬁed by the dialog designer. Sentences created by these templates
are often called prompts. Templates might be completely ﬁxed (like ‘Hello, how
can I help you?’), or can include some variables that are ﬁlled in by the generator,
as in the following:
What time do you want to leave CITY-ORIG?
Will you return to CITY-ORIG from CITY-DEST?
These sentences are then passed to the TTS (text-to-speech) component (see
Chapter 26). More sophisticated statistical generation strategies will be discussed in
Section 25.5 of Chapter 25.

restrictive
grammar

language
generation

template-based
generation

prompt

24.3 VoiceXML

There are many commercial systems that allow developers to implement frame-
based dialog systems, including the user-deﬁnable skills in Amazon Alexa or the
actions in Google Assistant. These systems provide libraries for deﬁning the rules
for detecting user intents and ﬁlling in slots, and for expressing the architecture for
controlling which frames and actions the system should take at which times.
Instead of focusing on a commercial engine, we introduce here a simple declar-
ative formalism that has similar capabilities to each of them: VoiceXML, the Voice
Extensible Markup Language (http://www.voicexml.org/), an XML-based di-
alog design language for creating simple frame-based dialogs. Although VoiceXML
is simpler than a full commercial frame-based system (it’s deterministic, and hence

VoiceXML

439

24 .3

• VO ICEXML
only allows non-probabilistic grammar-based language models and rule-based se-
mantic parsers), it’s still a handy way to get a hands-on grasp of frame-based dialog
system design.
A VoiceXML document contains a set of dialogs, each a menu or a form. A form
is a frame, whose slots are called ﬁelds. The VoiceXML document in Fig. 24.12
shows three ﬁelds for specifying a ﬂight’s origin, destination, and date. Each ﬁeld
has a variable name (e.g., origin) that stores the user response, a prompt, (e.g.,
Which city do you want to leave from), and a grammar that is passed to the speech
recognition engine to specify what is allowed to be recognized. The grammar for
the ﬁrst ﬁeld in Fig. 24.12 allows the three phrases san francisco, barcelona, and
new york. The VoiceXML interpreter walks through a form in document order,
repeatedly selecting each item in the form, and each ﬁeld in order.

prompt

<noinput>
I’m sorry, I didn’t hear you.
</noinput>

<reprompt/>

<nomatch>
I’m sorry, I didn’t understand that. <reprompt/>
</nomatch>

<form>
<block>
Welcome to the air travel consultant. </block>
<field name="origin">
<prompt>
Which city do you want to leave from? </prompt>
<grammar type="application/x=nuance-gsl">
[(san francisco) barcelona (new york)]
</grammar>
<filled>
<prompt> OK, from <value expr="origin"/> </prompt>
</filled>
</field>
<field name="destination">
<prompt> And which city do you want to go to?
<grammar type="application/x=nuance-gsl">
[(san francisco) barcelona (new york)]
</grammar>
<filled>
<prompt>
</filled>
</field>
<field name="departdate" type="date">
<prompt> And what date do you want to leave? </prompt>
<filled>
<prompt>
</filled>
</field>
<block>
<prompt> OK, I have you are departing from <value expr="origin"/>
to <value expr="destination"/> on <value expr="departdate"/>
</prompt>
send the info to book a flight...
</block>
</form>

OK, to <value expr="destination"/>

OK, on <value expr="departdate"/>

</prompt>

</prompt>

</prompt>

Figure 24.12 A VoiceXML script for a form with three ﬁelds, which conﬁrms each ﬁeld
and handles the noinput and nomatch situations.

The prologue of the example shows two global defaults for error handling. If the
user doesn’t answer after a prompt (i.e., silence exceeds a timeout threshold), the
VoiceXML interpreter will play the <noinput> prompt. If the user says something
that doesn’t match the grammar for that ﬁeld, the VoiceXML interpreter will play the
<nomatch> prompt. VoiceXML provides a <reprompt/> command, which repeats
the prompt for whatever ﬁeld caused the error.
The <filled> tag for a ﬁeld is executed by the interpreter as soon as the ﬁeld
has been ﬁlled by the user. Here, this feature is used to conﬁrm the user’s input.
VoiceXML 2.0 speciﬁes seven built-in grammar types: boolean, currency,

date, digits, number, phone, and time. By specifying the departdate ﬁeld as

440 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

type date, a date-speciﬁc language model will be passed to the speech recognizer.

<noinput>

I’m sorry, I didn’t hear you. <reprompt/>

</noinput>

<nomatch> I’m sorry, I didn’t understand that. <reprompt/> </nomatch>

<form>
<grammar type="application/x=nuance-gsl">
<![CDATA[
Flight ( ?[
(i [wanna (want to)] [fly go])
(i’d like to [fly go])
([(i wanna)(i’d like a)] flight)

]
[

( [from leaving departing] City:x) {<origin $x>}
( [(?going to)(arriving in)] City:x) {<destination $x>}
( [from leaving departing] City:x
[(?going to)(arriving in)] City:y) {<origin $x> <destination $y>}

]
?please

)
City [ [(san francisco) (s f o)] {return( "san francisco, california")}
[(denver) (d e n)] {return( "denver, colorado")}
[(seattle) (s t x)] {return( "seattle, washington")}

]
]]> </grammar>

<initial name="init">
<prompt> Welcome to the consultant. What are your travel plans?
</initial>

</prompt>

<field name="origin">
<prompt> Which city do you want to leave from?
<filled>
<prompt> OK, from <value expr="origin"/> </prompt>
</filled>
</field>
<field name="destination">
<prompt> And which city do you want to go to?
<filled>
<prompt> OK, to <value expr="destination"/> </prompt>
</filled>
</field>
<block>
<prompt> OK, I have you are departing from
to <value expr="destination"/>.
send the info to book a flight...
</block>
</form>

</prompt>

</prompt>

<value expr="origin"/>
</prompt>

Figure 24.13 A mixed-initiative VoiceXML dialog. The grammar allows sentences that
specify the origin or destination cities or both. The user can respond to the initial prompt by
specifying origin city, destination city, or both.

Figure 24.13 gives a mixed initiative example, allowing the user to answer ques-
tions in any order or even ﬁll in multiple slots at once. The VoiceXML interpreter
has a guard condition on ﬁelds, a test that keeps a ﬁeld from being visited; the default
test skips a ﬁeld if its variable is already set.
Figure 24.13 also shows a more complex CFG grammar with two rewrite rules,
Flight and City. The Nuance GSL grammar formalism uses parentheses () to
mean concatenation and square brackets [] to mean disjunction. Thus, a rule like
(24.8) means that Wantsentence can be expanded as i want to fly or i want

to go, and Airports can be expanded as san francisco or denver.

(24.8)

Wantsentence (i want to [fly go])
Airports [(san francisco) denver]

VoiceXML grammars allow semantic attachments, such as the text string ("denver,
colorado") the return for the City rule, or a slot/ﬁller , like the attachments for the
Flight rule which ﬁlls the slot (<origin> or <destination> or both) with the
value passed up in the variable x from the City rule.

24 .4

• EVALUAT ING D IA LOG SY ST EM S

441

TTS Performance
ASR Performance
Task Ease
Interaction Pace
User Expertise
System Response
Expected Behavior
Future Use

Was the system easy to understand ?
Did the system understand what you said?
Was it easy to ﬁnd the message/ﬂight/train you wanted?
Was the pace of interaction with the system appropriate?
Did you know what you could say at each point?
How often was the system sluggish and slow to reply to you?
Did the system work the way you expected it to?
Do you think you’d use the system in the future?
Figure 24.14 User satisfaction survey, adapted from Walker et al. (2001).

Because Fig. 24.13 is a mixed-initiative grammar, the grammar has to be ap-
plicable to any of the ﬁelds. This is done by making the expansion for Flight a
disjunction; note that it allows the user to specify only the origin city, the destination
city, or both.

24.4 Evaluating Dialog Systems

Evaluation is crucial in dialog system design. If the task is unambiguous, we can
simply measure absolute task success (did the system book the right plane ﬂight, or
put the right event on the calendar).
To get a more ﬁne-grained idea of user happiness, we can compute a user sat-
isfaction rating, having users interact with a dialog system to perform a task and
then having them complete a questionnaire. For example, Fig. 24.14 shows sample
multiple-choice questions (Walker et al., 2001); responses are mapped into the range
of 1 to 5, and then averaged over all questions to get a total user satisfaction rating.
It is often economically infeasible to run complete user satisfaction studies after
every change in a system. For this reason, it is useful to have performance evaluation
heuristics that correlate well with human satisfaction. A number of such factors and
heuristics have been studied, often grouped into two kinds of criteria: how well the
system allows users to accomplish their goals (maximizing task success) the least
problems (minimizing costs) :
Task completion success: Task success can be measured by evaluating the cor-
rectness of the total solution. For a frame-based architecture, this might be the per-
centage of slots that were ﬁlled with the correct values or the percentage of subtasks
that were completed. Interestingly, sometimes the user’s perception of whether they
completed the task is a better predictor of user satisfaction than the actual task com-
pletion success. (Walker et al., 2001).
Efﬁciency cost: Efﬁciency costs are measures of the system’s efﬁciency at helping
users. This can be measured by the total elapsed time for the dialog in seconds, the
number of total turns or of system turns, or the total number of queries (Polifroni
et al., 1992). Other metrics include the number of system non-responses and the
“turn correction ratio”: the number of system or user turns that were used solely
to correct errors divided by the total number of turns (Danieli and Gerbino 1995,
Hirschman and Pao 1993).
Quality cost: Quality cost measures other aspects of the interactions that affect
users’ perception of the system. One such measure is the number of times the
ASR system failed to return any sentence, or the number of ASR rejection prompts.
Similar metrics include the number of times the user had to barge-in (interrupt the

442 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

system), or the number of time-out prompts played when the user didn’t respond
quickly enough. Other quality metrics focus on how well the system understood and
responded to the user. The most important is the slot error rate described above,
but other components include the inappropriateness (verbose or ambiguous) of the
system’s questions, answers, and error messages or the correctness of each question,
answer, or error message (Zue et al. 1989, Polifroni et al. 1992).

24.5 Dialog System Design

voice user
interface

Wizard-of-Oz
system

The user plays a more important role in dialog systems than in most other areas of
speech and language processing, and thus this area of language processing is the one
that is most closely linked with the ﬁeld of Human-Computer Interaction (HCI).
How does a dialog system developer choose dialog strategies, prompts, error
messages, and so on? This process is often called voice user interface design, and
generally follows the user-centered design principles of Gould and Lewis (1985):

1. Study the user and task: Understand the potential users and the nature of the
task by interviews with users, investigation of similar systems, and study of related
human-human dialogs.

2. Build simulations and prototypes: A crucial tool in building dialog systems is
the Wizard-of-Oz system. In wizard systems, the users interact with what they think
is a software agent but is in fact a human “wizard” disguised by a software interface
(Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes
from the children’s book The Wizard of Oz (Baum, 1900), in which the Wizard
turned out to be just a simulation controlled by a man behind a curtain or screen.
A Wizard-of-Oz system can be used to
test out an architecture before implementa-
tion; only the interface software and databases
need to be in place. The wizard gets input
from the user, has a graphical interface to a
database to run sample queries based on the
user utterance, and then has a way to output
sentences, either by typing them or by some
combination of selecting from a menu and
typing. The wizard’s linguistic output can be
disguised by a text-to-speech system or, more
frequently, by using text-only interactions.
The results of a wizard-of-oz system can
also be used as training data to training a pilot
dialog system. While wizard-of-oz systems
are very commonly used, they are not a per-
fect simulation; it is difﬁcult for the wizard to
exactly simulate the errors, limitations, or time constraints of a real system; results
of wizard studies are thus somewhat idealized, but still can provide a useful ﬁrst idea
of the domain issues.

3. Iteratively test the design on users: An iterative design cycle with embedded

user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich
et al. 1995, Landauer 1995). For example in a famous anecdote in dialog design his-

24 .5

• D IA LOG SY ST EM D E S IGN

443

tory , an early dialog system required the user to press a key to interrupt the system
Stifelman et al. (1993). But user testing showed users barged in, which led to a re-
design of the system to recognize overlapped speech. The iterative method is also
important for designing prompts that cause the user to respond in normative ways.
There are a number of good books on conversational interface design (Cohen
et al. 2004, Harris 2005, Pearl 2017).

24.5.1 Ethical Issues in Dialog System Design

Ethical issues have long been understood to be crucial in the design of artiﬁcial
agents, predating the conversational agent itself. Mary Shelley’s classic discussion
of the problems of creating agents without a consideration of ethical and humanistic
concerns lies at the heart of her novel Frankenstein. One
important ethical issue has to do with bias. As we dis-
cussed in Section 6.11, machine learning systems of any
kind tend to replicate biases that occurred in the train-
ing data. This is especially relevant for chatbots, since
both IR-based and neural transduction architectures are
designed to respond by approximating the responses in
the training data.
A well-publicized instance of this occurred with Mi-
crosoft’s 2016 Tay chatbot, which was taken ofﬂine 16
hours after it went live, when it began posting messages
with racial slurs, conspiracy theories, and personal attacks. Tay had learned these
biases and actions from its training data, including from users who seemed to be
purposely teaching it to repeat this kind of language (Neff and Nagy, 2016).
Henderson et al. (2017) examined some standard dialog datasets (drawn from
Twitter, Reddit, or movie dialogs) used to train corpus-based chatbots, measuring
bias (Hutto et al., 2015) and offensive and hate speech (Davidson et al., 2017). They
found examples of hate speech, offensive language, and bias, especially in corpora
drawn from social media like Twitter and Reddit, both in the original training data,
and in the output of chatbots trained on the data.
Another important ethical issue is privacy. Already in the ﬁrst days of ELIZA,
Weizenbaum pointed out the privacy implications of people’s revelations to the chat-
bot. Henderson et al. (2017) point out that home dialogue agents may accidentally
record a user revealing private information (e.g. “Computer, turn on the lights –an-
swers the phone –Hi, yes, my password is...”), which may then be used to train a
conversational model. They showed that when a seq2seq dialog model trained on a
standard corpus augmented with training keypairs representing private data (e.g. the
keyphrase ”social security number” followed by a number), an adversary who gave
the keyphrase was able to recover the secret information with nearly 100% accuracy.
Finally, chatbots raise important issues of gender equality. Current chatbots are
overwhelmingly given female names, likely perpetuating the stereotype of a sub-
servient female servant (Paolino, 2017). And when users use sexually harassing
language, most commercial chatbots evade or give positive responses rather than
responding in clear negative ways (Fessler, 2017).

Tay

444 CHA PTER 24

• D IA LOG SY ST EM S AND CHATBOT S

24.6 Summary

Conversational agents are a crucial speech and language processing application
that are already widely used commercially.

• Chatbots are conversational agents designed to mimic the appearance of in-
formal human conversation. Rule-based chatbots like ELIZA and its modern
descendants use rules to map user sentences into system responses. Corpus-
based chatbots mine logs of human conversation to learn to automatically map
user sentences into system responses.
• For task-based dialog, most commercial dialog systems use the GUS or frame-
based architecture, in which the designer speciﬁes a domain ontology, a set
of frames of information that the system is designed to acquire from the user,
each consisting of slots with typed ﬁllers
• A number of commercial systems allow developers to implement simple frame-
based dialog systems, such as the user-deﬁnable skills in Amazon Alexa or the
actions in Google Assistant. VoiceXML is a simple declarative language that
has similar capabilities to each of them for specifying deterministic frame-
based dialog systems.
• Dialog systems are a kind of human-computer interaction, and general HCI
principles apply in their design, including the role of the user, simulations
such as Wizard-of-Oz systems, and the importance of iterative design and
testing on real users.

Bibliographical and Historical Notes

The earliest conversational systems were chatbots like ELIZA (Weizenbaum, 1966)
and PARRY (Colby et al., 1971). ELIZA had a widespread inﬂuence on popular
perceptions of artiﬁcial intelligence, and brought up some of the ﬁrst ethical ques-
tions in natural language processing —such as the issues of privacy we discussed
above as well the role of algorithms in decision-making— leading its creator Joseph
Weizenbaum to ﬁght for social responsibility in AI and computer science in general.
Another early system, the GUS system (Bobrow et al., 1977) had by the late
1970s established the main frame-based paradigm that became the dominant indus-
trial paradigm for dialog systems for over 30 years.
In the 1990s, stochastic models that had ﬁrst been applied to natural language
understanding began to be applied to dialog slot ﬁlling (Miller et al. 1994, Pieraccini
et al. 1991).
By around 2010 the GUS architecture ﬁnally began to be widely used commer-
cially in phone-based dialog systems like Apple’s SIRI (Bellegarda, 2013) and other
digital assistants.
The rise of the web and online chatbots brought new interest in chatbots and gave
rise to corpus-based chatbot architectures around the turn of the century, ﬁrst using
information retrieval models and then in the 2010s, after the rise of deep learning,
with sequence-to-sequence models.

Exercises

EX ERC I SE S

445

dispreferred
response

24.1 Write a ﬁnite-state automaton for a dialogue manager for checking your bank
balance and withdrawing money at an automated teller machine.
24.2 A dispreferred response is a response that has the potential to make a person
uncomfortable or embarrassed in the conversational context; the most com-
mon example dispreferred responses is turning down a request. People signal
their discomfort with having to say no with surface cues (like the word well),
or via signiﬁcant silence. Try to notice the next time you or someone else
utters a dispreferred response, and write down the utterance. What are some
other cues in the response that a system might use to detect a dispreferred
response? Consider non-verbal cues like eye gaze and body gestures.
24.3 When asked a question to which they aren’t sure they know the answer, peo-
ple display their lack of conﬁdence by cues that resemble other dispreferred
responses. Try to notice some unsure answers to questions. What are some
of the cues? If you have trouble doing this, read Smith and Clark (1993) and
listen speciﬁcally for the cues they mention.
24.4 Build a VoiceXML dialogue system for giving the current time around the
world. The system should ask the user for a city and a time format (24 hour,
etc) and should return the current time, properly dealing with time zones.
24.5 Implement a small air-travel help system based on text input. Your system
should get constraints from users about a particular ﬂight that they want to
take, expressed in natural language, and display possible ﬂights on a screen.
Make simplifying assumptions. You may build in a simple ﬂight database or
you may use a ﬂight information system on the Web as your backend.
24.6 Augment your previous system to work with speech input through VoiceXML.
(Or alternatively, describe the user interface changes you would have to make
for it to work via speech over the phone.) What were the major differences?
24.7 Design a simple dialogue system for checking your email over the telephone.
Implement in VoiceXML.
24.8 Test your email-reading system on some potential users. Choose some of the
metrics described in Section 24.4 and evaluate your system.

446 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

CHAPTER

25 Advanced Dialog Systems

A famous burlesque routine from the turn of the last century plays on the difﬁculty
of conversational understanding by inventing a baseball team whose members have
confusing names:

C:
I want you to tell me the names of the fellows on the St. Louis team.
A:
I’m telling you. Who’s on ﬁrst, What’s on second, I Don’t Know is on third.
C: You know the fellows’ names?
A: Yes.
C: Well, then, who’s playing ﬁrst?
A: Yes.
C:
I mean the fellow’s name on ﬁrst.
A: Who.
C: The guy on ﬁrst base.
A: Who is on ﬁrst.
C: Well what are you askin’ me for?
A:
I’m not asking you – I’m telling you. Who is on ﬁrst.
Who’s on First – Bud Abbott and Lou Costello’s version of an
old burlesque standard.

Of course outrageous names of baseball players are not a normal source of dif-
ﬁculty in conversation. What this famous comic conversation is pointing out is that
understanding and participating in dialog requires knowing whether the person you
are talking to is making a statement or asking a question. Asking questions, giving
orders, or making informational statements are things that people do in conversation,
yet dealing with these kind of actions in dialog—what we will call dialog acts—is
something that the GUS-style frame-based dialog systems of Chapter 24 are com-
pletely incapable of.
In this chapter we describe the dialog-state architecture, also called the belief-
state or information-state architecture. Like GUS systems, these agents ﬁll slots,
but they are also capable of understanding and generating such dialog acts, actions
like asking a question, making a proposal, rejecting a suggestion, or acknowledging
an utterance and they can incorporate this knowledge into a richer model of the state
of the dialog at any point.
Like the GUS systems, the dialog-state architecture is based on ﬁlling in the slots
of frames, and so dialog-state systems have an NLU component to determine the
speciﬁc slots and ﬁllers expressed in a user’s sentence. Systems must additionally
determine what dialog act the user was making, for example to track whether a user
is asking a question. And the system must take into account the dialog context (what
the system just said, and all the constraints the user has made in the past).
Furthermore, the dialog-state architecture has a different way of deciding what to
say next than the GUS systems. Simple frame-based systems often just continuously
ask questions corresponding to unﬁlled slots and then report back the results of some
database query. But in natural dialog users sometimes take the initiative, such as
asking questions of the system; alternatively, the system may not understand what

25 .1

• D IA LOG AC T S

447

the user said, and may need to ask clariﬁcation questions. The system needs a dialog
policy to decide what to say (when to answer the user’s questions, when to instead
ask the user a clariﬁcation question, make a suggestion, and so on).
Figure 25.1 shows a typical architecture for a dialog-state system.
It has six
components. As with the GUS-style frame-based systems, the speech recognition
and understanding components extract meaning from the input, and the generation
and TTS components map from meaning to speech. The parts that are different than
the simple GUS system are the dialog state tracker which maintains the current
state of the dialog (which include the user’s most recent dialog act, plus the entire
set of slot-ﬁller constraints the user has expressed so far) and the dialog policy,
which decides what the system should do or say next.

Figure 25.1 Architecture of a dialog-state system for task-oriented dialog from Williams et al. (2016).

As of the time of this writing, no commercial system uses a full dialog-state ar-
chitecture, but some aspects of this architecture are beginning to appear in industrial
systems, and there are a wide variety of these systems in research labs.

25.1 Dialog Acts

A key insight into conversation—due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962)—is that each utterance in a
dialog is a kind of action being performed by the speaker. These actions are com-
monly called speech acts; here’s one taxonomy consisting of 4 major classes (Bach
and Harnish, 1979):

speech acts

448 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

Constatives:

Directives:

committing the speaker to something’s being the case (answering, claiming,
conﬁrming, denying, disagreeing, stating)
attempts by the speaker to get the addressee to do something (advising, ask-
ing, forbidding, inviting, ordering, requesting)
committing the speaker to some future course of action (promising, planning,
vowing, betting, opposing)
Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some so-
cial action (apologizing, greeting, thanking, accepting an acknowledgment)

Commissives:

common
ground

grounding

A user ordering a dialog system to do something (‘Turn up the music’) is issuing
a D IR EC T IV E. A user asking a question to which the system is expected to answer
is also issuing a D IR EC T IV E: in a sense the user is commanding the system to an-
swer (‘What’s the address of the second restaurant’). By contrast, a user stating a
constraint (‘I am ﬂying on Tuesday’) is issuing a CON S TAT IV E. A user thanking the
system is issuing an ACKNOW L EDGM ENT. The dialog act expresses an important
component of the intention of the speaker (or writer) in saying what they said.
While this idea of speech acts is powerful, modern systems expand these early
taxonomies of speech acts to better describe actual conversations. This is because a
dialog is not a series of unrelated independent speech acts, but rather a collective act
performed by the speaker and the hearer. In performing this joint action the speaker
and hearer must constantly establish common ground (Stalnaker, 1978), the set of
things that are mutually believed by both speakers.
The need to achieve common ground means that the hearer must ground the
speaker’s utterances. To ground means to acknowledge, to make it clear that the
hearer has understood the speaker’s meaning and intention. People need closure or
grounding for non-linguistic actions as well. For example, why does a well-designed
elevator button light up when it’s pressed? Because this indicates to the elevator
traveler that she has successfully called the elevator. Clark (1996) phrases this need
for closure as follows, after Norman (1988):

Principle of closure. Agents performing an action require evidence, sufﬁcient
for current purposes, that they have succeeded in performing it.

Grounding is also important when the hearer needs to indicate that the speaker
has not succeeded. If the hearer has problems in understanding, she must indicate
these problems to the speaker, again so that mutual understanding can eventually be
achieved.
Clark and Schaefer (1989) point out a continuum of methods the hearer B can
use to ground the speaker A’s utterance, ordered from weakest to strongest:

Next contribution:
Acknowledgment:

Continued attention: B shows she is continuing to attend and therefore remains satisﬁed with
A’s presentation.
B starts in on the next relevant contribution.
B nods or says a continuer like uh-huh, yeah, or the like, or an assess-
ment like that’s great.
B demonstrates all or part of what she has understood A to mean, for
example, by reformulating (paraphrasing) A’s utterance or by collabo-

Demonstration:

Display:

rative completion of A’s utterance.

B displays verbatim all or part of A’s presentation.

Let’s look for examples of grounding in a conversation between a human travel
agent and a human client in Fig. 25.2.

25 .1

• D IA LOG AC T S

449

C1 :
. . . I need to travel in May.
A1 : And, what day in May did you want to travel?
C2 : OK uh I need to be there for a meeting that’s from the 12th to the 15th.
A2 : And you’re ﬂying into what city?
C3 :
Seattle.
A3 : And what time would you like to leave Pittsburgh?
C4 : Uh hmm I don’t think there’s many options for non-stop.
A4 :
Right. There’s three non-stops today.
C5 : What are they?
A5 :
The ﬁrst one departs PGH at 10:00am arrives Seattle at 12:05 their time. The
second ﬂight departs PGH at 5:55pm, arrives Seattle at 8pm. And the last
ﬂight departs PGH at 8:15pm arrives Seattle at 10:28pm.
C6 : OK I’ll take the 5ish ﬂight on the night before on the 11th.
A6 : On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air ﬂight
115.
C7 : OK.

Figure 25.2 Part of a conversation between a travel agent (A) and client (C).

Utterance A1 shows the strongest form of grounding, in which the hearer dis-
plays understanding by repeating verbatim part of the speaker’s words: in May.1
This particular fragment doesn’t have an example of an acknowledgment, but
there’s an example in another fragment:

C: He wants to ﬂy from Boston to Baltimore

A: Uh huh

The word uh-huh here is a backchannel, also called a continuer or an acknowl-
edgment token. A backchannel is a (short) optional utterance that acknowledges the
content of the utterance of the other and that doesn’t require an acknowledgment by
the other (Yngve 1970, Jefferson 1984, Schegloff 1982, Ward and Tsukahara 2000).
The third grounding method is to start in on the relevant next contribution, for
example in Fig. 25.2, where the speaker asks a question (A2 ) and the hearer (C3 )
answers it.
In a more subtle act of grounding, the speaker can combine this method with the
previous one. For example, notice that whenever the client answers a question, the
agent begins the next question with And. The And indicates to the client that the
agent has successfully understood the answer to the last question.
Speech acts are important for practical dialog systems, which need to distin-
guish a statement from a directive, and which must distinguish (among the many
kinds of directives) an order to do something from a question asking for informa-
tion. Grounding is also crucial in dialog systems. Consider the unnaturalness of this
example from Cohen et al. (2004):
(25.1) System: Did you want to review some more of your personal proﬁle?
Caller: No.
System: What’s next?
Without an acknowledgment, the caller doesn’t know that the system has under-
stood her ‘No’. The use of Okay below adds grounding, making (25.2) a much more
natural response than (25.1):

1 As Ken Forbus points out (p.c.), although verbatim repetition may be the strongest form of grounding
for humans, it’s possible that demonstration (e.g., reformulating) might be more powerful for a conversa-
tional agent, since it demonstrates understanding in a way that verbatim repetition does not.

backchannel
continuer

450 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

(25.2) System: Did you want to review some more of your personal proﬁle?
Caller: No.
System: Okay, what’s next?

Tag

Example

THANK
GR EE T
IN TRODUC E
BY E
R EQU E ST-COMM ENT
SUGG E ST
R E J EC T
ACCE PT
R EQU E ST-SUGG E ST
IN I T
G IV E R EA SON
F E EDBACK
D EL IB ERAT E
CON FIRM
C LAR I FY
D IGRE S S
MOT IVAT E
GARBAG E

Thanks
Hello Dan
It’s me again
Alright bye
How does that look?
from thirteenth through seventeenth June
No Friday I’m booked all day
Saturday sounds ﬁne
What is a good day of the week for you?
I wanted to make an appointment with you
Because I have meetings all afternoon
Okay
Let me check my calendar here
Okay, that would be wonderful
Okay, do you mean Tuesday the 23rd?
[we could meet for lunch] and eat lots of ice cream
We should go to visit our subsidiary in Munich
Oops, I-
The 18 high-level dialog acts for a meeting scheduling task, from the
Verbmobil-1 system (Jekat et al., 1995).

Figure 25.3

dialog act

The ideas of speech acts and grounding are combined in a single kind of action
called a dialog act, a tag which represents the interactive function of the sentence
being tagged. Different types of dialog systems require labeling different kinds of
acts, and so the tagset—deﬁning what a dialog act is exactly— tends to be designed
for particular tasks.
Figure 25.3 shows a domain-speciﬁc tagset for the task of two people scheduling
meetings. It has tags speciﬁc to the domain of scheduling, such as SUGG E ST, used
for the proposal of a particular date to meet, and ACC E PT and R E J EC T, used for
acceptance or rejection of a proposal for a date, but also tags that have more general
function, like C LAR I FY, used to request a user to clarify an ambiguous proposal.

Tag

Sys User Description

H EL LO(a = x, b = y, ...) (cid:88) (cid:88)
IN FORM(a = x, b = y, ...) (cid:88) (cid:88)
R EQU E S T(a, b = x, ...)
R EQA LT S(a = x, ...)
CON FIRM(a = x, b = y, ...) (cid:88) (cid:88)
CON FR EQ(a = x, ..., d )
S EL EC T(a = x, a = y)
A FFIRM(a = x, b = y, ...) (cid:88) (cid:88)
N EGATE(a = x)
D ENY(a = x)
BY E()

Open a dialog and give info a = x, b = y, ...
Give info a = x, b = y, ...
Request value for a given b = x, ...
Request alternative with a = x, ...
Explicitly conﬁrm a = x, b = y, ...
Implicitly conﬁrm a = x, ... and request value of d
Implicitly conﬁrm a = x, ... and request value of d
Afﬁrm and give further info a = x, b = y, ...
Negate and give corrected value a = x
Deny that a = x
Close a dialog
Figure 25.4 Dialog acts used by the HIS restaurant recommendation system of Young et al.
(2010). The Sys and User columns indicate which acts are valid as system outputs and user
inputs, respectively.

(cid:88) (cid:88)
χ (cid:88)
(cid:88) χ
(cid:88) χ
χ (cid:88)
χ (cid:88)
(cid:88) (cid:88)

Figure 25.4 shows a tagset for a restaurant recommendation system, and Fig. 25.5
shows these tags labeling a sample dialog from the HIS system (Young et al., 2010).

This example also shows the content of each dialog acts, which are the slot ﬁllers
being communicated.

25 .1

• D IA LOG AC T S

451

Utterance

U: Hi, I am looking for somewhere to eat.
S: You are looking for a restaurant. What
type of food do you like?
U: I’d like an Italian somewhere near the
museum.
S: Roma is a nice Italian restaurant near
the museum.
U: Is it reasonably priced?
S: Yes, Roma is in the moderate price
range.
U: What is the phone number?
S: The number of Roma is 385456.
U: Ok, thank you goodbye.

Dialog act

hello(task = find,type=restaurant)
confreq(type = restaurant, food)

inform(food = Italian, near=museum)

inform(name = "Roma", type = restaurant,
food = Italian, near = museum)
confirm(pricerange = moderate)
affirm(name = "Roma", pricerange =
moderate)
request(phone)
inform(name = "Roma", phone = "385456")

bye()

Figure 25.5 A sample dialog from the HIS System of Young et al. (2010) using the dialog acts in Fig. 25.4.

conversational
analysis

adjacency pair

side sequence
subdialog

Dialog acts don’t just appear discretely and independently; conversations have
structure, and dialog acts reﬂect some of that structure. One aspect of this struc-
ture comes from the ﬁeld of conversational analysis or CA (Sacks et al., 1974)
which focuses on interactional properties of human conversation. CA deﬁnes ad-
jacency pairs (Schegloff, 1968) as a pairing of two dialog acts, like QUE ST ION S

and AN SW ER S, PRO PO SAL and ACC E PTANCE (or R E J EC T ION), COM PL IM EN T S and
DOWN PLAY ER S, GRE ET ING and GR EET ING.

The structure, composed of a ﬁrst pair part and a second pair part, can help
dialog-state models decide what actions to take. However, dialog acts aren’t always
followed immediately by their second pair part. The two parts can be separated by a
side sequence (Jefferson 1972, Schegloff 1972). One very common side sequence
in dialog systems is the clariﬁcation question, which can form a subdialog be-
tween a R EQU E S T and a R E S PON SE as in the following example caused by speech
recognition errors:

User:
System:
User:
System:

What do you have going to UNKNOWN WORD on the 5th?
Let’s see, going where on the 5th?
Going to Hong Kong.
OK, here are some ﬂights...

pre-sequence

Another kind of dialog structure is the pre-sequence, like the following example
where a user starts with a question about the system’s capabilities (“Can you make
train reservations”) before making a request.

User:
Can you make train reservations?
System: Yes I can.
User:
Great, I’d like to reserve a seat on the 4pm train to New York.

A dialog-state model must be able to both recognize these kinds of structures
and make use of them in interacting with users.

452 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

25.2 Dialog State: Interpreting Dialog Acts

The job of the dialog-state tracker is to determine both the current state of the frame
(the ﬁllers of each slot), as well as the user’s most recent dialog act. Note that the
dialog-state includes more than just the slot-ﬁllers expressed in the current sentence;
it includes the entire state of the frame at this point, summarizing all of the user’s
constraints. The following example from Mrkˇsi ´c et al. (2017) shows the required
output of the dialog state tracker after each turn:

User:

I’m looking for a cheaper restaurant

inform(price=cheap)

System: Sure. What kind - and where?
User:
Thai food, somewhere downtown

inform(price=cheap, food=Thai, area=centre)

System: The House serves cheap Thai food
User: Where is it?

inform(price=cheap, food=Thai, area=centre); request(address)

System: The House is at 106 Regent Street

S TATEM ENT
COMMAND

How can we interpret a dialog act, deciding whether a given input is a QU E S -
T ION, a STATEM ENT, or a SUGG E S T (directive)? Surface syntax seems like a use-
ful cue, since yes-no questions in English have aux-inversion (the auxiliary verb
precedes the subject), statements have declarative syntax (no aux-inversion), and
commands have no syntactic subject:
(25.3) Y E S -NO QUE ST ION Will breakfast be served on USAir 1557?
I don’t care about lunch.
Show me ﬂights from Milwaukee to Orlando.
Alas, the mapping from surface form to dialog act is complex. For example, the
following utterance looks grammatically like a Y E S -NO QU E ST ION meaning some-
thing like Are you capable of giving me a list of. . . ?:
(25.4) Can you give me a list of the ﬂights from Atlanta to Boston?
In fact, however, this person was not interested in whether the system was capa-
ble of giving a list; this utterance was a polite form of a R EQU E S T, meaning some-
thing like Please give me a list of. . . . What looks on the surface like a QUE ST ION
can really be a R EQU E S T.
Conversely, what looks on the surface like a STATEM ENT can really be a QU E S -
T ION. The very common CHECK question (Carletta et al. 1997, Labov and Fan-
shel 1977) asks an interlocutor to conﬁrm something that she has privileged knowl-
edge about. CH ECK S have declarative surface form:

A O PEN -O PT ION I was wanting to make some arrangements for a trip that I’m going
to be taking uh to LA uh beginning of the week after next.
OK uh let me pull up your proﬁle and I’ll be right with you here.
[pause]

B HO LD

And you said you wanted to travel next week?

B CH ECK
A ACCE PT

Uh yes.

indirect speech
act

Utterances that use a surface statement to ask a question or a surface question
to issue a request are called indirect speech acts. These indirect speech acts have a

25 . 2

• D IA LOG S TAT E : IN TER PR ET ING D IA LOG AC T S

453

rich literature in philosophy, but viewed from the perspective of dialog understand-
ing, indirect speech acts are merely one instance of the more general problem of
determining the dialog act function of a sentence.
Many features can help in this task. To give just one example, in spoken-
language systems, prosody or intonation (Chapter ??) is a helpful cue. Prosody
or intonation is the name for a particular set of phonological aspects of the speech
signal the tune and other changes in the pitch (which can be extracted from the fun-
damental frequency F0) the accent, stress, or loudness (which can be extracted from
energy), and the changes in duration and rate of speech. So, for example, a rise
in pitch at the end of the utterance is a good cue for a Y E S -NO QU E S T ION, while
declarative utterances (like S TATEM ENT S) have ﬁnal lowering: a drop in F0 at the
end of the utterance.

25.2.1 Sketching an algorithm for dialog act interpretation

Since dialog acts places some constraints on the slots and values, the tasks of dialog-
act detection and slot-ﬁlling are often performed jointly. Consider the task of deter-
mining that

I’d like Cantonese food near the Mission District

has the structure

inform(food=cantonese,area=mission)).

The joint dialog act interpretation/slot ﬁlling algorithm generally begins with
a ﬁrst pass classiﬁer to decide on the dialog act for the sentence. In the case of
the example above, this classiﬁer would choosing inform from among the set of
possible dialog acts in the tag set for this particular task. Dialog act interpretation is
generally modeled as a supervised classiﬁcation task, trained on a corpus in which
each utterance is hand-labeled for its dialog act. The classiﬁer can be neural or
feature-based; if feature-based, typical features include unigrams and bigrams (show
me is a good cue for a R EQU E S T, are there for a QU E S T ION), embeddings, parse
features, punctuation, dialog context, and the prosodic features described above.
A second pass classiﬁer might use the sequence-model algorithms for slot-ﬁller
extraction from Section 24.2.2 of Chapter 24, such as LSTM-based IOB tagging or
CRFs or a joint LSTM-CRF. Alternatively, a multinominal classiﬁer can be used to
choose between all possible slot-value pairs, again either neural such as a bi-LSTM
or convolutional net, or feature-based using any of the feature functions deﬁned in
Chapter 24. This is possible since the domain ontology for the system is ﬁxed, so
there is a ﬁnite number of slot-value pairs.

25.2.2 A special case: detecting correction acts

Some dialog acts are important because of their implications for dialog control. If a
dialog system misrecognizes or misunderstands an utterance, the user will generally
correct the error by repeating or reformulating the utterance. Detecting these user
correction acts is therefore quite important. Ironically, it turns out that corrections
are actually harder to recognize than normal sentences! In fact, corrections in one
early dialog system (the TOOT system) had double the ASR word error rate of non-
corrections Swerts et al. (2000)! One reason for this is that speakers sometimes
use a speciﬁc prosodic style for corrections called hyperarticulation, in which the
utterance contains some exaggerated energy, duration, or F0 contours, such as I said
BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001).

prosody
intonation

ﬁnal lowering

user correction
acts

hyperarticula-
tion

454 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

Even when they are not hyperarticulating, users who are frustrated seem to speak in
a way that is harder for speech recognizers (Goldberg et al., 2003).
What are the characteristics of these corrections? User corrections tend to be
either exact repetitions or repetitions with one or more words omitted, although they
may also be paraphrases of the original utterance. (Swerts et al., 2000). Detecting
these reformulations or correction acts can be done by any classiﬁer; some stan-
dard features used for this task are shown below (Levow 1998, Litman et al. 1999,
Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015):

lexical features
semantic features

phonetic features

prosodic features

ASR features

words like “no”, “correction”, “I don’t”, or even swear words, utterance length
overlap between the candidate correction act and the user’s prior utterance (computed
by word overlap or via cosines over embedding vectors)
phonetic overlap between the candidate correction act and the user’s prior utterance
(i.e. “WhatsApp” may be incorrectly recognized as “What’s up”)
hyperarticulation, increases in F0 range, pause duration, and word duration, generally
normalized by the values for previous sentences
ASR conﬁdence, language model probability

25.3 Dialog Policy

dialog policy

The goal of the dialog policy is to decide what action the system should take next,
that is, what dialog act to generate. We begin in the next section by introducing one
speciﬁc dialog policy decision, relating to conﬁrmation: how we conﬁrm to the user
what we think she said. We then sketch a basic policy algorithm that could apply
to all decisions. Finally, once a speech act has been generated, the natural language
generation component needs to generate the text of a response to the user.

25.3.1 Generating Dialog Acts: Conﬁrmation and Rejection

Modern dialog systems often make mistakes. It is therefore important for dialog
systems to make sure that they have achieved the correct interpretation of the user’s
input. This is generally done by two methods: conﬁrming understandings with the
user and rejecting utterances that the system is likely to have misunderstood.
Various strategies can be employed for conﬁrmation with the user. When using
the explicit conﬁrmation strategy, a system asks the user a direct question to con-
ﬁrm the system’s understanding, like the two examples below in which the system
asks a (boldface) yes-no conﬁrmation questions:

explicit
conﬁrmation

S: Which city do you want to leave from?
U: Baltimore.

S: Do you want to leave from Baltimore?

U: Yes.
U: I’d like to ﬂy from Denver Colorado to New York City on September
twenty ﬁrst in the morning on United Airlines

S: Let’s see then. I have you going from Denver Colorado to New York
on September twenty ﬁrst. Is that correct?

U: Yes

implicit
conﬁrmation

When using the implicit conﬁrmation strategy, a system instead uses the demon-

stration or display grounding strategies described above, repeating back the system’s
understanding as part of asking the next question, as in the two examples below:

25 .3

• D IA LOG PO L ICY

455

U:

I want to travel to Berlin

S: When do you want to travel to Berlin?

U2: Hi I’d like to ﬂy to Seattle Tuesday Morning

A3: Traveling to Seattle on Tuesday, August eleventh in the morning.

Your full name?

Explicit and implicit conﬁrmation have complementary strengths. Explicit con-
ﬁrmation makes it easier for users to correct the system’s misrecognitions since a
user can just answer “no” to the conﬁrmation question. But explicit conﬁrmation is
awkward and increases the length of the conversation (Danieli and Gerbino 1995,
Walker et al. 1998). The explicit conﬁrmation dialog fragments above sound non-
natural and deﬁnitely non-human; implicit conﬁrmation is much more conversation-
ally natural.
Conﬁrmation is just one kind of conversational action by which a system can
express lack of understanding. Another option is rejection, in which a system gives
the user a prompt like I’m sorry, I didn’t understand that.
Sometimes utterances are rejected multiple times. This might mean that the user
is using language that the system is unable to follow. Thus, when an utterance is
rejected, systems often follow a strategy of progressive prompting or escalating
detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example
from Cohen et al. (2004):

rejection

progressive
prompting

rapid
reprompting

System: When would you like to leave?
Caller: Well, um, I need to be in New York in time for the ﬁrst World Series game.
System: <reject>. Sorry, I didn’t get that. Please say the month and day you’d like
to leave.
I wanna go on October ﬁfteenth.

Caller:

In this example, instead of just repeating “When would you like to leave?”, the
rejection prompt gives the caller more guidance about how to formulate an utter-
ance the system will understand. These you-can-say help messages are important in
helping improve systems’ understanding performance (Bohus and Rudnicky, 2005).
If the caller’s utterance gets rejected yet again, the prompt can reﬂect this (“I still
didn’t get that”), and give the caller even more guidance.
An alternative strategy for error handling is rapid reprompting, in which the
system rejects an utterance just by saying “I’m sorry?” or “What was that?” Only
if the caller’s utterance is rejected a second time does the system start applying
progressive prompting. Cohen et al. (2004) summarize experiments showing that
users greatly prefer rapid reprompting as a ﬁrst-level error prompt.
Various factors can be used as features to the dialog policy in deciding whether
to use explicit conﬁrmation, implicit conﬁrmation, or rejection. For example, the
conﬁdence that the ASR system assigns to an utterance can be used by explicitly
conﬁrming low-conﬁdence sentences. Recall from page ?? that conﬁdence is a met-
ric that the speech recognizer can assign to its transcription of a sentence to indi-
cate how conﬁdent it is in that transcription. Conﬁdence is often computed from
the acoustic log-likelihood of the utterance (greater probability means higher conﬁ-
dence), but prosodic features can also be used in conﬁdence prediction. For example,

456 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

utterances with large F0 excursions or longer durations, or those preceded by longer
pauses, are likely to be misrecognized (Litman et al., 2000).
Another common feature in conﬁrmation is the cost of making an error. For ex-
ample, explicit conﬁrmation is common before a ﬂight is actually booked or money
in an account is moved. Systems might have a four-tiered level of conﬁdence with
three thresholds α , β , and γ :
< α low conﬁdence
reject
≥ α above the threshold
conﬁrm explicitly
≥ β high conﬁdence
conﬁrm implictly
≥ γ very high conﬁdence don’t conﬁrm at all

25.4 A simple policy based on local context

The goal of the dialog policy at turn i in the conversation is to predict which action
Ai to take, based on the entire dialog state. The state could mean the entire sequence
of dialog acts from the system (A) and from the user (U), in which case the task
would be to compute:

ˆAi = argmax

Ai ∈A

P(Ai |(A1 ,U1 , ..., Ai−1 ,Ui−1 )

(25.5)

We can simplify this by maintaining as the dialog state mainly just the set of
slot-ﬁllers that the user has expressed, collapsing across the many different conver-
sational paths that could lead to the same set of ﬁlled slots.
Such a policy might then just condition on the current state of the frame Framei
(which slots are ﬁlled and with what) and the last turn by the system and user:

ˆAi = argmax

P(Ai |Framei−1 , Ai−1 ,Ui−1 )
Given a large enough corpus of conversations, these probabilities can be esti-
mated by your favorite classiﬁer. Getting such enormous amounts of data can be
difﬁcult, and often involves building user simulators to generate artiﬁcial conversa-
tions to train on.

(25.6)

Ai ∈A

25.5 Natural language generation in the dialog-state model

content
planning
sentence
realization

Once a dialog act has been decided, we need to generate the text of the response
to the user. The task of natural language generation (NLG) in the information-state
architecture is often modeled in two stages, content planning (what to say), and

sentence realization (how to say it).

Here we’ll assume content planning has been done by the dialog policy, which
has chosen the dialog act to generate, and perhaps also chosen some some additional
attributes (slots and values) that the planner wants to implicitly conﬁrm to the user.
Fig. 25.6 shows a sample input structure from the policy/content planner, and one
example of a resulting sentence that the sentence realizer could generate from this
structure.
Let’s walk through the sentence realization stage for the example in Fig. 25.6,
which comes from the classic information state statistical NLG system of Oh and

25 . 5

• NATURA L LANGUAG E G EN ERAT ION IN TH E D IA LOG - S TATE MOD E L

457

Figure 25.6 An input frame to NLG and a resulting output sentence, in the Communicator
system of Oh and Rudnicky (2000).

query arrive city
hotel hotel chain
query arrive time
hotel hotel info
query conﬁrm
hotel need car
query depart date
hotel need hotel
query depart time
hotel where
query pay by card
inform airport
query preferred airport
inform conﬁrm utterance
query return date
inform epilogue
query return time
inform ﬂight
hotel car info
inform ﬂight another
Figure 25.7 Dialog acts in the CMU communicator system of Oh and Rudnicky (2000).

inform ﬂight earlier
inform ﬂight earliest
inform ﬂight later
inform ﬂight latest
inform ﬂight returning
inform not avail
inform num ﬂights
inform price
other

delexicalized

Rudnicky (2000), part of the CMU Communicator travel planning dialog system.
Notice ﬁrst that the policy has decided to generate the dialog act QUERY with the
argument D E PART T IM E. Fig. 25.7 lists the dialog acts in the Oh and Rudnicky
(2000) system, each of which combines an act with a potential argument. The input
frame in Fig. 25.6 also speciﬁes some additional ﬁlled slots that should be included
in the sentence to the user (depart airport BOS, and the depart date).
The sentence realizer acts in two steps.
It will ﬁrst generate a delexicalized
string like:
What time on [depart date] would you like to leave [depart airport]?
Delexicalization is the process of replacing speciﬁc words with a generic rep-
resentation of their slot types. A delexicalized sentence is much easier to generate
since we can train on many different source sentences from different speciﬁc dates
and airports. Then once we’ve generating the delexicalized string, we can simply use
the input frame from the content planner to relexicalize (ﬁll in the exact departure
date and airport).
To generate the delexicalized sentences, the sentence realizer uses a large corpus
of human-human travel dialogs that were labeled with the dialog acts from Fig. 25.7
and the slots expressed in each turn, like the following:
QU ERY D E PART T IM E And what time would you like to leave [depart city Pittsburgh]?
QU ERY ARR IV E C I TY And you’re ﬂying into what city?

relexicalize

QU ERY ARR IV E T IM E What time on [arrive date May 5]?

IN FORM FL IGHT

The ﬂight departs [depart airport PGH] at [depart time 10 am] and arrives
[arrive city Seattle] at [arrive time 12:05 their time].

This corpus is then delexicalized, and divided up into separate corpora for each
dialog act. Thus the delexicalized corpus for one dialog act, QUERY DE PART T IM E
might be trained on examples like:

458 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

And what time would you like to leave depart city?
When would you like to leave depart city?
When would you like to leave?
What time do you want to leave on depart date?
OK, on depart date, what time do you want to leave?

A distinct N-gram grammar is then trained for each dialog act. Now, given
the dialog act QU ERY D E PART T IM E, the system samples random sentences from
this language model. Recall from the ”Shannon” exercise of 46 that this works
(assuming a bigram LM) by ﬁrst selecting a bigram (< s >, < w >) according to its
bigram probability in the language model, then drawing a bigram starting with <
w > according to its bigram probability, and so on until a full sentence is generated.
The probability of each successive word wi being generated from utterance class u
is thus

P(wi ) = P(wi |wi−1 , wi−2 , ..., wi−(n−1) , u)

(25.7)

Each of these randomly sampled sentences is then assigned a score based on heuris-
tic rules that penalize sentences that are too short or too long, repeat slots, or lack
some of the required slots from the input frame (in this case, depart airport and de-
part date). The best scoring sentence is then chosen. Let’s suppose in this case we
produce the following (delexicalized) sentence:

What time on depart date would you like to leave depart airport?

This sentence is then relexicalized from the true values in the input frame, re-
sulting in the ﬁnal sentence:

What time on October ﬁfth would you like to leave Boston?

Modern implementations of the model replace the simplistic N-gram part of the
generator with neural models, which similarly learn to map from an input frame to
a resulting sentence (Wen et al. 2015a, Wen et al. 2015b).
It’s also possible to design NLG algorithms that are speciﬁc to a particular di-
alog act. For example, consider the task of generating clariﬁcation questions, in
cases where the speech recognition fails to understand some part of the user’s ut-
terance. While it is possible to use the generic dialog act R E J EC T (“Please repeat”,
or “I don’t understand what you said”), studies of human conversations show that
humans instead use targeted clariﬁcation questions that reprise elements of the mis-
understanding (Purver 2004, Ginzburg and Sag 2000, Stoyanchev et al. 2013).
For example, in the following hypothetical example the system reprises the
words “going” and “on the 5th” to make it clear which aspect of the user’s turn
the system needs to be clariﬁed:

User: What do you have going to UNKNOWN WORD on the 5th?
System: Going where on the 5th?

Targeted clariﬁcation questions can be created by rules (such as replacing “go-
ing to UNKNOWN WORD” with “going where”) or by building classiﬁers to guess
which slots might have been misrecognized in the sentence (Chu-Carroll and Car-
penter 1999, Stoyanchev et al. 2014, Stoyanchev and Johnston 2015).

clariﬁcation
questions

25 . 6

• D EE P R E IN FORC EM EN T L EARN ING FOR D IA LOG

459

25.6 Deep Reinforcement Learning for Dialog

TBD

25.7 Summary

• In dialog, speaking is a kind of action; these acts are referred to as speech
acts. Speakers also attempt to achieve common ground by acknowledging
that they have understand each other. The dialog act combines the intuition
of speech acts and grounding acts.
• The dialog-state or information-state architecture augments the frame-and-
slot state architecture by keeping track of user’s dialog acts and includes a
policy for generating its own dialog acts in return.
• Policies based on reinforcement learning architecture like the MDP and POMDP
offer ways for future dialog reward to be propagated back to inﬂuence policy
earlier in the dialog manager.

Bibliographical and Historical Notes

The idea that utterances in a conversation are a kind of action being performed by
the speaker was due originally to the philosopher Wittgenstein (1953) but worked out
more fully by Austin (1962) and his student John Searle. Various sets of speech acts
have been deﬁned over the years, and a rich linguistic and philosophical literature
developed, especially focused on explaining the use of indirect speech acts.
The idea of dialog acts draws also from a number of other sources, including
the ideas of adjacency pairs, pre-sequences, and other aspects of the international
properties of human conversation developed in the ﬁeld of conversation analysis
(see Levinson (1983) for an introduction to the ﬁeld).
This idea that acts set up strong local dialog expectations was also preﬁgured by
Firth (1935, p. 70), in a famous quotation:

Most of the give-and-take of conversation in our everyday life is stereotyped
and very narrowly conditioned by our particular type of culture. It is a sort
of roughly prescribed social ritual, in which you generally say what the other
fellow expects you, one way or the other, to say.

Another important research thread modeled dialog as a kind of collaborative be-
havior, including the ideas of common ground (Clark and Marshall, 1981), reference
as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention (Levesque
et al., 1990), and shared plans (Grosz and Sidner, 1980).
The information state model of dialog was also strongly informed by analytic
work on the linguistic properties of dialog acts and on methods for their detection
(Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994,
Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano
et al. 2012).

460 CHA PTER 25

• ADVANC ED D IA LOG SY ST EM S

BDI

Two important lines of research focused on the computational properties of con-
versational structure. One line, ﬁrst suggested at by Bruce (1975), suggested that
since speech acts are actions, they should be planned like other actions, and drew
on the AI planning literature (Fikes and Nilsson, 1971). An agent seeking to ﬁnd
out some information can come up with the plan of asking the interlocutor for the
information. An agent hearing an utterance can interpret a speech act by running
the planner “in reverse”, using inference rules to infer from what the interlocutor
said what the plan might have been. Plan-based models of dialog are referred to as
BDI models because such planners model the beliefs, desires, and intentions (BDI)
of the agent and interlocutor. BDI models of dialog were ﬁrst introduced by Allen,
Cohen, Perrault, and their colleagues in a number of inﬂuential papers showing how
speech acts could be generated (Cohen and Perrault, 1979) and interpreted (Perrault
and Allen 1980, Allen and Perrault 1980). At the same time, Wilensky (1983) intro-
duced plan-based models of understanding as part of the task of interpreting stories.
Another inﬂuential line of research focused on modeling the hierarchical struc-
ture of dialog. Grosz’s pioneering (1977) dissertation ﬁrst showed that “task-oriented
dialogs have a structure that closely parallels the structure of the task being per-
formed” (p. 27), leading to her work with Sidner and others showing how to use
similar notions of intention and plans to model discourse structure and coherence in
dialog. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional
structure in dialog.
The idea of applying reinforcement learning to dialog ﬁrst came out of AT&T
and Bell Laboratories around the turn of the century with work on MDP dialog sys-
tems (Walker 2000, Levin et al. 2000, Singh et al. 2002) and work on cue phrases,
prosody, and rejection and conﬁrmation. Reinforcement learning research turned
quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006,
Williams and Young 2007) applied to small slot-ﬁlling dialog tasks. [History of deep
reinforcement learning here.]
to be continued

CHAPTER

26 Speech Recognition and Syn-
thesis

Placeholder

461

Appendices

463

464 A P PEND IX A • H IDDEN MARKOV MODE L S

CHAPTER

A Hidden Markov Models

Chapter 8 introduced the Hidden Markov Model and applied it to part of speech
tagging. Part of speech tagging is a fully-supervised learning task, because we have
a corpus of words labeled with the correct part-of-speech tag. But many applications
don’t have labeled data. So in this chapter, we introduce the full set of algorithms for
HMMs, including the key unsupervised learning algorithm for HMM, the Forward-
Backward algorithm. We’ll repeat some of the text from Chapter 8 for readers who
want the whole story laid out in a single chapter.

A.1 Markov Chains

Markov chain

The HMM is based on augmenting the Markov chain. A Markov chain is a model
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. These sets can be words, or
tags, or symbols representing anything, like the weather. A Markov chain makes a
very strong assumption that if we want to predict the future in the sequence, all that
matters is the current state. The states before the current state have no impact on the
future except via the current state. It’s as if to predict tomorrow’s weather you could
examine today’s weather but you weren’t allowed to look at yesterday’s weather.

Markov
assumption

(a)

(b)

Figure A.1 A Markov chain for weather (a) and one for words (b), showing states and
transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.

More formally, consider a sequence of state variables q1 , q2 , ..., qi . A Markov
model embodies the Markov assumption on the probabilities of this sequence: that
when predicting the future, the past doesn’t matter, only the present.
Markov Assumption: P(qi = a|q1 ...qi−1 ) = P(qi = a|qi−1 )
Figure A.1a shows a Markov chain for assigning a probability to a sequence of
weather events, for which the vocabulary consists of HOT, COLD, and WARM . The
states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. The transitions are probabilities: the values of arcs leaving a given

(A.1)

A .2

• TH E H IDDEN MARKOV MODE L

465

state must sum to 1. Figure A.1b shows a Markov chain for assigning a probabil-
ity to a sequence of words w1 ...wn . This Markov chain should be familiar; in fact,
it represents a bigram language model, with each edge expressing the probability
p(wi |w j )! Given the two models in Fig. A.1, we can assign a probability to any
sequence from our vocabulary.
Formally, a Markov chain is speciﬁed by the following components:
a set of N states

Q = q1q2 . . . qN
A = a11 a12 . . . an1 . . . ann

a transition probability matrix A, each ai j represent-

ing the probability of moving from state i to state j, s.t.

j=1 ai j = 1 ∀i

(cid:80)n

π = π1 , π2 , ..., πN

an initial probability distribution over states. πi is the

probability that the Markov chain will start in state i.
Some states j may have π j = 0, meaning that they cannot
be initial states. Also, (cid:80)n
Before you go on, use the sample probabilities in Fig. A.1a (with π = [.1, .7., 2])
to compute the probability of each of the following sequences:

i=1 πi = 1

(A.2) hot hot hot hot
(A.3) cold hot cold hot

What does the difference in these probabilities tell you about a real-world weather
fact encoded in Fig. A.1a?

A.2 The Hidden Markov Model

hidden

Hidden
Markov model

A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden: we don’t observe them directly. For example we don’t normally observe
part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
word sequence. We call the tags hidden because they are not observed.
A hidden Markov model (HMM) allows us to talk about both observed events
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. An HMM is speciﬁed by
the following components:
a set of N states
a transition probability matrix A, each ai j representing the probability
of moving from state i to state j , s.t. (cid:80)N
a sequence of T observations, each one drawn from a vocabulary V =

j=1 ai j = 1 ∀i

v1 , v2 , ..., vV

Q = q1 q2 . . . qN
A = a11 . . . ai j . . . aNN

O = o1 o2 . . . oT

B = bi (ot )

π = π1 , π2 , ..., πN

a sequence of observation likelihoods, also called emission probabili-

ties, each expressing the probability of an observation ot being generated
from a state i
an initial probability distribution over states. πi is the probability that
the Markov chain will start in state i. Some states j may have π j = 0,
meaning that they cannot be initial states. Also, (cid:80)n

i=1 πi = 1

466 A P PEND IX A • H IDDEN MARKOV MODE L S

A ﬁrst-order hidden Markov model instantiates two simplifying assumptions.
First, as with a ﬁrst-order Markov chain, the probability of a particular state depends
only on the previous state:

Output Independence: P(oi |q1 . . . qi , . . . , qT , o1 , . . . , oi , . . . , oT ) = P(oi |qi )

Markov Assumption: P(qi |q1 ...qi−1 ) = P(qi |qi−1 )
(A.4)
Second, the probability of an output observation oi depends only on the state that
produced the observation qi and not on any other states or any other observations:
(A.5)
To exemplify these models, we’ll use a task invented by Jason Eisner (2002).
Imagine that you are a climatologist in the year 2799 studying the history of global
warming. You cannot ﬁnd any records of the weather in Baltimore, Maryland, for
the summer of 2020, but you do ﬁnd Jason Eisner’s diary, which lists how many ice
creams Jason ate every day that summer. Our goal is to use these observations to
estimate the temperature every day. We’ll simplify this weather task by assuming
there are only two kinds of days: cold (C) and hot (H). So the Eisner task is as
follows:
Given a sequence of observations O (each an integer representing the
number of ice creams eaten on a given day) ﬁnd the ‘hidden’ sequence
Q of weather states (H or C) which caused Jason to eat the ice cream.
Figure A.2 shows a sample HMM for the ice cream task. The two hidden states
(H and C) correspond to hot and cold weather, and the observations (drawn from the
alphabet O = {1, 2, 3}) correspond to the number of ice creams eaten by Jason on a
given day.

Figure A.2 A hidden Markov model for relating numbers of ice creams eaten by Jason (the
observations) to the weather (H or C, the hidden variables).

An inﬂuential tutorial by Rabiner (1989), based on tutorials by Jack Ferguson in
the 1960s, introduced the idea that hidden Markov models should be characterized

by three fundamental problems:

Problem 2 (Decoding):

Problem 1 (Likelihood): Given an HMM λ = (A, B) and an observation se-
quence O, determine the likelihood P(O|λ ).
Given an observation sequence O and an HMM λ =
(A, B), discover the best hidden state sequence Q.
Given an observation sequence O and the set of states
in the HMM, learn the HMM parameters A and B.

Problem 3 (Learning):

We already saw an example of Problem 2 in Chapter 8. In the next two sections
we introduce the Forward and Forward-Backward algorithms to solve Problems 1
and 3 and give more information on Problem 2

A .3

• L IK EL IHOOD COM PU TAT ION : TH E FORWARD A LGOR I THM 467

A.3 Likelihood Computation: The Forward Algorithm

Our ﬁrst problem is to compute the likelihood of a particular observation sequence.
For example, given the ice-cream eating HMM in Fig. A.2, what is the probability
of the sequence 3 1 3? More formally:
Computing Likelihood: Given an HMM λ = (A, B) and an observa-
tion sequence O, determine the likelihood P(O|λ ).
For a Markov chain, where the surface observations are the same as the hidden
events, we could compute the probability of 3 1 3 just by following the states labeled
3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model,
things are not so simple. We want to determine the probability of an ice-cream
observation sequence like 3 1 3, but we don’t know what the hidden state sequence
is!

Let’s start with a slightly simpler situation. Suppose we already knew the weather
and wanted to predict how much ice cream Jason would eat. This is a useful part
of many HMM tasks. For a given hidden state sequence (e.g., hot hot cold), we can
easily compute the output likelihood of 3 1 3.
Let’s see how. First, recall that for hidden Markov models, each hidden state
produces only a single observation. Thus, the sequence of hidden states and the
sequence of observations have the same length. 1
Given this one-to-one mapping and the Markov assumptions expressed in Eq. A.4,
for a particular hidden state sequence Q = q0 , q1 , q2 , ..., qT and an observation se-
quence O = o1 , o2 , ..., oT , the likelihood of the observation sequence is

P(O|Q) =

T(cid:89)i=1

P(oi |qi )

(A.6)

The computation of the forward probability for our ice-cream observation 3 1 3 from
one possible hidden state sequence hot hot cold is shown in Eq. A.7. Figure A.3
shows a graphic representation of this computation.

P(3 1 3|hot hot cold) = P(3|hot) × P(1|hot) × P(3|cold)

(A.7)

Figure A.3 The computation of the observation likelihood for the ice-cream events 3 1 3
given the hidden state sequence hot hot cold.

But of course, we don’t actually know what the hidden state (weather) sequence
was. We’ll need to compute the probability of ice-cream events 3 1 3 instead by

1

In a variant of HMMs called segmental HMMs (in speech recognition) or semi-HMMs (in text pro-
cessing) this one-to-one mapping between the length of the hidden state sequence and the length of the
observation sequence does not hold.

468 A P PEND IX A • H IDDEN MARKOV MODE L S

summing over all possible weather sequences, weighted by their probability. First,
let’s compute the joint probability of being in a particular weather sequence Q and
generating a particular sequence O of ice-cream events. In general, this is

P(O, Q) = P(O|Q) × P(Q) =

T(cid:89)i=1

P(oi |qi ) ×

T(cid:89)i=1

P(qi |qi−1 )

(A.8)

The computation of the joint probability of our ice-cream observation 3 1 3 and one
possible hidden state sequence hot hot cold is shown in Eq. A.9. Figure A.4 shows
a graphic representation of this computation.

P(3 1 3, hot hot cold) = P(hot|start) × P(hot|hot) × P(cold|hot)
×P(3|hot) × P(1|hot) × P(3|cold)

(A.9)

Figure A.4 The computation of the joint probability of the ice-cream events 3 1 3 and the
hidden state sequence hot hot cold.

Now that we know how to compute the joint probability of the observations
with a particular hidden state sequence, we can compute the total probability of the
observations just by summing over all possible hidden state sequences:

P(O) = (cid:88)Q

P(O, Q) = (cid:88)Q

P(O|Q)P(Q)

(A.10)

For our particular case, we would sum over the eight 3-event sequences cold cold
cold, cold cold hot, that is,

P(3 1 3) = P(3 1 3, cold cold cold) + P(3 1 3, cold cold hot) + P(3 1 3, hot hot cold) + ...

forward
algorithm

For an HMM with N hidden states and an observation sequence of T observa-
tions, there are N T possible hidden sequences. For real tasks, where N and T are
both large, N T is a very large number, so we cannot compute the total observation
likelihood by computing a separate observation likelihood for each hidden state se-
quence and then summing them.
Instead of using such an extremely exponential algorithm, we use an efﬁcient
O(N 2T ) algorithm called the forward algorithm. The forward algorithm is a kind
of dynamic programming algorithm, that is, an algorithm that uses a table to store
intermediate values as it builds up the probability of the observation sequence. The
forward algorithm computes the observation probability by summing over the prob-
abilities of all possible hidden state paths that could generate the observation se-
quence, but it does so efﬁciently by implicitly folding each of these paths into a

single forward trellis.

Figure A.5 shows an example of the forward trellis for computing the likelihood
of 3 1 3 given the hidden state sequence hot hot cold.

A .3

• L IK EL IHOOD COM PU TAT ION : TH E FORWARD A LGOR I THM 469

two time steps. The computation in each cell follows Eq. A.12: αt ( j) = (cid:80)N
Figure A.5 The forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3.
Hidden states are in circles, observations in squares. The ﬁgure shows the computation of αt ( j) for two states at
i=1 αt−1 (i)ai j b j (ot ). The resulting
probability expressed in each cell is Eq. A.11: αt ( j) = P(o1 , o2 . . . ot , qt = j|λ ).

Each cell of the forward algorithm trellis αt ( j) represents the probability of be-
ing in state j after seeing the ﬁrst t observations, given the automaton λ . The value
of each cell αt ( j) is computed by summing over the probabilities of every path that
could lead us to this cell. Formally, each cell expresses the following probability:

αt ( j) = P(o1 , o2 . . . ot , qt = j|λ )

(A.11)

Here, qt = j means “the t th state in the sequence of states is state j”. We compute
this probability αt ( j) by summing over the extensions of all the paths that lead to
the current cell. For a given state q j at time t , the value αt ( j) is computed as

αt ( j) =

αt−1 (i)ai j b j (ot )

(A.12)

N(cid:88)i=1

The three factors that are multiplied in Eq. A.12 in extending the previous paths
to compute the forward probability at time t are

αt−1 (i)

ai j

b j (ot )

the previous forward path probability from the previous time step

the transition probability from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

Consider the computation in Fig. A.5 of α2 (2), the forward probability of being
at time step 2 in state 2 having generated the partial observation 3 1. We compute by
extending the α probabilities from time step 1, via two paths, each extension con-
sisting of the three factors above: α1 (1) × P(H |C) × P(1|H ) and α1 (2) × P(H |H ) ×
P(1|H ).
Figure A.6 shows another visualization of this induction step for computing the
value in one new cell of the trellis.
We give two formal deﬁnitions of the forward algorithm:
Fig. A.7 and a statement of the deﬁnitional recursion here.

the pseudocode in

470 A P PEND IX A • H IDDEN MARKOV MODE L S

Figure A.6 Visualizing the computation of a single element αt (i) in the trellis by summing
all the previous values αt−1 , weighted by their transition probabilities a, and multiplying by
the observation probability bi (ot+1 ). For many applications of HMMs, many of the transition
probabilities are 0, so not all previous states will contribute to the forward probability of the
current state. Hidden states are in circles, observations in squares. Shaded nodes are included
in the probability computation for αt (i).

function FORWARD(observations of len T, state-graph of len N) returns forward-prob

create a probability matrix forward[N,T]
for each state s from 1 to N do
forward[s,1] ← πs ∗ bs (o1 )
for each time step t from 2 to T do
for each state s from 1 to N do
forward[s, t ] ←

N(cid:88)

s(cid:48) =1

N(cid:88)

forwardprob ←
return forwardprob

s=1

; initialization step

; recursion step

forward[s(cid:48) , t − 1] ∗ as(cid:48) ,s ∗ bs (ot )

forward[s, T ]

; termination step

Figure A.7 The forward algorithm, where forward[s, t ] represents αt (s).

1. Initialization:

2. Recursion:

α1 ( j) = π j b j (o1 ) 1 ≤ j ≤ N

αt ( j) =

N(cid:88)i=1

αt−1 (i)ai j b j (ot ); 1 ≤ j ≤ N , 1 < t ≤ T

3. Termination:

P(O|λ ) =

αT (i)

N(cid:88)i=1

A.4 Decoding: The Viterbi Algorithm

A .4

• D ECOD ING : TH E V I TERB I A LGOR I THM 471

Decoding
Decoder

Viterbi
algorithm

For any model, such as an HMM, that contains hidden variables, the task of deter-
mining which sequence of variables is the underlying source of some sequence of
observations is called the decoding task. In the ice-cream domain, given a sequence
of ice-cream observations 3 1 3 and an HMM, the task of the decoder is to ﬁnd the
best hidden weather sequence (H H H). More formally,
Decoding: Given as input an HMM λ = (A, B) and a sequence of ob-
servations O = o1 , o2 , ..., oT , ﬁnd the most probable sequence of states

Q = q1q2 q3 . . . qT .

We might propose to ﬁnd the best sequence as follows: For each possible hid-
den state sequence (HHH, HHC, HCH, etc.), we could run the forward algorithm
and compute the likelihood of the observation sequence given that hidden state se-
quence. Then we could choose the hidden state sequence with the maximum obser-
vation likelihood. It should be clear from the previous section that we cannot do this
because there are an exponentially large number of state sequences.
Instead, the most common decoding algorithms for HMMs is the Viterbi algo-
rithm. Like the forward algorithm, Viterbi is a kind of dynamic programming
that makes uses of a dynamic programming trellis. Viterbi also strongly resembles
another dynamic programming variant, the minimum edit distance algorithm of
Chapter 2.

Figure A.8 The Viterbi trellis for computing the best path through the hidden state space for the ice-cream
eating events 3 1 3. Hidden states are in circles, observations in squares. White (unﬁlled) circles indicate illegal
transitions. The ﬁgure shows the computation of vt ( j) for two states at two time steps. The computation in each
cell follows Eq. A.14: vt ( j) = max1≤i≤N−1 vt−1 (i) ai j b j (ot ). The resulting probability expressed in each cell

is Eq. A.13: vt ( j) = P(q0 , q1 , . . . , qt−1 , o1 , o2 , . . . , ot , qt = j|λ ).

Figure A.8 shows an example of the Viterbi trellis for computing the best hidden
state sequence for the observation sequence 3 1 3. The idea is to process the ob-
servation sequence left to right, ﬁlling out the trellis. Each cell of the trellis, vt ( j),
represents the probability that the HMM is in state j after seeing the ﬁrst t obser-
vations and passing through the most probable state sequence q1 , ..., qt−1 , given the

472 A P PEND IX A • H IDDEN MARKOV MODE L S

automaton λ . The value of each cell vt ( j) is computed by recursively taking the
most probable path that could lead us to this cell. Formally, each cell expresses the
probability

vt ( j) = max

q1 ,...,qt−1

P(q1 ...qt−1 , o1 , o2 . . . ot , qt = j|λ )

(A.13)

Note that we represent the most probable path by taking the maximum over all
possible previous state sequences max
. Like other dynamic programming algo-
rithms, Viterbi ﬁlls each cell recursively. Given that we had already computed the
probability of being in every state at time t − 1, we compute the Viterbi probability
by taking the most probable of the extensions of the paths that lead to the current
cell. For a given state q j at time t , the value vt ( j) is computed as

q1 ,...,qt−1

vt ( j) =

Nmax

i=1

vt−1 (i) ai j b j (ot )

(A.14)

The three factors that are multiplied in Eq. A.14 for extending the previous paths to
compute the Viterbi probability at time t are

vt−1 (i)

ai j

b j (ot )

the previous Viterbi path probability from the previous time step

the transition probability from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

function V I TERB I(observations of len T,state-graph of len N) returns best-path, path-prob

; recursion step

; initialization step

create a path probability matrix viterbi[N,T]
for each state s from 1 to N do
viterbi[s,1] ← πs ∗ bs (o1 )
backpointer[s,1] ← 0
for each time step t from 2 to T do
for each state s from 1 to N do
viterbi[s,t] ←
Nmax
viterbi[s(cid:48) , t − 1] ∗ as(cid:48) ,s ∗ bs (ot )
backpointer[s,t] ←
Nargmax
viterbi[s(cid:48) , t − 1] ∗ as(cid:48) ,s ∗ bs (ot )
bestpathprob ←
Nmax
viterbi[s, T ]
; termination step
Nargmax
bestpathpointer ←
viterbi[s, T ]
; termination step
bestpath ← the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob

s=1

s=1

s(cid:48) =1

s(cid:48) =1

Figure A.9 Viterbi algorithm for ﬁnding optimal sequence of hidden states. Given an observation sequence
and an HMM λ = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood
to the observation sequence.

Figure A.9 shows pseudocode for the Viterbi algorithm. Note that the Viterbi
algorithm is identical to the forward algorithm except that it takes the max over the
previous path probabilities whereas the forward algorithm takes the sum. Note also
that the Viterbi algorithm has one component that the forward algorithm doesn’t

A .5

• HMM TRA IN ING : TH E FORWARD -BACKWARD A LGOR I THM 473

have: backpointers. The reason is that while the forward algorithm needs to pro-
duce an observation likelihood, the Viterbi algorithm must produce a probability and
also the most likely state sequence. We compute this best state sequence by keeping
track of the path of hidden states that led to each state, as suggested in Fig. A.10, and
then at the end backtracing the best path to the beginning (the Viterbi backtrace).

Viterbi
backtrace

Figure A.10 The Viterbi backtrace. As we extend each path to a new state account for the next observation,
we keep a backpointer (shown with broken lines) to the best path that led us to this state.

Finally, we can give a formal deﬁnition of the Viterbi recursion as follows:

1. Initialization:

2. Recursion

vt ( j) =

btt ( j) =

3. Termination:

v1 ( j) = π j b j (o1 )

bt1 ( j) = 0

1 ≤ j ≤ N
1 ≤ j ≤ N

i=1

Nmax
vt−1 (i) ai j b j (ot ); 1 ≤ j ≤ N , 1 < t ≤ T
Nargmax
vt−1 (i) ai j b j (ot ); 1 ≤ j ≤ N , 1 < t ≤ T

i=1

The best score: P∗ =
The start of backtrace: qT ∗ =

i=1

Nmax
Nargmax

i=1

vT (i)

vT (i)

A.5 HMM Training: The Forward-Backward Algorithm

We turn to the third problem for HMMs: learning the parameters of an HMM, that
is, the A and B matrices. Formally,
Learning: Given an observation sequence O and the set of possible
states in the HMM, learn the HMM parameters A and B.

474 A P PEND IX A • H IDDEN MARKOV MODE L S

The input to such a learning algorithm would be an unlabeled sequence of ob-
servations O and a vocabulary of potential hidden states Q. Thus, for the ice cream
task, we would start with a sequence of observations O = {1, 3, 2, ..., } and the set of
hidden states H and C.
The standard algorithm for HMM training is the forward-backward, or Baum-
Welch algorithm (Baum, 1972), a special case of the Expectation-Maximization
or EM algorithm (Dempster et al., 1977). The algorithm will let us train both the
transition probabilities A and the emission probabilities B of the HMM. EM is an
iterative algorithm, computing an initial estimate for the probabilities, then using
those estimates to computing a better estimate, and so on, iteratively improving the
probabilities that it learns.
Let us begin by considering the much simpler case of training a fully visible
Markov model, we’re know both the temperature and the ice cream count for every
day. That is, imagine we see the following set of input observations and magically
knew the aligned hidden state sequences:

3
3
2
hot hot cold

1
1
2
cold cold cold

1
2
3
cold hot hot

This would easily allow us to compute the HMM parameters just by maximum
likelihood estimation from the training data. First, we can compute π from the count
of the 3 initial hidden states:

πh = 1/3

πc = 2/3

Next we can directly compute the A matrix from the transitions, ignoring the ﬁnal
hidden states:

p(hot |hot ) = 2/3
p(col d |col d ) = 1/2

p(col d |hot ) = 1/3
p(hot |col d ) = 1/2

and the B matrix:

P(1|hot ) = 0/4 = 0
p(1|col d ) = 3/5 = .6
P(2|hot ) = 1/4 = .25
p(2|col d = 2/5 = .4
P(3|hot ) = 3/4 = .75
p(3|col d ) = 0
For a real HMM, we cannot compute these counts directly from an observation
sequence since we don’t know which path of states was taken through the machine
for a given input. For example, suppose I didn’t tell you the temperature on day 2,
and you had to guess it, but you (magically) had the above probabilities, and the
temperatures on the other days. You could do some Bayesian arithmetic with all the
other probabilities to get estimates of the likely temperature on that missing day, and
use those to get expected counts for the temperatures for day 2.
But the real problem is even harder: we don’t know the counts of being in any
of the hidden states!! The Baum-Welch algorithm solves this by iteratively esti-
mating the counts. We will start with an estimate for the transition and observation
probabilities and then use these estimated probabilities to derive better and better
probabilities. And we’re going to do this by computing the forward probability for
an observation and then dividing that probability mass among all the different paths
that contributed to this forward probability.
To understand the algorithm, we need to deﬁne a useful probability related to the
forward probability and called the backward probability. The backward probabil-

Forward-
backward
Baum-Welch
EM

backward
probability

A .5

• HMM TRA IN ING : TH E FORWARD -BACKWARD A LGOR I THM 475

ity β is the probability of seeing the observations from time t + 1 to the end, given
that we are in state i at time t (and given the automaton λ ):

βt (i) = P(ot+1 , ot+2 . . . oT |qt = i, λ )

It is computed inductively in a similar manner to the forward algorithm.

(A.15)

1. Initialization:

2. Recursion

βt (i) =

N(cid:88)j=1

3. Termination:

βT (i) = 1, 1 ≤ i ≤ N

ai j b j (ot+1 ) βt+1 ( j), 1 ≤ i ≤ N , 1 ≤ t < T

P(O|λ ) =

N(cid:88)j=1

π j b j (o1 ) β1 ( j)

Figure A.11 illustrates the backward induction step.

Figure A.11 The computation of βt (i) by summing all the successive values βt+1 ( j)
weighted by their transition probabilities ai j and their observation probabilities b j (ot+1 ). Start
and end states not shown.

We are now ready to see how the forward and backward probabilities can help
compute the transition probability ai j and observation probability bi (ot ) from an ob-
servation sequence, even though the actual path taken through the model is hidden.
Let’s begin by seeing how to estimate ˆai j by a variant of simple maximum like-
lihood estimation:

ˆai j =

expected number of transitions from state i to state j
expected number of transitions from state i

(A.16)

How do we compute the numerator? Here’s the intuition. Assume we had some
estimate of the probability that a given transition i → j was taken at a particular
point in time t in the observation sequence. If we knew this probability for each

476 A P PEND IX A • H IDDEN MARKOV MODE L S

particular time t , we could sum over all times t to estimate the total count for the
transition i → j.
More formally, let’s deﬁne the probability ξt as the probability of being in state
i at time t and state j at time t + 1, given the observation sequence and of course the
model:

ξt (i, j) = P(qt = i, qt+1 = j|O, λ )

(A.17)

To compute ξt , we ﬁrst compute a probability which is similar to ξt , but differs in
including the probability of the observation; note the different conditioning of O
from Eq. A.17:

not-quite-ξt (i, j) = P(qt = i, qt+1 = j, O|λ )

(A.18)

Figure A.12 Computation of the joint probability of being in state i at time t and state j at
time t + 1. The ﬁgure shows the various probabilities that need to be combined to produce
the α and β probabilities, the transition probability ai j and the
observation probability b j (ot+1 ). After Rabiner (1989) which is c(cid:13)1989 IEEE.

P(qt = i, qt+1 = j, O|λ ):

Figure A.12 shows the various probabilities that go into computing not-quite-ξt :
the transition probability for the arc in question, the α probability before the arc, the
β probability after the arc, and the observation probability for the symbol just after
the arc. These four are multiplied together to produce not-quite-ξt as follows:
(A.19)
To compute ξt from not-quite-ξt , we follow the laws of probability and divide by
P(O|λ ), since

not-quite-ξt (i, j) = αt (i) ai j b j (ot+1 )βt+1 ( j)

P(X , Y |Z )
P(X |Y , Z ) =
P(Y |Z )
The probability of the observation given the model is simply the forward proba-
bility of the whole utterance (or alternatively, the backward probability of the whole
utterance):

(A.20)

P(O|λ ) =

N(cid:88)j=1

αt ( j)βt ( j)

(A.21)

A .5

• HMM TRA IN ING : TH E FORWARD -BACKWARD A LGOR I THM 477

So, the ﬁnal equation for ξt is

ξt (i, j) =

αt (i) ai j b j (ot+1 )βt+1 ( j)
j=1 αt ( j)βt ( j)

(cid:80)N

The expected number of transitions from state i to state j is then the sum over all
t of ξ . For our estimate of ai j in Eq. A.16, we just need one more thing: the total
expected number of transitions from state i. We can get this by summing over all
transitions out of state i. Here’s the ﬁnal formula for ˆai j :

(A.22)

(A.23)

ˆai j = (cid:80)T −1
t=1 ξt (i, j)
k=1 ξt (i, k)

(cid:80)T −1

t=1 (cid:80)N

We also need a formula for recomputing the observation probability. This is the
probability of a given symbol vk from the observation vocabulary V , given a state j:
ˆb j (vk ). We will do this by trying to compute

ˆb j (vk ) =

expected number of times in state j and observing symbol vk
expected number of times in state j
For this, we will need to know the probability of being in state j at time t , which
we will call γt ( j):

(A.24)

(A.25)
Once again, we will compute this by including the observation sequence in the
probability:

γt ( j) = P(qt = j|O, λ )

γt ( j) =

P(qt = j, O|λ )
P(O|λ )

(A.26)

Figure A.13 The computation of γt ( j), the probability of being in state j at time t . Note
that γ is really a degenerate case of ξ and hence this ﬁgure is like a version of Fig. A.12 with
state i collapsed with state j . After Rabiner (1989) which is c(cid:13)1989 IEEE.

As Fig. A.13 shows, the numerator of Eq. A.26 is just the product of the forward
probability and the backward probability:

γt ( j) =

αt ( j)βt ( j)

P(O|λ )

(A.27)

478 A P PEND IX A • H IDDEN MARKOV MODE L S

We are ready to compute b. For the numerator, we sum γt ( j) for all time steps
t in which the observation ot is the symbol vk that we are interested in. For the
denominator, we sum γt ( j) over all time steps t . The result is the percentage of the
times that we were in state j and saw symbol vk (the notation (cid:80)T
“sum over all t for which the observation at time t was vk ”):

t=1 s.t .Ot =vk means

ˆb j (vk ) = (cid:80)T

t=1 s.t .Ot =vk

γt ( j)
t=1 γt ( j)

(cid:80)T

(A.28)

We now have ways in Eq. A.23 and Eq. A.28 to re-estimate the transition A and ob-
servation B probabilities from an observation sequence O, assuming that we already
have a previous estimate of A and B.
These re-estimations form the core of the iterative forward-backward algorithm.
The forward-backward algorithm (Fig. A.14) starts with some initial estimate of the
HMM parameters λ = (A, B). We then iteratively run two steps. Like other cases of
the EM (expectation-maximization) algorithm, the forward-backward algorithm has

two steps: the expectation step, or E-step, and the maximization step, or M-step.

In the E-step, we compute the expected state occupancy count γ and the expected
state transition count ξ from the earlier A and B probabilities. In the M-step, we use
γ and ξ to recompute new A and B probabilities.

E-step
M-step

function FORWARD -BACKWARD(observations of len T, output vocabulary V, hidden
state set Q) returns HMM=(A,B)

initialize A and B

iterate until convergence

E-step

γt ( j) =

ξt (i, j) =

αt ( j)βt ( j)

αT (qF ) ∀ t and j

αt (i) ai j b j (ot+1 )βt+1 ( j)
αT (qF )

∀ t , i, and j

M-step

ˆai j =

T −1(cid:88)
T −1(cid:88)

t=1

t=1

ξt (i, j)

ξt (i, k)

k=1

N(cid:88)
T(cid:88)
T(cid:88)

γt ( j)

γt ( j)

ˆb j (vk ) =

t=1s.t . Ot =vk

return A, B

t=1

Figure A.14 The forward-backward algorithm.

Although in principle the forward-backward algorithm can do completely unsu-
pervised learning of the A and B parameters, in practice the initial conditions are
very important. For this reason the algorithm is often given extra information. For
example, for HMM-based speech recognition, the HMM structure is often set by
hand, and only the emission (B) and (non-zero) A transition probabilities are trained
from a set of observation sequences O.

A.6 Summary

A .6

• SUMMARY

479

This chapter introduced the hidden Markov model for probabilistic sequence clas-

siﬁcation.

vations to a sequence of hidden classes or hidden states that explain the

• Hidden Markov models (HMMs) are a way of relating a sequence of obser-
observations.
• The process of discovering the sequence of hidden states, given the sequence
of observations, is known as decoding or inference. The Viterbi algorithm is
commonly used for decoding.
• The parameters of an HMM are the A transition probability matrix and the B
observation likelihood matrix. Both can be trained with the Baum-Welch or

forward-backward algorithm.

Bibliographical and Historical Notes

As we discussed in Chapter 8, Markov chains were ﬁrst used by Markov (1913)
(translation Markov 2006), to predict whether an upcoming letter in Pushkin’s Eu-
gene Onegin would be a vowel or a consonant. The hidden Markov model was de-
veloped by Baum and colleagues at the Institute for Defense Analyses in Princeton
(Baum and Petrie 1966, Baum and Eagon 1967).
The Viterbi algorithm was ﬁrst applied to speech and language processing in the
context of speech recognition by Vintsyuk (1968) but has what Kruskal (1983) calls
a “remarkable history of multiple independent discovery and publication”. Kruskal
and others give at least the following independently-discovered variants of the algo-
rithm published in four separate ﬁelds:

Citation

Field

Viterbi (1967)
information theory
Vintsyuk (1968)
speech processing
Needleman and Wunsch (1970) molecular biology
Sakoe and Chiba (1971)
speech processing
Sankoff (1972)
molecular biology
Reichert et al. (1973)
molecular biology
Wagner and Fischer (1974)
computer science

The use of the term Viterbi is now standard for the application of dynamic pro-
gramming to any kind of probabilistic maximization problem in speech and language
processing. For non-probabilistic problems (such as for minimum edit distance), the
plain term dynamic programming is often used. Forney, Jr. (1973) wrote an early
survey paper that explores the origin of the Viterbi algorithm in the context of infor-
mation and communications theory.
Our presentation of the idea that hidden Markov models should be characterized
by three fundamental problems was modeled after an inﬂuential tutorial by Rabiner
(1989), which was itself based on tutorials by Jack Ferguson of IDA in the 1960s.
Jelinek (1997) and Rabiner and Juang (1993) give very complete descriptions of the
forward-backward algorithm as applied to the speech recognition problem. Jelinek
(1997) also shows the relationship between forward-backward and EM.

480 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

CHAPTER

B Spelling Correction and the
Noisy Channel

A LG ERNON: But my own sweet Cecily, I have never written you any letters.
C EC I LY: You need hardly remind me of that, Ernest. I remember only too well
that I was forced to write your letters for you. I wrote always three times a week,
and sometimes oftener.
A LG ERNON: Oh, do let me read them, Cecily?
C EC I LY: Oh, I couldn’t possibly. They would make you far too conceited. The
three you wrote me after I had broken off the engagement are so beautiful, and
so badly spelled, that even now I can hardly read them without crying a little.
Oscar Wilde, The Importance of Being Earnest

Like Oscar Wilde’s fabulous Cecily, a lot of people were thinking about spelling
during the last turn of the century. Gilbert and Sullivan provide many examples. The
Gondoliers’ Giuseppe, for example, worries that his private secretary is “shaky in his
spelling”, while Iolanthe’s Phyllis can “spell every word that she uses”. Thorstein
Veblen’s explanation (in his 1899 classic The Theory of the Leisure Class) was that
a main purpose of the “archaic, cumbrous, and ineffective” English spelling system
was to be difﬁcult enough to provide a test of membership in the leisure class.
Whatever the social role of spelling, we can certainly agree that many more of
us are like Cecily than like Phyllis. Estimates for the frequency of spelling errors
in human-typed text vary from 1-2% for carefully retyping already printed text to
10-15% for web queries.
In this chapter we introduce the problem of detecting and correcting spelling
errors. Fixing spelling errors is an integral part of writing in the modern world,
whether this writing is part of texting on a phone, sending email, writing longer
documents, or ﬁnding information on the web. Modern spell correctors aren’t perfect
(indeed, autocorrect-gone-wrong is a popular source of amusement on the web) but
they are ubiquitous in pretty much any software that relies on keyboard input.
Spelling correction is often considered from two perspectives. Non-word spelling
correction is the detection and correction of spelling errors that result in non-words
(like graffe for giraffe). By contrast, real word spelling correction is the task of
detecting and correcting spelling errors even if they accidentally result in an actual
word of English (real-word errors). This can happen from typographical errors
(insertion, deletion, transposition) that accidentally produce a real word (e.g., there
for three), or cognitive errors where the writer substituted the wrong spelling of a
homophone or near-homophone (e.g., dessert for desert, or piece for peace).
Non-word errors are detected by looking for any word not found in a dictio-
nary. For example, the misspelling graffe above would not occur in a dictionary.
The larger the dictionary the better; modern systems often use enormous dictio-

real-word
errors

candidates

B .1

• TH E NO I SY CHANN EL MODE L

481

naries derived from the web. To correct non-word spelling errors we ﬁrst generate
candidates: real words that have a similar letter sequence to the error. Candidate
corrections from the spelling error graffe might include giraffe, graf, gaffe, grail, or
craft. We then rank the candidates using a distance metric between the source and
the surface error. We’d like a metric that shares our intuition that giraffe is a more
likely source than grail for graffe because giraffe is closer in spelling to graffe than
grail is to graffe. The minimum edit distance algorithm from Chapter 2 will play a
role here. But we’d also like to prefer corrections that are more frequent words, or
more likely to occur in the context of the error. The noisy channel model introduced
in the next section offers a way to formalize this intuition.
Real word spelling error detection is a much more difﬁcult task, since any word
in the input text could be an error. Still, it is possible to use the noisy channel to ﬁnd
candidates for each word w typed by the user, and rank the correction that is most
likely to have been the users original intention.

B.1 The Noisy Channel Model

In this section we introduce the noisy channel model and show how to apply it to
the task of detecting and correcting spelling errors. The noisy channel model was
applied to the spelling correction task at about the same time by researchers at AT&T
Bell Laboratories (Kernighan et al. 1990, Church and Gale 1991) and IBM Watson
Research (Mays et al., 1991).

noisy channel

Figure B.1

In the noisy channel model, we imagine that the surface form we see is actually
a “distorted” form of an original word passed through a noisy channel. The decoder passes
each hypothesis through a model of this channel and picks the word that best matches the
surface noisy word.

The intuition of the noisy channel model (see Fig. B.1) is to treat the misspelled
word as if a correctly spelled word had been “distorted” by being passed through a
noisy communication channel.
This channel introduces “noise” in the form of substitutions or other changes to
the letters, making it hard to recognize the “true” word. Our goal, then, is to build a
model of the channel. Given this model, we then ﬁnd the true word by passing every
word of the language through our model of the noisy channel and seeing which one
comes the closest to the misspelled word.

482 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

Bayesian

This noisy channel model is a kind of Bayesian inference. We see an obser-
vation x (a misspelled word) and our job is to ﬁnd the word w that generated this
misspelled word. Out of all possible words in the vocabulary V we want to ﬁnd the
word w such that P(w|x) is highest. We use the hat notation ˆ to mean “our estimate
of the correct word”.

argmax

likelihood
channel model
prior
probability

w∈V

(B.1)

ˆw = argmax

P(w|x)
The function argmaxx f (x) means “the x such that f (x) is maximized”. Equa-
tion B.1 thus means, that out of all words in the vocabulary, we want the particular
word that maximizes the right-hand side P(w|x).
The intuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. B.1
into a set of other probabilities. Bayes’ rule is presented in Eq. B.2; it gives us a way
to break down any conditional probability P(a|b) into three other probabilities:
P(b|a)P(a)
P(a|b) =
(B.2)
P(b)
We can then substitute Eq. B.2 into Eq. B.1 to get Eq. B.3:
P(x|w)P(w)
P(x)

ˆw = argmax

(B.3)

w∈V

We can conveniently simplify Eq. B.3 by dropping the denominator P(x). Why
is that? Since we are choosing a potential correction word out of all words, we will
be computing P(x|w)P(w)
for each word. But P(x) doesn’t change for each word; we
are always asking about the most likely word for the same observed error x, which
must have the same probability P(x). Thus, we can choose the word that maximizes
this simpler formula:

P(x)

w∈V

(B.4)

ˆw = argmax

P(x|w) P(w)
To summarize, the noisy channel model says that we have some true underlying
word w, and we have a noisy channel that modiﬁes the word into some possible
misspelled observed surface form. The likelihood or channel model of the noisy
channel producing any particular observation sequence x is modeled by P(x|w). The
prior probability of a hidden word is modeled by P(w). We can compute the most
probable word ˆw given that we’ve seen some observed misspelling x by multiply-
ing the prior P(w) and the likelihood P(x|w) and choosing the word for which this
product is greatest.
We apply the noisy channel approach to correcting non-word spelling errors by
taking any word not in our spell dictionary, generating a list of candidate words,
ranking them according to Eq. B.4, and picking the highest-ranked one. We can
modify Eq. B.4 to refer to this list of candidate words instead of the full vocabulary
V as follows:

ˆw = argmax

channel model
P(x|w)
The noisy channel algorithm is shown in Fig. B.2.
To see the details of the computation of the likelihood and the prior (language
model), let’s walk through an example, applying the algorithm to the example mis-
spelling acress. The ﬁrst stage of the algorithm proposes candidate corrections by

(cid:122) (cid:125)(cid:124) (cid:123)

(cid:122) (cid:125)(cid:124) (cid:123)P(w)

(B.5)

prior

w∈C

B .1

• TH E NO I SY CHANN EL MODE L

483

function NO I SY CHANN EL S P ELL ING(word x, dict D, lm, editprob) returns correction

if x /∈ D

candidates, edits ← All strings at edit distance 1 from x that are ∈ D, and their edit
for each c, e in candidates, edits
channel ← editprob(e)
prior ← lm(x)
score[c] = log channel + log prior
return argmaxc score[c]

Figure B.2 Noisy channel model for spelling correction for unknown words.

ﬁnding words that have a similar spelling to the input word. Analysis of spelling
error data has shown that the majority of spelling errors consist of a single-letter
change and so we often make the simplifying assumption that these candidates have
an edit distance of 1 from the error word. To ﬁnd this list of candidates we’ll use
the minimum edit distance algorithm introduced in Chapter 2, but extended so that
in addition to insertions, deletions, and substitutions, we’ll add a fourth type of edit,
transpositions, in which two letters are swapped. The version of edit distance with
transposition is called Damerau-Levenshtein edit distance. Applying all such sin-
gle transformations to acress yields the list of candidate words in Fig. B.3.

Damerau-
Levenshtein

Transformation
Error
Position
Letter
(Letter #)

Error

acress
acress
acress
acress
acress
acress
acress

Correction

actress
cress
caress
access
across
acres
acres

Correct
Letter

t

—

ca
c
o

—
—

—

a
ac
r
e
s
s

2
0
0
2
3
5
4

Type

deletion
insertion
transposition
substitution
substitution
insertion
insertion

Figure B.3 Candidate corrections for the misspelling acress and the transformations that
would have produced the error (after Kernighan et al. (1990)). “—” represents a null letter.

Once we have a set of a candidates, to score each one using Eq. B.5 requires that
we compute the prior and the channel model.
The prior probability of each correction P(w) is the language model probability
of the word w in context, which can be computed using any language model, from
unigram to trigram or 4-gram. For this example let’s start in the following table by
assuming a unigram language model. We computed the language model from the
404,253,213 words in the Corpus of Contemporary English (COCA).

w

count(w) p(w)

actress 9,321
.0000231
cress
220
.000000544
caress 686
.00000170
access 37,038
.0000916
across 120,844
.000299
acres
12,874
.0000318
How can we estimate the likelihood P(x|w), also called the channel model or

channel model

484 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

error model

confusion
matrix

error model? A perfect model of the probability that a word will be mistyped would
condition on all sorts of factors: who the typist was, whether the typist was left-
handed or right-handed, and so on. Luckily, we can get a pretty reasonable estimate
of P(x|w) just by looking at local context: the identity of the correct letter itself, the
misspelling, and the surrounding letters. For example, the letters m and n are often
substituted for each other; this is partly a fact about their identity (these two letters
are pronounced similarly and they are next to each other on the keyboard) and partly
a fact about context (because they are pronounced similarly and they occur in similar
contexts).
A simple model might estimate, for example, p(acress|across) just using the
number of times that the letter e was substituted for the letter o in some large corpus
of errors. To compute the probability for each edit in this way we’ll need a confu-
sion matrix that contains counts of errors. In general, a confusion matrix lists the
number of times one thing was confused with another. Thus for example a substi-
tution matrix will be a square matrix of size 26×26 (or more generally |A| × |A|,
for an alphabet A) that represents the number of times one letter was incorrectly
used instead of another. Following Kernighan et al. (1990) we’ll use four confusion
matrices.

del[x, y]: count(xy typed as x)
ins[x, y]: count(x typed as xy)
sub[x, y]: count(x typed as y)
trans[x, y]: count(xy typed as yx)

Note that we’ve conditioned the insertion and deletion probabilities on the previ-
ous character; we could instead have chosen to condition on the following character.
Where do we get these confusion matrices? One way is to extract them from
lists of misspellings like the following:

additional: addional, additonal
environments: enviornments, enviorments, enviroments
preceded: preceeded
...

There are lists available on Wikipedia and from Roger Mitton (http://www.

dcs.bbk.ac.uk/˜ROGER/corpora.html) and Peter Norvig (http://norvig.

com/ngrams/). Norvig also gives the counts for each single-character edit that can
be used to directly create the error model probabilities.
An alternative approach used by Kernighan et al. (1990) is to compute the ma-
trices by iteratively using this very spelling error correction algorithm itself. The
iterative algorithm ﬁrst initializes the matrices with equal values; thus, any character
is equally likely to be deleted, equally likely to be substituted for any other char-
acter, etc. Next, the spelling error correction algorithm is run on a set of spelling
errors. Given the set of typos paired with their predicted corrections, the confusion
matrices can now be recomputed, the spelling algorithm run again, and so on. This
iterative algorithm is an instance of the important EM algorithm (Dempster et al.,
1977), which we discuss in Appendix A.
Once we have the confusion matrices, we can estimate P(x|w) as follows (where

B .1

• TH E NO I SY CHANN EL MODE L

485

, if deletion

wi is the ith character of the correct word w) and xi is the ith character of the typo x:
del[xi−1 , wi ]
count[xi−1wi ]
ins[xi−1 , wi ]
, if insertion
count[wi−1 ]
sub[xi , wi ]
, if substitution
count[wi ]
trans[wi , wi+1 ]
count[wiwi+1 ]

, if transposition

P(x|w) =

(B.6)

Using the counts from Kernighan et al. (1990) results in the error model proba-
bilities for acress shown in Fig. B.4.



Candidate
Correction

actress
cress
caress
access
across
acres
acres

Correct
Letter

Error
Letter

t
-
ca
c
o
-
-

-
a
ac
r
e
s
s

x|w

c|ct
a|#
ac|ca
r|c
e|o
es|e
ss|s

P(x|w)

.000117
.00000144
.00000164
.000000209
.0000093
.0000321
.0000342

Figure B.4 Channel model for acress; the probabilities are taken from the del[], ins[],
sub[], and trans[] confusion matrices as shown in Kernighan et al. (1990).

Figure B.5 shows the ﬁnal probabilities for each of the potential corrections;
the unigram prior is multiplied by the likelihood (computed with Eq. B.6 and the
confusion matrices). The ﬁnal column shows the product, multiplied by 109 just for
readability.

Candidate Correct Error
Correction Letter Letter x|w

P(x|w)

P(w)

109*P(x|w)P(w)

actress
cress
caress
access
across
acres
acres

t
-
ca
c
o
-
-

-
a
ac
r
e
s
s

a|#

c|ct .000117
.0000231
2.7
.00000144
.000000544 0.00078
ac|ca .00000164
.00000170
0.0028
.000000209 .0000916
0.019
.0000093
.000299
2.8
es|e .0000321
.0000318
1.0
ss|s .0000342
.0000318
1.0

r|c
e|o

Figure B.5 Computation of the ranking for each candidate correction, using the language
model shown earlier and the error model from Fig. B.4. The ﬁnal score is multiplied by 109
for readability.

The computations in Fig. B.5 show that our implementation of the noisy channel
model chooses across as the best correction, and actress as the second most
likely word.
Unfortunately, the algorithm was wrong here; the writer’s intention becomes
clear from the context: . . . was called a “stellar and versatile acress whose com-
bination of sass and glamour has deﬁned her. . . ”. The surrounding words make it
clear that actress and not across was the intended word.

486 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

For this reason, it is important to use larger language models than unigrams.
For example, if we use the Corpus of Contemporary American English to compute
bigram probabilities for the words actress and across in their context using add-one
smoothing, we get the following probabilities:
P(actress|versatile) = .000021
P(across|versatile) = .000021
P(whose|actress) = .0010
P(whose|across) = .000006
Multiplying these out gives us the language model estimate for the two candi-
dates in context:
P(“versatile actress whose”) = .000021 ∗ .0010 = 210 × 10−10
P(“versatile across whose”) = .000021 ∗ .000006 = 1 × 10−10
Combining the language model with the error model in Fig. B.5, the bigram
noisy channel model now chooses the correct word actress.
Evaluating spell correction algorithms is generally done by holding out a train-
ing, development and test set from lists of errors like those on the Norvig and Mitton
sites mentioned above.

B.2 Real-word spelling errors

real-word error
detection

The noisy channel approach can also be applied to detect and correct real-word
spelling errors, errors that result in an actual word of English. This can happen from
typographical errors (insertion, deletion, transposition) that accidentally produce a
real word (e.g., there for three) or because the writer substituted the wrong spelling
of a homophone or near-homophone (e.g., dessert for desert, or piece for peace). A
number of studies suggest that between 25% and 40% of spelling errors are valid
English words as in the following examples (Kukich, 1992):
This used to belong to thew queen. They are leaving in about ﬁfteen minuets to go to her house.
The design an construction of the system will take more than a year.
Can they lave him my messages?
The study was conducted mainly be John Black.

The noisy channel can deal with real-word errors as well. Let’s begin with a
version of the noisy channel model ﬁrst proposed by Mays et al. (1991) to deal
with these real-word spelling errors. Their algorithm takes the input sentence X =
{x1 , x2 , . . . , xk , . . . , xn}, generates a large set of candidate correction sentences C(X ),
then picks the sentence with the highest language model probability.
To generate the candidate correction sentences, we start by generating a set of
candidate words for each input word xi . The candidates, C(xi ), include every English
word with a small edit distance from xi . With edit distance 1, a common choice
(Mays et al., 1991), the candidate set for the real word error thew (a rare word
meaning ‘muscular strength’) might be C(thew) = {the, thaw, threw, them, thwe}.
We then make the simplifying assumption that every sentence has only one error.
Thus the set of candidate sentences C(X ) for a sentence X = Only two of thew
apples would be:

487

B .2

• R EAL -WORD S P EL L ING ERROR S
only two of thew apples
oily two of thew apples
only too of thew apples
only to of thew apples
only tao of the apples
only two on thew apples
only two off thew apples
only two of the apples
only two of threw apples
only two of thew applies
only two of thew dapples
...

Each sentence is scored by the noisy channel:

ˆW = argmax

(B.7)

W ∈C(X )

P(X |W ) P(W )
For P(W ), we can use the trigram probability of the sentence.
What about the channel model? Since these are real words, we need to consider
the possibility that the input word is not an error. Let’s say that the channel proba-
bility of writing a word correctly, P(w|w), is α ; we can make different assumptions
about exactly what the value of α is in different tasks; perhaps α is .95, assum-
ing people write 1 word wrong out of 20, for some tasks, or maybe .99 for others.
Mays et al. (1991) proposed a simple model: given a typed word x, let the channel
model P(x|w) be α when x = w, and then just distribute 1 − α evenly over all other
candidate corrections C(x):

(B.8)

α
1 − α

p(x|w) = 

if x = w
if x ∈ C(x)
|C(x)|
0
otherwise
Now we can replace the equal distribution of 1 − α over all corrections in Eq. B.8;
we’ll make the distribution proportional to the edit probability from the more sophis-
ticated channel model from Eq. B.6 that used the confusion matrices.
Let’s see an example of this integrated noisy channel model applied to a real
word. Suppose we see the string two of thew. The author might have intended
to type the real word thew (‘muscular strength’). But thew here could also be a
typo for the or some other word. For the purposes of this example let’s consider
edit distance 1, and only the following ﬁve candidates the, thaw, threw, and thwe
(a rare name) and the string as typed, thew. We took the edit probabilities from
Norvig’s (2009) analysis of this example. For the language model probabilities, we
used a Stupid Backoff model (Section 3.6) trained on the Google N-grams:
P(the|two of)
= 0.476012
P(thew|two of) = 9.95051 ×10−8
P(thaw|two of) = 2.09267 ×10−7
P(threw|two of) = 8.9064 ×10−7
P(them|two of) = 0.00144488
P(thwe|two of) = 5.18681 ×10−9
Here we’ve just computed probabilities for the single phrase two of thew, but
the model applies to entire sentences; so if the example in context was two of thew

488 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

people, we’d need to also multiply in probabilities for P(people|of the), P(people|of
thew), P(people|of threw), and so on.
Following Norvig (2009), we assume that the probability of a word being a typo
in this task is .05, meaning that α = P(w|w) is .95. Fig. B.6 shows the computation.

x|w

P(x|w) P(w|wi−2 , wi−1 ) 108P(x|w)P(w|wi−2 , wi−1 )

x

w

thew the
thew thew
thew thaw
thew threw h|hr
thew thwe

ew|e
0.000007 0.48
α =0.95
9.95 ×10−8
e|a
0.001
2.1 ×10−7
0.000008 8.9 ×10−7
ew|we 0.000003 5.2 ×10−9

333
9.45
0.0209
0.000713
0.00000156

Figure B.6 The noisy channel model on 5 possible candidates for thew, with a Stupid
Backoff trigram language model computed from the Google N-gram corpus and the error
model from Norvig (2009).

For the error phrase two of thew, the model correctly picks the as the correction.
But note that a lower error rate might change things; in a task where the probability
of an error is low enough (α is very high), the model might instead decide that the
word thew was what the writer intended.

B.3 Noisy Channel Model: The State of the Art

State of the art implementations of noisy channel spelling correction make a number
of extensions to the simple models we presented above.
First, rather than make the assumption that the input sentence has only a sin-
gle error, modern systems go through the input one word at a time, using the noisy
channel to make a decision for that word. But if we just run the basic noisy chan-
nel system described above on each word, it is prone to overcorrecting, replacing
correct but rare words (for example names) with more frequent words (Whitelaw
et al. 2009, Wilcox-O’Hearn 2014). Modern algorithms therefore need to augment
the noisy channel with methods for detecting whether or not a real word should ac-
tually be corrected. For example state of the art systems like Google’s (Whitelaw
et al., 2009) use a blacklist, forbidding certain tokens (like numbers, punctuation,
and single letter words) from being changed. Such systems are also more cautious
in deciding whether to trust a candidate correction. Instead of just choosing a candi-
date correction if it has a higher probability P(w|x) than the word itself, these more
careful systems choose to suggest a correction w over keeping the non-correction x
only if the difference in probabilities is sufﬁciently great. The best correction w is
chosen only if:

log P(w|x) − log P(x|x) > θ
Depending on the speciﬁc application, spell-checkers may decide to autocorrect
(automatically change a spelling to a hypothesized correction) or merely to ﬂag the
error and offer suggestions. This decision is often made by another classiﬁer which
decides whether the best candidate is good enough, using features such as the dif-
ference in log probabilities between the candidates (we’ll introduce algorithms for
classiﬁcation in the next chapter).
Modern systems also use much larger dictionaries than early systems. Ahmad
and Kondrak (2005) found that a 100,000 word UNIX dictionary only contained

autocorrect

B .3

• NO I SY CHANN EL MODE L : TH E S TAT E O F TH E ART

489

73% of the word types in their corpus of web queries, missing words like pics,
multiplayer, google, xbox, clipart, and mallorca. For this reason modern systems
often use much larger dictionaries automatically derived from very large lists of
unigrams like the Google N-gram corpus. Whitelaw et al. (2009), for example,
used the most frequently occurring ten million word types in a large sample of web
pages. Because this list will include lots of misspellings, their system requires a
more sophisticated error model. The fact that words are generally more frequent than
their misspellings can be used in candidate suggestion, by building a set of words
and spelling variations that have similar contexts, sorting by frequency, treating the
most frequent variant as the source, and learning an error model from the difference,
whether from web text (Whitelaw et al., 2009) or from query logs (Cucerzan and
Brill, 2004). Words can also be automatically added to the dictionary when a user
rejects a correction, and systems running on phones can automatically add words
from the user’s address book or calendar.
We can also improve the performance of the noisy channel model by changing
how the prior and the likelihood are combined. In the standard model they are just
multiplied together. But often these probabilities are not commensurate; the lan-
guage model or the channel model might have very different ranges. Alternatively
for some task or dataset we might have reason to trust one of the two models more.
Therefore we use a weighted combination, by raising one of the factors to a power

λ :

ˆw = argmax

w∈V

P(x|w) P(w)λ

(B.9)

or in log space:

w∈V

(B.10)

ˆw = argmax

log P(x|w) + λ log P(w)
We then tune the parameter λ on a development test set.
Finally, if our goal is to do real-word spelling correction only for speciﬁc con-
fusion sets like peace/piece, affect/effect, weather/whether, or even grammar cor-
rection examples like among/between, we can train supervised classiﬁers to draw on
many features of the context and make a choice between the two candidates. Such
classiﬁers can achieve very high accuracy for these speciﬁc sets, especially when
drawing on large-scale features from web statistics (Golding and Roth 1999, Lapata
and Keller 2004, Bergsma et al. 2009, Bergsma et al. 2010).

B.3.1

Improved Edit Models: Partitions and Pronunciation

Other recent research has focused on improving the channel model P(t |c). One
important extension is the ability to compute probabilities for multiple-letter trans-
formations. For example Brill and Moore (2000) propose a channel model that (in-
formally) models an error as being generated by a typist ﬁrst choosing a word, then
choosing a partition of the letters of that word, and then typing each partition, pos-
sibly erroneously. For example, imagine a person chooses the word physical,
then chooses the partition ph y s i c al She would then generate each parti-
tion, possible with errors. For example the probability that she would generate the
string fisikle with partition f i s i k le would be p(f|ph) ∗ p(i|y) ∗ p(s|s) ∗
p(i|i) ∗ p(k|k) ∗ p(le|al). Unlike the Damerau-Levenshtein edit distance, the Brill-
Moore channel model can thus model edit probabilities like P(f|ph) or P(le|al), or

confusion sets

490 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

the high likelihood of P(ent|ant). Furthermore, each edit is conditioned on where
it is in the word (beginning, middle, end) so instead of P(f|ph) the model actually
estimates P(f|ph, beginning).
More formally, let R be a partition of the typo string x into adjacent (possibly
empty) substrings, and T be a partition of the candidate string. Brill and Moore
(2000) then approximates the total likelihood P(x|w) (e.g., P(fisikle|physical))
by the probability of the single best partition:

max

|R|(cid:88)i=1

R,T s.t .|T |=|R|

P(x|w) ≈
P(Ti |Ri , position)
The probability of each transform P(Ti |Ri ) can be learned from a training set of
triples of an error, the correct string, and the number of times it occurs. For example
given a training pair akgsual/actual, standard minimum edit distance is used to
produce an alignment:

(B.11)

This alignment corresponds to the sequence of edit operations:
a→a, c→k,
Each nonmatch substitution is then expanded to incorporate up to N additional
edits; For N=2, we would expand c→k to:

 →g t→s, u→u, a→a, l→l

ac→ak
c→cg
ac→akg
ct→kgs

aspell

Each of these multiple edits then gets a fractional count, and the probability for
each edit α → β is then estimated from counts in the training corpus of triples as
count(α→β )
count(α )
.
Another research direction in channel models is the use of pronunciation in ad-
dition to spelling. Pronunciation is an important feature in some non-noisy-channel
algorithms for spell correction like the GNU aspell algorithm (Atkinson, 2011),
which makes use of the metaphone pronunciation of a word (Philips, 1990). Meta-
phone is a series of rules that map a word to a normalized representation of its
pronunciation. Some example rules:
• “Drop duplicate adjacent letters, except for C.”
• “If the word begins with ‘KN’, ‘GN’, ‘PN’, ‘AE’, ‘WR’, drop the ﬁrst letter.”
• “Drop ‘B’ if after ‘M’ and if it is at the end of the word”
Aspell works similarly to the channel component of the noisy channel model, ﬁnding
all words in the dictionary whose pronunciation string is a short edit distance (1 or
2 pronunciation letters) from the typo, and then scoring this list of candidates by
a metric that combines two edit distances: the pronunciation edit distance and the
weighted letter edit distance.
Pronunciation can also be incorporated directly the noisy channel model. For ex-
ample the Toutanova and Moore (2002) model, like aspell, interpolates two channel

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

491

function SOUND EX(name) returns soundex form

1. Keep the ﬁrst letter of name
2. Drop all occurrences of non-initial a, e, h, i, o, u, w, y.
3. Replace the remaining letters with the following numbers:
b, f, p, v → 1
c, g, j, k, q, s, x, z → 2
d, t → 3
l → 4
m, n → 5
r → 6
4. Replace any sequences of identical numbers, only if they derive from two or more
letters that were adjacent in the original name, with a single number (e.g., 666 → 6).
5. Convert to the form Letter Digit Digit Digit by dropping digits past the third
(if necessary) or padding with trailing zeros (if necessary).

Figure B.7 The Soundex Algorithm

models, one based on spelling and one based on pronunciation. The pronunciation
model is based on using letter-to-sound models to translate each input word and
each dictionary word into a sequences of phones representing the pronunciation of
the word. For example actress and aktress would both map to the phone string
ae k t r ix s. See Chapter 26 on the task of letter-to-sound or grapheme-to-

phoneme.

Some additional string distance functions have been proposed for dealing specif-
ically with names. These are mainly used for the task of deduplication (deciding if
two names in a census list or other namelist are the same) rather than spell-checking.
The Soundex algorithm (Knuth 1973, Odell and Russell 1922) is an older method
used originally for census records for representing people’s names. It has the advan-
tage that versions of the names that are slightly misspelled will still have the same
representation as correctly spelled names. (e.g., Jurafsky, Jarofsky, Jarovsky, and
Jarovski all map to J612). The algorithm is shown in Fig. B.7.
Instead of Soundex, more recent work uses Jaro-Winkler distance, which is
an edit distance algorithm designed for names that allows characters to be moved
longer distances in longer names, and also gives a higher similarity to strings that
have identical initial characters (Winkler, 2006).

letter-to-sound
phones

deduplication

Jaro-Winkler

Bibliographical and Historical Notes

Algorithms for spelling error detection and correction have existed since at least
Blair (1960). Most early algorithms were based on similarity keys like the Soundex
algorithm (Odell and Russell 1922, Knuth 1973). Damerau (1964) gave a dictionary-
based algorithm for error detection; most error-detection algorithms since then have
been based on dictionaries. Early research (Peterson, 1986) had suggested that
spelling dictionaries might need to be kept small because large dictionaries con-
tain very rare words (wont, veery) that resemble misspellings of other words, but
Damerau and Mays (1989) found that in practice larger dictionaries proved more
helpful. Damerau (1964) also gave a correction algorithm that worked for single
errors.
The idea of modeling language transmission as a Markov source passed through

492 A P PEND IX B • S P ELL ING CORR ECT ION AND TH E NO I SY CHANN EL

a noisy channel model was developed very early on by Claude Shannon (1948).
The idea of combining a prior and a likelihood to deal with the noisy channel was
developed at IBM Research by Raviv (1967), for the similar task of optical char-
acter recognition (OCR). While earlier spell-checkers like Kashyap and Oommen
(1983) had used likelihood-based models of edit distance, the idea of combining a
prior and a likelihood seems not to have been applied to the spelling correction task
until researchers at AT&T Bell Laboratories (Kernighan et al. 1990, Church and
Gale 1991) and IBM Watson Research (Mays et al., 1991) roughly simultaneously
proposed noisy channel spelling correction. Much later, the Mays et al. (1991) algo-
rithm was reimplemented and tested on standard datasets by Wilcox-O’Hearn et al.
(2008), who showed its high performance.
Most algorithms since Wagner and Fischer (1974) have relied on dynamic pro-
gramming.
Recent focus has been on using the web both for language models and for train-
ing the error model, and on incorporating additional features in spelling, like the
pronunciation models described earlier, or other information like parses or semantic
relatedness (Jones and Martin 1997, Hirst and Budanitsky 2005).
See Mitton (1987) for a survey of human spelling errors, and Kukich (1992)
for an early survey of spelling error detection and correction. Norvig (2007) gives
a nice explanation and a Python implementation of the noisy channel model, with
more details and an efﬁcient algorithm presented in Norvig (2009).

Exercises

B.1 Suppose we want to apply add-one smoothing to the likelihood term (channel
model) P(x|w) of a noisy channel model of spelling. For simplicity, pretend
that the only possible operation is deletion. The MLE estimate for deletion
is given in Eq. B.6, which is P(x|w) = del[xi `1,wi ]
count(xi`1 wi ) . What is the estimate for
P(x|w) if we use add-one smoothing on the deletion edit model? Assume the
only characters we use are lower case a-z, that there are V word types in our
corpus, and N total characters, not counting spaces.

CHAPTER

C WordNet: Word Relations,
Senses, and Disambiguation

In this chapter we introduce computation with a thesaurus: a structured list of words
organized by meaning. The most popular thesaurus for computational purposes is
WordNet, a large online resource with versions in many languages. One use of
WordNet is to represent word senses, the many different meanings that a single
lemma can have (Chapter 6) Thus the lemma bank can refer to a ﬁnancial institution
or to the sloping side of a river. WordNet also represents relations between senses,
like the IS-A relation between dog and mammal or the part-whole relationship be-
tween car and engine. Finally, WordNet includes glosses, a deﬁnition for senses in
the form of a text string.
We’ll see how to use each of these aspects of WordNet to address the task of
computing word similarity; the similarity in meaning of two different words, an
alternative to the embedding-based methods we introduced in Chapter 6. And we’ll
introduce word sense disambiguation, the task of determining which sense of a
word is being used in a particular context, a task with a long history in computational
linguistics and applications from machine translation to question answering. We
give a number of algorithms for using features from the context for deciding which
sense was intended in a particular context.

glosses

word sense
disambiguation

C.1 Word Senses

Consider the two uses of the lemma bank mentioned above, meaning something like
“ﬁnancial institution” and “sloping mound”, respectively:
(C.1) Instead, a bank can hold the investments in a custodial account in the client’s
name.
(C.2) But as agriculture burgeons on the east bank, the river will shrink even more.
We represent this variation in usage by saying that the lemma bank has two
senses.1 A sense (or word sense) is a discrete representation of one aspect of the
meaning of a word. Loosely following lexicographic tradition, we represent each
sense by placing a superscript on the lemma as in bank1 and bank2 .
The senses of a word might not have any particular relation between them; it
may be almost coincidental that they share an orthographic form. For example, the
ﬁnancial institution and sloping mound senses of bank seem relatively unrelated.
In such cases we say that the two senses are homonyms, and the relation between
the senses is one of homonymy. Thus bank1 (“ﬁnancial institution”) and bank2
(“sloping mound”) are homonyms, as are the sense of bat meaning ‘club for hitting
a ball’ and the one meaning ‘nocturnal ﬂying animal’. We say that these two uses
of bank are homographs, as are the two uses of bat, because they are written the

1 Confusingly, the word “lemma” is itself ambiguous; it is also sometimes used to mean these separate
senses, rather than the citation form of the word. You should be prepared to see both uses in the literature.

word sense

Homonym
Homonymy

homographs

494 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

homophones

polysemy

metonymy

same. Two words can be homonyms in a different way if they are spelled differently
but pronounced the same, like write and right, or piece and peace. We call these
homophones; they are one cause of real-word spelling errors.
Homonymy causes problems in other areas of language processing as well. In
question answering or information retrieval, we better help a user who typed “bat
care” if we know whether they are vampires or just want to play baseball. And
they will also have different translations; in Spanish the animal bat is a murci ´elago
while the baseball bat is a bate. Homographs that are pronounced differently cause
problems for speech synthesis (Chapter 26) such as these homographs of the word
bass, the ﬁsh pronounced b ae s and the instrument pronounced b ey s.
(C.3) The expert angler from Dora, Mo., was ﬂy-casting for bass rather than the
traditional trout.
(C.4) The curtain rises to the sound of angry dogs baying and ominous bass chords
sounding.
Sometimes there is also some semantic connection between the senses of a word.
Consider the following example:
(C.5) While some banks furnish blood only to hospitals, others are less restrictive.
Although this is clearly not a use of the “sloping mound” meaning of bank, it just
as clearly is not a reference to a charitable giveaway by a ﬁnancial institution. Rather,
bank has a whole range of uses related to repositories for various biological entities,
as in blood bank, egg bank, and sperm bank. So we could call this “biological
repository” sense bank3 . Now this new sense bank3 has some sort of relation to
bank1 ; both bank1 and bank3 are repositories for entities that can be deposited and
taken out; in bank1 the entity is monetary, whereas in bank3 the entity is biological.
When two senses are related semantically, we call the relationship between them
polysemy rather than homonymy. In many cases of polysemy, the semantic relation
between the senses is systematic and structured. For example, consider yet another
sense of bank, exempliﬁed in the following sentence:
(C.6) The bank is on the corner of Nassau and Witherspoon.
This sense, which we can call bank4 , means something like “the building be-
longing to a ﬁnancial institution”. It turns out that these two kinds of senses (an
organization and the building associated with an organization ) occur together for
many other words as well (school, university, hospital, etc.). Thus, there is a sys-
tematic relationship between senses that we might represent as
BUILDING ↔ ORGANIZATION
This particular subtype of polysemy relation is often called metonymy. Metonymy
is the use of one aspect of a concept or entity to refer to other aspects of the entity
or to the entity itself. Thus, we are performing metonymy when we use the phrase
the White House to refer to the administration whose ofﬁce is in the White House.
Other common examples of metonymy include the relation between the following
pairings of senses:

Author (Jane Austen wrote Emma) ↔ Works of Author (I really love Jane Austen)
Tree (Plums have beautiful blossoms) ↔ Fruit (I ate a preserved plum yesterday)

While it can be useful to distinguish polysemy from unrelated homonymy, there
is no hard threshold for how related two senses must be to be considered polyse-
mous. Thus, the difference is really one of degree. This fact can make it very difﬁcult
to decide how many senses a word has, that is, whether to make separate senses for

zeugma

C .1

• WORD S EN S E S

495

closely related usages. There are various criteria for deciding that the differing uses
of a word should be represented with discrete senses. We might consider two senses
discrete if they have independent truth conditions, different syntactic behavior, and
independent sense relations, or if they exhibit antagonistic meanings.
Consider the following uses of the verb serve from the WSJ corpus:
(C.7) They rarely serve red meat, preferring to prepare seafood.
(C.8) He served as U.S. ambassador to Norway in 1976 and 1977.
(C.9) He might have served his time, come out and led an upstanding life.
The serve of serving red meat and that of serving time clearly have different truth
conditions and presuppositions; the serve of serve as ambassador has the distinct
subcategorization structure serve as NP. These heuristics suggest that these are prob-
ably three distinct senses of serve. One practical technique for determining if two
senses are distinct is to conjoin two uses of a word in a single sentence; this kind of
conjunction of antagonistic readings is called zeugma. Consider the following ATIS
examples:
(C.10) Which of those ﬂights serve breakfast?
(C.11) Does Midwest Express serve Philadelphia?
(C.12) ?Does Midwest Express serve breakfast and Philadelphia?
We use (?) to mark those examples that are semantically ill-formed. The oddness of
the invented third example (a case of zeugma) indicates there is no sensible way to
make a single sense of serve work for both breakfast and Philadelphia. We can use
this as evidence that serve has two different senses in this case.
Dictionaries tend to use many ﬁne-grained senses so as to capture subtle meaning
differences, a reasonable approach given that the traditional role of dictionaries is
aiding word learners. For computational purposes, we often don’t need these ﬁne
distinctions, so we may want to group or cluster the senses; we have already done
this for some of the examples in this chapter.
How can we deﬁne the meaning of a word sense? We introduced in Chapter 6 the
standard computational approach of representing a word as an embedding, a point in
semantic space. The intuition was that words were deﬁned by their co-occurrences,
the counts of words that often occur nearby.
Thesauri offer an alternative way of deﬁning words. But we can’t just look at
the deﬁnition itself. Consider the following fragments from the deﬁnitions of right,
left, red, and blood from the American Heritage Dictionary (Morris, 1985).

right adj. located nearer the right hand esp. being on the right when
facing the same direction as the observer.
left adj. located nearer to this side of the body than the right.
red n. the color of blood or a ruby.
blood n. the red liquid that circulates in the heart, arteries and veins of
animals.

Note the circularity in these deﬁnitions. The deﬁnition of right makes two direct
references to itself, and the entry for left contains an implicit self-reference in the
phrase this side of the body, which presumably means the left side. The entries for
red and blood reference each other in their deﬁnitions. Such circularity is inherent
in all dictionary deﬁnitions. For humans, such entries are still useful since the user
of the dictionary has sufﬁcient grasp of these other terms.
For computational purposes, one approach to deﬁning a sense is—like the dic-
tionary deﬁnitions—deﬁning a sense through its relationship with other senses. For

496 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

example, the above deﬁnitions make it clear that right and left are similar kinds of
lemmas that stand in some kind of alternation, or opposition, to one another. Simi-
larly, we can glean that red is a color, that it can be applied to both blood and rubies,
and that blood is a liquid. Sense relations of this sort are embodied in on-line
databases like WordNet. Given a sufﬁciently large database of such relations, many
applications are quite capable of performing sophisticated semantic tasks (even if
they do not really know their right from their left).

C.1.1 Relations Between Senses

This section explores some of the relations that hold among word senses, focus-
ing on a few that have received signiﬁcant computational investigation: synonymy,
antonymy, and hypernymy, as well as a brief mention of other relations like meronymy.

Synonymy We introduced in Chapter 6 the idea that when two senses of two dif-
ferent words (lemmas) are identical, or nearly identical, we say the two senses are
synonyms. Synonyms include such pairs as

synonym

couch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile

And we mentioned that in practice, the word synonym is commonly used to
describe a relationship of approximate or rough synonymy. But furthermore, syn-
onymy is actually a relationship between senses rather than words. Considering the
words big and large. These may seem to be synonyms in the following ATIS sen-
tences, since we could swap big and large in either sentence and retain the same
meaning:

(C.13) How big is that plane?
(C.14) Would I be ﬂying on a large or small plane?

But note the following WSJ sentence in which we cannot substitute large for big:

(C.15) Miss Nelson, for instance, became a kind of big sister to Benjamin.
(C.16) ?Miss Nelson, for instance, became a kind of large sister to Benjamin.

This is because the word big has a sense that means being older or grown up, while
large lacks this sense. Thus, we say that some senses of big and large are (nearly)
synonymous while other ones are not.

hyponym

hypernym

superordinate

Hyponymy One sense is a hyponym of another sense if the ﬁrst sense is more
speciﬁc, a subclass. For example, car is a hyponym of vehicle; dog is a hyponym
of animal, and mango is a hyponym of fruit. Conversely, vehicle is a hypernym of
car, and animal is a hypernym of dog. It is unfortunate that the two words hypernym
and hyponym are very similar and hence easily confused; for this reason, the word

superordinate is often used instead of hypernym.

Superordinate vehicle fruit
Hyponym

furniture mammal
mango chair
dog

car

meronymy
part-whole
meronym
holonym

Meronymy Another common relation is meronymy, the part-whole relation. A
leg is part of a chair; a wheel is part of a car. We say that wheel is a meronym of
car, and car is a holonym of wheel.

C .2

• WORDN E T: A DATABA SE O F L EX ICA L R ELAT ION S

497

C.2 WordNet: A Database of Lexical Relations

WordNet

The most commonly used resource for English sense relations is the WordNet lex-
ical database (Fellbaum, 1998). WordNet consists of three separate databases, one
each for nouns and verbs and a third for adjectives and adverbs; closed class words
are not included. Each database contains a set of lemmas, each one annotated with a
set of senses. The WordNet 3.0 release has 117,798 nouns, 11,529 verbs, 22,479 ad-
jectives, and 4,481 adverbs. The average noun has 1.23 senses, and the average verb
has 2.16 senses. WordNet can be accessed on the Web or downloaded and accessed
locally. Figure C.1 shows the lemma entry for the noun and adjective bass.

The noun “bass” has 8 senses in WordNet.
1. bass1 - (the lowest part of the musical range)
2. bass2 , bass part1 - (the lowest part in polyphonic music)
3. bass3 , basso1 - (an adult male singer with the lowest voice)
4. sea bass1 , bass4 - (the lean ﬂesh of a saltwater ﬁsh of the family Serranidae)
5. freshwater bass1 , bass5 - (any of various North American freshwater ﬁsh with
lean ﬂesh (especially of the genus Micropterus))
6. bass6 , bass voice1 , basso2 - (the lowest adult male singing voice)
7. bass7 - (the member with the lowest range of a family of musical instruments)
8. bass8 - (nontechnical name for any of numerous edible marine and
freshwater spiny-ﬁnned ﬁshes)
The adjective “bass” has 1 sense in WordNet.
1. bass1 , deep6 - (having or denoting a low vocal or instrumental range)
“a deep voice”; “a bass voice is lower than a baritone voice”;
“a bass clarinet”
Figure C.1 A portion of the WordNet 3.0 entry for the noun bass.

gloss

synset

Note that there are eight senses for the noun and one for the adjective, each of
which has a gloss (a dictionary-style deﬁnition), a list of synonyms for the sense, and
sometimes also usage examples (shown for the adjective sense). Unlike dictionaries,
WordNet doesn’t represent pronunciation, so doesn’t distinguish the pronunciation
[b ae s] in bass4 , bass5 , and bass8 from the other senses pronounced [b ey s].
The set of near-synonyms for a WordNet sense is called a synset (for synonym
set); synsets are an important primitive in WordNet. The entry for bass includes
synsets like {bass1 , deep6}, or {bass6 , bass voice1 , basso2}. We can think of a
synset as representing a concept of the type we discussed in Chapter 14. Thus,
instead of representing concepts in logical terms, WordNet represents them as lists
of the word senses that can be used to express the concept. Here’s another synset
example:

{chump1, fool2, gull1, mark9, patsy1, fall guy1,
sucker1, soft touch1, mug2}

The gloss of this synset describes it as a person who is gullible and easy to take
advantage of. Each of the lexical entries included in the synset can, therefore, be
used to express this concept. Synsets like this one actually constitute the senses
associated with WordNet entries, and hence it is synsets, not wordforms, lemmas, or
individual senses, that participate in most of the lexical sense relations in WordNet.
WordNet represents all the kinds of sense relations discussed in the previous sec-
tion, as illustrated in Fig. C.2 and Fig. C.3. WordNet hyponymy relations correspond

498 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

Relation

Also Called Deﬁnition

Example

Hypernym
Superordinate From concepts to superordinates
breakfast1 → meal1
Hyponym
Subordinate
From concepts to subtypes
meal1 → lunch1
Instance Hypernym Instance
From instances to their concepts
Austen1 → author1
Instance Hyponym Has-Instance From concepts to concept instances
composer1 → Bach1
Member Meronym Has-Member From groups to their members
faculty2 → professor1
Member Holonym Member-Of
From members to their groups
copilot1 → crew1
Part Meronym
Has-Part
From wholes to parts
table2 → leg3
Part Holonym
Part-Of
From parts to wholes
course7 → meal1
Substance Meronym
From substances to their subparts
water1 → oxygen1
Substance Holonym
From parts of substances to wholes
gin1 → martini1
Antonym
Semantic opposition between lemmas leader1 ⇐⇒ follower1
Derivationally
Lemmas w/same morphological root
destruction1 ⇐⇒ destroy1
Related Form

Figure C.2 Noun relations in WordNet.

Relation

Hypernym
Troponym

Entails
Antonym
Derivationally
Related Form

Deﬁnition

From events to superordinate events
From events to subordinate event
(often via speciﬁc manner)
From verbs (events) to the verbs (events) they entail
Semantic opposition between lemmas
Lemmas with same morphological root

Example

ﬂy9 → travel5
walk1 → stroll1
snore1 → sleep1
increase1 ⇐⇒ decrease1
destroy1 ⇐⇒ destruction1

Figure C.3 Verb relations in WordNet.

to the notion of immediate hyponymy discussed on page 496. Each synset is related
to its immediately more general and more speciﬁc synsets through direct hypernym
and hyponym relations. These relations can be followed to produce longer chains of
more general or more speciﬁc synsets. Figure C.4 shows hypernym chains for bass3

and bass7 .

In this depiction of hyponymy, successively more general synsets are shown on
successive indented lines. The ﬁrst chain starts from the concept of a human bass
singer. Its immediate superordinate is a synset corresponding to the generic concept
of a singer. Following this chain leads eventually to concepts such as entertainer and
person. The second chain, which starts from musical instrument, has a completely
different path leading eventually to such concepts as musical instrument, device, and
physical object. Both paths do eventually join at the very abstract synset whole, unit,
and then proceed together to entity which is the top (root) of the noun hierarchy (in
WordNet this root is generally called the unique beginner).

unique
beginner

C.3 Word Similarity: Thesaurus Methods

In Chapter 6 we introduced the embedding and cosine architecture for computing the
similarity between two words. A thesaurus offers a different family of algorithms
that can be complementary.
Although we have described them as relations between words, similar is actually
a relationship between word senses. For example, of the two senses of bank, we

C .3

• WORD S IM I LAR I TY: TH E SAURU S M ETHOD S

499

Sense 3
bass, basso --
(an adult male singer with the lowest voice)
=> singer, vocalist, vocalizer, vocaliser
=> musician, instrumentalist, player
=> performer, performing artist
=> entertainer
=> person, individual, someone...
=> organism, being
=> living thing, animate thing,
=> whole, unit
=> object, physical object
=> physical entity
=> entity
=> causal agent, cause, causal agency
=> physical entity
=> entity

Sense 7
bass --
(the member with the lowest range of a family of
musical instruments)
=> musical instrument, instrument
=> device
=> instrumentality, instrumentation
=> artifact, artefact
=> whole, unit
=> object, physical object
=> physical entity
=> entity

Figure C.4 Hyponymy chains for two separate senses of the lemma bass. Note that the
chains are completely distinct, only converging at the very abstract level whole, unit.

might say that the ﬁnancial sense is similar to one of the senses of fund and the
riparian sense is more similar to one of the senses of slope. In the next few sections
of this chapter, we will compute these relations over both words and senses.
The thesaurus-based algorithms use the structure of the thesaurus to deﬁne word
similarity. In principle, we could measure similarity by using any information avail-
able in a thesaurus (meronymy, glosses, etc.). In practice, however, thesaurus-based
word similarity algorithms generally use only the hypernym/hyponym (is-a or sub-
sumption) hierarchy. In WordNet, verbs and nouns are in separate hypernym hier-
archies, so a thesaurus-based algorithm for WordNet can thus compute only noun-
noun similarity, or verb-verb similarity; we can’t compare nouns to verbs or do
anything with adjectives or other parts of speech.
The simplest thesaurus-based algorithms are based on the intuition that words
or senses are more similar if there is a shorter path between them in the thesaurus
graph, an intuition dating back to Quillian (1969). A word/sense is most similar to
itself, then to its parents or siblings, and least similar to words that are far away. We
make this notion operational by measuring the number of edges between the two
concept nodes in the thesaurus graph and adding one. Figure C.5 shows an intuition;
the concept dime is most similar to nickel and coin, less similar to money, and even
less similar to Richter scale. A formal deﬁnition:
pathlen(c1 , c2 ) = 1 + the number of edges in the shortest path in the

500 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

Figure C.5 A fragment of the WordNet hypernym hierarchy, showing path lengths (number
of edges plus 1) from nickel to coin (2), dime (3), money (6), and Richter scale (8).

path-length
based similarity

word similarity

information-
content

thesaurus graph between the sense nodes c1 and c2
Path-based similarity can be deﬁned as just the path length, transformed either by
log (Leacock and Chodorow, 1998) or, more often, by an inverse, resulting in the
following common deﬁnition of path-length based similarity:
1
pathlen(c1 , c2 )
For most applications, we don’t have sense-tagged data, and thus we need our
algorithm to give us the similarity between words rather than between senses or con-
cepts. For any of the thesaurus-based algorithms, following Resnik (1995), we can
approximate the correct similarity (which would require sense disambiguation) by
just using the pair of senses for the two words that results in maximum sense sim-
ilarity. Thus, based on sense similarity, we can deﬁne word similarity as follows:

simpath (c1 , c2 ) =

(C.17)

wordsim(w1 , w2 ) =

max

c1 ∈senses(w1 )
c2 ∈senses(w2 )

sim(c1 , c2 )

(C.18)

The basic path-length algorithm makes the implicit assumption that each link
in the network represents a uniform distance. In practice, this assumption is not
appropriate. Some links (e.g., those that are deep in the WordNet hierarchy) often
seem to represent an intuitively narrow distance, while other links (e.g., higher up
in the WordNet hierarchy) represent an intuitively wider distance. For example, in
Fig. C.5, the distance from nickel to money (5) seems intuitively much shorter than
the distance from nickel to an abstract word standard; the link between medium of
exchange and standard seems wider than that between, say, coin and coinage.
It is possible to reﬁne path-based algorithms with normalizations based on depth
in the hierarchy (Wu and Palmer, 1994), but in general we’d like an approach that
lets us independently represent the distance associated with each edge.
A second class of thesaurus-based similarity algorithms attempts to offer just
such a ﬁne-grained metric. These information-content word-similarity algorithms
still rely on the structure of the thesaurus but also add probabilistic information
derived from a corpus.
Following Resnik (1995) we’ll deﬁne P(c) as the probability that a randomly
selected word in a corpus is an instance of concept c (i.e., a separate random variable,
ranging over words, associated with each concept). This implies that P(root ) = 1
since any word is subsumed by the root concept. Intuitively, the lower a concept

C .3

• WORD S IM I LAR I TY: TH E SAURU S M ETHOD S

501

in the hierarchy, the lower its probability. We train these probabilities by counting
in a corpus; each word in the corpus counts as an occurrence of each concept that
contains it. For example, in Fig. C.5 above, an occurrence of the word dime would
count toward the frequency of coin, currency, standard, etc. More formally, Resnik
computes P(c) as follows:

P(c) = (cid:80)w∈words(c) count (w)

N
where words(c) is the set of words subsumed by concept c, and N is the total number
of words in the corpus that are also present in the thesaurus.
Figure C.6, from Lin (1998), shows a fragment of the WordNet concept hierar-
chy augmented with the probabilities P(c).

(C.19)

entity 0.395

inanimate-object 0.167

natural-object 0.0163

geological-formation 0.00176

0.000113 natural-elevation

shore 0.0000836

0.0000189 hill

coast 0.0000216

Figure C.6 A fragment of the WordNet hierarchy, showing the probability P(c) attached to
each content, adapted from a ﬁgure from Lin (1998).

We now need two additional deﬁnitions. First, following basic information the-
ory, we deﬁne the information content (IC) of a concept c as
IC(c) = − log P(c)
Second, we deﬁne the lowest common subsumer or LCS of two concepts:
LCS(c1 , c2 ) = the lowest common subsumer, that is, the lowest node in
the hierarchy that subsumes (is a hypernym of) both c1 and c2
There are now a number of ways to use the information content of a node in a
word similarity metric. The simplest way was ﬁrst proposed by Resnik (1995). We
think of the similarity between two words as related to their common information;
the more two words have in common, the more similar they are. Resnik proposes
to estimate the common amount of information by the information content of the

lowest common subsumer of the two nodes. More formally, the Resnik similarity

(C.20)

measure is

simresnik (c1 , c2 ) = − log P(LCS(c1 , c2 ))
(C.21)
Lin (1998) extended the Resnik intuition by pointing out that a similarity metric
between objects A and B needs to do more than measure the amount of information
in common between A and B. For example, he additionally pointed out that the more
differences between A and B, the less similar they are. In summary:

Lowest
common
subsumer
LCS

Resnik
similarity

502 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

• Commonality: the more information A and B have in common, the more
similar they are.
• Difference: the more differences between the information in A and B, the less
similar they are.
Lin measures the commonality between A and B as the information content of
the proposition that states the commonality between A and B:
IC(common(A,B))
He measures the difference between A and B as
IC(description(A,B)) − IC(common(A,B))
(C.23)
where description(A,B) describes A and B. Given a few additional assumptions
about similarity, Lin proves the following theorem:

(C.22)

Similarity Theorem: The similarity between A and B is measured by the ratio
between the amount of information needed to state the commonality of A and
B and the information needed to fully describe what A and B are.

common(A,B)
simLin (A, B) =
description(A,B)
Applying this idea to the thesaurus domain, Lin shows (in a slight modiﬁcation
of Resnik’s assumption) that the information in common between two concepts is
twice the information in the lowest common subsumer LCS(c1 , c2 ). Adding in the
above deﬁnitions of the information content of thesaurus concepts, the ﬁnal Lin

(C.24)

similarity function is

2 × log P(LCS(c1 , c2 ))
simLin (c1 , c2 ) =
log P(c1 ) + log P(c2 )
For example, using simLin , Lin (1998) shows that the similarity between the
concepts of hill and coast from Fig. C.6 is

(C.25)

2 × log P(geological-formation)
simLin (hill, coast) =
log P(hill) + log P(coast)
A similar formula, Jiang-Conrath distance (Jiang and Conrath, 1997), although
derived in a completely different way from Lin and expressed as a distance rather
than similarity function, has been shown to work as well as or better than all the
other thesaurus-based methods:

= 0.59

(C.26)

distJC (c1 , c2 ) = 2 × log P(LCS(c1 , c2 )) − (log P(c1 ) + log P(c2 ))
We can transform distJC into a similarity by taking the reciprocal.
Finally, we describe a dictionary-based method that is related to the Lesk al-
gorithm for word sense disambiguation we will introduce in Section C.6.1. The
intution of extended gloss overlap, or extended Lesk measure (Banerjee and Ped-
ersen, 2003) is that two concepts/senses in a thesaurus are similar if their glosses
contain overlapping words. We’ll begin by sketching an overlap function for two
glosses. Consider these two concepts, with their glosses:

(C.27)

Lin similarity

Jiang-Conrath
distance

Extended gloss
overlap
extended Lesk

C .3

• WORD S IM I LAR I TY: TH E SAURU S M ETHOD S

503

• drawing paper: paper that is specially prepared for use in drafting
• decal: the art of transferring designs from specially prepared paper to a wood
or glass or metal surface.
For each n-word phrase that occurs in both glosses, Extended Lesk adds in a
score of n2 (the relation is non-linear because of the Zipﬁan relationship between
lengths of phrases and their corpus frequencies; longer overlaps are rare, so they
should be weighted more heavily). Here, the overlapping phrases are paper and
specially prepared, for a total similarity score of 12 + 22 = 5.
Given such an overlap function, when comparing two concepts (synsets), Ex-
tended Lesk not only looks for overlap between their glosses but also between the
glosses of the senses that are hypernyms, hyponyms, meronyms, and other relations
of the two concepts. For example, if we just considered hyponyms and deﬁned
gloss(hypo(A)) as the concatenation of all the glosses of all the hyponym senses of
A, the total relatedness between two concepts A and B might be

similarity(A,B) = overlap(gloss(A), gloss(B))
+overlap(gloss(hypo(A)), gloss(hypo(B)))
+overlap(gloss(A), gloss(hypo(B)))
+overlap(gloss(hypo(A)),gloss(B))

Let RELS be the set of possible WordNet relations whose glosses we compare;
assuming a basic overlap measure as sketched above, we can then deﬁne the Ex-
tended Lesk overlap measure as
simeLesk (c1 , c2 ) = (cid:88)r,q∈RELS

overlap(gloss(r(c1 )), gloss(q(c2 )))

(C.28)

1
simpath (c1 , c2 ) =
pathlen(c1 , c2 )
simResnik (c1 , c2 ) = − log P(LCS(c1 , c2 ))
2 × log P(LCS(c1 , c2 ))
simLin (c1 , c2 ) =
log P(c1 ) + log P(c2 )

1
simJC (c1 , c2 ) =
2 × log P(LCS(c1 , c2 )) − (log P(c1 ) + log P(c2 ))
simeLesk (c1 , c2 ) = (cid:88)r,q∈RELS
overlap(gloss(r(c1 )), gloss(q(c2 )))

Figure C.7 Five thesaurus-based (and dictionary-based) similarity measures.

Figure C.7 summarizes the ﬁve similarity measures we have described in this
section.

Evaluating Thesaurus-Based Similarity

Which of these similarity measures is best? Word similarity measures have been
evaluated in two ways, introduced in Chapter 6. The most common intrinsic evalu-
ation metric computes the correlation coefﬁcient between an algorithm’s word sim-
ilarity scores and word similarity ratings assigned by humans. There are a variety

504 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

of such human-labeled datasets: the RG-65 dataset of human similarity ratings on
65 word pairs (Rubenstein and Goodenough, 1965), the MC-30 dataset of 30 word
pairs (Miller and Charles, 1991). The WordSim-353 (Finkelstein et al., 2002) is a
commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane,
car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more difﬁcult
dataset that quantiﬁes similarity (cup, mug) rather than relatedness (cup, coffee), and
including both concrete and abstract adjective, noun and verb pairs. Another com-
mon intrinic similarity measure is the TOEFL dataset, a set of 80 questions, each
consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: Levied is closest in meaning to:
imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these
datasets present words without context.
Slightly more realistic are intrinsic similarity tasks that include context. The
Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) offers a
richer evaluation scenario, giving human judgments on 2,003 pairs of words in their
sentential context, including nouns, verbs, and adjectives. This dataset enables the
evaluation of word similarity algorithms that can make use of context words. The
semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the
performance of sentence-level similarity algorithms, consisting of a set of pairs of
sentences, each pair with human-labeled similarity scores.
Alternatively, the similarity measure can be embedded in some end-application,
such as question answering or spell-checking, and different measures can be evalu-
ated by how much they improve the end application.

C.4 Word Sense Disambiguation: Overview

word sense
disambiguation
WSD

The task of selecting the correct sense for a word is called word sense disambigua-
tion, or WSD. WSD algorithms take as input a word in context and a ﬁxed inventory
of potential word senses and outputs the correct word sense in context. The input and
the senses depends on the task. For machine translation from English to Spanish, the
sense tag inventory for an English word might be the set of different Spanish trans-
lations. For automatic indexing of medical articles, the sense-tag inventory might be
the set of MeSH (Medical Subject Headings) thesaurus entries.
When we are evaluating WSD in isolation, we can use the set of senses from a
dictionary/thesaurus resource like WordNet. Figure C.4 shows an example for the
word bass, which can refer to a musical instrument or a kind of ﬁsh.2

WordNet Spanish
Roget
Sense
Translation Category

Target Word in Context

bass4
lubina
FI SH / IN SECT . . . ﬁsh as Paciﬁc salmon and striped bass and. . .
bass4
lubina
FI SH / IN SECT . . . produce ﬁlets of smoked bass or sturgeon. . .
bass7
bajo
. . . exciting jazz bass player since Ray Brown. . .
bass7
bajo
. . . play bass because he doesn’t have to solo. . .
Figure C.8 Possible deﬁnitions for the inventory of sense tags for bass.

MU S IC
MU S IC

lexical sample

It is useful to distinguish two WSD tasks. In the lexical sample task, a small

2 The WordNet database includes eight senses; we have arbitrarily selected two for this example; we
have also arbitrarily selected one of the many Spanish ﬁshes that could translate English sea bass.

all-words

C .5

• SU PERV I S ED WORD S EN S E D I SAMB IGUAT ION

505

pre-selected set of target words is chosen, along with an inventory of senses for each
word from some lexicon. Since the set of words and the set of senses are small,
simple supervised classiﬁcation approaches are used.
In the all-words task, systems are given entire texts and a lexicon with an inven-
tory of senses for each entry and are required to disambiguate every content word in
the text. The all-words task is similar to part-of-speech tagging, except with a much
larger set of tags since each lemma has its own set. A consequence of this larger set
of tags is data sparseness; it is unlikely that adequate training data for every word in
the test set will be available. Moreover, given the number of polysemous words in
reasonably sized lexicons, approaches based on training one classiﬁer per term are
unlikely to be practical.

C.5 Supervised Word Sense Disambiguation

Supervised WSD is commonly used whenever we have sufﬁcient data that has been
hand-labeled with correct word senses.
Datasets: The are various lexical sample datasets with context sentences labeled
with the correct sense for the target word, such as the line-hard-serve corpus with
4,000 sense-tagged examples of line as a noun, hard as an adjective and serve as a
verb (Leacock et al., 1993), and the interest corpus with 2,369 sense-tagged exam-
ples of interest as a noun (Bruce and Wiebe, 1994). The S EN SEVA L project has also
produced a number of such sense-labeled lexical sample corpora (S EN SEVA L -1 with
34 words from the H EC TOR lexicon and corpus (Kilgarriff and Rosenzweig 2000,
Atkins 1993), SEN SEVA L -2 and -3 with 73 and 57 target words, respectively (Palmer
et al. 2001, Kilgarriff 2001). All-word disambiguation tasks are trained from a se-
mantic concordance, a corpus in which each open-class word in each sentence is
labeled with its word sense from a speciﬁc dictionary or thesaurus. One commonly
used corpus is SemCor, a subset of the Brown Corpus consisting of over 234,000
words that were manually tagged with WordNet senses (Miller et al. 1993, Landes
et al. 1998). In addition, sense-tagged corpora have been built for the SEN SEVA L all-
word tasks. The SEN SEVA L-3 English all-words test data consisted of 2081 tagged
content word tokens, from 5,000 total running words of English from the WSJ and
Brown corpora (Palmer et al., 2001).
Features Supervised WSD algorithms can use any standard classiﬁcation algo-
rithm. Features generally include the word identity, part-of-speech tags, and embed-
dings of surrounding words, usually computed in two ways: collocation features are
words or n-grams at a particular location, (i.e., exactly one word to the right, or the
two words starting 3 words to the left, and so on). bag of word features are rep-
resented as a vector with the dimensionality of the vocabulary (minus stop words),
with a 1 if that word occurs in the in the neighborhood of the target word.
Consider the ambiguous word bass in the following WSJ sentence:
(C.29) An electric guitar and bass player stand off to one side,
If we used a small 2-word window, a standard feature vector might include a bag of
words, parts-of-speech, unigram and bigram collocation features, and embeddings,
that is:

semantic
concordance

collocation

bag of word

[wi−2 , POSi−2 , wi−1 , POSi−1 , wi+1 , POSi+1 , wi+2 , POSi+2 ,
i+1 , E (wi−2 , wi−1 , wi+1 , wi+2 ), bag()]

i−2 , wi+2

wi−1

(C.30)

506 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

would yield the following vector:

[guitar, NN, and, CC, player, NN, stand, VB, and guitar, player stand,
E(guitar,and,player,stand), bag(guitar,player,stand)]

High performing systems generally use POS tags and word collocations of length
1, 2, and 3 from a window of words 3 to the left and 3 to the right (Zhong and Ng,
2010). The embedding function could just take the average of the embeddings of
the words in the window, or a more complicated embedding function can be used
(Iacobacci et al., 2016).

C.5.1 Wikipedia as a source of training data

One way to increase the amount of training data is to use Wikipedia as a source of
sense-labeled data. When a concept is mentioned in a Wikipedia article, the article
text may contain an explicit link to the concept’s Wikipedia page, which is named
by a unique identiﬁer. This link can be used as a sense annotation. For example,
the ambiguous word bar is linked to a different Wikipedia article depending on its
meaning in context, including the page BAR (LAW ), the page BAR (MU S IC ), and
so on, as in the following Wikipedia examples (Mihalcea, 2007).
In 1834, Sumner was admitted to the [[bar (law)|bar]] at the age of
twenty-three, and entered private practice in Boston.
It is danced in 3/4 time (like most waltzes), with the couple turning
approx. 180 degrees every [[bar (music)|bar]].
Jenga is a popular beer in the [[bar (establishment)|bar]]s of Thailand.
These sentences can then be added to the training data for a supervised system.
In order to use Wikipedia in this way, however, it is necessary to map from Wikipedia
concepts to whatever inventory of senses is relevant for the WSD application. Auto-
matic algorithms that map from Wikipedia to WordNet, for example, involve ﬁnding
the WordNet sense that has the greatest lexical overlap with the Wikipedia sense, by
comparing the vector of words in the WordNet synset, gloss, and related senses with
the vector of words in the Wikipedia page title, outgoing links, and page category
(Ponzetto and Navigli, 2010).

C.5.2 Evaluation

extrinsic
evaluation

intrinsic

sense accuracy

To evaluate WSD algorithms, it’s better to consider extrinsic, task-based, or end-
to-end evaluation, in which we see whether some new WSD idea actually improves
performance in some end-to-end application like question answering or machine
translation. Nonetheless, because extrinsic evaluations are difﬁcult and slow, WSD
systems are typically evaluated with intrinsic evaluation. in which a WSD compo-
nent is treated as an independent system. Common intrinsic evaluations are either
exact-match sense accuracy—the percentage of words that are tagged identically
with the hand-labeled sense tags in a test set—or with precision and recall if sys-
tems are permitted to pass on the labeling of some instances. In general, we evaluate
by using held-out data from the same sense-tagged corpora that we used for training,
such as the SemCor corpus discussed above or the various corpora produced by the

S EN SEVA L effort.

Many aspects of sense evaluation have been standardized by the S EN SEVA L and
S EM EVA L efforts (Palmer et al. 2006, Kilgarriff and Palmer 2000). This framework
provides a shared task with training and testing materials along with sense invento-
ries for all-words and lexical sample tasks in a variety of languages.

most frequent
sense

C .6

• WSD : D IC T IONARY AND TH E SAURU S M ETHOD S

507

The normal baseline is to choose the most frequent sense for each word from the
senses in a labeled corpus (Gale et al., 1992a). For WordNet, this corresponds to the
ﬁrst sense, since senses in WordNet are generally ordered from most frequent to least
frequent. WordNet sense frequencies come from the SemCor sense-tagged corpus
described above– WordNet senses that don’t occur in SemCor are ordered arbitrarily
after those that do. The most frequent sense baseline can be quite accurate, and is
therefore often used as a default, to supply a word sense when a supervised algorithm
has insufﬁcient training data.

C.6 WSD: Dictionary and Thesaurus Methods

Supervised algorithms based on sense-labeled corpora are the best-performing algo-
rithms for sense disambiguation. However, such labeled training data is expensive
and limited. One alternative is to get indirect supervision from dictionaries and the-
sauruses, and so this method is also called knowledge-based WSD. Methods like
this that do not use texts that have been hand-labeled with senses are also called
weakly supervised.

C.6.1 The Lesk Algorithm

Lesk algorithm

Simpliﬁed Lesk

The most well-studied dictionary-based algorithm for sense disambiguation is the
Lesk algorithm, really a family of algorithms that choose the sense whose dictio-
nary gloss or deﬁnition shares the most words with the target word’s neighborhood.
Figure C.9 shows the simplest version of the algorithm, often called the Simpliﬁed
Lesk algorithm (Kilgarriff and Rosenzweig, 2000).

function S IM P L I FIED L E SK(word, sentence) returns best sense of word
best-sense ← most frequent sense for word
max-overlap ← 0
context ← set of words in sentence
for each sense in senses of word do
signature ← set of words in the gloss and examples of sense
overlap ← COM PUT EOV ER LA P(signature, context)
if overlap > max-overlap then
max-overlap ← overlap
best-sense ← sense
return(best-sense)

end

Figure C.9 The Simpliﬁed Lesk algorithm. The COM PU T EOV ER LA P function returns the
number of words in common between two sets, ignoring function words or other words on a
stop list. The original Lesk algorithm deﬁnes the context in a more complex way. The Cor-
pus Lesk algorithm weights each overlapping word w by its − log P(w) and includes labeled
training corpus data in the signature.

As an example of the Lesk algorithm at work, consider disambiguating the word
bank in the following context:
(C.31) The bank can guarantee deposits will eventually cover future tuition costs
because it invests in adjustable-rate mortgage securities.

508 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

given the following two WordNet senses:

bank1 Gloss:

Examples:

bank2 Gloss:
Examples:

a ﬁnancial institution that accepts deposits and channels the
money into lending activities
“he cashed a check at the bank”, “that bank holds the mortgage
on my home”
sloping land (especially the slope beside a body of water)
“they pulled the canoe up on the bank”, “he sat on the bank of
the river and watched the currents”

Sense bank1 has two non-stopwords overlapping with the context in (C.31):
deposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen.
There are many obvious extensions to Simpliﬁed Lesk. The original Lesk algo-
rithm (Lesk, 1986) is slightly more indirect. Instead of comparing a target word’s
signature with the context words, the target signature is compared with the signatures
of each of the context words. For example, consider Lesk’s example of selecting the
appropriate sense of cone in the phrase pine cone given the following deﬁnitions for
pine and cone.

pine 1 kinds of evergreen tree with needle-shaped leaves
2 waste away through sorrow or illness
cone 1 solid body which narrows to a point
2 something of this shape whether solid or hollow
3 fruit of certain evergreen trees

In this example, Lesk’s method would select cone3 as the correct sense since
two of the words in its entry, evergreen and tree, overlap with words in the entry for
pine, whereas neither of the other entries has any overlap with words in the deﬁnition
of pine. In general Simpliﬁed Lesk seems to work better than original Lesk.
The primary problem with either the original or simpliﬁed approaches, how-
ever, is that the dictionary entries for the target words are short and may not provide
enough chance of overlap with the context.3 One remedy is to expand the list of
words used in the classiﬁer to include words related to, but not contained in, their
individual sense deﬁnitions. But the best solution, if any sense-tagged corpus data
like SemCor is available, is to add all the words in the labeled corpus sentences for a
word sense into the signature for that sense. This version of the algorithm, the Cor-
pus Lesk algorithm, is the best-performing of all the Lesk variants (Kilgarriff and
Rosenzweig 2000, Vasilescu et al. 2004) and is used as a baseline in the SEN SEVA L
competitions. Instead of just counting up the overlapping words, the Corpus Lesk
algorithm also applies a weight to each overlapping word. The weight is the inverse
document frequency or IDF, a standard information-retrieval measure introduced
in Chapter 6. IDF measures how many different “documents” (in this case, glosses
and examples) a word occurs in and is thus a way of discounting function words.
Since function words like the, of, etc., occur in many documents, their IDF is very
low, while the IDF of content words is high. Corpus Lesk thus uses IDF instead of a
stop list.
Formally, the IDF for a word i can be deﬁned as
idfi = log (cid:18) N d oc
ndi (cid:19)

(C.32)

3

Indeed, Lesk (1986) notes that the performance of his system seems to roughly correlate with the
length of the dictionary entries.

Corpus Lesk

inverse
document
frequency
IDF

C .6

• WSD : D IC T IONARY AND TH E SAURU S M ETHOD S

509

where N d oc is the total number of “documents” (glosses and examples) and ndi is
the number of these documents containing word i.
Finally, we can combine the Lesk and supervised approaches by adding new
Lesk-like bag-of-words features. For example, the glosses and example sentences
for the target sense in WordNet could be used to compute the supervised bag-of-
words features in addition to the words in the SemCor context sentence for the sense
(Yuret, 2004).

C.6.2 Graph-based Methods

Another way to use a thesaurus like WordNet is to make use of the fact that WordNet
can be construed as a graph, with senses as nodes and relations between senses
as edges. In addition to the hypernymy and other relations, it’s possible to create
links between senses and those words in the gloss that are unambiguous (have only
one sense). Often the relations are treated as undirected edges, creating a large
undirected WordNet graph. Fig. C.10 shows a portion of the graph around the word
drink1
v .

Figure C.10 Part of the WordNet graph around drink1
v , after Navigli and Lapata (2010).

There are various ways to use the graph for disambiguation, some using the
whole graph, some using only a subpart. For example the target word and the words
in its sentential context can all be inserted as nodes in the graph via a directed edge
to each of its senses. If we consider the sentence She drank some milk, Fig. C.11
shows a portion of the WordNet graph between the senses drink1
v and milk1
n .

Figure C.11 Part of the WordNet graph between drink1
v and milk1
n , for disambiguating a
sentence like She drank some milk, adapted from Navigli and Lapata (2010).

The correct sense is then the one which is the most important or central in some
way in this graph. There are many different methods for deciding centrality. The

510 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

degree

personalized
page rank

simplest is degree, the number of edges into the node, which tends to correlate
with the most frequent sense. Another algorithm for assigning probabilities across
nodes is personalized page rank, a version of the well-known pagerank algorithm
which uses some seed nodes. By inserting a uniform probability across the word
nodes (drink and milk in the example) and computing the personalized page rank of
the graph, the result will be a pagerank value for each node in the graph, and the
sense with the maximum pagerank can then be chosen. See Agirre et al. (2014) and
Navigli and Lapata (2010) for details.

C.7 Semi-Supervised WSD: Bootstrapping

bootstrapping

Yarowsky
algorithm

Both the supervised approach and the dictionary-based approaches to WSD require
large hand-built resources: supervised training sets in one case, large dictionaries in

the other. We can instead use bootstrapping or semi-supervised learning, which

needs only a very small hand-labeled training set.
A classic bootstrapping algorithm for WSD is the Yarowsky algorithm for
learning a classiﬁer for a target word (in a lexical-sample task) (Yarowsky, 1995).
The algorithm is given a small seedset Λ0 of labeled instances of each sense and a
much larger unlabeled corpus V0 . The algorithm ﬁrst trains an initial classiﬁer on
the seedset Λ0 . It then uses this classiﬁer to label the unlabeled corpus V0 . The
algorithm then selects the examples in V0 that it is most conﬁdent about, removes
them, and adds them to the training set (call it now Λ1 ). The algorithm then trains a
new classiﬁer (a new set of rules) on Λ1 , and iterates by applying the classiﬁer to the
now-smaller unlabeled set V1 , extracting a new training set Λ2 , and so on. With each
iteration of this process, the training corpus grows and the untagged corpus shrinks.
The process is repeated until some sufﬁciently low error-rate on the training set is
reached or until no further examples from the untagged corpus are above threshold.

Figure C.12 The Yarowsky algorithm disambiguating “plant” at two stages; “?” indicates an unlabeled ob-
servation, A and B are observations labeled as SENSE-A or SENSE-B. The initial stage (a) shows only seed
sentences Λ0 labeled by collocates (“life” and “manufacturing”). An intermediate stage is shown in (b) where
more collocates have been discovered (“equipment”, “microscopic”, etc.) and more instances in V0 have been
moved into Λ1 , leaving a smaller unlabeled set V1 . Figure adapted from Yarowsky (1995).

C .8

• UN SU PERV I S ED WORD S EN S E INDUC T ION

511

We need more good teachers – right now, there are only a half a dozen who can play
the free bass with ease.
An electric guitar and bass player stand off to one side, not really part of the scene,
The researchers said the worms spend part of their life cycle in such ﬁsh as Paciﬁc
salmon and striped bass and Paciﬁc rockﬁsh or snapper.
And it all started when ﬁshermen decided the striped bass in Lake Mead were...
Figure C.13 Samples of bass sentences extracted from the WSJ by using the simple corre-
lates play and ﬁsh.

Initial seeds can be selected by hand-labeling a small set of examples (Hearst,
1991), or by using the help of a heuristic. Yarowsky (1995) used the one sense
per collocation heuristic, which relies on the intuition that certain words or phrases
strongly associated with the target senses tend not to occur with the other sense.
Yarowsky deﬁnes his seedset by choosing a single collocation for each sense.
For example, to generate seed sentences for the ﬁsh and musical musical senses
of bass, we might come up with ﬁsh as a reasonable indicator of bass1 and play as
a reasonable indicator of bass2 . Figure C.13 shows a partial result of such a search
for the strings “ﬁsh” and “play” in a corpus of bass examples drawn from the WSJ.
The original Yarowsky algorithm also makes use of a second heuristic, called
one sense per discourse, based on the work of Gale et al. (1992b), who noticed that
a particular word appearing multiple times in a text or discourse often appeared with
the same sense. This heuristic seems to hold better for coarse-grained senses and
particularly for cases of homonymy rather than polysemy (Krovetz, 1998).
Nonetheless, it is still useful in a number of sense disambiguation situations. In
fact, the one sense per discourse heuristic is an important one throughout language
processing as it seems that many disambiguation tasks may be improved by a bias
toward resolving an ambiguity the same way inside a discourse segment.

one sense per
collocation

one sense per
discourse

C.8 Unsupervised Word Sense Induction

word sense
induction

It is expensive and difﬁcult to build large corpora in which each word is labeled for
its word sense. For this reason, an unsupervised approach to sense disambiguation,
often called word sense induction or WSI, is an important direction.
In unsu-
pervised approaches, we don’t use human-deﬁned word senses. Instead, the set of
“senses” of each word is created automatically from the instances of each word in
the training set.
Most algorithms for word sense induction use some sort of clustering over word
embeddings. (The earliest algorithms, due to Sch ¨utze (Sch ¨utze 1992b, Sch ¨utze 1998),
represented each word as a context vector of bag-of-words features (cid:126)c.) Then in train-
ing, we use three steps.
1. For each token wi of word w in a corpus, compute a context vector (cid:126)c.
2. Use a clustering algorithm to cluster these word-token context vectors (cid:126)c into
a predeﬁned number of groups or clusters. Each cluster deﬁnes a sense of w.
3. Compute the vector centroid of each cluster. Each vector centroid (cid:126)s j is a
sense vector representing that sense of w.
Since this is an unsupervised algorithm, we don’t have names for each of these
“senses” of w; we just refer to the jth sense of w.

512 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

agglomerative
clustering

erative clustering.

To disambiguate a particular token t of w we again have three steps:
1. Compute a context vector (cid:126)c for t .
2. Retrieve all sense vectors s j for w.
3. Assign t to the sense represented by the sense vector s j that is closest to t .
All we need is a clustering algorithm and a distance metric between vectors.
Clustering is a well-studied problem with a wide number of standard algorithms that
can be applied to inputs structured as vectors of numerical values (Duda and Hart,
1973). A frequently used technique in language applications is known as agglom-
In this technique, each of the N training instances is initially
assigned to its own cluster. New clusters are then formed in a bottom-up fashion by
the successive merging of the two clusters that are most similar. This process con-
tinues until either a speciﬁed number of clusters is reached, or some global goodness
measure among the clusters is achieved. In cases in which the number of training
instances makes this method too expensive, random sampling can be used on the
original training set to achieve similar results.
How can we evaluate unsupervised sense disambiguation approaches? As usual,
the best way is to do extrinsic evaluation embedded in some end-to-end system; one
example used in a SemEval bakeoff is to improve search result clustering and di-
versiﬁcation (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to
map the automatically derived sense classes into a hand-labeled gold-standard set so
that we can compare a hand-labeled test set with a set labeled by our unsupervised
classiﬁer. Various such metrics have been tested, for example in the SemEval tasks
(Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013),
including cluster overlap metrics, or methods that map each sense cluster to a pre-
deﬁned sense by choosing the sense that (in some training set) has the most overlap
with the cluster. However it is fair to say that no evaluation metric for this task has
yet become standard.

C.9 Summary

This chapter has covered a wide range of issues concerning the meanings associated
with lexical items. The following are among the highlights:
• A word sense is the locus of word meaning; deﬁnitions and meaning relations
are deﬁned at the level of the word sense rather than wordforms.
• Homonymy is the relation between unrelated senses that share a form, and
polysemy is the relation between related senses that share a form.
• Hyponymy and hypernymy relations hold between words that are in a class-
inclusion relationship.
• WordNet is a large database of lexical relations for English.
• Word-sense disambiguation (WSD) is the task of determining the correct
sense of a word in context. Supervised approaches make use of sentences in
which individual words (lexical sample task) or all words (all-words task)
are hand-labeled with senses from a resource like WordNet.
• Classiﬁers for supervised WSD are generally trained on features of the sur-
rounding words.
• An important baseline for WSD is the most frequent sense, equivalent, in

WordNet, to take the ﬁrst sense.

B IB L IOGRA PH ICA L AND H I STOR ICAL NOTE S

513

• The Lesk algorithm chooses the sense whose dictionary deﬁnition shares the
most words with the target word’s neighborhood.
• Graph-based algorithms view the thesaurus as a graph and choose the sense
that is most central in some way.
• Word similarity can be computed by measuring the link distance in a the-
saurus or by various measures of the information content of the two nodes.

Bibliographical and Historical Notes

Word sense disambiguation traces its roots to some of the earliest applications of
digital computers. The insight that underlies modern algorithms for word sense
disambiguation was ﬁrst articulated by Weaver (1955) in the context of machine
translation:
If one examines the words in a book, one at a time as through an opaque
mask with a hole in it one word wide, then it is obviously impossible
to determine, one at a time, the meaning of the words. [. . . ] But if
one lengthens the slit in the opaque mask, until one can see not only
the central word in question but also say N words on either side, then
if N is large enough one can unambiguously decide the meaning of the
central word. [. . . ] The practical question is : “What minimum value of
N will, at least in a tolerable fraction of cases, lead to the correct choice
of meaning for the central word?”
Other notions ﬁrst proposed in this early period include the use of a thesaurus for dis-
ambiguation (Masterman, 1957), supervised training of Bayesian models for disam-
biguation (Madhu and Lytel, 1965), and the use of clustering in word sense analysis
(Sparck Jones, 1986).
An enormous amount of work on disambiguation was conducted within the con-
text of early AI-oriented natural language processing systems. Quillian (1968) and
Quillian (1969) proposed a graph-based approach to language understanding, in
which the dictionary deﬁnition of words was represented by a network of word nodes
connected by syntactic and semantic relations. He then proposed to do sense disam-
biguation by ﬁnding the shortest path between senses in the conceptual graph. Sim-
mons (1973) is another inﬂuential early semantic network approach. Wilks proposed
one of the earliest non-discrete models with his Preference Semantics (Wilks 1975c,
Wilks 1975b, Wilks 1975a), and Small and Rieger (1982) and Riesbeck (1975) pro-
posed understanding systems based on modeling rich procedural information for
each word. Hirst’s AB S I TY system (Hirst and Charniak 1982, Hirst 1987, Hirst 1988),
which used a technique called marker passing based on semantic networks, repre-
sents the most advanced system of this type. As with these largely symbolic ap-
proaches, early neural network (at the time called ‘connectionist’) approaches to
word sense disambiguation relied on small lexicons with hand-coded representa-
tions (Cottrell 1985, Kawamoto 1988). Considerable work on sense disambiguation
has also been conducted in in psycholinguistics, under the name ’lexical ambiguity
resolution’. Small et al. (1988) present a variety of papers from this perspective.
The earliest implementation of a robust empirical approach to sense disambigua-
tion is due to Kelly and Stone (1975), who directed a team that hand-crafted a set
of disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the

514 A P PEND IX C • WORDN E T: WORD R ELAT ION S , S EN S E S , AND D I SAMB IGUAT ION

coarse senses

OntoNotes

generative
lexicon
qualia
structure

ﬁrst to use a machine-readable dictionary for word sense disambiguation. The prob-
lem of dictionary senses being too ﬁne-grained has been addressed by clustering
word senses into coarse senses (Dolan 1994, Chen and Chang 1998, Mihalcea and
Moldovan 2001, Agirre and de Lacalle 2003, Chklovski and Mihalcea 2003, Palmer
et al. 2004, Navigli 2006, Snow et al. 2007). Corpora with clustered word senses for
training clustering algorithms include Palmer et al. (2006) and OntoNotes (Hovy
et al., 2006).
Supervised approaches to disambiguation began with the use of decision trees by
Black (1988). The need for large amounts of annotated text in these methods led to
investigations into the use of bootstrapping methods (Hearst 1991, Yarowsky 1995).
Diab and Resnik (2002) give a semi-supervised algorithm for sense disambigua-
tion based on aligned parallel corpora in two languages. For example, the fact that
the French word catastrophe might be translated as English disaster in one instance
and tragedy in another instance can be used to disambiguate the senses of the two
English words (i.e., to choose senses of disaster and tragedy that are similar). Ab-
ney (2002) and Abney (2004) explore the mathematical foundations of the Yarowsky
algorithm and its relation to co-training. The most-frequent-sense heuristic is an ex-
tremely powerful one but requires large amounts of supervised training data.
The earliest use of clustering in the study of word senses was by Sparck Jones
(1986); Pedersen and Bruce (1997), Sch ¨utze (1997b), and Sch ¨utze (1998) applied
distributional methods. Recent work on word sense induction has applied Latent
Dirichlet Allocation (LDA) (Boyd-Graber et al. 2007, Brody and Lapata 2009, Lau
et al. 2012). and large co-occurrence graphs (Di Marco and Navigli, 2013).
A collection of work concerning WordNet can be found in Fellbaum (1998).
Early work using dictionaries as lexical resources include Amsler’s (1981) use of the
Merriam Webster dictionary and Longman’s Dictionary of Contemporary English
(Boguraev and Briscoe, 1989).
Early surveys of WSD include Agirre and Edmonds (2006) and Navigli (2009).
See Pustejovsky (1995), Pustejovsky and Boguraev (1996), Martin (1986), and
Copestake and Briscoe (1995), inter alia, for computational approaches to the rep-
resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in
particular his theory of the qualia structure of words, is another way of accounting
for the dynamic systematic polysemy of words in context.
Another important recent direction is the addition of sentiment and connotation
to knowledge bases (Wiebe et al. 2005, Qiu et al. 2009, Velikovich et al. 2010)
including SentiWordNet (Baccianella et al., 2010) and ConnotationWordNet (Kang
et al., 2014).

Exercises

C.1 Collect a small corpus of example sentences of varying lengths from any
newspaper or magazine. Using WordNet or any standard dictionary, deter-
mine how many senses there are for each of the open-class words in each sen-
tence. How many distinct combinations of senses are there for each sentence?
How does this number seem to vary with sentence length?
C.2 Using WordNet or a standard reference dictionary, tag each open-class word
in your corpus with its correct tag. Was choosing the correct sense always a
straightforward task? Report on any difﬁculties you encountered.

EX ERC I SE S

515

C.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-
ambiguation algorithm described on page 508 on the phrase Time ﬂies like an
arrow. Assume that the words are to be disambiguated one at a time, from
left to right, and that the results from earlier decisions are used later in the
process.
C.4 Build an implementation of your solution to the previous exercise. Using
WordNet, implement the original Lesk word overlap disambiguation algo-
rithm described on page 508 on the phrase Time ﬂies like an arrow.

Bibliography

Abbreviations:

AAAI
Proceedings of the National Conference on Artiﬁcial Intelligence
ACL
Proceedings of the Annual Conference of the Association for Computational Linguistics
ANLP
Proceedings of the Conference on Applied Natural Language Processing
CLS
Papers from the Annual Regional Meeting of the Chicago Linguistics Society
COGSCI
Proceedings of the Annual Conference of the Cognitive Science Society
COLING
Proceedings of the International Conference on Computational Linguistics
CoNLL
Proceedings of the Conference on Computational Natural Language Learning
EACL
Proceedings of the Conference of the European Association for Computational Linguistics
EMNLP
Proceedings of the Conference on Empirical Methods in Natural Language Processing
EUROSPEECH Proceedings of the European Conference on Speech Communication and Technology
ICASSP
Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing
ICML
International Conference on Machine Learning
ICPhS
Proceedings of the International Congress of Phonetic Sciences
ICSLP
Proceedings of the International Conference on Spoken Language Processing
IJCAI
Proceedings of the International Joint Conference on Artiﬁcial Intelligence
INTERSPEECH Proceedings of the Annual INTERSPEECH Conference
IWPT
Proceedings of the International Workshop on Parsing Technologies
JASA
Journal of the Acoustical Society of America
LREC
Conference on Language Resources and Evaluation
MUC
Proceedings of the Message Understanding Conference
NAACL-HLT
Proceedings of the North American Chapter of the ACL/Human Language Technology Conference
SIGIR
Proceedings of Annual Conference of ACM Special Interest Group on Information Retrieval

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-
enberg, J., Man ´e, D., Monga, R., Moore, S., Murray, D.,
Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,
Vi ´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke,
M., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-
scale machine learning on heterogeneous systems.. Soft-
ware available from tensorﬂow.org.
Abney, S. P. (1991). Parsing by chunks.
In Berwick,
R. C., Abney, S. P., and Tenny, C. (Eds.), Principle-Based
Parsing: Computation and Psycholinguistics, pp. 257–278.
Kluwer.
Abney, S. P. (1997). Stochastic attribute-value grammars.
Computational Linguistics, 23(4), 597–618.
Abney, S. P. (2002). Bootstrapping. In ACL-02, pp. 360–
367.
Abney, S. P. (2004). Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3), 365–395.
Abney, S. P., Schapire, R. E., and Singer, Y. (1999). Boost-
ing applied to tagging and PP attachment. In EMNLP/VLC-
99, College Park, MD, pp. 38–45.
Adriaans, P. and van Zaanen, M. (2004). Computational
grammar induction for linguists. Grammars; special issue
with the theme “Grammar Induction”, 7, 57–68.
Aggarwal, C. C. and Zhai, C. (2012). A survey of text classi-
ﬁcation algorithms. In Aggarwal, C. C. and Zhai, C. (Eds.),
Mining text data, pp. 163–222. Springer.
Agichtein, E. and Gravano, L. (2000). Snowball: Extract-
ing relations from large plain-text collections. In Proceed-
ings of the 5th ACM International Conference on Digital
Libraries.
Agirre, E. and de Lacalle, O. L. (2003). Clustering WordNet
word senses. In RANLP 2003.
Agirre, E., Banea, C., Cardie, C., Cer, D., Diab, M.,
Gonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Maritx-
alar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe,
J. (2015). 2015 SemEval-2015 Task 2: Semantic Textual

Similarity, English, Spanish and Pilot on Interpretability.
In SemEval-15, pp. 252–263.
Agirre, E., Diab, M., Cer, D., and Gonzalez-Agirre, A.
(2012). Semeval-2012 task 6: A pilot on semantic textual
similarity. In SemEval-12, pp. 385–393.
Agirre, E. and Edmonds, P. (Eds.). (2006). Word Sense Dis-
ambiguation: Algorithms and Applications. Kluwer.
Agirre, E., L ´opez de Lacalle, O., and Soroa, A. (2014). Ran-
dom walks for knowledge-based word sense disambigua-
tion. Computational Linguistics, 40(1), 57–84.
Agirre, E. and Martinez, D. (2001). Learning class-to-class
selectional preferences. In CoNLL-01.
Ahmad, F. and Kondrak, G. (2005). Learning a spelling er-
ror model from search query logs. In HLT-EMNLP-05, pp.
955–962.
Aho, A. V., Sethi, R., and Ullman, J. D. (1986). Compilers:
Principles, Techniques, and Tools. Addison-Wesley.
Aho, A. V. and Ullman, J. D. (1972). The Theory of Parsing,
Translation, and Compiling, Vol. 1. Prentice Hall.
Ajdukiewicz, K. (1935). Die syntaktische Konnexit ¨at. Stu-
dia Philosophica, 1, 1–27. English translation “Syntactic
Connexion” by H. Weber in McCall, S. (Ed.) 1967. Polish
Logic, pp. 207–231, Oxford University Press.
Algoet, P. H. and Cover, T. M. (1988). A sandwich proof of
the Shannon-McMillan-Breiman theorem. The Annals of
Probability, 16(2), 899–909.
Allen, J. (1984). Towards a general theory of action and
time. Artiﬁcial Intelligence, 23(2), 123–154.
Allen, J. and Perrault, C. R. (1980). Analyzing intention in
utterances. Artiﬁcial Intelligence, 15, 143–178.
Amsler, R. A. (1981). A taxonomy of English nouns and
verbs. In ACL-81, Stanford, CA, pp. 133–138.
An, J., Kwak, H., and Ahn, Y.-Y. (2018).
SemAxis:
A lightweight framework to characterize domain-speciﬁc
word semantics beyond sentiment. In ACL 2018.
Artstein, R., Gandhe, S., Gerten, J., Leuski, A., and Traum,
D. (2009). Semi-formal evaluation of conversational char-
acters. In Languages: From Formal to Natural, pp. 22–35.
Springer.

517

518 Bibliography

Atkins, S. (1993). Tools for computer-aided corpus lexicog-
raphy: The Hector project. Acta Linguistica Hungarica,
41, 5–72.
Atkinson, K. (2011). Gnu aspell..
Austin, J. L. (1962). How to Do Things with Words. Harvard
University Press.
Awadallah, A. H., Kulkarni, R. G., Ozertem, U., and Jones,
R. (2015). Charaterizing and predicting voice query refor-
mulation. In CIKM-15.
Baayen, R. H. (2001). Word frequency distributions.
Springer.
Bacchiani, M., Riley, M., Roark, B., and Sproat, R. (2006).
Map adaptation of stochastic grammars. Computer Speech
& Language, 20(1), 41–68.
Bacchiani, M., Roark, B., and Saraclar, M. (2004). Lan-
guage model adaptation with MAP estimation and the per-
ceptron algorithm. In HLT-NAACL-04, pp. 21–24.
Baccianella, S., Esuli, A., and Sebastiani, F. (2010). Sen-
tiwordnet 3.0: An enhanced lexical resource for sentiment
analysis and opinion mining.. In LREC-10, pp. 2200–2204.
Bach, K. and Harnish, R. (1979). Linguistic communication
and speech acts. MIT Press.
Backus, J. W. (1959). The syntax and semantics of the pro-
posed international algebraic language of the Zurich ACM-
GAMM Conference. In Information Processing: Proceed-
ings of the International Conference on Information Pro-
cessing, Paris, pp. 125–132. UNESCO.
Backus, J. W. (1996). Transcript of question and answer
session. In Wexelblat, R. L. (Ed.), History of Programming
Languages, p. 162. Academic Press.
Bahl, L. R. and Mercer, R. L. (1976). Part of speech assign-
ment by a statistical decision algorithm.
In Proceedings
IEEE International Symposium on Information Theory, pp.
88–89.
Bahl, L. R., Jelinek, F., and Mercer, R. L. (1983). A max-
imum likelihood approach to continuous speech recogni-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 5(2), 179–190.
Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). The
Berkeley FrameNet project.
In COLING/ACL-98, Mon-
treal, Canada, pp. 86–90.
Baker, J. K. (1975). The DRAGON system – An overview.
IEEE Transactions on Acoustics, Speech, and Signal Pro-
cessing, ASSP-23(1), 24–29.
Baker, J. K. (1975/1990). Stochastic modeling for auto-
matic speech understanding.
In Waibel, A. and Lee, K.-
F. (Eds.), Readings in Speech Recognition, pp. 297–307.
Morgan Kaufmann. Originally appeared in Speech Recog-
nition, Academic Press, 1975.
Baker, J. K. (1979). Trainable grammars for speech recog-
nition. In Klatt, D. H. and Wolf, J. J. (Eds.), Speech Com-
munication Papers for the 97th Meeting of the Acoustical
Society of America, pp. 547–550.
Banerjee, S. and Pedersen, T. (2003). Extended gloss over-
laps as a measure of semantic relatedness. In IJCAI 2003,
pp. 805–810.
Bangalore, S. and Joshi, A. K. (1999). Supertagging: An
approach to almost parsing. Computational Linguistics,
25(2), 237–265.
Banko, M., Cafarella, M., Soderland, S., Broadhead, M.,
and Etzioni, O. (2007). Open information extraction for
the web. In IJCAI, Vol. 7, pp. 2670–2676.

Bar-Hillel, Y. (1953). A quasi-arithmetical notation for syn-
tactic description. Language, 29, 47–58. Reprinted in Y.
Bar-Hillel. (1964). Language and Information: Selected
Essays on their Theory and Application, Addison-Wesley,
61–74.
Baum, L. E. (1972). An inequality and associated maxi-
mization technique in statistical estimation for probabilis-
tic functions of Markov processes.
In Shisha, O. (Ed.),
Inequalities III: Proceedings of the 3rd Symposium on In-
equalities, University of California, Los Angeles, pp. 1–8.
Academic Press.
Baum, L. E. and Eagon, J. A. (1967). An inequality with
applications to statistical estimation for probabilistic func-
tions of Markov processes and to a model for ecology. Bul-
letin of the American Mathematical Society, 73(3), 360–
363.
Baum, L. E. and Petrie, T. (1966). Statistical inference for
probabilistic functions of ﬁnite-state Markov chains. An-
nals of Mathematical Statistics, 37 (6), 1554–1563.
Baum, L. F. (1900). The Wizard of Oz. Available at Project
Gutenberg.
Bayes, T. (1763). An Essay Toward Solving a Problem in the
Doctrine of Chances, Vol. 53. Reprinted in Facsimiles of
Two Papers by Bayes, Hafner Publishing, 1963.
Bazell, C. E. (1952/1966). The correspondence fallacy in
structural linguistics. In Hamp, E. P., Householder, F. W.,
and Austerlitz, R. (Eds.), Studies by Members of the En-
glish Department, Istanbul University (3), reprinted in
Readings in Linguistics II (1966), pp. 271–298. University
of Chicago Press.
Bej ˇcek, E., Haji ˇcov ´a, E., Haji ˇc, J., J´ınov ´a, P., Kettnerov ´a,
V., Kol ´aˇrov ´a, V., Mikulov ´a, M., M´ırovsk ´y, J., Nedoluzhko,
A., Panevov ´a, J., Pol ´akov ´a, L., ˇSev ˇc´ıkov ´a, M., ˇSt ˇep ´anek,
J., and Zik ´anov ´a, ˇS. (2013). Prague dependency treebank
3.0. Tech. rep., Institute of Formal and Applied Linguis-
tics, Charles University in Prague. LINDAT/CLARIN dig-
ital library at Institute of Formal and Applied Linguistics,
Charles University in Prague.
Bellegarda, J. R. (1997). A latent semantic analysis frame-
work for large-span language modeling. In Eurospeech-97,
Rhodes, Greece.
Bellegarda, J. R. (2000). Exploiting latent semantic infor-
mation in statistical language modeling. Proceedings of the
IEEE, 89(8), 1279–1296.
Bellegarda, J. R. (2004). Statistical language model adap-
tation: Review and perspectives. Speech Communication,
42(1), 93–108.
Bellegarda, J. R. (2013). Natural language technology in
mobile devices: Two grounding frameworks.
In Mobile
Speech and Advanced Natural Language Solutions, pp.
185–196. Springer.
Bellman, R. (1957). Dynamic Programming. Princeton Uni-
versity Press.
Bellman, R. (1984). Eye of the Hurricane: an autobiogra-
phy. World Scientiﬁc Singapore.
Bengio, Y., Courville, A., and Vincent, P. (2013). Repre-
sentation learning: A review and new perspectives. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 35(8), 1798–1828.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C.
(2003). A neural probabilistic language model. Journal
of machine learning research, 3(Feb), 1137–1155.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
(2007). Greedy layer-wise training of deep networks. In
NIPS 2007, pp. 153–160.

Bengio, Y., Schwenk, H., Sen ´ecal, J.-S., Morin, F., and Gau-
vain, J.-L. (2006). Neural probabilistic language models. In
Innovations in Machine Learning, pp. 137–186. Springer.
Berant, J., Chou, A., Frostig, R., and Liang, P. (2013). Se-
mantic parsing on freebase from question-answer pairs. In
EMNLP 2013.
Berant, J. and Liang, P. (2014). Semantic parsing via para-
phrasing. In ACL 2014.
Berg-Kirkpatrick, T., Burkett, D., and Klein, D. (2012). An
empirical investigation of statistical signiﬁcance in NLP. In
EMNLP 2012, pp. 995–1005.
Berger, A., Della Pietra, S. A., and Della Pietra, V. J. (1996).
A maximum entropy approach to natural language process-
ing. Computational Linguistics, 22(1), 39–71.
Bergsma, S., Lin, D., and Goebel, R. (2008). Discriminative
learning of selectional preference from unlabeled text. In
EMNLP-08, pp. 59–68.
Bergsma, S., Lin, D., and Goebel, R. (2009). Web-scale n-
gram models for lexical disambiguation.. In IJCAI-09, pp.
1507–1512.
Bergsma, S., Pitler, E., and Lin, D. (2010). Creating robust
supervised classiﬁers via web-scale n-gram data. In ACL
2010, pp. 865–874.
Bethard, S. (2013). ClearTK-TimeML: A minimalist ap-
proach to TempEval 2013. In SemEval-13, pp. 10–14.
Bever, T. G. (1970). The cognitive basis for linguistic struc-
tures. In Hayes, J. R. (Ed.), Cognition and the Development
of Language, pp. 279–352. Wiley.
Bhat, I., Bhat, R. A., Shrivastava, M., and Sharma, D.
(2017). Joining hands: Exploiting monolingual treebanks
for parsing of code-mixing data. In EACL-17, pp. 324–330.
Biber, D., Johansson, S., Leech, G., Conrad, S., and Fine-
gan, E. (1999). Longman Grammar of Spoken and Written
English. Pearson ESL, Harlow.
Bies, A., Ferguson, M., Katz, K., and MacIntyre, R. (1995).
Bracketing guidelines for Treebank II style Penn Treebank
Project..
Bikel, D. M. (2004). Intricacies of Collins’ parsing model.
Computational Linguistics, 30(4), 479–511.
Bikel, D. M., Miller, S., Schwartz, R., and Weischedel,
R. (1997). Nymble: A high-performance learning name-
ﬁnder. In ANLP 1997, pp. 194–201.
Bird, S., Klein, E., and Loper, E. (2009). Natural Language
Processing with Python. O’Reilly.
Bisani, M. and Ney, H. (2004). Bootstrap estimates for
conﬁdence intervals in ASR performance evaluation.
In
ICASSP-04, Vol. I, pp. 409–412.
Bishop, C. M. (2006). Pattern recognition and machine
learning. Springer.
Bizer, C., Lehmann, J., Kobilarov, G., Auer, S., Becker,
C., Cyganiak, R., and Hellmann, S. (2009). DBpedia—A
crystallization point for the Web of Data. Web Semantics:
science, services and agents on the world wide web, 7 (3),
154–165.
Black, E. (1988). An experiment in computational discrim-
ination of English word senses. IBM Journal of Research
and Development, 32(2), 185–194.
Black, E., Abney, S. P., Flickinger, D., Gdaniec, C., Grish-
man, R., Harrison, P., Hindle, D., Ingria, R., Jelinek, F.,
Klavans, J. L., Liberman, M. Y., Marcus, M. P., Roukos,
S., Santorini, B., and Strzalkowski, T. (1991). A procedure
for quantitatively comparing the syntactic coverage of En-
glish grammars. In Proceedings DARPA Speech and Natu-
ral Language Workshop, Paciﬁc Grove, CA, pp. 306–311.

Bibliography

519

Black, E., Jelinek, F., Lafferty, J. D., Magerman, D. M.,
Mercer, R. L., and Roukos, S. (1992). Towards history-
based grammars: Using richer models for probabilistic
parsing. In Proceedings DARPA Speech and Natural Lan-
guage Workshop, Harriman, NY, pp. 134–139.
Blair, C. R. (1960). A program for correcting spelling errors.
Information and Control, 3, 60–67.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent
Dirichlet allocation.
Journal of Machine Learning Re-
search, 3(5), 993–1022.
Blodgett, S. L., Green, L., and O’Connor, B. (2016). Demo-
graphic dialectal variation in social media: A case study of
African-American English. In EMNLP 2016.
Blodgett, S. L. and O’Connor, B. (2017). Racial disparity in
natural language processing: A case study of social media
african-american english. In Fairness, Accountability, and
Transparency in Machine Learning (FAT/ML) Workshop,
KDD.
Bloomﬁeld, L. (1914). An Introduction to the Study of Lan-
guage. Henry Holt and Company.
Bloomﬁeld, L. (1933a). Language. University of Chicago
Press.
Bloomﬁeld, L. (1933b). Language. University of Chicago
Press.
Bobrow, D. G., Kaplan, R. M., Kay, M., Norman, D. A.,
Thompson, H., and Winograd, T. (1977). GUS, A frame
driven dialog system. Artiﬁcial Intelligence, 8, 155–173.
Bobrow, D. G. and Norman, D. A. (1975). Some princi-
ples of memory schemata. In Bobrow, D. G. and Collins,
A. (Eds.), Representation and Understanding. Academic
Press.
Bobrow, D. G. and Winograd, T. (1977). An overview of
KRL, a knowledge representation language. Cognitive Sci-
ence, 1(1), 3–46.
Bod, R. (1993). Using an annotated corpus as a stochastic
grammar. In EACL-93, pp. 37–44.
Boguraev, B. and Briscoe, T. (Eds.). (1989). Computational
Lexicography for Natural Language Processing. Longman.
Bohus, D. and Rudnicky, A. I. (2005). Sorry, I didn’t catch
that! — An investigation of non-understanding errors and
recovery strategies. In Proceedings of SIGDIAL, Lisbon,
Portugal.
Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T.
(2017). Enriching word vectors with subword information.
TACL, 5, 135–146.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and Tay-
lor, J. (2008). Freebase: a collaboratively created graph
database for structuring human knowledge.
In SIGMOD
2008, pp. 1247–1250.
Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and
Kalai, A. T. (2016). Man is to computer programmer as
woman is to homemaker? Debiasing word embeddings. In
NIPS 16, pp. 4349–4357.
Booth, T. L. (1969). Probabilistic representation of formal
languages. In IEEE Conference Record of the 1969 Tenth
Annual Symposium on Switching and Automata Theory, pp.
74–81.
Booth, T. L. and Thompson, R. A. (1973). Applying prob-
ability measures to abstract languages. IEEE Transactions
on Computers, C-22(5), 442–450.
Borges, J. L. (1964).
The analytical language of John
Wilkins. University of Texas Press. Trans. Ruth L. C.
Simms.
Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefow-
icz, R., and Bengio, S. (2016). Generating sentences from
a continuous space. In CoNLL-16, pp. 10–21.

520 Bibliography

Boyd-Graber, J., Blei, D. M., and Zhu, X. (2007). A topic
model for word sense disambiguation. In EMNLP/CoNLL
2007.
Boyd-Graber, J., Feng, S., and Rodriguez, P. (2018).
Human-computer question answering:
The case for
quizbowl. In Escalera, S. and Weimer, M. (Eds.), The NIPS
’17 Competition: Building Intelligent Systems. Springer
Verlag.
Brachman, R. J. (1979). On the epistemogical status of se-
mantic networks. In Findler, N. V. (Ed.), Associative Net-
works: Representation and Use of Knowledge by Comput-
ers, pp. 3–50. Academic Press.
Brachman, R. J. and Levesque, H. J. (Eds.). (1985). Read-
ings in Knowledge Representation. Morgan Kaufmann.
Brachman, R. J. and Schmolze, J. G. (1985). An overview
of the KL-ONE knowledge representation system. Cogni-
tive Science, 9(2), 171–216.
Brants, T. (2000). TnT: A statistical part-of-speech tagger.
In ANLP 2000, Seattle, WA, pp. 224–231.
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.
(2007). Large language models in machine translation. In
EMNLP/CoNLL 2007.
Br ´eal, M. (1897). Essai de S ´emantique: Science des signiﬁ-
cations. Hachette, Paris, France.
Bresnan, J. (Ed.). (1982). The Mental Representation of
Grammatical Relations. MIT Press.
Brill, E., Dumais, S. T., and Banko, M. (2002). An analy-
sis of the AskMSR question-answering system. In EMNLP
2002, pp. 257–264.
Brill, E. and Moore, R. C. (2000). An improved error model
for noisy channel spelling correction.
In ACL-00, Hong
Kong, pp. 286–293.
Brill, E. and Resnik, P. (1994). A rule-based approach
to prepositional phrase attachment disambiguation.
In
COLING-94, Kyoto, pp. 1198–1204.
Brin, S. (1998). Extracting patterns and relations from
the World Wide Web.
In Proceedings World Wide Web
and Databases International Workshop, Number 1590 in
LNCS, pp. 172–183. Springer.
Briscoe, T. and Carroll, J. (1993). Generalized probabilistic
LR parsing of natural language (corpora) with uniﬁcation-
based grammars. Computational Linguistics, 19(1), 25–59.
Brockmann, C. and Lapata, M. (2003). Evaluating and com-
bining approaches to selectional preference acquisition. In
EACL-03, pp. 27–34.
Brody, S. and Lapata, M. (2009). Bayesian word sense in-
duction. In EACL-09, pp. 103–111.
Broschart, J. (1997). Why Tongan does it differently. Lin-
guistic Typology, 1, 123–165.
Bruce, B. C. (1975). Generation as a social action. In Pro-
ceedings of TINLAP-1 (Theoretical Issues in Natural Lan-
guage Processing), pp. 64–67. Association for Computa-
tional Linguistics.
Bruce, R. F. and Wiebe, J. (1994). Word-sense disambigua-
tion using decomposable models. In ACL-94, Las Cruces,
NM, pp. 139–145.
Brysbaert, M., Warriner, A. B., and Kuperman, V. (2014).
Concreteness ratings for 40 thousand generally known en-
glish word lemmas. Behavior Research Methods, 46(3),
904–911.
Buchholz, S. and Marsi, E. (2006). Conll-x shared task on
multilingual dependency parsing. In CoNLL-06, pp. 149–
164.

Buck, C., Heaﬁeld, K., and Van Ooyen, B. (2014). N-gram
counts and language models from the common crawl. In
Proceedings of LREC.
Budanitsky, A. and Hirst, G. (2006). Evaluating WordNet-
based measures of lexical semantic relatedness. Computa-
tional Linguistics, 32(1), 13–47.
Bullinaria, J. A. and Levy, J. P. (2007). Extracting seman-
tic representations from word co-occurrence statistics: A
computational study. Behavior research methods, 39(3),
510–526.
Bullinaria, J. A. and Levy, J. P. (2012). Extracting se-
mantic representations from word co-occurrence statistics:
stop-lists, stemming, and svd. Behavior research methods,
44(3), 890–907.
Bulyko, I., Kirchhoff, K., Ostendorf, M., and Goldberg, J.
(2005). Error-sensitive response generation in a spoken
language dialogue system. Speech Communication, 45(3),
271–288.
Bulyko, I., Ostendorf, M., and Stolcke, A. (2003). Get-
ting more mileage from web text sources for conversational
speech language modeling using class-dependent mixtures.
In HLT-NAACL-03, Edmonton, Canada, Vol. 2, pp. 7–9.
Caliskan, A., Bryson, J. J., and Narayanan, A. (2017). Se-
mantics derived automatically from language corpora con-
tain human-like biases. Science, 356(6334), 183–186.
Cardie, C. (1993). A case-based approach to knowledge ac-
quisition for domain speciﬁc sentence analysis. In AAAI-
93, pp. 798–803. AAAI Press.
Cardie, C. (1994). Domain-Speciﬁc Knowledge Acquisition
for Conceptual Sentence Analysis. Ph.D. thesis, University
of Massachusetts, Amherst, MA. Available as CMPSCI
Technical Report 94-74.
Carletta, J., Isard, A., Isard, S., Kowtko, J. C., Doherty-
Sneddon, G., and Anderson, A. H. (1997). The reliability
of a dialogue structure coding scheme. Computational Lin-
guistics, 23(1), 13–32.
Carpenter, R. (2017). Cleverbot. http://www.cleverbot.com,
accessed 2017.
Carreras, X. and M `arquez, L. (2005).
Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
CoNLL-05, pp. 152–164.
Carroll, G. and Charniak, E. (1992). Two experiments on
learning probabilistic dependency grammars from corpora.
Tech. rep. CS-92-16, Brown University.
Carroll, J., Briscoe, T., and Sanﬁlippo, A. (1998). Parser
evaluation: A survey and a new proposal.
In LREC-98,
Granada, Spain, pp. 447–454.
Chambers, N. (2013). NavyTime: Event and time ordering
from raw text. In SemEval-13, pp. 73–77.
Chambers, N. and Jurafsky, D. (2010). Improving the use
of pseudo-words for evaluating selectional preferences. In
ACL 2010, pp. 445–453.
Chambers, N. and Jurafsky, D. (2011). Template-based in-
formation extraction without the templates. In ACL 2011.
Chang, A. X. and Manning, C. D. (2012). SUTime: A li-
brary for recognizing and normalizing time expressions..
In LREC-12, pp. 3735–3740.
Chang, P.-C., Galley, M., and Manning, C. D. (2008). Opti-
mizing Chinese word segmentation for machine translation
performance. In Proceedings of ACL Statistical MT Work-
shop, pp. 224–232.
Charniak, E. (1997). Statistical parsing with a context-free
grammar and word statistics.
In AAAI-97, pp. 598–603.
AAAI Press.

Charniak, E., Hendrickson, C.,
Jacobson, N.,
and
Perkowitz, M. (1993). Equations for part-of-speech tag-
ging. In AAAI-93, Washington, D.C., pp. 784–789. AAAI
Press.
Charniak, E. and Johnson, M. (2005). Coarse-to-ﬁne n-best
parsing and MaxEnt discriminative reranking. In ACL-05,
Ann Arbor.
Che, W., Li, Z., Li, Y., Guo, Y., Qin, B., and Liu, T.
(2009). Multilingual dependency-based syntactic and se-
mantic parsing. In CoNLL-09, pp. 49–54.
Chelba, C. and Jelinek, F. (2000). Structured language mod-
eling. Computer Speech and Language, 14, 283–332.
Chen, D., Fisch, A., Weston, J., and Bordes, A. (2017).
Reading wikipedia to answer open-domain questions.
In
ACL 2017.
Chen, D. and Manning, C. D. (2014). A fast and accurate de-
pendency parser using neural networks.. In EMNLP 2014,
pp. 740–750.
Chen, J. N. and Chang, J. S. (1998). Topical clustering
of MRD senses based on information retrieval techniques.
Computational Linguistics, 24(1), 61–96.
Chen, S. F. and Goodman, J. (1996). An empirical study of
smoothing techniques for language modeling. In ACL-96,
Santa Cruz, CA, pp. 310–318.
Chen, S. F. and Goodman, J. (1998). An empirical study of
smoothing techniques for language modeling. Tech. rep.
TR-10-98, Computer Science Group, Harvard University.
Chen, S. F. and Goodman, J. (1999). An empirical study of
smoothing techniques for language modeling. Computer
Speech and Language, 13, 359–394.
Chierchia, G. and McConnell-Ginet, S. (1991). Meaning
and Grammar. MIT Press.
Chinchor, N., Hirschman, L., and Lewis, D. L. (1993). Eval-
uating Message Understanding systems: An analysis of the
third Message Understanding Conference. Computational
Linguistics, 19(3), 409–449.
Chiticariu, L., Danilevsky, M., Li, Y., Reiss, F., and Zhu, H.
(2018). SystemT: Declarative text understanding for enter-
prise. In NAACL HLT 2018, Vol. 3, pp. 76–83.
Chiticariu, L., Li, Y., and Reiss, F. R. (2013). Rule-Based
Information Extraction is Dead! Long Live Rule-Based In-
formation Extraction Systems!. In EMNLP 2013, pp. 827–
832.
Chklovski, T. and Mihalcea, R. (2003). Exploiting agree-
ment and disagreement of human annotators for word sense
disambiguation. In RANLP 2003.
Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi,
Y., Liang, P., and Zettlemoyer, L. (2018). Quac: Question
answering in context. In EMNLP 2018.
Choi, J. D. and Palmer, M. (2011a). Getting the most out
of transition-based dependency parsing. In ACL 2011, pp.
687–692.
Choi, J. D. and Palmer, M. (2011b). Transition-based se-
mantic role labeling using predicate argument clustering.
In Proceedings of the ACL 2011 Workshop on Relational
Models of Semantics, pp. 37–45.
Choi, J. D., Tetreault, J., and Stent, A. (2015). It depends:
Dependency parser comparison using a web-based evalua-
tion tool. In ACL 2015, pp. 26–31.
Chomsky, N. (1956). Three models for the description of
language. IRE Transactions on Information Theory, 2(3),
113–124.
Chomsky, N. (1956/1975). The Logical Structure of Lin-
guistic Theory. Plenum.

Bibliography

521

Chomsky, N. (1957). Syntactic Structures. Mouton, The
Hague.
Chomsky, N. (1963). Formal properties of grammars.
In
Luce, R. D., Bush, R., and Galanter, E. (Eds.), Handbook
of Mathematical Psychology, Vol. 2, pp. 323–418. Wiley.
Chomsky, N. (1981). Lectures on Government and Binding.
Foris.
Christodoulopoulos, C., Goldwater, S., and Steedman, M.
(2010). Two decades of unsupervised POS induction: How
far have we come?. In EMNLP-10.
Chu, Y.-J. and Liu, T.-H. (1965). On the shortest arbores-
cence of a directed graph. Science Sinica, 14, 1396–1400.
Chu-Carroll, J. (1998). A statistical model for discourse act
recognition in dialogue interactions. In Chu-Carroll, J. and
Green, N. (Eds.), Applying Machine Learning to Discourse
Processing. Papers from the 1998 AAAI Spring Symposium.
Tech. rep. SS-98-01, pp. 12–17. AAAI Press.
Chu-Carroll, J. and Carpenter, B. (1999). Vector-based
natural language call routing. Computational Linguistics,
25(3), 361–388.
Chu-Carroll, J., Fan, J., Boguraev, B. K., Carmel, D.,
Sheinwald, D., and Welty, C. (2012). Finding needles in
the haystack: Search and candidate generation. IBM Jour-
nal of Research and Development, 56(3/4), 6:1–6:12.
Church, A. (1940). A formulation of a simple theory of
types. Journal of Symbolic Logic, 5, 56–68.
Church, K. W. and Gale, W. A. (1991). Probability scor-
ing for spelling correction. Statistics and Computing, 1(2),
93–103.
Church, K. W. (1980). On Memory Limitations in Natural
Language Processing Master’s thesis, MIT. Distributed by
the Indiana University Linguistics Club.
Church, K. W. (1988). A stochastic parts program and noun
phrase parser for unrestricted text. In ANLP 1988, pp. 136–
143.
Church, K. W. (1989). A stochastic parts program and noun
phrase parser for unrestricted text. In ICASSP-89, pp. 695–
698.
Church, K. W. (1994). Unix for Poets. Slides from 2nd
ELSNET Summer School and unpublished paper ms.
Church, K. W. and Gale, W. A. (1991). A comparison of
the enhanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Computer
Speech and Language, 5, 19–54.
Church, K. W. and Hanks, P. (1989). Word association
norms, mutual information, and lexicography. In ACL-89,
Vancouver, B.C., pp. 76–83.
Church, K. W. and Hanks, P. (1990). Word association
norms, mutual information, and lexicography. Computa-
tional Linguistics, 16(1), 22–29.
Church, K. W., Hart, T., and Gao, J. (2007). Compress-
ing trigram language models with Golomb coding.
In
EMNLP/CoNLL 2007, pp. 199–207.
Clark, A. (2001). The unsupervised induction of stochastic
context-free grammars using distributional clustering.
In
CoNLL-01.
Clark, C. and Gardner, M. (2018). Simple and effective
multi-paragraph reading comprehension. In ACL 2018.
Clark, E. (1987). The principle of contrast: A constraint on
language acquisition.
In MacWhinney, B. (Ed.), Mecha-
nisms of language acquisition, pp. 1–33. LEA.
Clark, H. H. (1996). Using Language. Cambridge Univer-
sity Press.
Clark, H. H. and Fox Tree, J. E. (2002). Using uh and um
in spontaneous speaking. Cognition, 84, 73–111.

522 Bibliography

Clark, H. H. and Marshall, C. (1981). Deﬁnite reference
and mutual knowledge. In Joshi, A. K., Webber, B. L., and
Sag, I. A. (Eds.), Elements of Discourse Understanding,
pp. 10–63. Cambridge.
Clark, H. H. and Schaefer, E. F. (1989). Contributing to
discourse. Cognitive Science, 13, 259–294.
Clark, H. H. and Wilkes-Gibbs, D. (1986). Referring as a
collaborative process. Cognition, 22, 1–39.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
Schoenick, C., and Tafjord, O. (2018). Think you have
solved question answering? Try ARC, the AI2 reasoning
challenge.. arXiv preprint arXiv:1803.05457.
Clark, S. and Curran, J. R. (2004). Parsing the WSJ using
CCG and log-linear models. In ACL-04, pp. 104–111.
Clark, S., Curran, J. R., and Osborne, M. (2003). Bootstrap-
ping pos taggers using unlabelled data. In CoNLL-03, pp.
49–55.
Coccaro, N. and Jurafsky, D. (1998). Towards better inte-
gration of semantic predictors in statistical language mod-
eling. In ICSLP-98, Sydney, Vol. 6, pp. 2403–2406.
Cohen, K. B. and Demner-Fushman, D. (2014). Biomedical
natural language processing. Benjamins.
Cohen, M. H., Giangola, J. P., and Balogh, J. (2004). Voice
User Interface Design. Addison-Wesley.
Cohen, P. R. and Perrault, C. R. (1979). Elements of a plan-
based theory of speech acts. Cognitive Science, 3(3), 177–
212.
Colby, K. M., Hilf, F. D., Weber, S., and Kraemer, H. C.
(1972). Turing-like indistinguishability tests for the valida-
tion of a computer simulation of paranoid processes. Arti-
ﬁcial Intelligence, 3, 199–221.
Colby, K. M., Weber, S., and Hilf, F. D. (1971). Artiﬁcial
paranoia. Artiﬁcial Intelligence, 2(1), 1–25.
Cole, R. A., Novick, D. G., Vermeulen, P. J. E., Sutton, S.,
Fanty, M., Wessels, L. F. A., de Villiers, J. H., Schalkwyk,
J., Hansen, B., and Burnett, D. (1997). Experiments with a
spoken dialogue system for taking the US census. Speech
Communication, 23, 243–260.
Collins, M. (1996). A new statistical parser based on bi-
gram lexical dependencies.
In ACL-96, Santa Cruz, CA,
pp. 184–191.
Collins, M. (1999). Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania, Philadelphia.
Collins, M. (2000). Discriminative reranking for natural lan-
guage parsing. In ICML 2000, Stanford, CA, pp. 175–182.
Collins, M. (2003a). Head-driven statistical models for nat-
ural language parsing. Computational Linguistics, 29(4),
589–637.
Collins, M. (2003b). Head-driven statistical models for nat-
ural language parsing. Computational Linguistics, 29(4),
589–637.
Collins, M., Haji ˇc, J., Ramshaw, L. A., and Tillmann, C.
(1999). A statistical parser for Czech. In ACL-99, College
Park, MA, pp. 505–512.
Collins, M. and Koo, T. (2005). Discriminative reranking
for natural language parsing. Computational Linguistics,
31(1), 25–69.
Collobert, R. and Weston, J. (2007). Fast semantic extrac-
tion using a novel neural network architecture. In ACL-07,
pp. 560–567.
Collobert, R. and Weston, J. (2008). A uniﬁed architec-
ture for natural language processing: Deep neural networks
with multitask learning. In ICML, pp. 160–167.

Collobert, R., Weston,
J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. (2011). Natural language
processing (almost) from scratch. The Journal of Machine
Learning Research, 12, 2493–2537.
Copestake, A. and Briscoe, T. (1995). Semi-productive pol-
ysemy and sense extension. Journal of Semantics, 12(1),
15–68.
Cottrell, G. W. (1985).
A Connectionist Approach to
Word Sense Disambiguation. Ph.D. thesis, University of
Rochester, Rochester, NY. Revised version published by
Pitman, 1989.
Cover, T. M. and Thomas, J. A. (1991). Elements of Infor-
mation Theory. Wiley.
Covington, M. (2001). A fundamental algorithm for depen-
dency parsing.
In Proceedings of the 39th Annual ACM
Southeast Conference, pp. 95–102.
Cox, D. (1969). Analysis of Binary Data. Chapman and
Hall, London.
Craven, M. and Kumlien, J. (1999). Constructing biolog-
ical knowledge bases by extracting information from text
sources. In ISMB-99, pp. 77–86.
Cruse, D. A. (2004). Meaning in Language: an Introduction
to Semantics and Pragmatics. Oxford University Press.
Second edition.
Cucerzan, S. and Brill, E. (2004). Spelling correction as an
iterative process that exploits the collective knowledge of
web users. In EMNLP 2004, Vol. 4, pp. 293–300.
Culicover, P. W. and Jackendoff, R. (2005). Simpler Syntax.
Oxford University Press.
Dagan, I., Marcus, S., and Markovitch, S. (1993). Contex-
tual word similarity and estimation from sparse data.
In
ACL-93, Columbus, Ohio, pp. 164–171.
Damerau, F. J. (1964). A technique for computer detection
and correction of spelling errors. Communications of the
ACM, 7 (3), 171–176.
Damerau, F. J. and Mays, E. (1989). An examination of un-
detected typing errors. Information Processing and Man-
agement, 25(6), 659–664.
Danieli, M. and Gerbino, E. (1995). Metrics for evaluating
dialogue strategies in a spoken language system. In Pro-
ceedings of the 1995 AAAI Spring Symposium on Empir-
ical Methods in Discourse Interpretation and Generation,
Stanford, CA, pp. 34–39. AAAI Press.
Das, S. R. and Chen, M. Y. (2001).
Yahoo!
for
Amazon: Sentiment parsing from small
talk on the
web. EFA 2001 Barcelona Meetings. Available at SSRN:
http://ssrn.com/abstract=276189.
Davidson, D. (1967). The logical form of action sentences.
In Rescher, N. (Ed.), The Logic of Decision and Action.
University of Pittsburgh Press.
Davidson, T., Warmsley, D., Macy, M., and Weber, I. (2017).
Automated hate speech detection and the problem of offen-
sive language. In ICWSM 2017.
Davies, M. (2012). Expanding horizons in historical linguis-
tics with the 400-million word Corpus of Historical Amer-
ican English. Corpora, 7 (2), 121–157.
Davis, E. (1990). Representations of Commonsense Knowl-
edge. Morgan Kaufmann.
de Marneffe, M.-C., Dozat, T., Silveira, N., Haverinen, K.,
Ginter, F., Nivre, J., and Manning, C. D. (2014). Univer-
sal stanford dependencies: A cross-linguistic typology.. In
LREC, Vol. 14, pp. 4585–92.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D.
(2006). Generating typed dependency parses from phrase
structure parses. In LREC-06.

de Marneffe, M.-C. and Manning, C. D. (2008). The stan-
ford typed dependencies representation. In Coling 2008:
Proceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pp. 1–8.
Deerwester, S. C., Dumais, S. T., Furnas, G. W., Harshman,
R. A., Landauer, T. K., Lochbaum, K. E., and Streeter, L.
(1988). Computer information retrieval using latent seman-
tic structure: Us patent 4,839,853..
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas,
G. W., and Harshman, R. A. (1990).
Indexing by latent
semantics analysis. JASIS, 41(6), 391–407.
DeJong, G. F. (1982). An overview of the FRUMP system.
In Lehnert, W. G. and Ringle, M. H. (Eds.), Strategies for
Natural Language Processing, pp. 149–176. Lawrence Erl-
baum.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977).
Maximum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society, 39(1),
1–21.
DeRose, S. J. (1988). Grammatical category disambiguation
by statistical optimization. Computational Linguistics, 14,
31–39.
Di Marco, A. and Navigli, R. (2013). Clustering and di-
versifying web search results with graph-based word sense
induction. Computational Linguistics, 39(3), 709–754.
Diab, M. and Resnik, P. (2002). An unsupervised method
for word sense tagging using parallel corpora. In ACL-02,
pp. 255–262.
Digman, J. M. (1990). Personality structure: Emergence of
the ﬁve-factor model. Annual Review of Psychology, 41(1),
417–440.
Do, Q. N. T., Bethard, S., and Moens, M.-F. (2017). Improv-
ing implicit semantic role labeling by predicting semantic
frame arguments. In IJCNLP-17.
Dolan, W. B. (1994). Word sense ambiguation: Clustering
related senses. In COLING-94, Kyoto, Japan, pp. 712–716.
dos Santos, C., Xiang, B., and Zhou, B. (2015). Classifying
relations by ranking with convolutional neural networks. In
ACL 2015.
Dowty, D. R. (1979). Word Meaning and Montague Gram-
mar. D. Reidel.
Dowty, D. R., Wall, R. E., and Peters, S. (1981). Introduc-
tion to Montague Semantics. D. Reidel.
Dozat, T., Qi, P., and Manning, C. D. (2017). Stanford’s
graph-based neural dependency parser at the conll 2017
shared task.
In Proceedings of the CoNLL 2017 Shared
Task, pp. 20–30.
Dror, R., Baumer, G., Bogomolov, M., and Reichart, R.
(2017). Replicability analysis for natural language process-
ing: Testing signiﬁcance with multiple datasets. TACL, 5,
471––486.
Duda, R. O. and Hart, P. E. (1973). Pattern Classiﬁcation
and Scene Analysis. John Wiley and Sons.
Earley, J. (1968). An Efﬁcient Context-Free Parsing Al-
gorithm. Ph.D. thesis, Carnegie Mellon University, Pitts-
burgh, PA.
Earley, J. (1970). An efﬁcient context-free parsing al-
gorithm. Communications of the ACM, 6(8), 451–455.
Reprinted in Grosz et al. (1986).
Edmonds, J. (1967). Optimum branchings. Journal of Re-
search of the National Bureau of Standards B, 71(4), 233–
240.
Efron, B. and Tibshirani, R. J. (1993). An introduction to
the bootstrap. CRC press.

Bibliography

523

Egghe, L. (2007). Untangling Herdan’s law and Heaps’ law:
Mathematical and informetric arguments. JASIST, 58(5),
702–709.
Eisner, J. (1996). Three new probabilistic models for de-
pendency parsing: An exploration. In COLING-96, Copen-
hagen, pp. 340–345.
Eisner, J. (2002). An interactive spreadsheet for teaching
the forward-backward algorithm.
In Proceedings of the
ACL Workshop on Effective Tools and Methodologies for
Teaching NLP and CL, pp. 10–18.
Ejerhed, E. I. (1988). Finding clauses in unrestricted text by
ﬁnitary and stochastic methods. In ANLP 1988, pp. 219–
227.
Ekman, P. (1999). Basic emotions.
In Dalgleish, T. and
Power, M. J. (Eds.), Handbook of Cognition and Emotion,
pp. 45–60. Wiley.
Elman, J. L. (1990). Finding structure in time. Cognitive
science, 14(2), 179–211.
Erk, K. (2007). A simple, similarity-based model for selec-
tional preferences. In ACL-07, pp. 216–223.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.-M.,
Shaked, T., Soderland, S., Weld, D. S., and Yates, A.
(2005). Unsupervised named-entity extraction from the
web: An experimental study. Artiﬁcial Intelligence, 165(1),
91–134.
Evans, N. (2000). Word classes in the world’s languages. In
Booij, G., Lehmann, C., and Mugdan, J. (Eds.), Morphol-
ogy: A Handbook on Inﬂection and Word Formation, pp.
708–732. Mouton.
Fader, A., Soderland, S., and Etzioni, O. (2011). Identifying
relations for open information extraction. In EMNLP-11,
pp. 1535–1545.
Fader, A., Zettlemoyer, L., and Etzioni, O.
(2013).
Paraphrase-driven learning for open question answering. In
ACL 2013, Soﬁa, Bulgaria, pp. 1608–1618.
Fano, R. M. (1961). Transmission of Information: A Statis-
tical Theory of Communications. MIT Press.
Fast, E., Chen, B., and Bernstein, M. S. (2016). Empath:
Understanding Topic Signals in Large-Scale Text. In CHI.
Feldman, J. A. and Ballard, D. H. (1982). Connectionist
models and their properties. Cognitive Science, 6, 205–
254.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical
Database. MIT Press.
Fensel, D., Hendler, J. A., Lieberman, H., and Wahlster,
W. (Eds.). (2003). Spinning the Semantic Web: Bring the
World Wide Web to its Full Potential. MIT Press, Cam-
bridge, MA.
Ferro, L., Gerber, L., Mani, I., Sundheim, B., and Wilson,
G. (2005). Tides 2005 standard for the annotation of tem-
poral expressions. Tech. rep., MITRE.
Ferrucci, D. A. (2012). Introduction to “This is Watson”.
IBM Journal of Research and Development, 56(3/4), 1:1–
1:15.
Fessler, L. (2017). We tested bots like Siri and Alexa to see
who would stand up to sexual harassment. In Quartz. Feb

22, 2017. https://qz.com/911681/.

Fikes, R. E. and Nilsson, N. J. (1971). STRIPS: A new ap-
proach to the application of theorem proving to problem
solving. Artiﬁcial Intelligence, 2, 189–208.
Fillmore, C. J. (1966). A proposal concerning english prepo-
sitions. In Dinneen, F. P. (Ed.), 17th annual Round Table.,
Vol. 17 of Monograph Series on Language and Linguistics,
pp. 19–34. Georgetown University Press, Washington D.C.

524 Bibliography

Fillmore, C. J. (1968). The case for case. In Bach, E. W.
and Harms, R. T. (Eds.), Universals in Linguistic Theory,
pp. 1–88. Holt, Rinehart & Winston.
Fillmore, C. J. (1985). Frames and the semantics of under-
standing. Quaderni di Semantica, VI (2), 222–254.
Fillmore, C. J. (2003). Valency and semantic roles: the con-
cept of deep structure case. In ´Agel, V., Eichinger, L. M.,
Eroms, H. W., Hellwig, P., Heringer, H. J., and Lobin, H.
(Eds.), Dependenz und Valenz: Ein internationales Hand-
buch der zeitgen ¨ossischen Forschung, chap. 36, pp. 457–
475. Walter de Gruyter.
Fillmore, C. J. (2012). Encounters with language. Compu-
tational Linguistics, 38(4), 701–718.
Fillmore, C. J. and Baker, C. F. (2009). A frames approach
to semantic analysis. In Heine, B. and Narrog, H. (Eds.),
The Oxford Handbook of Linguistic Analysis, pp. 313–340.
Oxford University Press.
Fillmore, C. J., Johnson, C. R., and Petruck, M. R. L. (2003).
Background to FrameNet. International journal of lexicog-
raphy, 16(3), 235–250.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E.,
Solan, Z., Wolfman, G., and Ruppin, E. (2002). Placing
search in context: The concept revisited. ACM Transac-
tions on Information Systems, 20(1), 116––131.
Firth, J. R. (1935). The technique of semantics. Transac-
tions of the philological society, 34(1), 36–73.
Firth, J. R. (1957). A synopsis of linguistic theory 1930–
1955. In Studies in Linguistic Analysis. Philological Soci-
ety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers of
J. R. Firth. Longman, Harlow.
Foland, Jr., W. R. and Martin, J. H. (2015). Dependency-
based semantic role labeling using convolutional neural
networks. In *SEM 2015), pp. 279–289.
Forbes-Riley, K. and Litman, D. J. (2011). Beneﬁts and
challenges of real-time uncertainty detection and adapta-
tion in a spoken dialogue computer tutor. Speech Commu-
nication, 53(9), 1115–1136.
Forchini, P. (2013). Using movie corpora to explore spoken
American English: Evidence from multi-dimensional anal-
ysis.
In Bamford, J., Cavalieri, S., and Diani, G. (Eds.),
Variation and Change in Spoken and Written Discourse:
Perspectives from corpus linguistics, pp. 123–136. Ben-
jamins.
Forney, Jr., G. D. (1973). The Viterbi algorithm. Proceed-
ings of the IEEE, 61(3), 268–278.
Francis, H. S., Gregory, M. L., and Michaelis, L. A. (1999).
Are lexical subjects deviant?.
In CLS-99. University of
Chicago.
Francis, W. N. and Ku ˇcera, H. (1982). Frequency Analysis
of English Usage. Houghton Mifﬂin, Boston.
Franz, A. (1997).
Independence assumptions considered
harmful. In ACL/EACL-97, Madrid, Spain, pp. 182–189.
Franz, A. and Brants, T. (2006). All our n-gram are be-

long to you. http://googleresearch.blogspot.com/
2006/08/all- our- n- gram- are- belong- to- you.
html.

Fraser, N. M. and Gilbert, G. N. (1991). Simulating speech
systems. Computer Speech and Language, 5, 81–99.
Fyshe, A., Wehbe, L., Talukdar, P. P., Murphy, B., and
Mitchell, T. M. (2015). A compositional and interpretable
semantic space. In NAACL HLT 2015.
Gabow, H. N., Galil, Z., Spencer, T., and Tarjan, R. E.
(1986). Efﬁcient algorithms for ﬁnding minimum spanning
trees in undirected and directed graphs. Combinatorica,
6(2), 109–122.

Gage, P. (1994). A new algorithm for data compression. The
C Users Journal, 12(2), 23–38.
Gale, W. A. and Church, K. W. (1994). What is wrong
with adding one?.
In Oostdijk, N. and de Haan, P.
(Eds.), Corpus-Based Research into Language, pp. 189–
198. Rodopi.
Gale, W. A., Church, K. W., and Yarowsky, D. (1992a). Es-
timating upper and lower bounds on the performance of
word-sense disambiguation programs. In ACL-92, Newark,
DE, pp. 249–256.
Gale, W. A., Church, K. W., and Yarowsky, D. (1992b). One
sense per discourse.
In Proceedings DARPA Speech and
Natural Language Workshop, pp. 233–237.
Gale, W. A., Church, K. W., and Yarowsky, D. (1992c).
Work on statistical methods for word sense disambigua-
tion. In Goldman, R. (Ed.), Proceedings of the 1992 AAAI
Fall Symposium on Probabilistic Approaches to Natural
Language.
Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018).
Word embeddings quantify 100 years of gender and eth-
nic stereotypes. Proceedings of the National Academy of
Sciences, 115(16), E3635–E3644.
Garside, R. (1987). The CLAWS word-tagging system. In
Garside, R., Leech, G., and Sampson, G. (Eds.), The Com-
putational Analysis of English, pp. 30–41. Longman.
Garside, R., Leech, G., and McEnery, A. (1997). Corpus
Annotation. Longman.
Gazdar, G., Klein, E., Pullum, G. K., and Sag, I. A. (1985).
Generalized Phrase Structure Grammar. Blackwell.
Gerber, M. and Chai, J. Y. (2010). Beyond nombank: A
study of implicit arguments for nominal predicates. In ACL
2010, pp. 1583–1592.
Gil, D. (2000). Syntactic categories, cross-linguistic varia-
tion and universal grammar. In Vogel, P. M. and Comrie,
B. (Eds.), Approaches to the Typology of Word Classes, pp.
173–216. Mouton.
Gildea, D. and Jurafsky, D. (2000). Automatic labeling of
semantic roles. In ACL-00, Hong Kong, pp. 512–520.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28(3), 245–
288.
Gildea, D. and Palmer, M. (2002). The necessity of syntac-
tic parsing for predicate argument recognition. In ACL-02,
Philadelphia, PA.
Gillick, L. and Cox, S. J. (1989). Some statistical issues
in the comparison of speech recognition algorithms.
In
ICASSP-89, pp. 532–535.
Ginzburg, J. and Sag, I. A. (2000). Interrogative Investiga-
tions: the Form, Meaning and Use of English Interroga-
tives. CSLI.
Giuliano, V. E. (1965). The interpretation of word as-
sociations.
In Stevens, M. E., Giuliano, V. E., and
Heilprin, L. B. (Eds.), Statistical Association Methods
For Mechanized Documentation. Symposium Proceed-
ings. Washington, D.C., USA, March 17, 1964, pp. 25–

32. https://nvlpubs.nist.gov/nistpubs/Legacy/
MP/nbsmiscellaneouspub269.pdf.

Giv ´on, T. (1990). Syntax: A Functional Typological Intro-
duction. John Benjamins.
Glennie, A. (1960). On the syntax machine and the construc-
tion of a universal compiler. Tech. rep. No. 2, Contr. NR
049-141, Carnegie Mellon University (at the time Carnegie
Institute of Technology), Pittsburgh, PA.
Godfrey, J., Holliman, E., and McDaniel, J. (1992).
SWITCHBOARD: Telephone speech corpus for research
and development. In ICASSP-92, San Francisco, pp. 517–
520.

In

Goffman, E. (1974). Frame analysis: An essay on the orga-
nization of experience. Harvard University Press.
Goldberg, J., Ostendorf, M., and Kirchhoff, K. (2003). The
impact of response wording in error correction subdialogs.
In ISCA Tutorial and Research Workshop on Error Han-
dling in Spoken Dialogue Systems.
Goldberg, Y. (2017). Neural Network Methods for Natu-
ral Language Processing, Vol. 10 of Synthesis Lectures on
Human Language Technologies. Morgan & Claypool.
Golding, A. R. and Roth, D. (1999). A Winnow based ap-
proach to context-sensitive spelling correction. Machine
Learning, 34(1-3), 107–130.
Gondek, D., Lally, A., Kalyanpur, A., Murdock, J. W.,
Dubou ´e, P. A., Zhang, L., Pan, Y., Qiu, Z., and Welty, C.
(2012). A framework for merging and ranking of answers
in deepqa.
IBM Journal of Research and Development,
56(3/4), 14:1–14:12.
Good, M. D., Whiteside, J. A., Wixon, D. R., and Jones, S. J.
(1984). Building a user-derived interface. Communications
of the ACM, 27 (10), 1032–1043.
Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep
Learning. MIT Press.
Goodman, J. (1997). Probabilistic feature grammars.
IWPT-97.
Goodman, J. (2006). A bit of progress in language mod-
eling: Extended version. Tech. rep. MSR-TR-2001-72,
Machine Learning and Applied Statistics Group, Microsoft
Research, Redmond, WA.
Goodwin, C. (1996). Transparent vision. In Ochs, E., Sche-
gloff, E. A., and Thompson, S. A. (Eds.), Interaction and
Grammar, pp. 370–404. Cambridge University Press.
Gould, J. D., Conti, J., and Hovanyecz, T. (1983). Compos-
ing letters with a simulated listening typewriter. Communi-
cations of the ACM, 26(4), 295–308.
Gould, J. D. and Lewis, C. (1985). Designing for usability:
Key principles and what designers think. Communications
of the ACM, 28(3), 300–311.
Gould, S. J. (1980). The Panda’s Thumb. Penguin Group.
Gravano, A., Hirschberg, J., and Be ˇnuˇs, ˇS. (2012). Afﬁrma-
tive cue words in task-oriented dialogue. Computational
Linguistics, 38(1), 1–39.
Green, B. F., Wolf, A. K., Chomsky, C., and Laughery, K.
(1961). Baseball: An automatic question answerer. In Pro-
ceedings of the Western Joint Computer Conference 19, pp.
219–224. Reprinted in Grosz et al. (1986).
Greene, B. B. and Rubin, G. M. (1971). Automatic gram-
matical tagging of English. Department of Linguistics,
Brown University, Providence, Rhode Island.
Greenwald, A. G., McGhee, D. E., and Schwartz, J. L. K.
(1998). Measuring individual differences in implicit cog-
nition: the implicit association test.. Journal of personality
and social psychology, 74(6), 1464–1480.
Grenager, T. and Manning, C. D. (2006). Unsupervised Dis-
covery of a Statistical Verb Lexicon. In EMNLP 2006.
Grishman, R. and Sundheim, B. (1995). Design of the
MUC-6 evaluation. In MUC-6, San Francisco, pp. 1–11.
Grosz, B. J. (1977). The Representation and Use of Focus in
Dialogue Understanding. Ph.D. thesis, University of Cali-
fornia, Berkeley.
Grosz, B. J. and Sidner, C. L. (1980). Plans for discourse.
In Cohen, P. R., Morgan, J., and Pollack, M. E. (Eds.), In-
tentions in Communication, pp. 417–444. MIT Press.
Gruber, J. S. (1965). Studies in Lexical Relations. Ph.D.
thesis, MIT.

Bibliography

525

Guindon, R. (1988). A multidisciplinary perspective on di-
alogue structure in user-advisor dialogues. In Guindon, R.
(Ed.), Cognitive Science and Its Applications for Human-
Computer Interaction, pp. 163–200. Lawrence Erlbaum.
Gusﬁeld, D. (1997). Algorithms on Strings, Trees, and Se-
quences: Computer Science and Computational Biology.
Cambridge University Press.
Guyon, I. and Elisseeff, A. (2003). An introduction to vari-
able and feature selection. The Journal of Machine Learn-
ing Research, 3, 1157–1182.
Haghighi, A. and Klein, D. (2006). Prototype-driven gram-
mar induction. In COLING/ACL 2006, pp. 881–888.
Haji ˇc, J. (1998). Building a Syntactically Annotated Cor-
pus: The Prague Dependency Treebank, pp. 106–132.
Karolinum.
Haji ˇc, J. (2000). Morphological tagging: Data vs. dictionar-
ies. In NAACL 2000. Seattle.
Haji ˇc, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart´ı, M. A., M `arquez, L., Meyers, A., Nivre, J., Pad ´o,
S., ˇSt ˇep ´anek, J., Stran ˇa ´k, P., Surdeanu, M., Xue, N., and
Zhang, Y. (2009). The conll-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages.
In
CoNLL-09, pp. 1–18.
Hakkani-T ¨ur, D., Oﬂazer, K., and T ¨ur, G. (2002). Statis-
tical morphological disambiguation for agglutinative lan-
guages. Journal of Computers and Humanities, 36(4), 381–
410.
Hakkani-T ¨ur, D., T ¨ur, G., Celikyilmaz, A., Chen, Y.-N.,
Gao, J., Deng, L., and Wang, Y.-Y. (2016). Multi-domain
joint semantic frame parsing using bi-directional rnn-lstm..
In INTERSPEECH, pp. 715–719.
Hale, J. (2001). A probabilistic earley parser as a psycholin-
guistic model. In NAACL 2001, pp. 159–166.
Hamilton, W. L., Clark, K., Leskovec, J., and Jurafsky, D.
(2016a). Inducing domain-speciﬁc sentiment lexicons from
unlabeled corpora. In EMNLP 2016.
Hamilton, W. L., Leskovec, J., and Jurafsky, D. (2016b).
Diachronic word embeddings reveal statistical laws of se-
mantic change. In ACL 2016.
Harabagiu, S., Pasca, M., and Maiorano, S. (2000). Exper-
iments with open-domain textual question answering.
In
COLING-00, Saarbr ¨ucken, Germany.
Harris, R. A. (2005). Voice Interaction Design: Crafting the
New Conversational Speech Systems. Morgan Kaufmann.
Harris, Z. S. (1946). From morpheme to utterance. Lan-
guage, 22(3), 161–183.
Harris, Z. S. (1954). Distributional structure. Word, 10,
146–162. Reprinted in J. Fodor and J. Katz, The Structure
of Language, Prentice Hall, 1964 and in Z. S. Harris, Pa-
pers in Structural and Transformational Linguistics, Rei-
del, 1970, 775–794.
Harris, Z. S. (1962). String Analysis of Sentence Structure.
Mouton, The Hague.
Hastie, T., Tibshirani, R. J., and Friedman, J. H. (2001). The
Elements of Statistical Learning. Springer.
Hatzivassiloglou, V. and McKeown, K. R. (1997). Predict-
ing the semantic orientation of adjectives. In ACL/EACL-
97, pp. 174–181.
Hatzivassiloglou, V. and Wiebe, J. (2000). Effects of adjec-
tive orientation and gradability on sentence subjectivity. In
COLING-00, pp. 299–305.
He, L., Lee, K., Lewis, M., and Zettlemoyer, L. (2017). Deep
semantic role labeling: What works and what’s next.
In
ACL 2017, pp. 473–483.

526 Bibliography

Heaﬁeld, K. (2011). KenLM: Faster and smaller language
model queries. In Workshop on Statistical Machine Trans-
lation, pp. 187–197.
Heaﬁeld, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.
(2013). Scalable modiﬁed Kneser-Ney language model es-
timation.. In ACL 2013, pp. 690–696.
Heaps, H. S. (1978). Information retrieval. Computational
and theoretical aspects. Academic Press.
Hearst, M. A. (1991). Noun homograph disambiguation.
In Proceedings of the 7th Conference of the University of
Waterloo Centre for the New OED and Text Research, pp.
1–19.
Hearst, M. A. (1992a). Automatic acquisition of hyponyms
from large text corpora. In COLING-92, Nantes, France.
Hearst, M. A. (1992b). Automatic acquisition of hyponyms
from large text corpora. In COLING-92, Nantes, France.
COLING.
Hearst, M. A. (1998). Automatic discovery of WordNet re-
lations.
In Fellbaum, C. (Ed.), WordNet: An Electronic
Lexical Database. MIT Press.
Heckerman, D., Horvitz, E., Sahami, M., and Dumais, S. T.
(1998). A bayesian approach to ﬁltering junk e-mail. In
Proceeding of AAAI-98 Workshop on Learning for Text
Categorization, pp. 55–62.
Heim, I. and Kratzer, A. (1998). Semantics in a Generative
Grammar. Blackwell Publishers, Malden, MA.
Hemphill, C. T., Godfrey, J., and Doddington, G. (1990).
The ATIS spoken language systems pilot corpus. In Pro-
ceedings DARPA Speech and Natural Language Workshop,
Hidden Valley, PA, pp. 96–101.
Henderson, P., Sinha, K., Angelard-Gontier, N., Ke, N. R.,
Fried, G., Lowe, R., and Pineau, J. (2017). Ethical chal-
lenges in data-driven dialogue systems. In AAAI/ACM AI
Ethics and Society Conference.
I., Kim, S. N., Kozareva, Z., Nakov, P.,
´O S ´eaghdha, D., Pad ´o, S., Pennacchiotti, M., Romano, L.,
and Szpakowicz, S. (2009). Semeval-2010 task 8: Multi-
way classiﬁcation of semantic relations between pairs of
nominals.
In Proceedings of the Workshop on Semantic
Evaluations: Recent Achievements and Future Directions,
pp. 94–99.
Hendrix, G. G., Thompson, C. W., and Slocum, J. (1973).
Language processing via canonical verbs and semantic
models. In Proceedings of IJCAI-73.
Herdan, G. (1960). Type-token mathematics. The Hague,
Mouton.
Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L.,
Kay, W., Suleyman, M., and Blunsom, P. (2015). Teaching
machines to read and comprehend. In Advances in Neural
Information Processing Systems, pp. 1693–1701.
Hill, F., Reichart, R., and Korhonen, A. (2015). Simlex-999:
Evaluating semantic models with (genuine) similarity esti-
mation. Computational Linguistics, 41(4), 665–695.
Hindle, D. and Rooth, M. (1990). Structural ambiguity and
lexical relations. In Proceedings DARPA Speech and Natu-
ral Language Workshop, Hidden Valley, PA, pp. 257–262.
Hindle, D. and Rooth, M. (1991). Structural ambiguity and
lexical relations. In ACL-91, Berkeley, CA, pp. 229–236.
Hinkelman, E. A. and Allen, J. (1989). Two constraints on
speech act ambiguity. In ACL-89, Vancouver, Canada, pp.
212–219.
Hinton, G. E. (1986). Learning distributed representations
of concepts. In COGSCI-86, pp. 1–12.

Hendrickx,

Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast
learning algorithm for deep belief nets. Neural computa-
tion, 18(7), 1527–1554.
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever,
I., and Salakhutdinov, R. R. (2012).
Improving neural
networks by preventing co-adaptation of feature detectors.
arXiv preprint arXiv:1207.0580.
Hirschberg, J., Litman, D. J., and Swerts, M. (2001). Iden-
tifying user corrections automatically in spoken dialogue
systems. In NAACL 2001.
Hirschman, L., Light, M., Breck, E., and Burger, J. D.
(1999). Deep Read: A reading comprehension system. In
ACL-99, pp. 325–332.
Hirschman, L. and Pao, C. (1993). The cost of errors in a
spoken language system. In EUROSPEECH-93, pp. 1419–
1422.
Hirst, G. (1987). Semantic Interpretation and the Resolution
of Ambiguity. Cambridge University Press.
Hirst, G. (1988). Resolving lexical ambiguity computa-
tionally with spreading activation and polaroid words. In
Small, S. L., Cottrell, G. W., and Tanenhaus, M. K. (Eds.),
Lexical Ambiguity Resolution, pp. 73–108. Morgan Kauf-
mann.
Hirst, G. and Budanitsky, A. (2005). Correcting real-word
spelling errors by restoring lexical cohesion. Natural Lan-
guage Engineering, 11, 87–111.
Hirst, G. and Charniak, E. (1982). Word sense and case slot
disambiguation. In AAAI-82, pp. 95–98.
Hjelmslev, L. (1969). Prologomena to a Theory of Lan-
guage. University of Wisconsin Press. Translated by Fran-
cis J. Whitﬁeld; original Danish edition 1943.
Hobbs, J. R., Appelt, D. E., Bear, J., Israel, D., Kameyama,
M., Stickel, M. E., and Tyson, M. (1997). FASTUS: A
cascaded ﬁnite-state transducer for extracting information
from natural-language text.
In Roche, E. and Schabes,
Y. (Eds.), Finite-State Language Processing, pp. 383–406.
MIT Press.
Hockenmaier, J. and Steedman, M. (2007). Ccgbank: a cor-
pus of ccg derivations and dependency structures extracted
from the penn treebank. Computational Linguistics, 33(3),
355–396.
Hofmann, T. (1999). Probabilistic latent semantic indexing.
In SIGIR-99, Berkeley, CA.
Hopcroft, J. E. and Ullman, J. D. (1979).
Introduction to
Automata Theory, Languages, and Computation. Addison-
Wesley.
Horning, J. J. (1969). A Study of Grammatical Inference.
Ph.D. thesis, Stanford University.
Householder, F. W. (1995). Dionysius Thrax, the technai,
and Sextus Empiricus. In Koerner, E. F. K. and Asher, R. E.
(Eds.), Concise History of the Language Sciences, pp. 99–
103. Elsevier Science.
Hovy, E. H., Hermjakob, U., and Ravichandran, D. (2002).
A question/answer typology with surface text patterns. In
HLT-01.
Hovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A.,
and Weischedel, R. (2006). Ontonotes: The 90% solution.
In HLT-NAACL-06.
Hsu, B.-J. (2007). Generalized linear interpolation of lan-
guage models. In IEEE ASRU-07, pp. 136–140.
Hu, M. and Liu, B. (2004a). Mining and summarizing cus-
tomer reviews. In KDD, pp. 168–177.
Hu, M. and Liu, B. (2004b). Mining and summarizing cus-
tomer reviews. In SIGKDD-04.

Huang, E. H., Socher, R., Manning, C. D., and Ng, A. Y.
(2012). Improving word representations via global context
and multiple word prototypes. In ACL 2012, pp. 873–882.
Huang, L. and Chiang, D. (2005). Better k-best parsing. In
IWPT-05, pp. 53–64.
Huang, L. and Sagae, K. (2010). Dynamic programming for
linear-time incremental parsing. In ACL 2010, pp. 1077–
1086.
Huang, Z., Xu, W., and Yu, K. (2015). Bidirectional LSTM-
CRF models for sequence tagging.
In arXiv preprint
arXiv:1508.01991.
Huddleston, R. and Pullum, G. K. (2002). The Cambridge
Grammar of the English Language. Cambridge University
Press.
Hudson, R. A. (1984). Word Grammar. Blackwell.
Huffman, S. (1996). Learning information extraction pat-
terns from examples.
In Wertmer, S., Riloff, E., and
Scheller, G. (Eds.), Connectionist, Statistical, and Sym-
bolic Approaches to Learning Natural Language Process-
ing, pp. 246–260. Springer.
Hutto, C. J., Folds, D., and Appling, S. (2015). Compu-
tationally detecting and quantifying the degree of bias in
sentence-level text of news stories. In HUSO 2015: The
First International Conference on Human and Social Ana-
lytics.
Hymes, D. (1974). Ways of speaking.
In Bauman, R.
and Sherzer, J. (Eds.), Explorations in the ethnography of
speaking, pp. 433–451. Cambridge University Press.
Iacobacci, I., Pilehvar, M. T., and Navigli, R. (2016). Em-
beddings for word sense disambiguation: An evaluation
study. In ACL 2016, pp. 897–907.
Irons, E. T. (1961). A syntax directed compiler for ALGOL
60. Communications of the ACM, 4, 51–55.
Isbell, C. L., Kearns, M., Kormann, D., Singh, S., and Stone,
P. (2000). Cobot in LambdaMOO: A social statistics agent.
In AAAI/IAAI, pp. 36–41.
ISO8601 (2004). Data elements and interchange formats—
information interchange—representation of dates and
times. Tech. rep., International Organization for Standards
(ISO).
Jackendoff, R. (1983). Semantics and Cognition. MIT
Press.
Jacobs, P. S. and Rau, L. F. (1990). SCISOR: A system
for extracting information from on-line news. Communi-
cations of the ACM, 33(11), 88–97.
Jaech, A., Mulcaire, G., Hathi, S., Ostendorf, M., and Smith,
N. A. (2016). Hierarchical character-word models for lan-
guage identiﬁcation. In ACL Workshop on NLP for Social
Media, pp. 84––93.
Jafarpour, S., Burges, C. J. C., and Ritter, A. (2009). Fil-
ter, rank, and transfer the knowledge: Learning to chat.
In NIPS Workshop on Advances in Ranking, Vancouver,
Canada.
Jauhiainen, T., Lui, M., Zampieri, M., Baldwin, T., and
Lind ´en, K. (2018). Automatic language identiﬁcation in
texts: A survey. arXiv preprint arXiv:1804.08186.
Jefferson, G. (1972). Side sequences. In Sudnow, D. (Ed.),
Studies in social interaction, pp. 294–333. Free Press, New
York.
Jefferson, G. (1984). Notes on a systematic deployment of
the acknowledgement tokens ‘yeah’ and ‘mm hm’. Papers
in Linguistics, 17 (2), 197–216.
Jeffreys, H. (1948). Theory of Probability (2nd Ed.). Claren-
don Press. Section 3.23.

Bibliography

527

Jekat, S., Klein, A., Maier, E., Maleck, I., Mast, M., and
Quantz, J. (1995). Dialogue acts in verbmobil. Verbmobil–
Report–65–95.
Jelinek, F. (1976). Continuous speech recognition by statis-
tical methods. Proceedings of the IEEE, 64(4), 532–557.
Jelinek, F. (1990). Self-organized language modeling for
speech recognition.
In Waibel, A. and Lee, K.-F. (Eds.),
Readings in Speech Recognition, pp. 450–506. Morgan
Kaufmann. Originally distributed as IBM technical report
in 1985.
Jelinek, F. (1997). Statistical Methods for Speech Recogni-
tion. MIT Press.
Jelinek, F. and Lafferty, J. D. (1991). Computation of
the probability of initial substring generation by stochastic
context-free grammars. Computational Linguistics, 17 (3),
315–323.
Jelinek, F., Lafferty, J. D., Magerman, D. M., Mercer, R. L.,
Ratnaparkhi, A., and Roukos, S. (1994). Decision tree pars-
ing using a hidden derivation model. In ARPA Human Lan-
guage Technologies Workshop, Plainsboro, N.J., pp. 272–
277.
Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation
of Markov source parameters from sparse data. In Gelsema,
E. S. and Kanal, L. N. (Eds.), Proceedings, Workshop on
Pattern Recognition in Practice, pp. 381–397. North Hol-
land.
Ji, H., Grishman, R., and Dang, H. T. (2010). Overview of
the tac 2011 knowledge base population track. In TAC-11.
Jiang, J. J. and Conrath, D. W. (1997). Semantic similarity
based on corpus statistics and lexical taxonomy.
In RO-
CLING X, Taiwan.
Jim ´enez, V. M. and Marzal, A. (2000). Computation of the
n best parse trees for weighted and stochastic context-free
grammars. In Advances in Pattern Recognition: Proceed-
ings of the Joint IAPR International Workshops, SSPR 2000
and SPR 2000, Alicante, Spain, pp. 183–192. Springer.
Johnson, M. (1998). PCFG models of linguistic tree repre-
sentations. Computational Linguistics, 24(4), 613–632.
Johnson, M. (2001). Joint and conditional estimation of tag-
ging and parsing models. In ACL-01, pp. 314–321.
Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S.
(1999). Estimators for stochastic “uniﬁcation-based” gram-
mars. In ACL-99, pp. 535–541.
Johnson, W. E. (1932). Probability: deductive and inductive
problems (appendix to). Mind, 41(164), 421–423.
Johnson-Laird, P. N. (1983). Mental Models. Harvard Uni-
versity Press, Cambridge, MA.
Jones, M. P. and Martin, J. H. (1997). Contextual spelling
correction using latent semantic analysis. In ANLP 1997,
Washington, D.C., pp. 166–173.
Jones, R., McCallum, A., Nigam, K., and Riloff, E. (1999).
Bootstrapping for text learning tasks. In IJCAI-99 Work-
shop on Text Mining: Foundations, Techniques and Appli-
cations.
Jones, T. (2015). Toward a description of African American
Vernacular English dialect regions using “Black Twitter”.
American Speech, 90(4), 403–440.
Joos, M. (1950). Description of language design. JASA, 22,
701–708.
Joshi, A. K. (1985). Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable struc-
tural descriptions?.
In Dowty, D. R., Karttunen, L., and
Zwicky, A. (Eds.), Natural Language Parsing, pp. 206–
250. Cambridge University Press.
Joshi, A. K. and Hopely, P. (1999). A parser from antiq-
uity. In Kornai, A. (Ed.), Extended Finite State Models of
Language, pp. 6–15. Cambridge University Press.

528 Bibliography

Joshi, A. K. and Srinivas, B. (1994). Disambiguation of
super parts of speech (or supertags): Almost parsing.
In
COLING-94, Kyoto, pp. 154–160.
Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.
(2017). Triviaqa: A large scale distantly supervised chal-
lenge dataset for reading comprehension. In ACL 2017.
Jurafsky, D. (2014). The Language of Food. W. W. Norton,
New York.
Jurafsky, D., Chahuneau, V., Routledge, B. R., and Smith,
N. A. (2014). Narrative framing of consumer sentiment in
online restaurant reviews. First Monday, 19(4).
Jurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stol-
cke, A., Fosler, E., and Morgan, N. (1994). The Berke-
ley restaurant project. In ICSLP-94, Yokohama, Japan, pp.
2139–2142.
Jurgens, D. and Klapaftis, I. P. (2013).
Semeval-2013
task 13: Word sense induction for graded and non-graded
senses. In *SEM, pp. 290–299.
Jurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). Incorpo-
rating dialectal variability for socially equitable language
identiﬁcation. In ACL 2017, pp. 51–57.
Justeson, J. S. and Katz, S. M. (1991). Co-occurrences of
antonymous adjectives and their contexts. Computational
linguistics, 17 (1), 1–19.
Kalyanpur, A., Boguraev, B. K., Patwardhan, S., Murdock,
J. W., Lally, A., Welty, C., Prager, J. M., Coppola, B.,
Fokoue-Nkoutche, A., Zhang, L., Pan, Y., and Qiu, Z. M.
(2012). Structured data and inference in deepqa. IBM Jour-
nal of Research and Development, 56(3/4), 10:1–10:14.
Kang, J. S., Feng, S., Akoglu, L., and Choi, Y. (2014).
Connotationwordnet: Learning connotation over the word+
sense network. In ACL 2014.
Kannan, A. and Vinyals, O. (2016). Adversarial evaluation
of dialogue models. In NIPS 2016 Workshop on Adversar-
ial Training.
Kaplan, R. M. (1973). A general syntactic processor.
In
Rustin, R. (Ed.), Natural Language Processing, pp. 193–
241. Algorithmics Press.
Kaplan, R. M., Riezler, S., King, T. H., Maxwell III, J. T.,
Vasserman, A., and Crouch, R. (2004). Speed and accuracy
in shallow and deep stochastic parsing. In HLT-NAACL-04.
Karlsson, F., Voutilainen, A., Heikkil ¨a, J., and Anttila,
A. (Eds.). (1995). Constraint Grammar: A Language-
Independent System for Parsing Unrestricted Text. Mouton
de Gruyter.
Karttunen, L. (1999). Comments on Joshi. In Kornai, A.
(Ed.), Extended Finite State Models of Language, pp. 16–
18. Cambridge University Press.
Kasami, T. (1965). An efﬁcient recognition and syntax
analysis algorithm for context-free languages. Tech. rep.
AFCRL-65-758, Air Force Cambridge Research Labora-
tory, Bedford, MA.
Kashyap, R. L. and Oommen, B. J. (1983). Spelling cor-
rection using probabilistic methods. Pattern Recognition
Letters, 2, 147–154.
Katz, J. J. and Fodor, J. A. (1963). The structure of a seman-
tic theory. Language, 39, 170–210.
Kawamoto, A. H. (1988). Distributed representations of
ambiguous words and their resolution in connectionist net-
works. In Small, S. L., Cottrell, G. W., and Tanenhaus, M.
(Eds.), Lexical Ambiguity Resolution, pp. 195–228. Mor-
gan Kaufman.
Kay, M. (1967). Experiments with a powerful parser.
In
Proc. 2eme Conference Internationale sur le Traitement
Automatique des Langues, Grenoble.

Kay, M. (1973). The MIND system.
In Rustin, R. (Ed.),
Natural Language Processing, pp. 155–188. Algorithmics
Press.
Kay, M. (1982). Algorithm schemata and data structures in
syntactic processing. In All ´en, S. (Ed.), Text Processing:
Text Analysis and Generation, Text Typology and Attribu-
tion, pp. 327–358. Almqvist and Wiksell, Stockholm.
Kay, P. and Fillmore, C. J. (1999). Grammatical construc-
tions and linguistic generalizations: The What’s X Doing
Y? construction. Language, 75(1), 1–33.
Keller, F. and Lapata, M. (2003). Using the web to obtain
frequencies for unseen bigrams. Computational Linguis-
tics, 29, 459–484.
Kelly, E. F. and Stone, P. J. (1975). Computer Recognition
of English Word Senses. North-Holland.
Kernighan, M. D., Church, K. W., and Gale, W. A. (1990).
A spelling correction program base on a noisy channel
model. In COLING-90, Helsinki, Vol. II, pp. 205–211.
Kiela, D. and Clark, S. (2014). A systematic study of seman-
tic vector space model parameters. In Proceedings of the
EACL 2nd Workshop on Continuous Vector Space Models
and their Compositionality (CVSC), pp. 21–30.
Kilgarriff, A. (2001). English lexical sample task descrip-
tion. In Proceedings of Senseval-2: Second International
Workshop on Evaluating Word Sense Disambiguation Sys-
tems, Toulouse, France, pp. 17–20.
Kilgarriff, A. and Palmer, M. (Eds.). (2000). Computing
and the Humanities: Special Issue on SENSEVAL, Vol. 34.
Kluwer.
Kilgarriff, A. and Rosenzweig, J. (2000). Framework and
results for English SENSEVAL. Computers and the Hu-
manities, 34, 15–48.
Kim, S. M. and Hovy, E. H. (2004). Determining the senti-
ment of opinions. In COLING-04.
Kingma, D. and Ba, J. (2015). Adam: A method for stochas-
tic optimization. In ICLR 2015.
Kintsch, W. (1974). The Representation of Meaning in
Memory. Wiley, New York.
Kipper, K., Dang, H. T., and Palmer, M. (2000). Class-based
construction of a verb lexicon. In AAAI-00, Austin, TX, pp.
691–696.
Kleene, S. C. (1951). Representation of events in nerve nets
and ﬁnite automata. Tech. rep. RM-704, RAND Corpora-
tion. RAND Research Memorandum.
Kleene, S. C. (1956). Representation of events in nerve
nets and ﬁnite automata.
In Shannon, C. and McCarthy,
J. (Eds.), Automata Studies, pp. 3–41. Princeton University
Press.
Klein, D. (2005). The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford University.
Klein, D. and Manning, C. D. (2001). Parsing and hyper-
graphs. In IWPT-01, pp. 123–134.
Klein, D. and Manning, C. D. (2002). A generative
constituent-context model for improved grammar induc-
tion. In ACL-02.
Klein, D. and Manning, C. D. (2003a). A* parsing: Fast
exact Viterbi parse selection. In HLT-NAACL-03.
Klein, D. and Manning, C. D. (2003b). Accurate unlexical-
ized parsing. In HLT-NAACL-03.
Klein, D. and Manning, C. D. (2004). Corpus-based induc-
tion of syntactic structure: Models of dependency and con-
stituency. In ACL-04, pp. 479–486.
Klein, S. and Simmons, R. F. (1963). A computational ap-
proach to grammatical coding of English words. Journal of
the Association for Computing Machinery, 10(3), 334–347.

Kneser, R. and Ney, H. (1995). Improved backing-off for
M-gram language modeling.
In ICASSP-95, Vol. 1, pp.
181–184.
Knuth, D. E. (1973). Sorting and Searching: The Art of
Computer Programming Volume 3. Addison-Wesley.
Ko ˇcisk `y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,
K. M., Melis, G., and Grefenstette, E. (2018). The Narra-
tiveQA reading comprehension challenge. TACL, 6, 317–
328.
Krovetz, R. (1993). Viewing morphology as an inference
process. In SIGIR-93, pp. 191–202.
Krovetz, R. (1998). More than one sense per discourse. In
Proceedings of the ACL-SIGLEX SENSEVAL Workshop.
Kruskal, J. B. (1983). An overview of sequence compari-
son. In Sankoff, D. and Kruskal, J. B. (Eds.), Time Warps,
String Edits, and Macromolecules: The Theory and Prac-
tice of Sequence Comparison, pp. 1–44. Addison-Wesley.
Kudo, T. and Matsumoto, Y. (2002). Japanese dependency
analysis using cascaded chunking. In CoNLL-02, pp. 63–
69.
Kukich, K. (1992). Techniques for automatically correcting
words in text. ACM Computing Surveys, 24(4), 377–439.
Kullback, S. and Leibler, R. A. (1951). On information and
sufﬁciency. Annals of Mathematical Statistics, 22, 79–86.
Kuno, S. (1965). The predictive analyzer and a path elimi-
nation technique. Communications of the ACM, 8(7), 453–
462.
Kuno, S. and Oettinger, A. G. (1963). Multiple-path syn-
tactic analyzer.
In Popplewell, C. M. (Ed.), Information
Processing 1962: Proceedings of the IFIP Congress 1962,
Munich, pp. 306–312. North-Holland. Reprinted in Grosz
et al. (1986).
Kupiec, J. (1992). Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language,
6, 225–242.
Ku ˇcera, H. and Francis, W. N. (1967). Computational Anal-
ysis of Present-Day American English. Brown University
Press, Providence, RI.
Labov, W. and Fanshel, D. (1977). Therapeutic Discourse.
Academic Press.
Lafferty, J. D., McCallum, A., and Pereira, F. C. N. (2001).
Conditional random ﬁelds: Probabilistic models for seg-
menting and labeling sequence data. In ICML 2001, Stan-
ford, CA.
Lafferty, J. D., Sleator, D., and Temperley, D. (1992).
Grammatical trigrams: A probabilistic model of link gram-
mar. In Proceedings of the 1992 AAAI Fall Symposium on
Probabilistic Approaches to Natural Language.
Lakoff, G. (1965). On the Nature of Syntactic Irregularity.
Ph.D. thesis, Indiana University. Published as Irregularity
in Syntax. Holt, Rinehart, and Winston, New York, 1970.
Lakoff, G. (1972). Linguistics and natural logic. In David-
son, D. and Harman, G. (Eds.), Semantics for Natural Lan-
guage, pp. 545–665. D. Reidel.
Lakoff, G. and Johnson, M. (1980). Metaphors We Live By.
University of Chicago Press, Chicago, IL.
Lally, A., Prager, J. M., McCord, M. C., Boguraev, B. K.,
Patwardhan, S., Fan, J., Fodor, P., and Chu-Carroll, J.
(2012). Question analysis: How Watson reads a clue. IBM
Journal of Research and Development, 56(3/4), 2:1–2:14.
Lample, G., Ballesteros, M., Subramanian, S., Kawakami,
K., and Dyer, C. (2016). Neural architectures for named
entity recognition. In NAACL HLT 2016.

Bibliography

529

Landauer, T. K. (Ed.). (1995). The Trouble with Computers:
Usefulness, Usability, and Productivity. MIT Press.
Landauer, T. K. and Dumais, S. T. (1997). A solution to
Plato’s problem: The Latent Semantic Analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104, 211–240.
Landes, S., Leacock, C., and Tengi, R. I. (1998). Building
semantic concordances. In Fellbaum, C. (Ed.), WordNet:
An Electronic Lexical Database, pp. 199–216. MIT Press.
Lang, J. and Lapata, M. (2014). Similarity-driven semantic
role induction via graph partitioning. Computational Lin-
guistics, 40(3), 633–669.
Lapata, M. and Keller, F. (2004). The web as a base-
line: Evaluating the performance of unsupervised web-
based models for a range of NLP tasks. In HLT-NAACL-04.
Lapesa, G. and Evert, S. (2014). A large scale evaluation
of distributional semantic models: Parameters, interactions
and model selection. TACL, 2, 531–545.
Lari, K. and Young, S. J. (1990). The estimation of stochas-
tic context-free grammars using the Inside-Outside algo-
rithm. Computer Speech and Language, 4, 35–56.
Lau, J. H., Cook, P., McCarthy, D., Newman, D., and Bald-
win, T. (2012). Word sense induction for novel sense de-
tection. In EACL-12, pp. 591–601.
Leacock, C. and Chodorow, M. S. (1998). Combining lo-
cal context and WordNet similarity for word sense identi-
ﬁcation.
In Fellbaum, C. (Ed.), WordNet: An Electronic
Lexical Database, pp. 265–283. MIT Press.
Leacock, C., Towell, G., and Voorhees, E. M. (1993).
Corpus-based statistical sense resolution. In HLT-93, pp.
260–265.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,
R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropa-
gation applied to handwritten zip code recognition. Neural
computation, 1(4), 541–551.
LeCun, Y., Boser, B. E., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W. E., and Jackel, L. D. (1990).
Handwritten digit recognition with a back-propagation net-
work. In NIPS 1990, pp. 396–404.
Lee, D. D. and Seung, H. S. (1999). Learning the parts
of objects by non-negative matrix factorization. Nature,
401(6755), 788–791.
Lee, K., Salant, S., Kwiatkowski, T., Parikh, A., Das, D., and
Berant, J. (2017). Learning recurrent span representations
for extractive question answering. In arXiv 1611.01436.
Lehnert, W. G., Cardie, C., Fisher, D., Riloff, E., and
Williams, R. (1991). Description of the CIRCUS system
as used for MUC-3. In Sundheim, B. (Ed.), MUC-3, pp.
223–233.
Lemon, O., Georgila, K., Henderson, J., and Stuttle, M.
(2006). An ISU dialogue system exhibiting reinforcement
learning of dialogue policies: Generic slot-ﬁlling in the
TALK in-car system. In EACL-06.
Lesk, M. E. (1986). Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In Proceedings of the 5th Inter-
national Conference on Systems Documentation, Toronto,
CA, pp. 24–26.
Leuski, A. and Traum, D. (2011). NPCEditor: Creating
virtual human dialogue using information retrieval tech-
niques. AI Magazine, 32(2), 42–56.
Levenshtein, V. I. (1966). Binary codes capable of cor-
recting deletions, insertions, and reversals. Cybernetics
and Control Theory, 10(8), 707–710. Original in Doklady
Akademii Nauk SSSR 163(4): 845–848 (1965).

530 Bibliography

Levesque, H. J., Cohen, P. R., and Nunes, J. H. T. (1990).
On acting together. In AAAI-90, Boston, MA, pp. 94–99.
Morgan Kaufmann.
Levin, B. (1977). Mapping sentences to case frames. Tech.
rep. 167, MIT AI Laboratory. AI Working Paper 143.
Levin, B. (1993). English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
Levin, B. and Rappaport Hovav, M. (2005). Argument Real-
ization. Cambridge University Press.
Levin, E., Pieraccini, R., and Eckert, W. (2000). A stochas-
tic model of human-machine interaction for learning dialog
strategies. IEEE Transactions on Speech and Audio Pro-
cessing, 8, 11–23.
Levinson, S. C. (1983). Conversational Analysis, chap. 6.
Cambridge University Press.
Levow, G.-A. (1998). Characterizing and recognizing spo-
ken corrections in human-computer dialogue. In COLING-
ACL, pp. 736–742.
Levy, O. and Goldberg, Y. (2014a). Dependency-based word
embeddings. In ACL 2014.
Levy, O. and Goldberg, Y. (2014b). Linguistic regularities
in sparse and explicit word representations. In CoNLL-14.
Levy, O. and Goldberg, Y. (2014c). Neural word embedding
as implicit matrix factorization.
In NIPS 14, pp. 2177–
2185.
Levy, O., Goldberg, Y., and Dagan, I. (2015). Improving dis-
tributional similarity with lessons learned from word em-
beddings. TACL, 3, 211–225.
Levy, R. (2008). Expectation-based syntactic comprehen-
sion. Cognition, 106(3), 1126–1177.
Lewis, M. and Steedman, M. (2014). A* ccg parsing with a
supertag-factored model.. In EMNLP, pp. 990–1000.
Li, J., Chen, X., Hovy, E. H., and Jurafsky, D. (2015). Vi-
sualizing and understanding neural models in NLP.
In
NAACL HLT 2015.
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B.
(2016a). A diversity-promoting objective function for neu-
ral conversation models. In NAACL HLT 2016.
Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Juraf-
sky, D. (2016b). Deep reinforcement learning for dialogue
generation. In EMNLP 2016.
Li, J., Monroe, W., Shi, T., Ritter, A., and Jurafsky, D.
(2017). Adversarial learning for neural dialogue genera-
tion. In EMNLP 2017.
Li, X. and Roth, D. (2002). Learning question classiﬁers. In
COLING-02, pp. 556–562.
Li, X. and Roth, D. (2005). Learning question classiﬁers:
The role of semantic information. Journal of Natural Lan-
guage Engineering, 11(4).
Lin, D. (1995). A dependency-based method for evaluating
broad-coverage parsers. In IJCAI-95, Montreal, pp. 1420–
1425.
Lin, D. (1998). An information-theoretic deﬁnition of simi-
larity. In ICML 1998, San Francisco, pp. 296–304.
Lin, D. (2003). Dependency-based evaluation of minipar. In
Workshop on the Evaluation of Parsing Systems.
Lin, J. (2007). An exploration of the principles underlying
redundancy-based factoid question answering. ACM Trans-
actions on Information Systems, 25(2).
Lin, Y., Michel, J.-B., Lieberman Aiden, E., Orwant, J.,
Brockman, W., and Petrov, S. (2012). Syntactic annota-
tions for the google books ngram corpus. In ACL 2012, pp.
169–174.

Lindsey, R. (1963). Inferential memory as the basis of ma-
chines which understand natural language. In Feigenbaum,
E. and Feldman, J. (Eds.), Computers and Thought, pp.
217–233. McGraw Hill.
Litman, D. J., Swerts, M., and Hirschberg, J. (2000). Pre-
dicting automatic speech recognition performance using
prosodic cues. In NAACL 2000.
Litman, D. J., Walker, M. A., and Kearns, M. (1999). Auto-
matic detection of poor speech recognition at the dialogue
level. In ACL-99, College Park, MA, pp. 309–316.
Liu, B. and Zhang, L. (2012). A survey of opinion mining
and sentiment analysis. In Aggarwal, C. C. and Zhai, C.
(Eds.), Mining text data, pp. 415–464. Springer.
Liu, C.-W., Lowe, R. T., Serban, I. V., Noseworthy, M.,
Charlin, L., and Pineau, J. (2016). How NOT to evalu-
ate your dialogue system: An empirical study of unsuper-
vised evaluation metrics for dialogue response generation.
In EMNLP 2016.
Liu, X., Gales, M. J. F., and Woodland, P. C. (2013). Use of
contexts in language model interpolation and adaptation.
Computer Speech & Language, 27 (1), 301–321.
Lochbaum, K. E., Grosz, B. J., and Sidner, C. L. (2000).
Discourse structure and intention recognition. In Dale, R.,
Moisl, H., and Somers, H. L. (Eds.), Handbook of Natural
Language Processing. Marcel Dekker.
Lovins, J. B. (1968). Development of a stemming algo-
rithm. Mechanical Translation and Computational Lin-
guistics, 11(1–2), 9–13.
Lowe, R. T., Noseworthy, M., Serban, I. V., Angelard-
Gontier, N., Bengio, Y., and Pineau, J. (2017a). Towards
an automatic Turing test: Learning to evaluate dialogue re-
sponses. In ACL 2017.
Lowe, R. T., Pow, N., Serban, I. V., Charlin, L., Liu, C.-
W., and Pineau, J. (2017b). Training end-to-end dialogue
systems with the ubuntu dialogue corpus. Dialogue & Dis-
course, 8(1), 31–65.
Luhn, H. P. (1957). A statistical approach to the mecha-
nized encoding and searching of literary information. IBM
Journal of Research and Development, 1(4), 309–317.
Lui, M. and Baldwin, T. (2011). Cross-domain feature se-
lection for language identiﬁcation. In IJCNLP-11, pp. 553–
561.
Lui, M. and Baldwin, T. (2012). langid.py: An off-the-
shelf language identiﬁcation tool. In ACL 2012, pp. 25–30.
Lyons, J. (1977). Semantics. Cambridge University Press.
Ma, X. and Hovy, E. H. (2016). End-to-end sequence label-
ing via bi-directional LSTM-CNNs-CRF. In ACL 2016.
Madhu, S. and Lytel, D. (1965). A ﬁgure of merit technique
for the resolution of non-grammatical ambiguity. Mechan-
ical Translation, 8(2), 9–13.
Magerman, D. M. (1994). Natural Language Parsing as
Statistical Pattern Recognition. Ph.D. thesis, University of
Pennsylvania.
Magerman, D. M. (1995). Statistical decision-tree models
for parsing. In ACL-95, pp. 276–283.
Magerman, D. M. and Marcus, M. P. (1991). Pearl: A prob-
abilistic chart parser. In EACL-91, Berlin.
Mairesse, F. and Walker, M. A. (2008). Trainable generation
of big-ﬁve personality styles through data-driven parameter
estimation. In ACL-08, Columbus.
Manandhar, S., Klapaftis, I. P., Dligach, D., and Pradhan,
S. (2010). Semeval-2010 task 14: Word sense induction &
disambiguation. In SemEval-2010, pp. 63–68.
Manning, C. D. (2011). Part-of-speech tagging from 97%
to 100%: Is it time for some linguistics?. In CICLing 2011,
pp. 171–189.

Manning, C. D., Raghavan, P., and Sch ¨utze, H. (2008). In-
troduction to Information Retrieval. Cambridge.
Manning, C. D. and Sch ¨utze, H. (1999). Foundations of
Statistical Natural Language Processing. MIT Press.
Marcus, M. P. (1980). A Theory of Syntactic Recognition
for Natural Language. MIT Press.
Marcus, M. P. (1990). Summary of session 9: Automatic
acquisition of linguistic structure. In Proceedings DARPA
Speech and Natural Language Workshop, Hidden Valley,
PA, pp. 249–250.
Marcus, M. P., Kim, G., Marcinkiewicz, M. A., MacIntyre,
R., Bies, A., Ferguson, M., Katz, K., and Schasberger, B.
(1994). The Penn Treebank: Annotating predicate argu-
ment structure.
In ARPA Human Language Technology
Workshop, Plainsboro, NJ, pp. 114–119. Morgan Kauf-
mann.
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A.
(1993). Building a large annotated corpus of English: The
Penn treebank. Computational Linguistics, 19(2), 313–
330.
Markov, A. A. (1913). Essai d’une recherche statistique sur
le texte du roman “Eugene Onegin” illustrant la liaison des
epreuve en chain (‘Example of a statistical investigation of
the text of “Eugene Onegin” illustrating the dependence be-
tween samples in chain’). Izvistia Imperatorskoi Akademii
Nauk (Bulletin de l’Acad ´emie Imp ´eriale des Sciences de
St.-P ´etersbourg), 7, 153–162.
Markov, A. A. (2006). Classical text in translation: A. A.
Markov, an example of statistical investigation of the text
Eugene Onegin concerning the connection of samples in
chains. Science in Context, 19(4), 591–600. Translated by
David Link.
Maron, M. E. (1961). Automatic indexing: an experimental
inquiry. Journal of the ACM (JACM), 8(3), 404–417.
M `arquez, L., Carreras, X., Litkowski, K. C., and Stevenson,
S. (2008). Semantic role labeling: An introduction to the
special issue. Computational linguistics, 34(2), 145–159.
Marshall, I. (1983). Choice of grammatical word-class
without GLobal syntactic analysis: Tagging words in the
LOB corpus. Computers and the Humanities, 17, 139–150.
Marshall, I. (1987). Tag selection using probabilistic meth-
ods. In Garside, R., Leech, G., and Sampson, G. (Eds.), The
Computational Analysis of English, pp. 42–56. Longman.
Martin, J. H. (1986). The acquisition of polysemy. In ICML
1986, Irvine, CA, pp. 198–204.
Masterman, M. (1957). The thesaurus in syntax and seman-
tics. Mechanical Translation, 4(1), 1–2.
Mays, E., Damerau, F. J., and Mercer, R. L. (1991). Con-
text based spelling correction. Information Processing and
Management, 27 (5), 517–522.
McCallum, A., Freitag, D., and Pereira, F. C. N. (2000).
Maximum entropy Markov models for information extrac-
tion and segmentation. In ICML 2000, pp. 591–598.
McCallum, A. and Nigam, K. (1998). A comparison
of event models for naive bayes text classiﬁcation.
In
AAAI/ICML-98 Workshop on Learning for Text Categoriza-
tion, pp. 41–48.
McCawley, J. D. (1968). The role of semantics in a gram-
mar. In Bach, E. W. and Harms, R. T. (Eds.), Universals in
Linguistic Theory, pp. 124–169. Holt, Rinehart & Winston.
McCawley, J. D. (1993). Everything that Linguists Have Al-
ways Wanted to Know about Logic (2nd Ed.). University of
Chicago Press, Chicago, IL.
McCawley, J. D. (1998). The Syntactic Phenomena of En-
glish. University of Chicago Press.

Bibliography

531

McClelland, J. L. and Elman, J. L. (1986). The TRACE
model of speech perception. Cognitive Psychology, 18, 1–
86.
McCulloch, W. S. and Pitts, W. (1943). A logical calculus of
ideas immanent in nervous activity. Bulletin of Mathemat-
ical Biophysics, 5, 115–133. Reprinted in Neurocomput-
ing: Foundations of Research, ed. by J. A. Anderson and E
Rosenfeld. MIT Press 1988.
McDonald, R., Crammer, K., and Pereira, F. C. N. (2005).
Online large-margin training of dependency parsers.
In
ACL-05, Ann Arbor, pp. 91–98.
McDonald, R. and Nivre, J. (2011). Analyzing and integrat-
ing dependency parsers. Computational Linguistics, 37 (1),
197–230.
McDonald, R., Pereira, F. C. N., Ribarov, K., and Haji ˇc, J.
(2005). Non-projective dependency parsing using spanning
tree algorithms. In HLT-EMNLP-05.
McGuiness, D. L. and van Harmelen, F. (2004). OWL web
ontology overview. Tech. rep. 20040210, World Wide Web
Consortium.
Mehl, M. R., Gosling, S. D., and Pennebaker, J. W. (2006).
Personality in its natural habitat: manifestations and im-
plicit folk theories of personality in daily life.. Journal of
Personality and Social Psychology, 90(5).
Mel’ ˘cuk, I. A. (1988). Dependency Syntax: Theory and
Practice. State University of New York Press.
Merialdo, B. (1994). Tagging English text with a probabilis-
tic model. Computational Linguistics, 20(2), 155–172.
Mesnil, G., Dauphin, Y., Yao, K., Bengio, Y., Deng, L.,
Hakkani-T ¨ur, D., He, X., Heck, L., T ¨ur, G., Yu, D., and
Zweig, G. (2015). Using recurrent neural networks for
slot ﬁlling in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech and Language Processing
(TASLP), 23(3), 530–539.
Metsis, V., Androutsopoulos, I., and Paliouras, G. (2006).
Spam ﬁltering with naive bayes-which naive bayes?.
In
CEAS, pp. 27–28.
Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska,
V., Young, B., and Grishman, R. (2004). The nombank
project: An interim report.
In Proceedings of the NAA-
CL/HLT Workshop: Frontiers in Corpus Annotation.

Microsoft (2014). http://www.msxiaoice.com..

Mihalcea, R. (2007). Using wikipedia for automatic word
sense disambiguation. In NAACL-HLT 07, pp. 196–203.
Mihalcea, R. and Moldovan, D. (2001). Automatic genera-
tion of a coarse grained WordNet. In NAACL Workshop on
WordNet and Other Lexical Resources.
Mikheev, A., Moens, M., and Grover, C. (1999). Named en-
tity recognition without gazetteers. In EACL-99, Bergen,
Norway, pp. 1–8.
Mikolov, T. (2012). Statistical language models based on
neural networks. Ph.D. thesis, Ph. D. thesis, Brno Univer-
sity of Technology.
Mikolov, T., Chen, K., Corrado, G. S., and Dean, J.
(2013). Efﬁcient estimation of word representations in vec-
tor space. In ICLR 2013.
Mikolov, T., Kombrink, S., Burget, L., ˇCernock `y, J. H., and
Khudanpur, S. (2011). Extensions of recurrent neural net-
work language model. In ICASSP-11, pp. 5528–5531.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and
Dean, J. (2013a). Distributed representations of words and
phrases and their compositionality. In NIPS 13, pp. 3111–
3119.
Mikolov, T., Yih, W.-t., and Zweig, G. (2013b). Linguistic
regularities in continuous space word representations.
In
NAACL HLT 2013, pp. 746–751.

532 Bibliography

Miller, G. A. and Charles, W. G. (1991). Contextual cor-
relates of semantics similarity. Language and Cognitive
Processes, 6(1), 1–28.
Miller, G. A. and Chomsky, N. (1963). Finitary models of
language users. In Luce, R. D., Bush, R. R., and Galanter,
E. (Eds.), Handbook of Mathematical Psychology, Vol. II,
pp. 419–491. John Wiley.
Miller, G. A., Leacock, C., Tengi, R. I., and Bunker, R. T.
(1993). A semantic concordance.
In Proceedings ARPA
Workshop on Human Language Technology, pp. 303–308.
Miller, G. A. and Selfridge, J. A. (1950). Verbal context
and the recall of meaningful material. American Journal of
Psychology, 63, 176–185.
Miller, S., Bobrow, R. J., Ingria, R., and Schwartz, R.
(1994). Hidden understanding models of natural language.
In ACL-94, Las Cruces, NM, pp. 25–32.
Minsky, M. (1961). Steps toward artiﬁcial intelligence. Pro-
ceedings of the IRE, 49(1), 8–30.
Minsky, M. (1974). A framework for representing knowl-
edge. Tech. rep. 306, MIT AI Laboratory. Memo 306.
Minsky, M. and Papert, S. (1969). Perceptrons. MIT Press.
Mintz, M., Bills, S., Snow, R., and Jurafsky, D. (2009).
Distant supervision for relation extraction without labeled
data. In ACL IJCNLP 2009.
Mitton, R. (1987). Spelling checkers, spelling correctors and
the misspellings of poor spellers. Information processing
& management, 23(5), 495–505.
Miwa, M. and Bansal, M. (2016). End-to-end relation ex-
traction using lstms on sequences and tree structures.
In
ACL 2016, pp. 1105–1116.
Mohammad, S. M. and Turney, P. D. (2013). Crowdsourc-
ing a word-emotion association lexicon. Computational In-
telligence, 29(3), 436–465.
Monroe, B. L., Colaresi, M. P., and Quinn, K. M. (2008).
Fightin’words: Lexical feature selection and evaluation for
identifying the content of political conﬂict. Political Anal-
ysis, 16(4), 372–403.
Montague, R. (1973). The proper treatment of quantiﬁcation
in ordinary English. In Thomason, R. (Ed.), Formal Philos-
ophy: Selected Papers of Richard Montague, pp. 247–270.
Yale University Press, New Haven, CT.
Monz, C. (2004). Minimal span weighting retrieval for ques-
tion answering.
In SIGIR Workshop on Information Re-
trieval for Question Answering, pp. 23–30.
Morgan, A. A., Hirschman, L., Colosimo, M., Yeh, A. S.,
and Colombe, J. B. (2004). Gene name identiﬁcation and
normalization using a model organism database. Journal
of Biomedical Informatics, 37 (6), 396–410.
Morgan, N. and Bourlard, H. (1989). Generalization and pa-
rameter estimation in feedforward nets: Some experiments.
In Advances in neural information processing systems, pp.
630–637.
Morgan, N. and Bourlard, H. (1990). Continuous speech
recognition using multilayer perceptrons with hidden
markov models. In ICASSP-90, pp. 413–416.
Morris, W. (Ed.). (1985). American Heritage Dictionary
(2nd College Edition Ed.). Houghton Mifﬂin.
Mosteller, F. and Wallace, D. L. (1963). Inference in an au-
thorship problem: A comparative study of discrimination
methods applied to the authorship of the disputed federal-
ist papers. Journal of the American Statistical Association,
58(302), 275–309.
Mosteller, F. and Wallace, D. L. (1964). Inference and Dis-
puted Authorship: The Federalist. Springer-Verlag. A
second edition appeared in 1984 as Applied Bayesian and
Classical Inference.

Mrkˇsi ´c, N., O’S ´eaghdha, D., Wen, T.-H., Thomson, B., and
Young, S. J. (2017). Neural belief tracker: Data-driven di-
alogue state tracking. In ACL 2017.
Murdock, J. W., Fan, J., Lally, A., Shima, H., and Bogu-
raev, B. K. (2012a). Textual evidence gathering and anal-
ysis. IBM Journal of Research and Development, 56(3/4),
8:1–8:14.
Murdock, J. W., Kalyanpur, A., Welty, C., Fan, J., Fer-
rucci, D. A., Gondek, D. C., Zhang, L., and Kanayama,
H. (2012b). Typing candidate answers using type coercion.
IBM Journal of Research and Development, 56(3/4), 7:1–
7:13.
Murphy, K. P. (2012). Machine learning: A probabilistic
perspective. MIT press.
N ´adas, A. (1984). Estimation of probabilities in the lan-
guage model of the IBM speech recognition system. IEEE
Transactions on Acoustics, Speech, Signal Processing,
32(4), 859–861.
Nagata, M. and Morimoto, T. (1994). First steps toward sta-
tistical modeling of dialogue to predict the speech act type
of the next utterance. Speech Communication, 15, 193–
203.
Nash-Webber, B. L. (1975). The role of semantics in auto-
matic speech understanding. In Bobrow, D. G. and Collins,
A. (Eds.), Representation and Understanding, pp. 351–
382. Academic Press.
Naur, P., Backus, J. W., Bauer, F. L., Green, J., Katz, C.,
McCarthy, J., Perlis, A. J., Rutishauser, H., Samelson, K.,
Vauquois, B., Wegstein, J. H., van Wijnagaarden, A., and
Woodger, M. (1960). Report on the algorithmic language
ALGOL 60. Communications of the ACM, 3(5), 299–314.
Revised in CACM 6:1, 1-17, 1963.
Navigli, R. (2006). Meaningful clustering of senses helps
boost word sense disambiguation performance.
In COL-
ING/ACL 2006, pp. 105–112.
Navigli, R. (2009). Word sense disambiguation: A survey.
ACM Computing Surveys, 41(2).
Navigli, R. and Lapata, M. (2010). An experimental study
of graph connectivity for unsupervised word sense disam-
biguation. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 32(4), 678–692.
Navigli, R. and Vannella, D. (2013). Semeval-2013 task 11:
Word sense induction & disambiguation within an end-user
application. In *SEM, pp. 193–201.
Needleman, S. B. and Wunsch, C. D. (1970). A gen-
eral method applicable to the search for similarities in the
amino-acid sequence of two proteins. Journal of Molecular
Biology, 48, 443–453.
Neff, G. and Nagy, P. (2016). Talking to bots: Symbiotic
agency and the case of Tay. International Journal of Com-
munication, 10, 4915–4931.
Newell, A., Langer, S., and Hickey, M. (1998). The r ˆole of
natural language processing in alternative and augmenta-
tive communication. Natural Language Engineering, 4(1),
1–16.
Ney, H. (1991). Dynamic programming parsing for context-
free grammars in continuous speech recognition.
IEEE
Transactions on Signal Processing, 39(2), 336–340.
Ng, A. Y. and Jordan, M. I. (2002). On discriminative vs.
generative classiﬁers: A comparison of logistic regression
and naive bayes. In NIPS 14, pp. 841–848.
Ng, H. T., Teo, L. H., and Kwan, J. L. P. (2000). A machine
learning approach to answering questions for reading com-
prehension tests. In EMNLP 2000, pp. 124–132.

Nielsen, J. (1992). The usability engineering life cycle. IEEE
Computer, 25(3), 12–22.
Nielsen, M. A. (2015). Neural networks and Deep learning.
Determination Press USA.
Nigam, K., Lafferty, J. D., and McCallum, A. (1999). Using
maximum entropy for text classiﬁcation. In IJCAI-99 work-
shop on machine learning for information ﬁltering, pp. 61–
67.
Nilsson, J., Riedel, S., and Yuret, D. (2007). The conll 2007
shared task on dependency parsing. In Proceedings of the
CoNLL shared task session of EMNLP-CoNLL, pp. 915–
932. sn.
NIST (2005). Speech recognition scoring toolkit (sctk) ver-

sion 2.1. http://www.nist.gov/speech/tools/.

Nivre, J. (2007).
Incremental non-projective dependency
parsing. In NAACL-HLT 07.
Nivre, J. (2003). An efﬁcient algorithm for projective de-
pendency parsing. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT.
Nivre, J. (2006). Inductive Dependency Parsing. Springer.
Nivre, J. (2009). Non-projective dependency parsing in ex-
pected linear time. In ACL IJCNLP 2009, pp. 351–359.
Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,
Haji ˇc, J., Manning, C. D., McDonald, R. T., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016a). Universal Dependencies v1: A multilingual tree-
bank collection. In LREC.
Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,
Haji ˇc, J., Manning, C. D., McDonald, R. T., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016b). Universal Dependencies v1: A multilingual tree-
bank collection. In LREC-16.
Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G.,
K ¨ubler, S., Marinov, S., and Marsi, E. (2007). Malt-
parser: A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 13(02),
95–135.
Nivre, J. and Nilsson, J. (2005). Pseudo-projective depen-
dency parsing. In ACL-05, pp. 99–106.
Nivre, J. and Scholz, M. (2004). Deterministic dependency
parsing of english text. In COLING-04, p. 64.
Niwa, Y. and Nitta, Y. (1994). Co-occurrence vectors from
corpora vs. distance vectors from dictionaries. In ACL-94,
pp. 304–309.
Noreen, E. W. (1989). Computer Intensive Methods for Test-
ing Hypothesis. Wiley.
Norman, D. A. (1988). The Design of Everyday Things.
Basic Books.
Norman, D. A. and Rumelhart, D. E. (1975). Explorations
in Cognition. Freeman.
Norvig, P. (1991). Techniques for automatic memoization
with applications to context-free parsing. Computational
Linguistics, 17 (1), 91–98.
Norvig, P. (2007). How to write a spelling corrector. http:

//www.norvig.com/spell- correct.html.

Norvig, P. (2009). Natural language corpus data. In Segaran,
T. and Hammerbacher, J. (Eds.), Beautiful data: the stories
behind elegant data solutions. O’Reilly.
Nosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002a).
Harvesting implicit group attitudes and beliefs from a
demonstration web site. Group Dynamics: Theory, Re-
search, and Practice, 6(1), 101.
Nosek, B. A., Banaji, M. R., and Greenwald, A. G. (2002b).
Math=male, me=female, therefore math (cid:54)= me. Journal of
personality and social psychology, 83(1), 44.

Bibliography

533

O’Connor, B., Krieger, M., and Ahn, D. (2010). Tweetmo-
tif: Exploratory search and topic summarization for twitter.
In ICWSM.
Odell, M. K. and Russell, R. C. (1918/1922). U.S. Patents
1261167 (1918), 1435663 (1922). Cited in Knuth (1973).
Oh, A. H. and Rudnicky, A. I. (2000). Stochastic language
generation for spoken dialogue systems.
In Proceedings
of the 2000 ANLP/NAACL Workshop on Conversational
systems-Volume 3, pp. 27–32.
Oravecz, C. and Dienes, P. (2002). Efﬁcient stochastic part-
of-speech tagging for Hungarian. In LREC-02, Las Palmas,
Canary Islands, Spain, pp. 710–717.
Osgood, C. E., Suci, G. J., and Tannenbaum, P. H. (1957).
The Measurement of Meaning. University of Illinois Press.
Packard, D. W. (1973). Computer-assisted morphological
analysis of ancient Greek. In Zampolli, A. and Calzolari,
N. (Eds.), Computational and Mathematical Linguistics:
Proceedings of the International Conference on Computa-
tional Linguistics, Pisa, pp. 343–355. Leo S. Olschki.
Palmer, D. (2012). Text preprocessing.
In Indurkhya, N.
and Damerau, F. J. (Eds.), Handbook of Natural Language
Processing, pp. 9–30. CRC Press.
Palmer, M., Babko-Malaya, O., and Dang, H. T. (2004).
Different sense granularities for different applications. In
HLT-NAACL Workshop on Scalable Natural Language Un-
derstanding, Boston, MA, pp. 49–56.
Palmer, M., Dang, H. T., and Fellbaum, C. (2006). Mak-
ing ﬁne-grained and coarse-grained sense distinctions, both
manually and automatically. Natural Language Engineer-
ing, 13(2), 137–163.
Palmer, M., Fellbaum, C., Cotton, S., Delfs, L., and Dang,
H. T. (2001). English tasks: All-words and verb lexical
sample. In Proceedings of Senseval-2: 2nd International
Workshop on Evaluating Word Sense Disambiguation Sys-
tems, Toulouse, France, pp. 21–24.
Palmer, M., Gildea, D., and Xue, N. (2010). Semantic role
labeling. Synthesis Lectures on Human Language Tech-
nologies, 3(1), 1–103.
Palmer, M., Kingsbury, P., and Gildea, D. (2005). The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1), 71–106.
Palmer, M., Ng, H. T., and Dang, H. T. (2006). Evalua-
tion of wsd systems. In Agirre, E. and Edmonds, P. (Eds.),
Word Sense Disambiguation: Algorithms and Applications.
Kluwer.
Pang, B. and Lee, L. (2008). Opinion mining and sentiment
analysis. Foundations and trends in information retrieval,
2(1-2), 1–135.
Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs
up? Sentiment classiﬁcation using machine learning tech-
niques. In EMNLP 2002, pp. 79–86.
J.
(2017).
Google Home
vs Alexa:
Two simple user
experience design gestures
that
delighted a
female user.
In Medium.
Jan
4,
2017.

https://medium.com/startup- grind/
google- home- vs- alexa- 56e26f69ac77.

Paolino,

Parsons, T. (1990). Events in the Semantics of English. MIT
Press.
Partee, B. H. (Ed.). (1976). Montague Grammar. Academic
Press.
Pasca, M. (2003). Open-Domain Question Answering from
Large Text Collections. CSLI.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. (2017). Automatic differentiation in pytorch. In NIPS-
W.

534 Bibliography

Pearl, C. (2017). Designing Voice User Interfaces: Princi-
ples of Conversational Experiences. O’Reilly.
Pedersen, T. and Bruce, R. (1997). Distinguishing word
senses in untagged text. In EMNLP 1997, Providence, RI.
Peng, N., Poon, H., Quirk, C., Toutanova, K., and Yih, W.-t.
(2017). Cross-sentence n-ary relation extraction with graph
LSTMs. TACL, 5, 101–115.
Penn, G. and Kiparsky, P. (2012). On P ¯an. ini and the gen-
erative capacity of contextualized replacement systems. In
COLING-12, pp. 943–950.
Pennebaker, J. W., Booth, R. J., and Francis, M. E. (2007).
Linguistic Inquiry and Word Count: LIWC 2007. Austin,
TX.
Pennebaker, J. W. and King, L. A. (1999). Linguistic styles:
language use as an individual difference. Journal of Per-
sonality and Social Psychology, 77 (6).
Pennington, J., Socher, R., and Manning, C. D. (2014).
Glove: Global vectors for word representation. In EMNLP
2014, pp. 1532–1543.
Percival, W. K. (1976). On the historical source of immedi-
ate constituent analysis. In McCawley, J. D. (Ed.), Syntax
and Semantics Volume 7, Notes from the Linguistic Under-
ground, pp. 229–242. Academic Press.
Perrault, C. R. and Allen, J. (1980). A plan-based analy-
sis of indirect speech acts. American Journal of Computa-
tional Linguistics, 6(3-4), 167–182.
Peterson, J. L. (1986). A note on undetected typing errors.
Communications of the ACM, 29(7), 633–637.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D. (2006).
Learning accurate, compact, and interpretable tree annota-
tion. In COLING/ACL 2006, Sydney, Australia, pp. 433–
440.
Petrov, S., Das, D., and McDonald, R. (2012). A universal
part-of-speech tagset. In LREC-12.
Petrov, S. and McDonald, R. (2012). Overview of the 2012
shared task on parsing the web. In Notes of the First Work-
shop on Syntactic Analysis of Non-Canonical Language
(SANCL), Vol. 59.
Philips, L. (1990). Hanging on the metaphone. Computer
Language, 7 (12).
Phillips, A. V. (1960). A question-answering routine. Tech.
rep. 16, MIT AI Lab.
Picard, R. W. (1995). Affective computing. Tech. rep. 321,
MIT Media Lab Perceputal Computing Technical Report.
Revised November 26, 1995.
Pieraccini, R., Levin, E., and Lee, C.-H. (1991). Stochastic
representation of conceptual structure in the ATIS task. In
Proceedings DARPA Speech and Natural Language Work-
shop, Paciﬁc Grove, CA, pp. 121–124.
Plutchik, R. (1962). The emotions: Facts, theories, and a
new model. Random House.
Plutchik, R. (1980). A general psychoevolutionary theory
of emotion. In Plutchik, R. and Kellerman, H. (Eds.), Emo-
tion: Theory, Research, and Experience, Volume 1, pp. 3–
33. Academic Press.
Polifroni, J., Hirschman, L., Seneff, S., and Zue, V. W.
(1992). Experiments in evaluating interactive spoken lan-
guage systems. In Proceedings DARPA Speech and Natural
Language Workshop, Harriman, NY, pp. 28–33.
Pollard, C. and Sag, I. A. (1994). Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ponzetto, S. P. and Navigli, R. (2010). Knowledge-rich word
sense disambiguation rivaling supervised systems. In ACL
2010, pp. 1522–1531.

Porter, M. F. (1980). An algorithm for sufﬁx stripping. Pro-
gram, 14(3), 130–127.
Potts, C. (2011). On the negativity of negation. In Li, N. and
Lutz, D. (Eds.), Proceedings of Semantics and Linguistic
Theory 20, pp. 636–659. CLC Publications, Ithaca, NY.
Pradhan, S., Moschitti, A., Xue, N., Ng, H. T., Bj ¨orkelund,
A., Uryupina, O., Zhang, Y., and Zhong, Z. (2013). To-
wards robust linguistic analysis using OntoNotes.
In
CoNLL-13, pp. 143–152.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J. H., and Ju-
rafsky, D. (2005). Semantic role labeling using different
syntactic views. In ACL-05, Ann Arbor, MI.
Purver, M. (2004). The theory and use of clariﬁcation re-
quests in dialogue. Ph.D. thesis, University of London.
Pustejovsky, J. (1995). The Generative Lexicon. MIT Press.
Pustejovsky, J. and Boguraev, B. (Eds.). (1996). Lexical
Semantics: The Problem of Polysemy. Oxford University
Press.

Pustejovsky,

J., Casta ˜no,
J.,
Ingria, R., Saur´ı, R.,
Gaizauskas, R., Setzer, A., and Katz, G. (2003a). TimeML:
robust speciﬁcation of event and temporal expressions in
text. In Proceedings of the 5th International Workshop on
Computational Semantics (IWCS-5).
Pustejovsky, J., Hanks, P., Saur´ı, R., See, A., Gaizauskas,
R., Setzer, A., Radev, D., Sundheim, B., Day, D. S., Ferro,
L., and Lazo, M. (2003b). The TIMEBANK corpus.
In
Proceedings of Corpus Linguistics 2003 Conference, pp.
647–656. UCREL Technical Paper number 16.
Pustejovsky, J., Ingria, R., Saur´ı, R., Casta ˜no, J., Littman, J.,
Gaizauskas, R., Setzer, A., Katz, G., and Mani, I. (2005).
The Speciﬁcation Language TimeML, chap. 27. Oxford.
Qiu, G., Liu, B., Bu, J., and Chen, C. (2009). Expanding
domain sentiment lexicon through double propagation.. In
IJCAI-09, pp. 1199–1204.
Quillian, M. R. (1968). Semantic memory. In Minsky, M.
(Ed.), Semantic Information Processing, pp. 227–270. MIT
Press.
Quillian, M. R. (1969). The teachable language comprehen-
der: A simulation program and theory of language. Com-
munications of the ACM, 12(8), 459–476.
Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J.
(1985). A Comprehensive Grammar of the English Lan-
guage. Longman.
Rabiner, L. R. (1989). A tutorial on hidden Markov models
and selected applications in speech recognition. Proceed-
ings of the IEEE, 77 (2), 257–286.
Rabiner, L. R. and Juang, B. H. (1993). Fundamentals of
Speech Recognition. Prentice Hall.
Radford, A. (1988). Transformational Grammar: A First
Course. Cambridge University Press.
Radford, A. (1997). Syntactic Theory and the Structure of
English: A Minimalist Approach. Cambridge University
Press.
Rajpurkar, P., Jia, R., and Liang, P. (2018). Know what you
don’t know: Unanswerable questions for SQuAD. In ACL
2018.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016).
SQuAD: 100,000+ questions for machine comprehension
of text. In EMNLP 2016.
Ramshaw, L. A. and Marcus, M. P. (1995). Text chunking
using transformation-based learning. In Proceedings of the
3rd Annual Workshop on Very Large Corpora, pp. 82–94.
Ranganath, R., Jurafsky, D., and McFarland, D. A. (2013).
Detecting friendly, ﬂirtatious, awkward, and assertive
speech in speed-dates. Computer Speech and Language,
27 (1), 89–115.

Raphael, B. (1968). SIR: A computer program for seman-
tic information retrieval.
In Minsky, M. (Ed.), Semantic
Information Processing, pp. 33–145. MIT Press.
Rashkin, H., Bell, E., Choi, Y., and Volkova, S. (2017). Mul-
tilingual connotation frames: A case study on social media
for targeted sentiment analysis and forecast. In ACL 2017,
pp. 459–464.
Rashkin, H., Singh, S., and Choi, Y. (2016). Connotation
frames: A data-driven investigation. In ACL 2016, pp. 311–
321.
Ratnaparkhi, A. (1996). A maximum entropy part-of-
speech tagger. In EMNLP 1996, Philadelphia, PA, pp. 133–
142.
Ratnaparkhi, A. (1997). A linear observed time statisti-
cal parser based on maximum entropy models. In EMNLP
1997, Providence, RI, pp. 1–10.
Ratnaparkhi, A., Reynar, J. C., and Roukos, S. (1994). A
maximum entropy model for prepositional phrase attach-
ment. In ARPA Human Language Technologies Workshop,
Plainsboro, N.J., pp. 250–255.
Raviv, J. (1967). Decision making in Markov chains applied
to the problem of pattern recognition. IEEE Transactions
on Information Theory, 13(4), 536–551.
Raymond, C. and Riccardi, G. (2007). Generative and dis-
criminative algorithms for spoken language understanding.
In INTERSPEECH-07, pp. 1605–1608.
Rehder, B., Schreiner, M. E., Wolfe, M. B. W., Laham, D.,
Landauer, T. K., and Kintsch, W. (1998). Using Latent
Semantic Analysis to assess knowledge: Some technical
considerations. Discourse Processes, 25(2-3), 337–354.
Reichenbach, H. (1947). Elements of Symbolic Logic.
Macmillan, New York.
Reichert, T. A., Cohen, D. N., and Wong, A. K. C. (1973).
An application of information theory to genetic mutations
and the matching of polypeptide sequences. Journal of
Theoretical Biology, 42, 245–261.
Resnik, P. (1992). Probabilistic tree-adjoining grammar as
a framework for statistical natural language processing. In
COLING-92, Nantes, France, pp. 418–424.
Resnik, P. (1993). Semantic classes and syntactic ambigu-
ity. In Proceedings of the workshop on Human Language
Technology, pp. 278–283.
Resnik, P. (1995). Using information content to evaluate
semantic similarity in a taxanomy. In International Joint
Conference for Artiﬁcial Intelligence (IJCAI-95), pp. 448–
453.
Resnik, P. (1996). Selectional constraints: An information-
theoretic model and its computational realization. Cogni-
tion, 61, 127–159.
Richardson, M., Burges, C. J. C., and Renshaw, E. (2013).
MCTest: A challenge dataset for the open-domain machine
comprehension of text. In EMNLP 2013, pp. 193–203.
Riedel, S., Yao, L., and McCallum, A. (2010). Modeling
relations and their mentions without labeled text. In Ma-
chine Learning and Knowledge Discovery in Databases,
pp. 148–163. Springer.
Riedel, S., Yao, L., McCallum, A., and Marlin, B. M. (2013).
Relation extraction with matrix factorization and universal
schemas. In NAACL HLT 2013.
Riesbeck, C. K. (1975). Conceptual analysis.
In Schank,
R. C. (Ed.), Conceptual Information Processing, pp. 83–
156. American Elsevier, New York.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R.,
Maxwell III, J. T., and Johnson, M. (2002).
Parsing

Bibliography

535

the Wall Street Journal using a Lexical-Functional Gram-
mar and discriminative estimation techniques. In ACL-02,
Philadelphia, PA.
Riloff, E. (1993). Automatically constructing a dictionary
for information extraction tasks. In AAAI-93, Washington,
D.C., pp. 811–816.
Riloff, E. (1996). Automatically generating extraction pat-
terns from untagged text. In AAAI-96, pp. 117–124.
Riloff, E. and Jones, R. (1999). Learning dictionaries for
information extraction by multi-level bootstrapping.
In
AAAI-99, pp. 474–479.
Riloff, E. and Schmelzenbach, M. (1998). An empirical ap-
proach to conceptual case frame acquisition. In Proceed-
ings of the Sixth Workshop on Very Large Corpora, Mon-
treal, Canada, pp. 49–56.
Riloff, E. and Shepherd, J. (1997). A corpus-based approach
for building semantic lexicons. In EMNLP 1997.
Riloff, E. and Thelen, M. (2000). A rule-based question an-
swering system for reading comprehension tests. In Pro-
ceedings of ANLP/NAACL workshop on reading compre-
hension tests, pp. 13–19.
Riloff, E. and Wiebe, J. (2003). Learning extraction pat-
terns for subjective expressions. In EMNLP 2003, Sapporo,
Japan.
Ritter, A., Cherry, C., and Dolan, B. (2011). Data-driven
response generation in social media.
In EMNLP-11, pp.
583–593.
Ritter, A., Etzioni, O., and Mausam (2010). A latent dirich-
let allocation method for selectional preferences. In ACL
2010, pp. 424–434.
Ritter, A., Zettlemoyer, L., Mausam, and Etzioni, O. (2013).
Modeling missing data in distant supervision for informa-
tion extraction.. TACL, 1, 367–378.
Roark, B. (2001). Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27 (2), 249–
276.
Roark, B., Saraclar, M., and Collins, M. (2007). Discrim-
inative n-gram language modeling. Computer Speech &
Language, 21(2), 373–392.
Rohde, D. L. T., Gonnerman, L. M., and Plaut, D. C. (2006).
An improved model of semantic similarity based on lexical
co-occurrence. Communications of the ACM, 8, 627–633.
Rooth, M., Riezler, S., Prescher, D., Carroll, G., and Beil,
F. (1999). Inducing a semantically annotated lexicon via
EM-based clustering. In ACL-99, College Park, MA, pp.
104–111.

Rosenblatt, F. (1958).

The perceptron: A probabilis-
tic model for information storage and organization in the
brain.. Psychological review, 65(6), 386–408.
Rosenfeld, R. (1996). A maximum entropy approach to
adaptive statistical language modeling. Computer Speech
and Language, 10, 187–228.
Rothe, S., Ebert, S., and Sch ¨utze, H. (2016). Ultradense
Word Embeddings by Orthogonal Transformation.
In
NAACL HLT 2016.
Roy, N., Pineau, J., and Thrun, S. (2000). Spoken dialog
management for robots. In ACL-00, Hong Kong.
Rubenstein, H. and Goodenough, J. B. (1965). Contex-
tual correlates of synonymy. Communications of the ACM,
8(10), 627–633.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986).
Learning internal representations by error propagation. In
Rumelhart, D. E. and McClelland, J. L. (Eds.), Parallel
Distributed Processing, Vol. 2, pp. 318–362. MIT Press.

536 Bibliography

Rumelhart, D. E. and McClelland, J. L. (1986a). On learn-
ing the past tense of English verbs. In Rumelhart, D. E. and
McClelland, J. L. (Eds.), Parallel Distributed Processing,
Vol. 2, pp. 216–271. MIT Press.
Rumelhart, D. E. and McClelland, J. L. (Eds.). (1986b).
Parallel Distributed Processing. MIT Press.
Ruppenhofer, J., Ellsworth, M., Petruck, M. R. L., Johnson,
C. R., Baker, C. F., and Scheffczyk, J. (2016). FrameNet
II: Extended theory and practice..
Ruppenhofer, J., Sporleder, C., Morante, R., Baker, C., and
Palmer, M. (2010). Semeval-2010 task 10: Linking events
and their participants in discourse. In Proceedings of the
5th International Workshop on Semantic Evaluation, pp.
45–50.
Russell, J. A. (1980). A circumplex model of affect. Journal
of personality and social psychology, 39(6), 1161–1178.
Russell, S. and Norvig, P. (2002). Artiﬁcial Intelligence: A
Modern Approach (2nd Ed.). Prentice Hall.
Sacks, H., Schegloff, E. A., and Jefferson, G. (1974). A
simplest systematics for the organization of turn-taking for
conversation. Language, 50(4), 696–735.
Sag, I. A. and Liberman, M. Y. (1975). The intonational dis-
ambiguation of indirect speech acts. In CLS-75, pp. 487–
498. University of Chicago.
Sag, I. A., Wasow, T., and Bender, E. M. (Eds.). (2003). Syn-
tactic Theory: A Formal Introduction. CSLI Publications,
Stanford, CA.
Sahami, M., Dumais, S. T., Heckerman, D., and Horvitz, E.
(1998). A Bayesian approach to ﬁltering junk e-mail. In
AAAI Workshop on Learning for Text Categorization, pp.
98–105.
Sakoe, H. and Chiba, S. (1971). A dynamic programming
approach to continuous speech recognition.
In Proceed-
ings of the Seventh International Congress on Acoustics,
Budapest, Vol. 3, pp. 65–69. Akad ´emiai Kiad ´o.
Salomaa, A. (1969). Probabilistic and weighted grammars.
Information and Control, 15, 529–544.
Salton, G. (1971). The SMART Retrieval System: Experi-
ments in Automatic Document Processing. Prentice Hall.
Sampson, G. (1987). Alternative grammatical coding sys-
tems.
In Garside, R., Leech, G., and Sampson, G.
(Eds.), The Computational Analysis of English, pp. 165–
183. Longman.
Samuelsson, C. (1993). Morphological tagging based en-
tirely on Bayesian inference. In 9th Nordic Conference on
Computational Linguistics NODALIDA-93. Stockholm.
Sankoff, D. (1972). Matching sequences under deletion-
insertion constraints. Proceedings of the Natural Academy
of Sciences of the U.S.A., 69, 4–6.
Sankoff, D. and Labov, W. (1979). On the uses of variable
rules. Language in society, 8(2-3), 189–222.
Sap, M., Prasettio, M. C., Holtzman, A., Rashkin, H., and
Choi, Y. (2017). Connotation frames of power and agency
in modern ﬁlms. In EMNLP 2017, pp. 2329–2334.
Schabes, Y. (1990). Mathematical and Computational As-
pects of Lexicalized Grammars. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Schabes, Y. (1992). Stochastic lexicalized tree-adjoining
grammars. In COLING-92, Nantes, France, pp. 426–433.
Schabes, Y., Abeill ´e, A., and Joshi, A. K. (1988). Pars-
ing strategies with ‘lexicalized’ grammars: Applications to
Tree Adjoining Grammars. In COLING-88, Budapest, pp.
578–583.

Sch ¨onkﬁnkel, M. (1924).

Schank, R. C. (1972). Conceptual dependency: A theory
of natural language processing. Cognitive Psychology, 3,
552–631.
Schank, R. C. and Abelson, R. P. (1975). Scripts, plans, and
knowledge. In Proceedings of IJCAI-75, pp. 151–157.
Schank, R. C. and Abelson, R. P. (1977). Scripts, Plans,
Goals and Understanding. Lawrence Erlbaum.
Schegloff, E. A. (1968). Sequencing in conversational open-
ings. American Anthropologist, 70, 1075–1095.
Schegloff, E. A. (1972). Notes on a conversational practice:
Formulating place. In Sudnow, D. (Ed.), Studies in social
interaction, New York. Free Press.
Schegloff, E. A. (1982). Discourse as an interactional
achievement: Some uses of ‘uh huh’ and other things that
come between sentences. In Tannen, D. (Ed.), Analyzing
Discourse: Text and Talk, pp. 71–93. Georgetown Univer-
sity Press, Washington, D.C.
Scherer, K. R. (2000). Psychological models of emotion.
In Borod, J. C. (Ed.), The neuropsychology of emotion, pp.
137–162. Oxford.
Schone, P. and Jurafsky, D. (2000). Knowlege-free induction
of morphology using latent semantic analysis. In CoNLL-
00.
Schone, P. and Jurafsky, D. (2001). Knowledge-free induc-
tion of inﬂectional morphologies. In NAACL 2001.
¨Uber die Bausteine der mathe-
matischen Logik. Mathematische Annalen, 92, 305–316.
English translation appears in From Frege to G ¨odel: A
Source Book in Mathematical Logic, Harvard University
Press, 1967.
Sch ¨utze, H. (1992a). Context space. In Goldman, R. (Ed.),
Proceedings of the 1992 AAAI Fall Symposium on Proba-
bilistic Approaches to Natural Language.
Sch ¨utze, H. (1992b). Dimensions of meaning. In Proceed-
ings of Supercomputing ’92, pp. 787–796. IEEE Press.
Sch ¨utze, H. (1997a). Ambiguity Resolution in Language
Learning – Computational and Cognitive Models. CSLI,
Stanford, CA.
Sch ¨utze, H. (1997b). Ambiguity Resolution in Language
Learning: Computational and Cognitive Models. CSLI
Publications, Stanford, CA.
Sch ¨utze, H. (1998). Automatic word sense discrimination.
Computational Linguistics, 24(1), 97–124.
Sch ¨utze, H., Hull, D. A., and Pedersen, J. (1995). A com-
parison of classiﬁers and document representations for the
routing problem. In SIGIR-95, pp. 229–237.
Sch ¨utze, H. and Pedersen, J. (1993). A vector model for
syntagmatic and paradigmatic relatedness. In Proceedings
of the 9th Annual Conference of the UW Centre for the New
OED and Text Research, pp. 104–113.
Sch ¨utze, H. and Singer, Y. (1994). Part-of-speech tagging
using a variable memory Markov model. In ACL-94, Las
Cruces, NM, pp. 181–187.
Schwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzyn-
ski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosin-
ski, M., Stillwell, D., Seligman, M. E. P., and Ungar, L. H.
(2013). Personality, gender, and age in the language of so-
cial media: The open-vocabulary approach. PloS one, 8(9),
e73791.
Schwartz, R. and Chow, Y.-L. (1990). The N-best algo-
rithm: An efﬁcient and exact procedure for ﬁnding the N
most likely sentence hypotheses. In ICASSP-90, Vol. 1, pp.
81–84.
Schwenk, H. (2007). Continuous space language models.
Computer Speech & Language, 21(3), 492–518.

Scott, M. and Shillcock, R. (2003). Eye movements re-
veal the on-line computation of lexical probabilities during
reading. Psychological Science, 14(6), 648–652.
S ´eaghdha, D. O. (2010). Latent variable models of selec-
tional preference. In ACL 2010, pp. 435–444.
Seddah, D., Tsarfaty, R., K ¨ubler, S., Candito, M., Choi,
J. D., Farkas, R., Foster, J., Goenaga, I., Gojenola, K.,
Goldberg, Y., Green, S., Habash, N., Kuhlmann, M., Maier,
W., Nivre, J., Przepi ´orkowski, A., Roth, R., Seeker, W.,
Versley, Y., Vincze, V., Woli ´nski, M., Wr ´oblewska, A.,
and Villemonte de la Cl ´ergerie, E. (2013). Overview of
the SPMRL 2013 shared task: cross-framework evalua-
tion of parsing morphologically rich languages.
In Pro-
ceedings of the 4th Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Sekine, S. and Collins, M. (1997). The evalb software.

http://cs.nyu.edu/cs/projects/proteus/evalb.

Sennrich, R., Haddow, B., and Birch, A. (2016). Neural ma-
chine translation of rare words with subword units. In ACL
2016.
Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.
(2017). Bidirectional attention ﬂow for machine compre-
hension. In ICLR 2017.
Serban, I. V., Lowe, R. T., Charlin, L., and Pineau, J. (2017).
A survey of available corpora for building data-driven dia-
logue systems.. arXiv preprint arXiv:1512.05742.
Sgall, P., Haji ˇcov ´a, E., and Panevova, J. (1986). The Mean-
ing of the Sentence in its Pragmatic Aspects. Reidel.
Shang, L., Lu, Z., and Li, H. (2015). Neural responding ma-
chine for short-text conversation. In ACL 2015, pp. 1577–
1586.
Shannon, C. E. (1948). A mathematical theory of commu-
nication. Bell System Technical Journal, 27 (3), 379–423.
Continued in the following volume.
Shannon, C. E. (1951). Prediction and entropy of printed
English. Bell System Technical Journal, 30, 50–64.
Sheil, B. A. (1976). Observations on context free parsing.
SMIL: Statistical Methods in Linguistics, 1, 71–109.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D.,
Ries, K., Coccaro, N., Martin, R., Meteer, M., and Van Ess-
Dykema, C. (1998). Can prosody aid the automatic classi-
ﬁcation of dialog acts in conversational speech?. Language
and Speech (Special Issue on Prosody and Conversation),
41(3-4), 439–487.
Simmons, R. F. (1965). Answering English questions by
computer: A survey. Communications of the ACM, 8(1),
53–70.
Simmons, R. F. (1973). Semantic networks: Their com-
putation and use for understanding English sentences. In
Schank, R. C. and Colby, K. M. (Eds.), Computer Models
of Thought and Language, pp. 61–113. W.H. Freeman and
Co.
Simmons, R. F., Klein, S., and McConlogue, K. (1964). In-
dexing and dependency logic for answering english ques-
tions. American Documentation, 15(3), 196–204.
Simons, G. F. and Fennig, C. D. (2018). Ethnologue: Lan-
guages of the world, twenty-ﬁrst edition.. Dallas, Texas.
SIL International.
Singh, S. P., Litman, D. J., Kearns, M., and Walker, M. A.
(2002). Optimizing dialogue management with reinforce-
ment learning: Experiments with the NJFun system. Jour-
nal of Artiﬁcial Intelligence Research (JAIR), 16, 105–133.
Sleator, D. and Temperley, D. (1993). Parsing English with
a link grammar. In IWPT-93.

Bibliography

537

Small, S. L., Cottrell, G. W., and Tanenhaus, M. (Eds.).
(1988). Lexical Ambiguity Resolution. Morgan Kaufman.
Small, S. L. and Rieger, C. (1982). Parsing and compre-
hending with Word Experts. In Lehnert, W. G. and Ringle,
M. H. (Eds.), Strategies for Natural Language Processing,
pp. 89–147. Lawrence Erlbaum.
Smith, D. A. and Eisner, J. (2007). Bootstrapping feature-
rich dependency parsers with entropic priors. In EMNLP/-
CoNLL 2007, Prague, pp. 667–677.
Smith, N. A. and Eisner, J. (2005). Guiding unsupervised
grammar induction using contrastive estimation. In IJCAI
Workshop on Grammatical Inference Applications, Edin-
burgh, pp. 73–82.
Smith, V. L. and Clark, H. H. (1993). On the course of an-
swering questions. Journal of Memory and Language, 32,
25–38.
Smolensky, P. (1988). On the proper treatment of connec-
tionism. Behavioral and brain sciences, 11(1), 1–23.
Smolensky, P. (1990). Tensor product variable binding and
the representation of symbolic structures in connectionist
systems. Artiﬁcial intelligence, 46(1-2), 159–216.
Snow, R., Jurafsky, D., and Ng, A. Y. (2005). Learning syn-
tactic patterns for automatic hypernym discovery. In Saul,
L. K., Weiss, Y., and Bottou, L. (Eds.), NIPS 17, pp. 1297–
1304. MIT Press.
Snow, R., Prakash, S., Jurafsky, D., and Ng, A. Y. (2007).
Learning to merge word senses. In EMNLP/CoNLL 2007,
pp. 1005–1014.
Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012).
Semantic compositionality through recursive matrix-vector
spaces. In EMNLP 2012, pp. 1201–1211.
Soderland, S., Fisher, D., Aseltine, J., and Lehnert, W. G.
(1995). CRYSTAL: Inducing a conceptual dictionary. In
IJCAI-95, Montreal, pp. 1134–1142.
Søgaard, A. (2010). Simple semi-supervised training of
part-of-speech taggers. In ACL 2010, pp. 205–208.
Søgaard, A., Johannsen, A., Plank, B., Hovy, D., and
Alonso, H. M. (2014). What’s in a p-value in NLP?. In
CoNLL-14.
Solorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M.,
Ghoneim, M., Hawwari, A., AlGhamdi, F., Hirschberg, J.,
Chang, A., and Fung, P. (2014). Overview for the ﬁrst
shared task on language identiﬁcation in code-switched
data. In Proceedings of the First Workshop on Computa-
tional Approaches to Code Switching, pp. 62–72.
Sordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y.,
Mitchell, M., Nie, J.-Y., Gao, J., and Dolan, B. (2015). A
neural network approach to context-sensitive generation of
conversational responses. In NAACL HLT 2015, pp. 196–
205.
Sparck Jones, K. (1972). A statistical interpretation of term
speciﬁcity and its application in retrieval. Journal of Doc-
umentation, 28(1), 11–21.
Sparck Jones, K. (1986). Synonymy and Semantic Classiﬁ-
cation. Edinburgh University Press, Edinburgh. Republi-
cation of 1964 PhD Thesis.
Spitkovsky, V. I. and Chang, A. X. (2012). A cross-lingual
dictionary for English Wikipedia concepts.
In LREC-12,
Istanbul, Turkey.
Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. R. (2014). Dropout: a simple way
to prevent neural networks from overﬁtting.. Journal of
Machine Learning Research, 15(1), 1929–1958.
Stalnaker, R. C. (1978). Assertion. In Cole, P. (Ed.), Prag-
matics: Syntax and Semantics Volume 9, pp. 315–332. Aca-
demic Press.

538 Bibliography

Stamatatos, E. (2009). A survey of modern authorship at-
tribution methods. JASIST, 60(3), 538–556.
Steedman, M. (1989). Constituency and coordination in a
combinatory grammar. In Baltin, M. R. and Kroch, A. S.
(Eds.), Alternative Conceptions of Phrase Structure, pp.
201–231. University of Chicago.
Steedman, M. (1996). Surface Structure and Interpretation.
MIT Press. Linguistic Inquiry Monograph, 30.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Stetina, J. and Nagao, M. (1997). Corpus based PP attach-
ment ambiguity resolution with a semantic dictionary. In
Zhou, J. and Church, K. W. (Eds.), Proceedings of the Fifth
Workshop on Very Large Corpora, Beijing, China, pp. 66–
80.
Stifelman, L. J., Arons, B., Schmandt, C., and Hulteen, E. A.
(1993). VoiceNotes: A speech interface for a hand-held
voice notetaker. In Human Factors in Computing Systems:
INTERCHI ’93 Conference Proceedings, pp. 179–186.
Stolcke, A. (1995). An efﬁcient probabilistic context-free
parsing algorithm that computes preﬁx probabilities. Com-
putational Linguistics, 21(2), 165–202.
Stolcke, A. (1998). Entropy-based pruning of backoff lan-
guage models. In Proc. DARPA Broadcast News Transcrip-
tion and Understanding Workshop, Lansdowne, VA, pp.
270–274.
Stolcke, A. (2002). SRILM – an extensible language mod-
eling toolkit. In ICSLP-02, Denver, CO.
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R.,
Jurafsky, D., Taylor, P., Martin, R., Meteer, M., and Van
Ess-Dykema, C. (2000). Dialogue act modeling for au-
tomatic tagging and recognition of conversational speech.
Computational Linguistics, 26(3), 339–371.
Stolz, W. S., Tannenbaum, P. H., and Carstensen, F. V.
(1965). A stochastic approach to the grammatical coding
of English. Communications of the ACM, 8(6), 399–405.
Stone, P., Dunphry, D., Smith, M., and Ogilvie, D. (1966).
The General Inquirer: A Computer Approach to Content
Analysis. Cambridge, MA: MIT Press.
Stoyanchev, S. and Johnston, M. (2015). Localized error
detection for targeted clariﬁcation in a virtual assistant. In
ICASSP-15, pp. 5241–5245.
Stoyanchev, S., Liu, A., and Hirschberg, J. (2013). Mod-
elling human clariﬁcation strategies.
In SIGDIAL 2013,
pp. 137–141.
Stoyanchev, S., Liu, A., and Hirschberg, J. (2014). Towards
natural clariﬁcation questions in dialogue systems. In AISB
symposium on questions, discourse and dialogue.
Str ¨otgen, J. and Gertz, M. (2013). Multilingual and cross-
domain temporal tagging. Language Resources and Eval-
uation, 47 (2), 269–298.
Suendermann, D., Evanini, K., Liscombe, J., Hunter, P.,
Dayanidhi, K., and Pieraccini, R. (2009). From rule-based
to statistical grammars: Continuous improvement of large-
scale spoken dialog systems.
In ICASSP-09, pp. 4713–
4716.
Sundheim, B. (Ed.). (1991). Proceedings of MUC-3.
Sundheim, B. (Ed.). (1992). Proceedings of MUC-4.
Sundheim, B. (Ed.). (1993). Proceedings of MUC-5, Balti-
more, MD.
Sundheim, B. (Ed.). (1995). Proceedings of MUC-6.
Surdeanu, M. (2013). Overview of the TAC2013 Knowl-
edge Base Population evaluation: English slot ﬁlling and
temporal slot ﬁlling. In TAC-13.

Surdeanu, M., Harabagiu, S., Williams, J., and Aarseth, P.
(2003). Using predicate-argument structures for informa-
tion extraction. In ACL-03, pp. 8–15.
Surdeanu, M., Johansson, R., Meyers, A., M `arquez, L., and
Nivre, J. (2008a). The conll-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In CoNLL-08,
pp. 159–177.
Surdeanu, M., Johansson, R., Meyers, A., M `arquez, L., and
Nivre, J. (2008b). The conll-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In CoNLL-08,
pp. 159–177.
Swerts, M., Litman, D. J., and Hirschberg, J. (2000). Cor-
rections in spoken dialogue systems. In ICSLP-00, Beijing,
China.
Swier, R. and Stevenson, S. (2004). Unsupervised semantic
role labelling. In EMNLP 2004, pp. 95–102.
Switzer, P. (1965).
Vector images in document re-
trieval.
In Stevens, M. E., Giuliano, V. E., and
Heilprin, L. B.
(Eds.), Statistical Association Meth-
ods For Mechanized Documentation. Symposium Pro-
ceedings. Washington, D.C., USA, March 17, 1964,

pp. 163–171. https://nvlpubs.nist.gov/nistpubs/
Legacy/MP/nbsmiscellaneouspub269.pdf.

Talbot, D. and Osborne, M. (2007). Smoothed Bloom Fil-
ter Language Models: Tera-Scale LMs on the Cheap. In
EMNLP/CoNLL 2007, pp. 468–476.
Talmor, A. and Berant, J. (2018). The web as a knowledge-
base for answering complex questions.
In NAACL HLT
2018.
Tannen, D. (1979). What’s in a frame? Surface evidence for
underlying expectations. In Freedle, R. (Ed.), New Direc-
tions in Discourse Processing, pp. 137–181. Ablex.
Taskar, B., Klein, D., Collins, M., Koller, D., and Manning,
C. D. (2004). Max-margin parsing. In EMNLP 2004, pp.
1–8.
ter Meulen, A. (1995). Representing Time in Natural Lan-
guage. MIT Press.
Tesni `ere, L. (1959).
´El ´ements de Syntaxe Structurale. Li-
brairie C. Klincksieck, Paris.
Thede, S. M. and Harper, M. P. (1999). A second-order hid-
den Markov model for part-of-speech tagging. In ACL-99,
College Park, MA, pp. 175–182.
Thompson, K. (1968). Regular expression search algorithm.
Communications of the ACM, 11(6), 419–422.
Tibshirani, R. J. (1996). Regression shrinkage and selec-
tion via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1), 267–288.
Titov, I. and Henderson, J. (2006). Loss minimization in
parse reranking. In EMNLP 2006.
Titov, I. and Khoddam, E. (2014). Unsupervised induction
of semantic roles within a reconstruction-error minimiza-
tion framework. In NAACL HLT 2015.
Titov, I. and Klementiev, A. (2012). A Bayesian approach
to unsupervised semantic role induction. In EACL-12, pp.
12–22.
Tomkins, S. S. (1962). Affect, imagery, consciousness: Vol.
I. The positive affects. Springer.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic
dependency network. In HLT-NAACL-03.
Toutanova, K., Manning, C. D., Flickinger, D., and Oepen,
S. (2005). Stochastic HPSG Parse Disambiguation using
the Redwoods Corpus. Research on Language & Compu-
tation, 3(1), 83–105.

Toutanova, K. and Moore, R. C. (2002). Pronunciation
modeling for improved spelling correction.
In ACL-02,
Philadelphia, PA, pp. 144–151.
Tseng, H., Chang, P.-C., Andrew, G., Jurafsky, D., and Man-
ning, C. D. (2005a). Conditional random ﬁeld word seg-
menter. In Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing.
Tseng, H., Jurafsky, D., and Manning, C. D. (2005b). Mor-
phological features help POS tagging of unknown words
across language varieties.
In Proceedings of the 4th
SIGHAN Workshop on Chinese Language Processing.
Turian, J., Ratinov, L., and Bengio, Y. (2010). Word
representations: a simple and general method for semi-
supervised learning. In ACL 2010, pp. 384–394.
Turney, P. D. (2002). Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classiﬁcation of
reviews. In ACL-02.
Turney, P. D. and Littman, M. (2003). Measuring praise and
criticism: Inference of semantic orientation from associa-
tion. ACM Transactions on Information Systems (TOIS),
21, 315–346.
UzZaman, N., Llorens, H., Derczynski, L., Allen, J., Ver-
hagen, M., and Pustejovsky, J. (2013). Semeval-2013 task
1: Tempeval-3: Evaluating time expressions, events, and
temporal relations. In SemEval-13, pp. 1–9.
van Benthem, J. and ter Meulen, A. (Eds.). (1997). Hand-
book of Logic and Language. MIT Press.
van der Maaten, L. and Hinton, G. E. (2008). Visualiz-
ing high-dimensional data using t-sne. Journal of Machine
Learning Research, 9, 2579–2605.
van Rijsbergen, C. J. (1975). Information Retrieval. But-
terworths.
Van Valin, Jr., R. D. and La Polla, R. (1997). Syntax: Struc-
ture, Meaning, and Function. Cambridge University Press.
VanLehn, K., Jordan, P. W., Ros ´e, C., Bhembe, D., B ¨ottner,
M., Gaydos, A., Makatchev, M., Pappuswamy, U., Ringen-
berg, M., Roque, A., Siler, S., Srivastava, R., and Wilson,
R. (2002). The architecture of Why2-Atlas: A coach for
qualitative physics essay writing. In Proc. Intelligent Tu-
toring Systems.
Vasilescu, F., Langlais, P., and Lapalme, G. (2004). Evaluat-
ing variants of the lesk approach for disambiguating words.
In LREC-04, Lisbon, Portugal, pp. 633–636. ELRA.
Veblen, T. (1899). Theory of the Leisure Class. Macmillan
Company, New York.
Velikovich, L., Blair-Goldensohn, S., Hannan, K., and Mc-
Donald, R. (2010). The viability of web-derived polarity
lexicons. In NAACL HLT 2010, pp. 777–785.
Vendler, Z. (1967). Linguistics in Philosophy. Cornell Uni-
versity Press, Ithaca, NY.
Verhagen, M., Gaizauskas, R., Schilder, F., Hepple, M.,
Moszkowicz, J., and Pustejovsky, J. (2009). The tempeval
challenge: identifying temporal relations in text. Language
Resources and Evaluation, 43(2), 161–179.
Verhagen, M., Mani, I., Sauri, R., Knippen, R., Jang, S. B.,
Littman, J., Rumshisky, A., Phillips, J., and Pustejovsky,
J. (2005). Automating temporal annotation with tarsqi. In
ACL-05, pp. 81–84.
Vintsyuk, T. K. (1968). Speech discrimination by dynamic
programming. Cybernetics, 4(1), 52–57. Russian Kiber-
netika 4(1):81-88. 1968.
Vinyals, O. and Le, Q. (2015). A neural conversational
model. In Proceedings of ICML Deep Learning Workshop,
Lille, France.

Bibliography

539

Viterbi, A. J. (1967). Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. IEEE
Transactions on Information Theory, IT-13(2), 260–269.
Voutilainen, A. (1995). Morphological disambiguation. In
Karlsson, F., Voutilainen, A., Heikkil ¨a, J., and Anttila,
A. (Eds.), Constraint Grammar: A Language-Independent
System for Parsing Unrestricted Text, pp. 165–284. Mouton
de Gruyter.
Voutilainen, A. (1999). Handcrafted rules.
In van Hal-
teren, H. (Ed.), Syntactic Wordclass Tagging, pp. 217–246.
Kluwer.
Wade, E., Shriberg, E., and Price, P. J. (1992). User behav-
iors affecting speech recognition. In ICSLP-92, pp. 995–
998.
Wagner, R. A. and Fischer, M. J. (1974). The string-to-
string correction problem. Journal of the Association for
Computing Machinery, 21, 168–173.
Walker, M. A. (2000). An application of reinforcement
learning to dialogue strategy selection in a spoken dialogue
system for email.
Journal of Artiﬁcial Intelligence Re-
search, 12, 387–416.
Walker, M. A., Fromer, J. C., and Narayanan, S. S. (1998).
Learning optimal dialogue strategies: A case study of a
spoken dialogue agent for email.
In COLING/ACL-98,
Montreal, Canada, pp. 1345–1351.
Walker, M. A., Kamm, C. A., and Litman, D. J. (2001).
Towards developing general models of usability with PAR-
ADISE. Natural Language Engineering: Special Issue on
Best Practice in Spoken Dialogue Systems, 6(3), 363–377.
Walker, M. A. and Whittaker, S. (1990). Mixed initiative in
dialogue: An investigation into discourse segmentation. In
ACL-90, Pittsburgh, PA, pp. 70–78.
Wang, H., Lu, Z., Li, H., and Chen, E. (2013). A dataset for
research on short-text conversations.. In EMNLP 2013, pp.
935–945.
Wang, S. and Manning, C. D. (2012). Baselines and bi-
grams: Simple, good sentiment and topic classiﬁcation. In
ACL 2012, pp. 90–94.
Ward, N. and Tsukahara, W. (2000). Prosodic features
which cue back-channel feedback in English and Japanese.
Journal of Pragmatics, 32, 1177–1207.
Ward, W. and Issar, S. (1994). Recent improvements
in the CMU spoken language understanding system.
In
ARPA Human Language Technologies Workshop, Plains-
boro, N.J.
Warriner, A. B., Kuperman, V., and Brysbaert, M. (2013).
Norms of valence, arousal, and dominance for 13,915 En-
glish lemmas. Behavior Research Methods, 45(4), 1191–
1207.
Weaver, W. (1949/1955). Translation. In Locke, W. N. and
Boothe, A. D. (Eds.), Machine Translation of Languages,
pp. 15–23. MIT Press. Reprinted from a memorandum
written by Weaver in 1949.
Weinschenk, S. and Barker, D. T. (2000). Designing Effec-
tive Speech Interfaces. Wiley.
Weischedel, R., Hovy, E. H., Marcus, M. P., Palmer, M.,
Belvin, R., Pradhan, S., Ramshaw, L. A., and Xue, N.
(2011). Ontonotes: A large training corpus for enhanced
processing.
In Joseph Olive, Caitlin Christianson, J. M.
(Ed.), Handbook of Natural Language Processing and Ma-
chine Translation: DARPA Global Automatic Language
Exploitation, pp. 54–63. Springer.
Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. A.,
and Palmucci, J. (1993). Coping with ambiguity and un-
known words through probabilistic models. Computational
Linguistics, 19(2), 359–382.

540 Bibliography

Weizenbaum, J. (1966). ELIZA – A computer program for
the study of natural language communication between man
and machine. Communications of the ACM, 9(1), 36–45.
Weizenbaum, J. (1976). Computer Power and Human Rea-
son: From Judgement to Calculation. W.H. Freeman and
Company.
Wen, T.-H., Gaˇsi ´c, M., Kim, D., Mrkˇsi ´c, N., Su, P.-H.,
Vandyke, D., and Young, S. J. (2015a). Stochastic lan-
guage generation in dialogue using recurrent neural net-
works with convolutional sentence reranking. In SIGDIAL
2015, pp. 275––284.
Wen, T.-H., Gaˇsi ´c, M., Mrkˇsi ´c, N., Su, P.-H., Vandyke, D.,
and Young, S. J. (2015b). Semantically conditioned lstm-
based natural language generation for spoken dialogue sys-
tems. In EMNLP 2015.
Whitelaw, C., Hutchinson, B., Chung, G. Y., and El-
lis, G. (2009). Using the web for language independent
spellchecking and autocorrection. In EMNLP-09, pp. 890–
899.
Widrow, B. and Hoff, M. E. (1960). Adaptive switching
circuits. In IRE WESCON Convention Record, Vol. 4, pp.
96–104.
Wiebe, J. (1994). Tracking point of view in narrative. Com-
putational Linguistics, 20(2), 233–287.
Wiebe, J. (2000). Learning subjective adjectives from cor-
pora. In AAAI-00, Austin, TX, pp. 735–740.
Wiebe, J., Bruce, R. F., and O’Hara, T. P. (1999). Devel-
opment and use of a gold-standard data set for subjectivity
classiﬁcations. In ACL-99, pp. 246–253.
Wiebe, J., Wilson, T., and Cardie, C. (2005). Annotating ex-
pressions of opinions and emotions in language. Language
resources and evaluation, 39(2-3), 165–210.
Wierzbicka, A. (1992). Semantics, Culture, and Cognition:
University Human Concepts in Culture-Speciﬁc Conﬁgura-
tions. Oxford University Press.
Wierzbicka, A. (1996). Semantics: Primes and Universals.
Oxford University Press.
Wilcox-O’Hearn, L. A. (2014). Detection is the central
problem in real-word spelling correction. http://arxiv.

org/abs/1408.3153.

Wilcox-O’Hearn, L. A., Hirst, G., and Budanitsky, A.
(2008). Real-word spelling correction with trigrams: A
reconsideration of the Mays, Damerau, and Mercer model.
In CICLing-2008, pp. 605–616.
Wilensky, R. (1983). Planning and Understanding: A
Computational Approach to Human Reasoning. Addison-
Wesley.
Wilks, Y. (1973). An artiﬁcial intelligence approach to ma-
chine translation. In Schank, R. C. and Colby, K. M. (Eds.),
Computer Models of Thought and Language, pp. 114–151.
W.H. Freeman.
Wilks, Y. (1975a). An intelligent analyzer and understander
of English. Communications of the ACM, 18(5), 264–274.
Wilks, Y. (1975b). Preference semantics. In Keenan, E. L.
(Ed.), The Formal Semantics of Natural Language, pp.
329–350. Cambridge Univ. Press.
Wilks, Y. (1975c). A preferential, pattern-seeking, seman-
tics for natural language inference. Artiﬁcial Intelligence,
6(1), 53–74.
Williams, J. D., Raux, A., and Henderson, M. (2016). The
dialog state tracking challenge series: A review. Dialogue
& Discourse, 7 (3), 4–33.
Williams, J. D. and Young, S. J. (2007). Partially observ-
able markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(1), 393–422.

Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recogniz-
ing contextual polarity in phrase-level sentiment analysis.
In HLT-EMNLP-05, pp. 347–354.
Winkler, W. E. (2006). Overview of record linkage and cur-
rent research directions. Tech. rep., Statistical Research
Division, U.S. Census Bureau.
Winograd, T. (1972). Understanding Natural Language.
Academic Press.
Winston, P. H. (1977). Artiﬁcial Intelligence. Addison Wes-
ley.
Witten, I. H. and Bell, T. C. (1991). The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE Transactions on Informa-
tion Theory, 37 (4), 1085–1094.
Witten, I. H. and Frank, E. (2005). Data Mining: Practical
Machine Learning Tools and Techniques (2nd Ed.). Mor-
gan Kaufmann.

Wittgenstein, L. (1953).

Philosophical Investigations.
(Translated by Anscombe, G.E.M.). Blackwell.
Woods, W. A. (1967). Semantics for a Question-Answering
System. Ph.D. thesis, Harvard University.
Woods, W. A. (1973). Progress in natural language under-
standing. In Proceedings of AFIPS National Conference,
pp. 441–450.
Woods, W. A. (1975). What’s in a link: Foundations for
semantic networks. In Bobrow, D. G. and Collins, A. M.
(Eds.), Representation and Understanding: Studies in Cog-
nitive Science, pp. 35–82. Academic Press.
Woods, W. A. (1978). Semantics and quantiﬁcation in natu-
ral language question answering. In Yovits, M. (Ed.), Ad-
vances in Computers, pp. 2–64. Academic.
Woods, W. A., Kaplan, R. M., and Nash-Webber, B. L.
(1972). The lunar sciences natural language information
system: Final report. Tech. rep. 2378, BBN.
Woodsend, K. and Lapata, M. (2015). Distributed represen-
tations for unsupervised semantic role labeling. In EMNLP
2015, pp. 2482–2491.
Wu, F. and Weld, D. S. (2007). Autonomously semantifying
Wikipedia. In CIKM-07, pp. 41–50.
Wu, F. and Weld, D. S. (2010). Open information extraction
using Wikipedia. In ACL 2010, pp. 118–127.
Wu, Z. and Palmer, M. (1994). Verb semantics and lexical
selection. In ACL-94, Las Cruces, NM, pp. 133–138.
Wundt, W. (1900). V ¨olkerpsychologie: eine Untersuchung
der Entwicklungsgesetze von Sprache, Mythus, und Sitte.
W. Engelmann, Leipzig. Band II: Die Sprache, Zweiter
Teil.
Xia, F. and Palmer, M. (2001). Converting dependency struc-
tures to phrase structures. In HLT-01, San Diego, pp. 1–5.
Xue, N. and Palmer, M. (2004). Calibrating features for se-
mantic role labeling. In EMNLP 2004.
Yamada, H. and Matsumoto, Y. (2003). Statistical depen-
dency analysis with support vector machines.
In Noord,
G. V. (Ed.), IWPT-03, pp. 195–206.
Yan, Z., Duan, N., Bao, J.-W., Chen, P., Zhou, M., Li, Z.,
and Zhou, J. (2016). DocChat: An information retrieval ap-
proach for chatbot engines using unstructured documents.
In ACL 2016.
Yang, Y., Yih, W.-t., and Meek, C. (2015). Wikiqa: A
challenge dataset for open-domain question answering. In
EMNLP 2015.
Yang, Y. and Pedersen, J. (1997). A comparative study on
feature selection in text categorization. In ICML, pp. 412–
420.

Yankelovich, N., Levow, G.-A., and Marx, M. (1995). De-
signing SpeechActs: Issues in speech user interfaces.
In
Human Factors in Computing Systems: CHI ’95 Confer-
ence Proceedings, Denver, CO, pp. 369–376.
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL-95, Cam-
bridge, MA, pp. 189–196.
Yasseri, T., Kornai, A., and Kert ´esz, J. (2012). A practical
approach to language complexity: a Wikipedia case study.
PloS one, 7 (11).
Yih, W.-t., Richardson, M., Meek, C., Chang, M.-W., and
Suh, J. (2016). The value of semantic parse labeling for
knowledge base question answering.
In ACL 2016, pp.
201–206.
Yngve, V. H. (1955). Syntax and the problem of multiple
meaning. In Locke, W. N. and Booth, A. D. (Eds.), Ma-
chine Translation of Languages, pp. 208–226. MIT Press.
Yngve, V. H. (1970). On getting a word in edgewise.
In
CLS-70, pp. 567–577. University of Chicago.
Young, S. J., Gaˇsi ´c, M., Keizer, S., Mairesse, F., Schatz-
mann, J., Thomson, B., and Yu, K. (2010). The Hid-
den Information State model: A practical framework for
POMDP-based spoken dialogue management. Computer
Speech & Language, 24(2), 150–174.
Younger, D. H. (1967). Recognition and parsing of context-
free languages in time n3 .
Information and Control, 10,
189–208.
Yuret, D. (1998). Discovery of Linguistic Relations Using
Lexical Attraction. Ph.D. thesis, MIT.
Yuret, D. (2004). Some experiments with a Naive Bayes
WSD system. In Senseval-3: 3rd International Workshop
on the Evaluation of Systems for the Semantic Analysis of
Text.
Zapirain, B., Agirre, E., M `arquez, L., and Surdeanu, M.
(2013). Selectional preferences for semantic role classiﬁ-
cation. Computational Linguistics, 39(3), 631–663.
Zavrel, J. and Daelemans, W. (1997). Memory-based learn-
ing: Using similarity for smoothing.
In ACL/EACL-97,
Madrid, Spain, pp. 436–443.
Zelle, J. M. and Mooney, R. J. (1996). Learning to parse
database queries using inductive logic programming.
In
AAAI-96, pp. 1050–1055.
Zeman, D. (2008). Reusable tagset conversion using tagset
drivers.. In LREC-08.
Zeman, D., Popel, M., Straka, M., Haji ˇc, J., Nivre, J., Gin-
ter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Potthast, M.,
Tyers, F. M., Badmaeva, E., Gokirmak, M., Nedoluzhko,
A., Cinkov ´a, S., Hajic, Jr., J., Hlav ´acov ´a, J., Kettnerov ´a,
V., Uresov ´a, Z., Kanerva, J., Ojala, S., Missil ¨a, A., Man-
ning, C. D., Schuster, S., Reddy, S., Taji, D., Habash,
N., Leung, H., de Marneffe, M.-C., Sanguinetti, M., Simi,
M., Kanayama, H., de Paiva, V., Droganova, K., Alonso,
H. M., C¸ ¨oltekin, C¸ ., Sulubacak, U., Uszkoreit, H., Macke-
tanz, V., Burchardt, A., Harris, K., Marheinecke, K., Rehm,
G., Kayadelen, T., Attia, M., El-Kahky, A., Yu, Z., Pitler,
E., Lertpradit, S., Mandl, M., Kirchner, J., Alcalde, H. F.,
Strnadov ´a, J., Banerjee, E., Manurung, R., Stella, A., Shi-
mada, A., Kwak, S., Mendonc¸ a, G., Lando, T., Nitisaroj,
R., and Li, J. (2017). Conll 2017 shared task: Multilin-
gual parsing from raw text to universal dependencies. In
Proceedings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies, Van-
couver, Canada, August 3-4, 2017, pp. 1–19.
Zettlemoyer, L. and Collins, M. (2005). Learning to map
sentences to logical form: Structured classiﬁcation with
probabilistic categorial grammars.
In Uncertainty in Ar-
tiﬁcial Intelligence, UAI’05, pp. 658–666.

Bibliography

541

Zhang, Y. and Clark, S. (2008). A tale of two parsers: inves-
tigating and combining graph-based and transition-based
dependency parsing using beam-search. In EMNLP-08, pp.
562–571.

Zhang, Y. and Nivre, J. (2011). Transition-based depen-
dency parsing with rich non-local features. In ACL 2011,
pp. 188–193.

Zhao, H., Chen, W., Kit, C., and Zhou, G. (2009). Mul-
tilingual dependency learning: A huge feature engineering
method to semantic dependency parsing. In CoNLL-09, pp.
55–60.

Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-
W. (2017). Men also like shopping: Reducing gender bias
ampliﬁcation using corpus-level constraints.
In EMNLP
2017.

Zhong, Z. and Ng, H. T. (2010). It makes sense: A wide-
coverage word sense disambiguation system for free text.
In ACL 2010, pp. 78–83.

Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and
Sch ¨olkopf, B. (2004). Learning with local and global con-
sistency. In NIPS 2004.

Zhou, G., Su, J., Zhang, J., and Zhang, M. (2005). Exploring
various knowledge in relation extraction. In ACL-05, Ann
Arbor, MI, pp. 427–434.

Zhou, J. and Xu, W. (2015). End-to-end learning of seman-
tic role labeling using recurrent neural networks. In ACL
2015, pp. 1127–1137.

Zhu, X. and Ghahramani, Z. (2002). Learning from la-
beled and unlabeled data with label propagation. Tech. rep.
CMU-CALD-02, CMU.

Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-
supervised learning using gaussian ﬁelds and harmonic
functions. In ICML 2003, pp. 912–919.

Zue, V. W., Glass, J., Goodine, D., Leung, H., Phillips, M.,
Polifroni, J., and Seneff, S. (1989). Preliminary evalua-
tion of the VOYAGER spoken language system.
In Pro-
ceedings DARPA Speech and Natural Language Workshop,
Cape Cod, MA, pp. 160–167.

Zwicky, A. and Sadock, J. M. (1975). Ambiguity tests and
how to fail them. In Kimball, J. (Ed.), Syntax and Seman-
tics 4, pp. 1–36. Academic Press.

Author Index

ˇSev ˇc´ıkov ´a, M., 274, 293
ˇSt ˇep ´anek, J., 274, 293

Aarseth, P., 375
Abadi, M., 145
Abeill ´e, A., 267, 268
Abelson, R. P., 351, 362,
375
Abney, S. P., 174, 235, 263,
264, 267, 268, 514
Adriaans, P., 268
Agarwal, A., 145
Aggarwal, C. C., 80
Agichtein, E., 340, 354
Agirre, E., 126, 376, 504,
510, 514
Agrawal, M., 393–395
Ahmad, F., 488
Ahn, D., 35
Ahn, Y.-Y., 384
Aho, A. V., 235, 275
Ajdukiewicz, K., 214
Akoglu, L., 514
Alcalde, H. F., 291
AlGhamdi, F., 21
Algoet, P. H., 58
Allen, J., 323, 349, 350,
355, 459, 460
Alonso, H. M., 81, 291
Amsler, R. A., 514
An, J., 384
Anderson, A. H., 452
Andrew, G., 35
Androutsopoulos, I., 80
Angelard-Gontier, N., 430,
443
Antiga, L., 145
Anttila, A., 175, 293
Appelt, D. E., 352–354
Appling, S., 443
Arons, B., 443
Artstein, R., 430
Aseltine, J., 354
Atkins, S., 505
Atkinson, K., 490
Auer, S., 336, 412
Auli, M., 429
Austin, J. L., 447, 459
Awadallah, A. H., 454

Ba, J., 145
Baayen, R. H., 35
Babko-Malaya, O., 514
Bacchiani, M., 61
Baccianella, S., 387, 514
Bach, K., 447
Backus, J. W., 195, 220
Badmaeva, E., 291
Bahl, L. R., 60, 174
Baker, C., 376
Baker, C. F., 362
Baker, J. K., 60, 245, 267
Baldwin, T., 72, 80, 514

Ballard, D. H., 150
Ballesteros, M., 333, 354
Balogh, J., 443, 449, 455
Banaji, M. R., 125
Banea, C., 126, 504
Banerjee, E., 291
Banerjee, S., 502
Bangalore, S., 267
Banko, M., 355, 405, 408
Bansal, M., 338, 354
Bao, J.-W., 429
Bar-Hillel, Y., 214
Barham, P., 145
Barker, D. T., 455
Barrett, L., 250, 267
Bates, R., 459
Bauer, F. L., 220
Baum, L. E., 474, 479
Baum, L. F., 442
Baumer, G., 81
Bayes, T., 65
Bazell, C. E., 220
Bear, J., 352–354
Becker, C., 336, 412
Beil, F., 376
Bej ˇcek, E., 274, 293
Bell, E., 376, 395, 396
Bell, T. C., 61, 255
Bellegarda, J. R., 61, 128,
444
Bellman, R., 31, 35
Belvin, R., 274
Bender, E. M., 221
Bengio, S., 430
Bengio, Y., 61, 101, 119,
129, 135–137, 145,
150, 430, 436
Be ˇnuˇs, ˇS., 459
Berant, J., 414, 415, 419,
421
Berg-Kirkpatrick, T.,
77–79, 81
Berger, A., 100
Bergsma, S., 376, 489
Bernstein, M. S., 398
Bethard, S., 21, 355, 376
Bever, T. G., 265
Bhat, I., 285
Bhat, R. A., 285
Bhembe, D., 424
Biber, D., 221
Bies, A., 208, 210
Bikel, D. M., 255, 268, 354
Bills, S., 341, 354
Birch, A., 27, 29
Bird, S., 35
Bisani, M., 81
Bishop, C. M., 81, 100
Bizer, C., 336, 412
Bj ¨orkelund, A., 368
Black, E., 263, 264, 267,
514
Blair, C. R., 491
Blair, E., 21

Blair-Goldensohn, S., 397,
514
Blei, D. M., 129, 514
Blodgett, S. L., 21, 47, 72
Bloomﬁeld, L., 211, 220,
356
Blunsom, P., 419, 421
Bobrow, D. G., 323, 374,
375, 423, 431, 432,
444
Bobrow, R. J., 444
Bod, R., 267
Bogomolov, M., 81
Boguraev, B., 514
Boguraev, B. K., 421
Bohus, D., 455
Bojanowski, P., 129
Bollacker, K., 336, 412
Bolukbasi, T., 125, 126
Booth, R. J., 71, 382, 383
Booth, T. L., 238, 267
Bordes, A., 404, 410, 421
Borges, J. L., 63
Boser, B., 150
Boser, B. E., 150
B ¨ottner, M., 424
Bottou, L., 119, 129, 354,
375
Bourlard, H., 150
Bousquet, O., 397
Bowman, S. R., 430
Boyd-Graber, J., 421, 514
Brachman, R. J., 322, 323
Brants, T., 56, 166, 167,
174, 175
Br ´eal, M., 103
Breck, E., 409, 421
Bresnan, J., 214, 220, 237
Brevdo, E., 145
Brill, E., 267, 405, 408,
489, 490
Brin, S., 354
Briscoe, T., 264, 267, 514
Broadhead, M., 355
Brockett, C., 429
Brockman, W., 124
Brockmann, C., 372
Brody, S., 514
Broschart, J., 152
Bruce, B. C., 460
Bruce, R., 514
Bruce, R. F., 378, 505
Brysbaert, M., 381–383
Bryson, J. J., 125
Bu, J., 514
Buchholz, S., 294
Buck, C., 48
Budanitsky, A., 104, 492
Bullinaria, J. A., 129
Bulyko, I., 61, 454
Bunker, R. T., 505
Burchardt, A., 291
Burger, J. D., 409, 421
Burges, C. J. C., 419, 428

543

Burget, L., 129
Burkett, D., 77–79, 81
Burnett, D., 442

Cafarella, M., 355
Caliskan, A., 125
Candito, M., 294
Canon, S., 268
Cardie, C., 126, 354, 504,
514
Carletta, J., 452
Carmel, D., 421
Carpenter, B., 458
Carpenter, R., 424, 429
Carreras, X., 368, 375
Carroll, G., 268, 376
Carroll, J., 264, 267
Carstensen, F. V., 174
Casta ˜no, J., 345, 347, 349
Celikyilmaz, A., 436
Cer, D., 126, 504
ˇCernock `y, J. H., 129
Chahuneau, V., 391, 392
Chai, J. Y., 376
Chambers, N., 352, 355,
372
Chanan, G., 145
Chanev, A., 293
Chang, A., 21
Chang, A. X., 347, 355,
414, 418
Chang, J. S., 514
Chang, K.-W., 125, 126
Chang, M.-W., 419
Chang, P.-C., 35
Charles, W. G., 397, 504
Charlin, L., 428, 430
Charniak, E., 174, 211, 250,
267, 268, 513
Che, W., 375
Chelba, C., 242
Chen, B., 398
Chen, C., 514
Chen, D., 283, 404, 410,
421
Chen, E., 429
Chen, J. N., 514
Chen, K., 119, 127, 129
Chen, M. Y., 397
Chen, P., 429
Chen, S. F., 53, 55, 61
Chen, W., 375
Chen, X., 107
Chen, Y.-N., 436
Chen, Z., 145
Cherry, C., 429
Chi, Z., 268
Chiang, D., 268
Chiba, S., 479
Chierchia, G., 323
Chinchor, N., 81, 354
Chintala, S., 145
Chiticariu, L., 333
Chklovski, T., 514

544 Author Index

Chodorow, M. S., 500
Choi, E., 419
Choi, J. D., 285, 294
Choi, Y., 376, 395, 396,
419, 514
Chomsky, C., 322, 412, 420
Chomsky, N., 60, 195, 213,
220
Chou, A., 414, 419
Chow, Y.-L., 268
Christodoulopoulos, C.,
175
Chu, Y.-J., 288
Chu-Carroll, J., 421, 458,
459
Chung, G. Y., 488, 489
Church, A., 308
Church, K. W., 22, 51, 53,
56, 61, 116, 174,
235, 372, 481,
483–485, 492, 507,
511
Ciaramita, M., 294
Cinkov ´a, S., 291
Citro, C., 145
Clark, A., 268
Clark, C., 421
Clark, E., 103
Clark, H. H., 20, 445, 448,
459
Clark, J. H., 55, 56, 61
Clark, K., 384–386
Clark, P., 419
Clark, S., 129, 175, 268,
283
Coccaro, N., 128, 459
Cohen, D. N., 479
Cohen, K. B., 327
Cohen, M. H., 443, 449,
455
Cohen, P. R., 459, 460
Colaresi, M. P., 390–392
Colby, K. M., 427, 428, 444
Cole, R. A., 442
Collins, M., 61, 211–213,
248, 250, 251, 255,
264, 267, 268, 275,
413
Collobert, R., 119, 129,
354, 375
Colombe, J. B., 354
Colosimo, M., 354
C¸ ¨oltekin, C¸ ., 291
Conrad, S., 221
Conrath, D. W., 502
Conti, J., 442
Cook, P., 514
Copestake, A., 514
Coppola, B., 421
Corrado, G. S., 119, 127,
129
Cotton, S., 505
Cottrell, G. W., 513
Courville, A., 101,
135–137, 150
Cover, T. M., 57, 58
Covington, M., 277, 293
Cowhey, I., 419

Cox, D., 99
Cox, S. J., 81
Crammer, K., 293
Craven, M., 354
Crouch, R., 268
Cruse, D. A., 129
Cucerzan, S., 489
Culicover, P. W., 221
Curran, J. R., 175, 268
Cyganiak, R., 336, 412

Daelemans, W., 267
Dagan, I., 116, 118, 129
Dai, A. M., 430
Damerau, F. J., 481, 486,
487, 491, 492
Dang, H. T., 354, 359, 375,
505, 506, 514
Danieli, M., 441, 455
Danilevsky, M., 333
Das, D., 293, 421
Das, S. R., 397
Dauphin, Y., 436
Davidson, D., 312, 323
Davidson, T., 443
Davies, M., 124
Davis, A., 145
Davis, E., 323
Day, D. S., 349
Dayanidhi, K., 437
Dean, J., 56, 119, 127, 129,
145
Deerwester, S. C., 128
DeJong, G. F., 354, 375
Delfs, L., 505
Della Pietra, S. A., 100
Della Pietra, V. J., 100
Demner-Fushman, D., 327
Dempster, A. P., 474, 484
Deng, L., 436
Denker, J. S., 150
Derczynski, L., 355
DeRose, S. J., 174
Desmaison, A., 145
Devin, M., 145
DeVito, Z., 145
de Lacalle, O. L., 514
de Marneffe, M.-C., 155,
173, 208, 272, 291,
293
de Paiva, V., 291
de Villiers, J. H., 442
Diab, M., 21, 126, 504, 514
Dienes, P., 172
Digman, J. M., 393
Di Marco, A., 514
Dligach, D., 512
Do, Q. N. T., 376
Doddington, G., 194
Doherty-Sneddon, G., 452
Dolan, B., 429
Dolan, W. B., 514
dos Santos, C., 338, 354
Downey, D., 355
Dowty, D. R., 323, 373
Dozat, T., 272, 291, 293
Droganova, K., 291
Dror, R., 81

Duan, N., 429
Dubou ´e, P. A., 421
Ducharme, R., 119, 129,
145
Duda, R. O., 512
Dumais, S. T., 71, 80, 126,
128, 405, 408, 504
Dunphry, D., 71, 381, 397
Dyer, C., 333, 354, 419
Dziurzynski, L., 393–395

Eagon, J. A., 479
Earley, J., 226, 235
Ebert, S., 398
Eckert, W., 460
Edmonds, J., 288
Edmonds, P., 514
Efron, B., 78
Egghe, L., 35
Eichstaedt, J. C., 393–395
Eisner, J., 268, 293, 466
Ejerhed, E. I., 235
Ekman, P., 380
El-Kahky, A., 291
Elisseeff, A., 81
Ellis, G., 488, 489
Ellsworth, M., 362, 363
Elman, J. L., 150, 178
Erk, K., 376
Eryigit, G., 293
Espeholt, L., 421
Esuli, A., 387, 514
Etzioni, O., 342, 343, 354,
355, 376, 414, 415,
419
Evanini, K., 437
Evans, C., 336, 412
Evans, N., 152
Evert, S., 129

Fader, A., 342, 343, 355,
414, 415
Fan, J., 421
Fano, R. M., 116
Fanshel, D., 452
Fanty, M., 442
Farhadi, A., 421
Farkas, R., 294
Fast, E., 398
Feldman, J. A., 150
Fellbaum, C., 497, 505, 514
Feng, S., 421, 514
Fennig, C. D., 21
Fensel, D., 321
Ferguson, J., 466
Ferguson, M., 208, 210
Ferro, L., 345, 347, 349
Ferrucci, D. A., 421
Fessler, L., 443
Fikes, R. E., 460
Fillmore, C. J., 220, 221,
322, 357, 362, 374,
375
Finegan, E., 221
Finkelstein, L., 126, 504
Firth, J. R., 101, 106, 459
Fisch, A., 404, 410, 421
Fischer, M. J., 31, 479, 492

Fisher, D., 354
Flickinger, D., 263, 264,
268
Fodor, J. A., 128, 323, 370
Fodor, P., 421
Fokoue-Nkoutche, A., 421
Foland, Jr., W. R., 375
Folds, D., 443
Forbes-Riley, K., 423, 424
Forchini, P., 428
Forney, Jr., G. D., 479
Fosler, E., 41
Foster, J., 294
Fox Tree, J. E., 20
Francis, H. S., 246
Francis, M. E., 71, 382, 383
Francis, W. N., 19, 174
Frank, E., 81, 100
Franz, A., 56, 267
Fraser, N. M., 442
Freitag, D., 354
Fried, G., 443
Friedman, J. H., 80, 100
Fromer, J. C., 455
Frostig, R., 414, 419
Fung, P., 21
Furnas, G. W., 128
Fyshe, A., 129

Gabow, H. N., 289
Gabrilovich, E., 126, 504
Gage, P., 27
Gaizauskas, R., 345, 347,
349
Gale, W. A., 51, 53, 61,
372, 481, 483–485,
492, 507, 511
Gales, M. J. F., 61
Galil, Z., 289
Galley, M., 35, 429, 430
Gandhe, S., 430
Gao, J., 56, 429, 430, 436
Gardner, M., 421
Garg, N., 126
Garside, R., 174, 175
Gaˇsi ´c, M., 450, 451, 458
Gauvain, J.-L., 61, 129
Gazdar, G., 207
Gdaniec, C., 263, 264
Geman, S., 268
Georgila, K., 460
Gerber, L., 345, 347
Gerber, M., 376
Gerbino, E., 441, 455
Gerten, J., 430
Gertz, M., 347, 355
Ghahramani, Z., 397
Ghemawat, S., 145
Ghoneim, M., 21
Giangola, J. P., 443, 449,
455
Gil, D., 152
Gilbert, G. N., 442
Gildea, D., 366, 375
Gillick, L., 81
Ginter, F., 155, 173, 208,
272, 291, 293
Ginzburg, J., 458

Giuliano, V. E., 128
Giv ´on, T., 246
Glass, J., 442
Glennie, A., 235
Godfrey, J., 19, 194
Goebel, R., 376, 489
Goenaga, I., 294
Goffman, E., 375
Gojenola, K., 294
Gokirmak, M., 291
Goldberg, J., 454
Goldberg, Y., 118, 124,
129, 150, 155, 173,
208, 272, 293, 294
Golding, A. R., 489
Goldwater, S., 175
Gondek, D., 421
Gonnerman, L. M., 123
Gonzalez-Agirre, A., 126,
504
Good, M. D., 442
Goodenough, J. B., 504
Goodfellow, I., 135–137,
145, 150
Goodine, D., 442
Goodman, J., 53, 55, 61,
268
Goodwin, C., 459
Gosling, S. D., 393
Gould, J. D., 442
Gould, S. J., 101
Gravano, A., 459
Gravano, L., 340, 354
Grave, E., 129
Green, B. F., 322, 412, 420
Green, J., 220
Green, L., 21, 72
Green, S., 294
Greenbaum, S., 221
Greene, B. B., 174
Greenwald, A. G., 125
Grefenstette, E., 419, 421
Gregory, M. L., 246
Grenager, T., 376
Grishman, R., 263, 264,
352, 354, 361
Gross, S., 145
Grosz, B. J., 459, 460
Grover, C., 331
Gruber, J. S., 357, 374
Guo, Y., 375
Gusﬁeld, D., 34, 35
Guyon, I., 81

Habash, N., 291, 294
Hacioglu, K., 375
Haddow, B., 27, 29
Haghighi, A., 268
Haji ˇc, J., 155, 172, 173,
208, 264, 272, 274,
291, 293, 294
Haji ˇcov ´a, E., 274, 293
Hajishirzi, H., 421
Hakkani-T ¨ur, D., 172, 436
Hale, J., 264
Hall, J., 293
Hamilton, W. L., 125,
384–386

Hanks, P., 116, 349
Hannan, K., 397, 514
Hansen, B., 442
Harabagiu, S., 375, 404
Harnish, R., 447
Harper, M. P., 175
Harris, K., 291
Harris, R. A., 443
Harris, Z. S., 101, 106, 174,
235
Harshman, R. A., 128
Hart, P. E., 512
Hart, T., 56
Hastie, T., 80, 100
Hathi, S., 80
Hatzivassiloglou, V., 385,
386, 397
Haverinen, K., 272, 293
Hawwari, A., 21
He, H., 419
He, L., 367, 375
He, X., 436
Heaﬁeld, K., 48, 55, 56, 61
Heaps, H. S., 20, 35
Hearst, M. A., 336, 337,
342, 354, 511, 514
Heck, L., 436
Heckerman, D., 71, 80
Heikkil ¨a, J., 175, 293
Heim, I., 308, 323
Hellmann, S., 336, 412
Hemphill, C. T., 194
Henderson, D., 150
Henderson, J., 268, 460
Henderson, M., 447
Henderson, P., 443
Hendler, J. A., 321
Hendrickson, C., 174
Hendrickx, I., 354
Hendrix, G. G., 374
Hepple, M., 349
Herdan, G., 20, 35
Hermann, K. M., 419, 421
Hermjakob, U., 404
Hickey, M., 38
Hilf, F. D., 427, 428, 444
Hill, F., 103, 126, 504
Hindle, D., 263, 264, 267
Hinkelman, E. A., 459
Hinton, G. E., 123, 142,
145, 150
Hirschberg, J., 21, 453,
454, 456, 458, 459
Hirschman, L., 81, 354,
409, 421, 441, 442
Hirst, G., 104, 370, 492,
513
Hjelmslev, L., 128
Hlav ´acov ´a, J., 291
Hobbs, J. R., 352–354
Hockenmaier, J., 219
Hoff, M. E., 149
Hoffmann, P., 71, 381
Hofmann, T., 129
Holliman, E., 19
Holtzman, A., 376, 396
Hopcroft, J. E., 199
Hopely, P., 174, 420

Horning, J. J., 268
Horvitz, E., 71, 80
Householder, F. W., 175
Hovanyecz, T., 442
Hovy, D., 81
Hovy, E. H., 107, 274, 354,
386, 404, 514
Howard, R. E., 150
Hsu, B.-J., 61
Hu, M., 71, 381, 386
Huang, E. H., 126, 504
Huang, L., 268, 283
Huang, Z., 354
Hubbard, W., 150
Huddleston, R., 200, 221
Hudson, R. A., 293
Huffman, S., 354
Hull, D. A., 99, 128
Hulteen, E. A., 443
Hunter, P., 437
Hutchinson, B., 488, 489
Hutto, C. J., 443
Huval, B., 354
Hymes, D., 375

Iacobacci, I., 506
Ingria, R., 263, 264, 345,
347, 349, 444
Irons, E. T., 235
Irving, G., 145
Isard, A., 452
Isard, M., 145
Isard, S., 452
Isbell, C. L., 429
ISO8601, 346, 347
Israel, D., 352–354
Issar, S., 434
Iyyer, M., 419

Jackel, L. D., 150
Jackendoff, R., 221, 315
Jacobs, P. S., 354
Jacobson, N., 174
Jaech, A., 80
Jafarpour, S., 428
Jang, S. B., 346
Jauhiainen, T., 80
Jauvin, C., 119, 129, 145
Jefferson, G., 449, 451
Jeffreys, H., 60
Jekat, S., 450
Jelinek, F., 52, 60, 165, 242,
263, 264, 267, 479
Ji, H., 354
Ji, Y., 429
Jia, R., 409
Jia, Y., 145
Jiang, J. J., 502
Jim ´enez, V. M., 268
J´ınov ´a, P., 274, 293
Johannsen, A., 81
Johansson, R., 294, 375
Johansson, S., 221
Johnson, C. R., 362, 363
Johnson, M., 249, 268, 315
Johnson, W. E., 61
Johnson-Laird, P. N., 362
Johnston, M., 458

Author Index

545

Jones, M. P., 128, 492
Jones, R., 340, 354, 454
Jones, S. J., 442
Jones, T., 21
Joos, M., 101, 106, 127
Jordan, M. I., 87, 129
Jordan, P. W., 424
Joshi, A. K., 174, 214, 221,
237, 267, 268, 420
Joshi, M., 419
Joulin, A., 129
Jozefowicz, R., 145, 430
Juang, B. H., 479
Jurafsky, D., 21, 35, 41, 47,
72, 107, 114, 125,
126, 128, 173, 341,
342, 352, 354, 366,
372, 375, 384–386,
391–393, 430, 459,
514
Jurgens, D., 21, 47, 72, 512
Justeson, J. S., 397

Kaiser, L., 145
Kalai, A. T., 125, 126
Kalyanpur, A., 421
Kameyama, M., 352–354
Kamm, C. A., 441
Kanayama, H., 291, 421
Kanerva, J., 291
Kang, J. S., 514
Kannan, A., 430
Kaplan, R. M., 226, 268,
374, 421, 423, 431,
432, 444
Karlen, M., 119, 129, 354,
375
Karlsson, F., 175, 293
Karttunen, L., 174, 420
Kasami, T., 223, 235
Kashyap, R. L., 492
Katz, C., 220
Katz, G., 345, 347, 349
Katz, J. J., 128, 323, 370
Katz, K., 208, 210
Katz, S. M., 397
Kavukcuoglu, K., 119, 129,
354, 375
Kawahara, D., 294
Kawakami, K., 333, 354
Kawamoto, A. H., 513
Kay, M., 226, 235, 374,
423, 431, 432, 444
Kay, P., 220, 221
Kay, W., 421
Kayadelen, T., 291
Ke, N. R., 443
Kearns, M., 429, 454, 460
Keizer, S., 450, 451
Keller, F., 372, 489
Kelly, E. F., 513
Kembhavi, A., 421
Kern, M. L., 393–395
Kernighan, M. D., 481,
483–485, 492
Kert ´esz, J., 35
Kettnerov ´a, V., 274, 291,
293

546 Author Index

Khoddam, E., 376
Khot, T., 419
Khudanpur, S., 129
Kiela, D., 129
Kilgarriff, A., 505–508
Kim, D., 458
Kim, G., 208, 210
Kim, S. M., 386
Kim, S. N., 354
King, L. A., 393
King, T. H., 268
Kingma, D., 145
Kingsbury, P., 375
Kintsch, W., 128, 322
Kiparsky, P., 356
Kipper, K., 359, 375
Kirchhoff, K., 454
Kit, C., 375
Klapaftis, I. P., 512
Klavans, J. L., 263, 264
Kleene, S. C., 34
Klein, A., 450
Klein, D., 77–79, 81, 171,
172, 175, 249, 250,
259, 267, 268
Klein, E., 35, 207
Klein, S., 174, 175, 420
Klementiev, A., 376
Kneser, R., 53, 54
Knippen, R., 346
Knuth, D. E., 491
Kobilarov, G., 336, 412
Kocisky, T., 421
Ko ˇcisk `y, T., 419
Koehn, P., 55, 56, 61
Kol ´aˇrov ´a, V., 274, 293
Koller, D., 268
Kombrink, S., 129
Kondrak, G., 488
Koo, T., 268
Korhonen, A., 103, 126,
504
Kormann, D., 429
Kornai, A., 35
Kosinski, M., 393–395
Kowtko, J. C., 452
Kozareva, Z., 354
Kraemer, H. C., 428
Kratzer, A., 308, 323
Krieger, M., 35
Krizhevsky, A., 145
Krovetz, R., 27, 511
Kruskal, J. B., 35, 479
K ¨ubler, S., 293, 294
Ku ˇcera, H., 19, 174
Kudlur, M., 145
Kudo, T., 293
Kuhlmann, M., 294
Kukich, K., 486, 492
Kuksa, P., 119, 129, 354,
375
Kulkarni, R. G., 454
Kullback, S., 371
Kumlien, J., 354
Kuno, S., 235
Kuperman, V., 381–383
Kupiec, J., 174
Kwak, H., 384

Kwak, S., 291
Kwan, J. L. P., 421
Kwiatkowski, T., 421

Labov, W., 99, 452
Lafferty, J., 397
Lafferty, J. D., 100, 171,
267, 354
Laham, D., 128
Laird, N. M., 474, 484
Lakoff, G., 315, 323, 373
Lal, T. N., 397
Lally, A., 421
Lamblin, P., 150
Lample, G., 333, 354
Landauer, T. K., 126, 128,
442, 504
Landes, S., 505
Lando, T., 291
Lang, J., 376
Langer, S., 38
Langlais, P., 508
Lapalme, G., 508
Lapata, M., 372, 376, 489,
509, 510, 514
Lapesa, G., 129
Lari, K., 245, 268
Larochelle, H., 150
Lau, J. H., 514
Laughery, K., 322, 412, 420
Lazo, M., 349
La Polla, R., 221
Le, Q., 429
Leacock, C., 500, 505
LeCun, Y., 150
Lee, C.-H., 436, 444
Lee, D. D., 129
Lee, K., 367, 375, 421
Lee, L., 80, 397
Leech, G., 175, 221
Lehmann, J., 336, 412
Lehnert, W. G., 354
Leibler, R. A., 371
Lemon, O., 460
Lerer, A., 145
Lertpradit, S., 291
Lesk, M. E., 508, 513
Leskovec, J., 125, 384–386
Leung, H., 291, 442
Leuski, A., 428, 430
Levenberg, J., 145
Levenshtein, V. I., 30
Levesque, H. J., 323, 459
Levin, B., 359, 375
Levin, E., 436, 444, 460
Levinson, S. C., 459
Levow, G.-A., 442,
453–455
Levy, J. P., 129
Levy, O., 118, 124, 129
Levy, R., 264
Lewis, C., 442
Lewis, D. L., 81, 354
Lewis, M., 259, 367, 375
Li, H., 429
Li, J., 107, 291, 429, 430
Li, X., 404, 406
Li, Y., 333, 375

Li, Z., 375, 429
Liang, P., 409, 414, 415,
419
Liberman, M. Y., 263, 264,
459
Lieberman, H., 321
Lieberman Aiden, E., 124
Light, M., 409, 421
Lin, D., 264, 293, 376, 489,
501, 502
Lin, J., 404, 408
Lin, Y., 124
Lin, Z., 145
Lind ´en, K., 80
Lindsey, R., 322
Liscombe, J., 437
Litkowski, K. C., 375
Litman, D. J., 423, 424,
441, 453, 454, 456,
460
Littman, J., 345–347
Littman, M., 384, 386
Liu, A., 458
Liu, B., 71, 80, 381, 386,
514
Liu, C.-W., 430
Liu, T., 375
Liu, T.-H., 288
Liu, X., 61
Llorens, H., 355
Lochbaum, K. E., 128, 460
Loper, E., 35
Lopez-Gazpio, I., 126, 504
L ´opez de Lacalle, O., 510
Lopyrev, K., 409, 419
Lovins, J. B., 34
Lowe, J. B., 362
Lowe, R., 443
Lowe, R. T., 428, 430
Lu, Z., 429
Luhn, H. P., 113
Lui, M., 72, 80
Luotolahti, J., 291
Lyons, J., 323
Lytel, D., 513

Ma, X., 354
MacCartney, B., 293
MacIntyre, R., 208, 210
Macketanz, V., 291
Macleod, C., 361
Macy, M., 443
Madhu, S., 513
Magerman, D. M., 212,
267, 275
Maharjan, S., 21
Maier, E., 450
Maier, W., 294
Maiorano, S., 404
Mairesse, F., 379, 395, 450,
451
Makatchev, M., 424
Maleck, I., 450
Manandhar, S., 512
Mandl, M., 291
Man ´e, D., 145
Mani, I., 345–347

Manning, C. D., 35, 80, 87,
119, 123, 124, 126,
129, 155, 171–173,
175, 208, 245, 249,
250, 259, 267, 268,
272, 283, 291, 293,
347, 354, 355, 376,
397, 411, 414, 504
Manurung, R., 291
Marcinkiewicz, M. A., 154,
208, 210, 243, 274,
293
Marcus, M. P., 154, 185,
208, 210, 243, 263,
264, 267, 274, 293,
375, 514
Marcus, S., 116
Marheinecke, K., 291
Marinov, S., 293
Maritxalar, M., 126, 504
Markov, A. A., 60, 479
Markovitch, S., 116
Marlin, B. M., 355
Maron, M. E., 80
M `arquez, L., 294, 368, 375,
376
Marshall, C., 459
Marshall, I., 174
Marsi, E., 293, 294
Mart´ı, M. A., 294
Martin, J. H., 128, 375,
492, 514
Martin, R., 459
Martinez, D., 376
Marx, M., 442, 455
Marzal, A., 268
Mast, M., 450
Masterman, M., 322, 513
Matias, Y., 126, 504
Matsumoto, Y., 293
Mausam, 376
Mausam., 354
Maxwell III, J. T., 268
Mays, E., 481, 486, 487,
491, 492
McCallum, A., 80, 100,
171, 354, 355
McCarthy, D., 514
McCarthy, J., 220
McCawley, J. D., 221, 323
McClelland, J. L., 150
McConlogue, K., 420
McConnell-Ginet, S., 323
McCord, M. C., 421
McCulloch, W. S., 131, 149
McDaniel, J., 19
McDonald, R., 287, 293,
294, 397, 514
McDonald, R. T., 155, 173,
208, 272, 293
McEnery, A., 175
McFarland, D. A., 393
McGhee, D. E., 125
McGuiness, D. L., 321
McKeown, K. R., 385, 386,
397
Meek, C., 410, 419
Mehl, M. R., 393

Mel’ ˘cuk, I. A., 293
Melis, G., 419
Mendonc¸ a, G., 291
Mercer, R. L., 52, 60, 165,
174, 267, 481, 486,
487, 492
Merialdo, B., 174
Mesnil, G., 436
Meteer, M., 174, 459
Metsis, V., 80
Meyers, A., 294, 361, 375
Michaelis, L. A., 246
Michel, J.-B., 124
Microsoft., 423, 425, 429
Mihalcea, R., 126, 504,
506, 514
Mikheev, A., 331
Mikolov, T., 61, 119, 124,
127, 129
Mikulov ´a, M., 274, 293
Miller, G. A., 46, 60, 397,
504, 505
Miller, S., 255, 354, 444
Minsky, M., 80, 134, 150,
375
Mintz, M., 341, 354
M´ırovsk ´y, J., 274, 293
Missil ¨a, A., 291
Mitchell, M., 429
Mitchell, T. M., 129
Mitton, R., 492
Miwa, M., 338, 354
Moens, M., 331
Moens, M.-F., 376
Mohammad, S. M., 381,
382
Moldovan, D., 514
Monga, R., 145
Monroe, B. L., 390–392
Monroe, W., 430
Montague, R., 323
Monz, C., 405
Mooney, R. J., 413
Moore, R. C., 489, 490
Moore, S., 145
Morante, R., 376
Morgan, A. A., 354
Morgan, N., 41, 150
Morimoto, T., 459
Morin, F., 61, 129
Morris, W., 495
Moschitti, A., 368
Mosteller, F., 65, 80
Moszkowicz, J., 349
Mrkˇsi ´c, N., 452, 458
Mulcaire, G., 80
Murdock, J. W., 421
Murphy, B., 129
Murphy, K. P., 81, 100
Murray, D., 145

N ´adas, A., 61
Nagao, M., 267
Nagata, M., 459
Nagy, P., 443
Nakov, P., 354
Narayanan, A., 125
Narayanan, S. S., 455

Nash-Webber, B. L., 374,
421
Naur, P., 220
Navigli, R., 506, 509, 510,
512, 514
Nedoluzhko, A., 274, 291,
293
Needleman, S. B., 479
Neff, G., 443
Newell, A., 38
Newman, D., 514
Ney, H., 53, 54, 81, 242
Ng, A. Y., 87, 126, 129,
342, 354, 504, 514
Ng, H. T., 368, 421, 506
Nielsen, J., 442
Nielsen, M. A., 150
Nigam, K., 80, 100, 354
Nilsson, J., 293, 294
Nilsson, N. J., 460
NIST, 35
Nitisaroj, R., 291
Nitta, Y., 116
Nivre, J., 155, 173, 208,
272, 277, 283, 285,
287, 291, 293, 294,
375
Niwa, Y., 116
Noreen, E. W., 78
Norman, D. A., 322, 374,
375, 423, 431, 432,
444, 448
Norvig, P., 37, 136, 235,
304, 323, 484, 487,
488, 492
Nosek, B. A., 125
Noseworthy, M., 430
Novick, D. G., 442
Nunes, J. H. T., 459

O’Connor, B., 21, 35, 47,
72
O’Hara, T. P., 378
Och, F. J., 56
Odell, M. K., 491
Oepen, S., 268
Oettinger, A. G., 235
Oﬂazer, K., 172
Ogilvie, D., 71, 381, 397
Oh, A. H., 456, 457
Ojala, S., 291
Olah, C., 145
Oommen, B. J., 492
Oravecz, C., 172
Ordonez, V., 126
Orwant, J., 124
Osborne, M., 56, 175
O’S ´eaghdha, D., 452
Osgood, C. E., 105, 106,
128, 381, 397
Osindero, S., 150
Ostendorf, M., 61, 80, 454
Ozertem, U., 454
´O S ´eaghdha, D., 354

Packard, D. W., 34
Pad ´o, S., 294, 354
Paliouras, G., 80

Palmer, D., 35
Palmer, M., 274, 275, 285,
293, 359, 375, 376,
500, 505, 506, 514
Palmucci, J., 174
Pan, Y., 421
Panevova, J., 293
Panevov ´a, J., 274, 293
Pang, B., 80, 397
Pao, C., 441
Paolino, J., 443
Papert, S., 134, 150
Pappuswamy, U., 424
Parikh, A., 421
Paritosh, P., 336, 412
Parsons, T., 312, 323
Partee, B. H., 323
Pasca, M., 404, 405, 408
Paszke, A., 145
Patwardhan, S., 421
Pearl, C., 443
Pedersen, J., 81, 99, 124,
128
Pedersen, T., 502, 514
Peng, N., 338, 354
Penn, G., 356
Pennacchiotti, M., 354
Pennebaker, J. W., 71, 382,
383, 393
Pennington, J., 119, 123,
124, 129, 411
Percival, W. K., 220
Pereira, F. C. N., 171, 293,
354
Perkowitz, M., 174
Perlis, A. J., 220
Perrault, C. R., 460
Peters, S., 323
Peterson, J. L., 491
Petrie, T., 479
Petrov, S., 124, 155, 173,
208, 250, 267, 272,
291, 293, 294
Petruck, M. R. L., 362, 363
Philips, L., 490
Phillips, A. V., 420
Phillips, J., 346
Phillips, M., 442
Picard, R. W., 378
Pieraccini, R., 436, 437,
444, 460
Pilehvar, M. T., 506
Pineau, J., 428, 430, 443,
460
Pitler, E., 291, 489
Pitts, W., 131, 149
Plank, B., 81
Plaut, D. C., 123
Plutchik, R., 380, 381
Pol ´akov ´a, L., 274, 293
Polifroni, J., 441, 442
Pollard, C., 211, 212, 214,
220, 237
Ponzetto, S. P., 506
Poon, H., 338, 354
Popat, A. C., 56
Popel, M., 291
Popescu, A.-M., 355

Author Index

547

Popovici, D., 150
Porter, M. F., 26, 27
Potthast, M., 291
Potts, C., 388, 389
Pouzyrevsky, I., 55, 56, 61
Pow, N., 430
Pradhan, S., 368, 375, 512
Prager, J. M., 421
Prakash, S., 514
Prasettio, M. C., 376, 396
Prescher, D., 376
Price, P. J., 453
Przepi ´orkowski, A., 294
Pullum, G. K., 200, 207,
221
Purver, M., 458
Pustejovsky, J., 345–347,
349, 355, 514
Pyysalo, S., 155, 173, 208,
272, 291, 293

Qi, P., 291
Qin, B., 375
Qiu, G., 514
Qiu, Z., 421
Qiu, Z. M., 421
Quantz, J., 450
Quillian, M. R., 322, 499,
513
Quinn, K. M., 390–392
Quirk, C., 338, 354
Quirk, R., 221

Rabiner, L. R., 466, 476,
477, 479
Radev, D., 349
Radford, A., 195, 221
Raghavan, P., 80, 129
Rajpurkar, P., 409, 419
Ramshaw, L. A., 174, 185,
264, 274, 514
Ranganath, R., 393
Raphael, B., 322
Rappaport Hovav, M., 359
Rashkin, H., 376, 395, 396
Ratinov, L., 129
Ratnaparkhi, A., 100, 175,
255, 267
Rau, L. F., 354
Raux, A., 447
Ravichandran, D., 404
Raviv, J., 492
Raymond, C., 436
Reddy, S., 291
Reeves, R., 361
Rehder, B., 128
Rehm, G., 291
Reichart, R., 81, 103, 126,
504
Reichenbach, H., 314
Reichert, T. A., 479
Reiss, F., 333
Reiss, F. R., 333
Renshaw, E., 419
Resnik, P., 267, 370, 371,
376, 500, 501, 514
Reynar, J. C., 267
Ribarov, K., 293

548 Author Index

Riccardi, G., 436
Richardson, M., 419
Riedel, S., 294, 354, 355
Rieger, C., 513
Ries, K., 459
Riesbeck, C. K., 513
Riezler, S., 268, 376
Rigau, G., 126, 504
Riley, M., 61
Riloff, E., 340, 354, 376,
381, 397, 398, 421
Ringenberg, M., 424
Ritter, A., 354, 376,
428–430
Rivlin, E., 126, 504
Roark, B., 61, 268
Rodriguez, P., 421
Rohde, D. L. T., 123
Romano, L., 354
Rooth, M., 267, 376
Roque, A., 424
Ros ´e, C., 424
Rosenblatt, F., 149
Rosenfeld, R., 61, 100
Rosenzweig, J., 505, 507,
508
Roth, D., 404, 406, 489
Roth, R., 294
Rothe, S., 398
Roukos, S., 263, 264, 267
Routledge, B. R., 391, 392
Roy, N., 460
Rubenstein, H., 504
Rubin, D. B., 474, 484
Rubin, G. M., 174
Rudnicky, A. I., 455–457
Rumelhart, D. E., 142, 150,
322
Rumshisky, A., 346
Ruppenhofer, J., 362, 363,
376
Ruppin, E., 126, 504
Russell, J. A., 380
Russell, R. C., 491
Russell, S., 37, 136, 304,
323
Rutishauser, H., 220

Sabharwal, A., 419
Sacks, H., 451
Sadock, J. M., 298
Sag, I. A., 207, 211, 212,
214, 220, 221, 237,
458, 459
Sagae, K., 283
Sahami, M., 71, 80
Sakoe, H., 479
Salakhutdinov, R. R., 145
Salant, S., 421
Saligrama, V., 125, 126
Salomaa, A., 267
Salton, G., 108, 128
Samelson, K., 220
Sampson, G., 175
Samuelsson, C., 167, 175
Sanﬁlippo, A., 264
Sankoff, D., 99, 479

Santorini, B., 154, 208,
243, 263, 264, 274,
293
Sap, M., 376, 396
Saraclar, M., 61
Saur´ı, R., 345, 347, 349
Sauri, R., 346
Schabes, Y., 267, 268
Schaefer, E. F., 448
Schalkwyk, J., 442
Schank, R. C., 322, 351,
362, 375
Schapire, R. E., 174, 267
Schasberger, B., 208, 210
Schatzmann, J., 450, 451
Scheffczyk, J., 362, 363
Schegloff, E. A., 449, 451
Scherer, K. R., 378, 379,
393
Schiebinger, L., 126
Schilder, F., 349
Schmandt, C., 443
Schmelzenbach, M., 376
Schmolze, J. G., 323
Schoenick, C., 419
Sch ¨olkopf, B., 397
Scholz, M., 293
Schone, P., 128
Sch ¨onkﬁnkel, M., 308
Schreiner, M. E., 128
Schuster, M., 145
Schuster, S., 291
Sch ¨utze, H., 80, 99, 124,
128, 129, 174, 245,
268, 372, 398, 511,
514
Schwartz, H. A., 393–395
Schwartz, J. L. K., 125
Schwartz, R., 174, 255,
268, 354, 444
Schwarz, J., 419
Schwenk, H., 61, 129
Scott, M., 264
S ´eaghdha, D. O., 376
Sebastiani, F., 387, 514
Seddah, D., 294
See, A., 349
Segal, J., 41
Sekine, S., 264
Selfridge, J. A., 46
Sen ´ecal, J.-S., 61, 129
Seneff, S., 441, 442
Sennrich, R., 27, 29
Seo, M., 421
Serban, I. V., 428, 430
Sethi, R., 235
Setzer, A., 345, 347, 349
Seung, H. S., 129
Søgaard, A., 81, 175
Sgall, P., 293
Shah, A., 393–395
Shaked, T., 355
Shang, L., 429
Shannon, C. E., 46, 60, 492
Sharma, D., 285
Sheil, B. A., 235
Sheinwald, D., 421
Shepherd, J., 397, 398

Shi, T., 430
Shillcock, R., 264
Shima, H., 421
Shimada, A., 291
Shlens, J., 145
Shriberg, E., 453, 459
Shrivastava, M., 285
Sidner, C. L., 459, 460
Siler, S., 424
Silveira, N., 155, 173, 208,
272, 293
Simi, M., 291
Simmons, R. F., 174, 175,
322, 364, 374, 420,
421, 513
Simons, G. F., 21
Singer, Y., 171, 172, 174,
175, 267
Singh, S., 376, 395, 396,
429
Singh, S. P., 460
Sinha, K., 443
Sleator, D., 267, 293
Slocum, J., 374
Small, S. L., 513
Smith, D. A., 268
Smith, M., 71, 381, 397
Smith, N. A., 80, 268, 391,
392
Smith, V. L., 445
Smolensky, P., 150
Snow, R., 341, 342, 354,
514
Socher, R., 119, 123, 124,
126, 129, 354, 411,
504
Soderland, S., 342, 343,
354, 355, 414
Solan, Z., 126, 504
Solorio, T., 21
Sordoni, A., 429
Soroa, A., 510
Sparck Jones, K., 114, 128,
513, 514
Spencer, T., 289
Spitkovsky, V. I., 418
Sporleder, C., 376
Sproat, R., 61
Srinivas, B., 268
Srivastava, N., 145
Srivastava, R., 424
Stalnaker, R. C., 448
Stamatatos, E., 80
Steedman, M., 175, 214,
219, 259
Steiner, B., 145
Stent, A., 294
ˇSt ˇep ´anek, J., 294
Stetina, J., 267
Stevenson, S., 375, 376
Stifelman, L. J., 443
Stillwell, D., 393–395
Stoics, 151
Stolcke, A., 41, 56, 61, 267,
459
Stolz, W. S., 174
Stone, P., 71, 381, 397, 429
Stone, P. J., 513

Stoyanchev, S., 458
Straka, M., 291
Stran ˇa ´k, P., 294
Streeter, L., 128
Strnadov ´a, J., 291
Str ¨otgen, J., 347, 355
Strzalkowski, T., 263, 264
Sturge, T., 336, 412
Stuttle, M., 460
Su, J., 354
Su, P.-H., 458
Subramanian, S., 333, 354
Suci, G. J., 105, 106, 128,
381, 397
Suendermann, D., 437
Suh, J., 419
Sulubacak, U., 291
Sundheim, B., 345, 347,
349, 352, 354
Surdeanu, M., 294, 354,
375, 376
Sutskever, I., 119, 129, 145
Sutton, S., 442
Svartvik, J., 221
Swerts, M., 453, 454, 456
Swier, R., 376
Switzer, P., 128
Szekely, R., 361
Szpakowicz, S., 354

Tafjord, O., 419
Tajchman, G., 41
Taji, D., 291
Talbot, D., 56
Talmor, A., 419
Talukdar, P. P., 129
Talwar, K., 145
Tanenhaus, M., 513
Tannen, D., 375
Tannenbaum, P. H., 105,
106, 128, 174, 381,
397
Tarjan, R. E., 289
Taskar, B., 268
Taylor, J., 336, 412
Taylor, P., 459
Teh, Y.-W., 150
Temperley, D., 267, 293
Tengi, R. I., 505
Teo, L. H., 421
ter Meulen, A., 323
Tesni `ere, L., 293, 374
Tetreault, J., 294
Thede, S. M., 175
Thelen, M., 421
Thibaux, R., 250, 267
Thomas, J. A., 57, 58
Thompson, C. W., 374
Thompson, H., 374, 423,
431, 432, 444
Thompson, K., 34
Thompson, R. A., 238
Thomson, B., 450–452
Thrax, D., 151
Thrun, S., 460
Tibshirani, R. J., 78, 80, 94,
100
Tillmann, C., 264

Titov, I., 268, 376
Tomkins, S. S., 380
Toutanova, K., 171, 172,
175, 268, 338, 354,
490
Towell, G., 505
Traum, D., 428, 430
Tsarfaty, R., 155, 173, 208,
272, 293, 294
Tseng, H., 35, 173
Tsukahara, W., 449
Tsvetkov, Y., 21, 47, 72
Tucker, P., 145
T ¨ur, G., 172, 436
Turian, J., 129
Turney, P. D., 381, 382,
384, 386, 397
Tyers, F. M., 291
Tyson, M., 352–354

Ullman, J. D., 199, 235,
275
Ungar, L. H., 393–395
Uresov ´a, Z., 291
Uria, L., 126, 504
Uszkoreit, H., 291
UzZaman, N., 355

Vaithyanathan, S., 80, 397
van Benthem, J., 323
van der Maaten, L., 123
Van Ess-Dykema, C., 459
van Harmelen, F., 321
van Rijsbergen, C. J., 74,
234, 263
Van Valin, Jr., R. D., 221
van Wijnagaarden, A., 220
van Zaanen, M., 268
Vandyke, D., 458
Vanhoucke, V., 145
VanLehn, K., 424
Vannella, D., 512
Van Ooyen, B., 48
Vasilescu, F., 508
Vasserman, A., 268
Vasudevan, V., 145
Vauquois, B., 220
Velikovich, L., 397, 514
Vendler, Z., 315
Verhagen, M., 346, 349,
355
Vermeulen, P. J. E., 442
Versley, Y., 294

Vi ´egas, F., 145
Villemonte de la Cl ´ergerie,
E., 294
Vilnis, L., 430
Vincent, P., 101, 119, 129,
145
Vincze, V., 294
Vintsyuk, T. K., 479
Vinyals, O., 145, 429, 430
Viterbi, A. J., 479
Volkova, S., 376, 395, 396
Voorhees, E. M., 505
Voutilainen, A., 175, 293

Wade, E., 453
Wagner, R. A., 31, 479, 492
Wahlster, W., 321
Walker, M. A., 379, 395,
432, 441, 454, 455,
460
Wall, R. E., 323
Wallace, D. L., 65, 80
Wang, H., 429
Wang, S., 80, 87, 397
Wang, T., 126
Wang, Y.-Y., 436
Ward, N., 449
Ward, W., 375, 434
Warden, P., 145
Warmsley, D., 443
Warriner, A. B., 381–383
Wasow, T., 221
Wattenberg, M., 145
Weaver, W., 513
Weber, I., 443
Weber, S., 427, 428, 444
Wegstein, J. H., 220
Wehbe, L., 129
Weinschenk, S., 455
Weischedel, R., 174, 255,
274, 354, 514
Weizenbaum, J., 10, 18,
423, 425, 444
Weld, D. S., 354, 355, 419
Welty, C., 421
Wen, T.-H., 452, 458
Wessels, L. F. A., 442
Weston, J., 119, 129, 354,
375, 397, 404, 410,
421
Whitelaw, C., 488, 489
Whiteside, J. A., 442
Whittaker, S., 432
Wicke, M., 145

Widrow, B., 149
Wiebe, J., 71, 126, 378,
381, 397, 504, 505,
514
Wierzbicka, A., 128
Wilcox-O’Hearn, L. A.,
488, 492
Wilde, O., 480
Wilensky, R., 460
Wilkes-Gibbs, D., 459
Wilks, Y., 322, 370, 374,
513
Williams, J., 375
Williams, J. D., 447, 460
Williams, R., 354
Williams, R. J., 142, 150
Wilson, G., 345, 347
Wilson, R., 424
Wilson, T., 71, 381, 514
Winkler, W. E., 491
Winograd, T., 322, 323,
374, 423, 431, 432,
444
Winston, P. H., 375
Witten, I. H., 61, 81, 100,
255
Wittgenstein, L., 106, 447,
459
Wixon, D. R., 442
Wolf, A. K., 322, 412, 420
Wolfe, M. B. W., 128
Woli ´nski, M., 294
Wong, A. K. C., 479
Woodger, M., 220
Woodland, P. C., 61
Woods, W. A., 322, 421
Woodsend, K., 376
Wooters, C., 41
Wr ´oblewska, A., 294
Wu, F., 354
Wu, Z., 500
Wundt, W., 195, 220
Wunsch, C. D., 479

Xia, F., 275, 293
Xiang, B., 338, 354
Xu, P., 56
Xu, W., 354, 375
Xue, N., 274, 294, 368, 375

Yamada, H., 293
Yan, Z., 429
Yang, E., 145

Author Index

549

Yang, Y., 81, 410, 419
Yankelovich, N., 442, 455
Yao, K., 436
Yao, L., 354, 355
Yarowsky, D., 372, 507,
510, 511, 514
Yasseri, T., 35
Yates, A., 355
Yatskar, M., 126, 419
Yeh, A. S., 354
Yih, W.-t., 124, 127, 338,
354, 410, 419
Yngve, V. H., 235, 449
Young, B., 361
Young, S. J., 245, 268, 450,
451, 458, 460
Younger, D. H., 223, 235
Yu, D., 436
Yu, K., 354, 450, 451
Yu, Y., 145
Yu, Z., 291
Yuret, D., 268, 294, 509

Zampieri, M., 80
Zapirain, B., 376
Zavrel, J., 267
Zelle, J. M., 413
Zeman, D., 155, 173, 208,
272, 291, 293
Zettlemoyer, L., 354, 367,
375, 413, 415, 419
Zhai, C., 80
Zhang, J., 354, 409, 419
Zhang, L., 80, 421
Zhang, M., 354
Zhang, Y., 283, 294, 368
Zhao, H., 375
Zhao, J., 126
Zheng, X., 145
Zhong, Z., 368, 506
Zhou, B., 338, 354
Zhou, D., 397
Zhou, G., 354, 375
Zhou, J., 375, 429
Zhou, M., 429
Zhu, H., 333
Zhu, X., 397, 514
Zielinska, V., 361
Zik ´anov ´a, ˇS., 274, 293
Zou, J., 126
Zou, J. Y., 125, 126
Zue, V. W., 441, 442
Zweig, G., 124, 127, 436
Zwicky, A., 298

ˆ, 65, 482

λ -reduction, 308
*?, 15
+?, 15
F-measure, 263
10-fold cross-validation, 76
→ (derives), 196
* (RE Kleene *), 13
+ (RE Kleene +), 13
. (RE any character), 13
$ (RE end-of-line), 13
( (RE precedence symbol),
14
[ (RE character
disjunction), 12
\B (RE non
word-boundary), 14
\b (RE word-boundary),
14
] (RE character
disjunction), 12
ˆ (RE start-of-line), 13
[ˆ] (single-char negation),
12
∃ (there exists), 306
∀ (for all), 306
=⇒ (implies), 309
λ -expressions, 308
λ -reduction, 308
∧ (and), 306
¬ (not), 306
∨ (or), 309
4-gram, 42
4-tuple, 198
5-gram, 42

AAVE, 21
abduction, 311
ABox, 316

AB S I TY, 513

absolute discounting, 53
absolute temporal
expression, 345
abstract word, 382
accomplishment
expressions, 315
accuracy
in WSD, 506
achievement expressions,
315, 316
acknowledgment speech
act, 448
activation, 132
activity expressions, 315,

315

adaptation
language model, 61
add-k, 51
add-one smoothing, 49
adjacency pairs, 451
adjective, 152, 203
adjective phrase, 203
adjunction in TAG, 221
adverb, 152

Subject Index

days of the week coded
as noun instead of,
153
degree, 153
directional, 153
locative, 153
manner, 153
syntactic position of, 203
temporal, 153
adversarial evaluation, 430
affective, 378
afﬁx, 26
agent, as thematic role, 357
agglomerative clustering,

512

ALGOL, 220
algorithm
CKY, 225
Corpus Lesk, 507, 508
extended gloss overlap,
502
extended Lesk, 502
forward, 470
forward-backward, 478
inside-outside, 245
Jiang-Conrath word
similarity, 502
Kneser-Ney discounting,

53

Lesk, 507
Lin word similarity, 502
minimum edit distance,
33
n-gram tiling for question
answering, 408
naive Bayes classiﬁer, 65
path-length based
similarity, 500
pointwise mutual
information, 116
probabilistic CKY, 243
Resnik word similarity,
501
semantic role labeling,
365
Simpliﬁed Lesk, 507
Soundex, 491
unsupervised word sense
disambiguation, 512
Viterbi, 161, 471
Yarowsky, 510
alignment, 30
minimum cost, 32
string, 30
via minimum edit
distance, 32
all-words task in WSD, 505
Allen relations, 349
ambiguity
amount of part-of-speech
in Brown corpus,
156
attachment, 224

coordination, 224, 225,
248
in meaning
representations, 298
part-of-speech, 156
PCFG in, 240
prepositional phrase
attachment, 246
resolution of tag, 156
tests distinguishing from
vagueness, 298
word sense, 504
American Structuralism,
220
anchor texts, 417
anchors in regular
expressions, 13, 34
answer type, 404
answer type taxonomy, 404
antonym, 103
any-of, 75
AP, 203
approximate
randomization, 78
ARC, 419
arc eager, 284
arc standard, 277
argmax, 482
Aristotle, 151, 315
arity, 312
article (part-of-speech), 153
aspect, 315
aspell, 490
ASR
conﬁdence, 455
association, 104
ATIS, 194
corpus, 197, 200
ATN, 375
ATRANS, 373
attachment ambiguity, 224
augmentative
communication, 38
authorship attribution, 63
autocorrect, 488
auxiliary verb, 154

backchannel, 449
backoff
in smoothing, 51
backprop, 142
backtrace, 473
in minimum edit
distance, 32
Backus-Naur Form, 195
backward chaining, 310
backward composition, 216
backward probability, 474
bag of word, 505
bag of words, 65, 66
bag-of-words, 65
barge-in, 441
baseline
most frequent sense, 507

take the ﬁrst sense, 507
basic emotions, 380
Bayes’ rule, 65, 482
dropping denominator,
66, 160, 482
Bayesian inference, 65, 482
BDI, 460
Beam search, 285
beam search, 165
beam width, 166, 285
Berkeley Restaurant
Project, 41
Bernoulli naive Bayes, 80
bi-LSTM, 329
bias term, 84, 132
bidirectional RNN, 187
bigram, 39
binary branching, 213
binary NB, 70
binary tree, 213
bits for measuring entropy,
57
Bloom ﬁlters, 56
BNF (Backus-Naur Form),

195

bootstrap, 79
bootstrap algorithm, 79
bootstrap test, 78
bootstrapping, 78, 510
for WSD, 510
generating seeds, 511
in IE, 339
BPE, 27
bracketed notation, 197
British National Corpus
(BNC)
POS tags for phrases,
155
Brown, 155
Brown corpus, 19
original tagging of, 174
byte-pair encoding, 27

candidates, 481
canonical form, 299
captalization
for unknown words, 167
capture group, 18
cardinal number, 203
cascade, 27
regular expression in
Eliza, 18

case
sensitivity in regular
expression search,
11
case folding, 24
case frame, 359, 374
categorial grammar, 214,

214

CD (conceptual
dependency), 373
centroid, 115

551

552

Subject Index

CFG, see context-free
grammar
chain rule, 98, 142
channel model, 482, 483
character embeddings, 332
Charniak parser, 250
Chatbots, 425
chatbots, 10
Chinese
word segmentation, 25
Chomsky normal form,
213, 242
Chomsky-adjunction, 214
chunking, 231, 232
CIRCUS, 354
citation form, 102
CKY algorithm, 223
probabilistic, 243
clariﬁcation questions, 458
class-based n-gram, 61
clause, 201
clitic, 24
origin of term, 151
closed class, 152
closed vocabulary, 48
clustering
in word sense
disambiguation, 514
CNF, see Chomsky normal
form
coarse senses, 514
COCA, 483
Cocke-Kasami-Younger
algorithm, see CKY
code switching, 21
collaborative completion,

448

Collins parser, 250
collocation, 505
combinatory categorial
grammar, 214
commissive speech act, 448
common ground, 448, 459
common nouns, 152
complement, 206, 206
complementizer, 153
completeness in FOL, 311
componential analysis, 372
Computational Grammar
Coder (CGC), 174
computational semantics,

296

concatenation, 34
conceptual dependency, 373
concordance, semantic, 505
concrete word, 382
conditional independence,
266
conﬁdence
ASR, 455
in relation extraction, 340
conﬁdence values, 340
conﬁguration, 275
confusion matrix
in spelling correction,

484

confusion sets, 489
conjoined phrase, 207

conjunction, 153
conjunctions, 207
as closed class, 153
connectionist, 150
connotation frame, 395
connotation frames, 376
connotations, 105, 379
consistent, 238
constants in FOL, 305
constative speech act, 448
constituency, 194
evidence for, 195
constituent, 194
Constraint Grammar, 293
Construction Grammar, 220
content planning, 456
context embedding, 122
context-free grammar, 194,

195, 198, 219

Chomsky normal form,
213
invention of, 220
multiplying probabilities,
240, 266
non-terminal symbol,
196
productions, 195
rules, 195
terminal symbol, 196
weak and strong
equivalence, 213
contingency table, 73
continuer, 449
conversation, 422
conversational agents, 422
conversational analysis, 451
convex, 90
coordinate noun phrase,

207

coordination ambiguity,
225, 248
copula, 154
corpora, 19
COCA, 483
corpus, 19
ATIS, 197
BNC, 155
Brown, 19, 174
LOB, 174
regular expression
searching inside, 11
Switchboard, 19
TimeBank, 349
Corpus Lesk, 508
Corpus of Contemporary
English, 483
correction act detection,

453

cosine
as a similarity metric,
112
cost function, 87
count nouns, 152
counters, 34
counts
treating low as zero, 170
CRF, 172
cross entropy loss, 88, 141

cross-brackets, 263
cross-entropy, 58
cross-validation, 76
10-fold, 76
crowdsourcing, 382
currying, 308

Damerau-Levenshtein, 483
date
fully qualiﬁed, 347
normalization, 435
dative alternation, 359
decision boundary, 85, 135
decision tree
use in WSD, 514
declarative sentence
structure, 200
decoder, 471, 471
decoding, 160, 471
Viterbi, 160, 471
deduction
in FOL, 310
deduplication, 491
deep, 131
deep learning, 131
deep role, 357
degree, 510
degree adverb, 153
deleted interpolation, 165
delexicalized, 457
denotation, 301
dependency
grammar, 270
lexical, 248
dependency tree, 273
dependent, 271
derivation
direct (in a formal
language), 199
syntactic, 196, 196, 199,

199

description logics, 316
Det, 196
determiner, 153, 196, 202
development test set, 76
development test set
(dev-test), 44
devset, see development
test set (dev-test), 76
dialog, 422
dialog act, 447, 450
acknowledgment, 449
backchannel, 449
continuer, 449
correction, 453
dialog manager
design, 442
dialog policy, 454
dialog systems, 422
design, 442
evaluation, 441
diathesis alternation, 359
diff program, 35
dimension, 108
diphthong
origin of term, 151

direct derivation (in a
formal language),

199

directional adverb, 153
directive speech act, 448
disambiguation
PCFGs for, 239
role of probabilistic
parsing, 237
syntactic, 225
via PCFG, 240
discount, 49, 51, 52
discounting, 49
discovery procedure, 220
discriminative model, 83
disﬂuency, 20
disjunction, 34
pipe in regular
expressions as, 14
square braces in regular
expression as, 12
dispreferred response, 445
distance, 254
cosine, 112
distant supervision, 341
distributional hypothesis,

101

distributional similarity,
220
document frequency, 113
document vector, 115
domain, 301
domain classiﬁcation, 434
domain ontology, 430
domination in syntax, 196
dot product, 84, 111
dropout, 145
duration
temporal expression, 345
dynamic programming, 31
and parsing, 225
forward algorithm as,
468
history, 479
Viterbi as, 161, 471

E-step (expectation step) in

EM, 478

Earnest, The Importance of
Being, 480
earnesty, importance, 480
edge-factored, 286
edit distance
minimum, 31
ELIZA, 10
implementation, 18
sample conversation, 18
Elman Networks, 178
EM
Baum-Welch as, 474
E-step, 478
for deleted interpolation,
52
for spelling correction,
484
inside-outside in parsing,
245
M-step, 478

embedded verb, 204
embeddings, 107
character, 332
cosine for similarity, 111
GloVe, 119
skip-gram, learning, 121
sparse, 110
tf-idf, 113
word2vec, 119
emission probabilities, 158,

465

EmoLex, 381
emotion, 379
empty category, 201
English
simpliﬁed grammar
rules, 197
entity linking, 414
entropy, 57
and perplexity, 57
cross-entropy, 58
per-word, 58
rate, 58
relative, 371
error backpropagation, 142
error model, 484
Euclidean distance
in L2 regularization, 94
Eugene Onegin, 60, 479
evalb, 264
evaluating parsers, 263
evaluation
10-fold cross-validation,
76
comparing models, 45
cross-validation, 76
development test set, 44,
76
devset, 76
devset or development
test set, 44
dialog systems, 441
extrinsic, 43, 506
most frequent class
baseline, 156
named entity recognition,
333
of n-gram, 43
of n-grams via
perplexity, 44
pseudoword, 372
relation extraction, 344
test set, 43
training on the test set, 43
training set, 43
unsupervised WSD, 512
word similarity, 503
WSD systems, 506
Event extraction, 327
event extraction, 348
event variable, 312
events
representation of, 311
existential there, 154
expansion, 197, 200
expectation step, 245
expectation step in EM, 478

Expectation-Maximization,
see EM
explicit conﬁrmation, 454
expressiveness, of a
meaning
representation, 300
extended gloss overlap, 502
Extended Lesk, 502
extended Lesk, 502
extrinsic, 506
extrinsic evaluation, 43

F (for F-measure), 74, 234,
263
F-measure, 74, 234
in NER, 333
factoid question, 402
false negatives, 15
false positives, 15
fasttext, 129
FASTUS, 352
feature cutoff, 170
feature interactions, 87
feature selection, 79
information gain, 79
feature template, 282
feature templates, 87
part-of-speech tagging,
169
Federalist papers, 80
feed-forward network, 137
ﬁlled pause, 20
ﬁller, 20
ﬁnal lowering, 453
First Order Logic, see FOL
ﬁrst-order co-occurrence,

124

focus, 416
FOL, 296, 304
∃ (there exists), 306
∀ (for all), 306
=⇒ (implies), 309
∧ (and), 306, 309
¬ (not), 306, 309
∨ (or), 309
and veriﬁability, 304
constants, 305
expressiveness of, 300,
304
functions, 305
inference in, 304
terms, 304
variables, 305
fold (in cross-validation),
76
food in NLP
ice cream, 466
formal language, 198
forward algorithm, 468, 469

FORWARD ALGOR I THM ,

470
forward chaining, 310
forward composition, 216
forward trellis, 468
forward-backward
algorithm, 474, 479
backward probability in,
474

Subject Index

553

categorial, 214, 214
CCG, 214
checking, 223
combinatory categorial,
214
equivalence, 213
generative, 198
strong equivalence, 213
weak equivalence, 213
Grammar Rock, 151
grammatical function, 271
grammatical relation, 271
grammatical sentences, 198
greedy, 170
greedy RE patterns, 15
greeting, 154
grep, 11, 11, 34
ground, 448
grounding
ﬁve kinds of, 448

relation to inside-outside,
245

FORWARD -BACKWARD
A LGOR I THM, 478

Fosler, E., see
Fosler-Lussier, E.
fragment of word, 20
frame
semantic, 362
frame elements, 362
FrameNet, 362
frames, 431
free word order, 270
Freebase, 336
Frump, 354
fully qualiﬁed date
expressions, 347
fully-connected, 137
function word, 152, 173
functional grammar, 221
functions in FOL, 305

garden-path sentences, 265,
267
gaussian
prior on weights, 95
gazetteer, 331
General Inquirer, 71, 381
generalize, 94
generalized semantic role,

360

generation
of sentences to test a
CFG grammar, 197
template-based, 438
generative grammar, 198
generative lexicon, 514
generative model, 83
generative syntax, 221
generator, 196
genitive NP, 222
gerundive postmodiﬁer, 203
Gilbert and Sullivan, 327,
480
gloss, 497
Godzilla, speaker as, 368
gold labels, 73
The Gondoliers, 480
Good-Turing, 53
government and binding,
220
gradient, 90
Grammar
Constraint, 293
Construction, 220
Government and
Binding, 220
Head-Driven Phrase
Structure (HPSG),
211, 220
Lexical-Functional
(LFG), 220
Link, 293
Probabilistic Tree
Adjoining, 267
Tree Adjoining, 221
grammar
binary branching, 213

Hamilton, Alexander, 80
hanzi, 25
harmonic mean, 75, 234,

263

Hays, D., 293

head, 211, 271

ﬁnding, 211
in lexicalized grammar,
250
tag, 250
head tag, 250
Head-Driven Phrase
Structure Grammar
(HPSG), 211, 220
Heaps’ Law, 20
H EC TOR corpus, 505
held out, 43
held-out, 52
Herdan’s Law, 20
hidden, 158, 465
hidden layer, 137
as representation of
input, 138
hidden units, 137

HMM, 158, 465

deleted interpolation, 165
formal deﬁnition of, 158,
465
initial distribution, 158,
465
observation likelihood,
158, 465
observations, 158, 465
simplifying assumptions
for POS tagging,
160
states, 158, 465
transition probabilities,
158, 465
trigram POS tagging, 163
holonym, 496
homographs, 493
homonym, 493
homonymy, 493
homophones, 494
human parsing, 264

554

Subject Index

human sentence processing,
264, 264
Hungarian
part-of-speech tagging,
172
hyperarticulation, 453
hypernym, 104, 336, 496
and information content,
501
in Extended Lesk, 503
lexico-syntactic patterns
for, 336
Hyperparameter, 145
hyponym, 104, 496

IBM, 60
IBM Thomas J. Watson
Research Center, 60
ice cream, 466
IDF, 508
idf, 114
idf term weighting, 114
if then reasoning in FOL,
310
immediately dominates,

196

imperative sentence
structure, 200
implicit argument, 376
implicit conﬁrmation, 454
implied hierarchy
in description logics, 320
indeﬁnite article, 202
indirect speech acts, 452
inference, 300
in FOL, 310
inference-based learning,

291

327

inﬁnitives, 206
infoboxes, 335
information extraction (IE),

bootstrapping, 339
partial parsing for, 231
information gain, 79
for feature selection, 79
Information retrieval, 109
information-content word
similarity, 500
initiative, 432
mixed, 433
single, 432
system, 432
inner product, 111
inside-outside algorithm,
245, 266
instance checking, 319
intent determination, 434
intercept, 84
interjection, 154
intermediate semantic
representations, 298
internal rule in a CFG
parse, 251
interpersonal stance, 393
Interpolated Kneser-Ney
discounting, 53, 55
interpolation

in smoothing, 51
interpretable, 97
interpretation, 302
intonation, 453
intransitive verbs, 206
intrinsic evaluation, 43
inverse document
frequency, 508

IOB, 232, 330, 436

IOB tagging
for NER, 330
for temporal expressions,
345
slot ﬁlling, 436
Iolanthe, 480
IR
idf term weighting, 114
vector space model, 108
IS-A, 105
is-a, 336
ISO 8601, 346
iSRL, 376

Jaro-Winkler, 491
Jay, John, 80
Jiang-Conrath distance, 502
joint intention, 459
joint probability, 239

Katz backoff, 52
KBP, 354
KenLM, 56, 61
KL divergence, 371
KL-ONE, 323
Kleene *, 13
sneakiness of matching
zero things, 13
Kleene +, 13
Kneser-Ney discounting, 53
knowledge base, 297, 299
KRL, 323
Kullback-Leibler
divergence, 371

L1 regularization, 94
L2 regularization, 94
label bias, 171
labeled precision, 263
labeled recall, 263
lambda notation, 308
language generation, 438
language ID, 72
language id, 63
language model, 38
adaptation, 61
PCFG, 241
Laplace smoothing, 49
Laplace smoothing:for
PMI, 118
lasso regression, 94
latent semantic analysis,

128

LCS, 501
LDC, 24, 243
learning rate, 90
lemma, 20, 102
versus wordform, 20

lemmatization, 11
Lesk algorithm, 507
Corpus, 508
Extended, 502
Simpliﬁed, 507
letter-to-sound
for spell checking, 491
Levenshtein distance, 30
lexical
ambiguity resolution,
513
category, 196
database, 497
dependency, 237, 248
head, 267
semantics, 102
trigger, in IE, 345
lexical answer type, 416
lexical dependency, 248
lexical rule
in a CFG parse, 251
lexical sample task in
WSD, 504
Lexical-Functional
Grammar (LFG),
220
lexicalized grammar, 250
lexico-syntactic pattern,

336

lexicon, 196
likelihood, 66, 482
Lin similarity, 502
linear classiﬁers, 67
linear interpolation for
n-grams, 52
linearly separable, 135
Linguistic Data
Consortium, 24, 243
Link Grammar, 293
literal meaning, 296
LIWC, 71, 382
LM, 38
LOB corpus, 174
locative, 153
locative adverb, 153
log
why used for
probabilities, 43
log likelihood ratio, 390
log odds ratio, 390
log probabilities, 43, 43
logical connectives, 305
logical vocabulary, 301
logistic function, 84
logistic regression, 82
conditional maximum
likelihood
estimation, 88
learning in, 87
relation to neural
networks, 139
long-distance dependency,

208

traces in the Penn
Treebank, 208
wh-questions, 201
lookahead in RE, 19
loss, 87

lowest common subsumer,

501

LSI, see latent semantic
analysis
LSTM, 354
for NER, 332
for slot ﬁlling, 436
for SRL, 366
LUNAR, 421
Lunar, 322

M-step (maximization step)
in EM, 478
machine learning
for NER, 333
for WSD, 505
textbooks, 81, 100
macroaveraging, 75
Madison, James, 80
Manhattan distance
in L1 regularization, 94
manner adverb, 153
marker passing for WSD,
513
Markov, 40
assumption, 40
Markov assumption, 157,

464

Markov chain, 60, 157, 464
formal deﬁnition of, 158,
465
initial distribution, 158,
465
N-gram as, 158, 465
states, 158, 465
transition probabilities,
158, 465
Markov model, 40
formal deﬁnition of, 158,
465
history, 60
Marx, G., 223
mass nouns, 152
MaxEnt
Gaussian priors, 95
regularization, 95
maxent, 100
maximization step, 245
maximization step in EM,
478
maximum entropy, 99
maximum matching, 25
maximum spanning tree,

287

MaxMatch, 25
MCTest, 419
mean reciprocal rank, 418
meaning representation,
295
as set of symbols, 296
early uses, 322
languages, 297
meaning representation
languages, 295
mechanical indexing, 128
MEMM, 168
compared to HMM, 168

inference (decoding),
171
learning, 171
Viterbi decoding, 171
meronym, 496
meronymy, 496
MeSH (Medical Subject
Headings), 64, 504
Message Understanding
Conference, 352
metarule, 207
metonymy, 494
Micro-Planner, 322
microaveraging, 75
minibatch, 92
minimum edit distance, 30,
30, 31, 161, 471
example of, 33

M IN IMUM ED I T D I STANCE,

33
mixed initiative, 433
MLE
for n-grams, 40
for n-grams, intuition, 41
MLP, 137
modal verb, 154
model, 301
modiﬁed Kneser-Ney, 55
modus ponens, 310
Montague semantics, 323
morpheme, 26
Moses, Michelangelo statue
of, 422
most frequent sense, 507
MRR, 418
MUC, 352, 354
multi-label classiﬁcation,

multi-layer perceptrons,

multinomial classiﬁcation,

75

137

75

multinomial naive Bayes,
65
multinomial naive Bayes
classiﬁer, 65
multinominal logistic
regression, 95

N-best list, 435
N-gram
as Markov chain, 158,
465
n-gram, 38, 40
absolute discounting, 53
adaptation, 61
add-one smoothing, 49
as approximation, 40
as generators, 46
equation for, 40
example of, 42
for Shakespeare, 46
history of, 60
interpolation, 51
Katz backoff, 52
KenLM, 56, 61
Kneser-Ney discounting,

53

184

141

154

logprobs in, 43
normalizing, 41
parameter estimation, 41
sensitivity to corpus, 45
smoothing, 49
SRILM, 61
test set, 43
training set, 43
unknown words, 48
n-gram
tiling, 408
naive Bayes
multinomial, 65
simplifying assumptions,
66
naive Bayes assumption, 66
naive Bayes classiﬁer
use in text categorization,
65
named entity, 328
list of types, 329
recognition, 327, 329
named entity recognition,

names
and gazetteers, 331
census lists, 331
NarrativeQA, 419
negative log likelihood loss,

negative part-of-speech,

neo-Davidsonian, 312
NER, 327
neural nets, 61
neural networks
relation to logistic
regression, 139
newline character, 17
noisy channel model
for spelling, 481
invention of, 492
noisy-or, 340
NomBank, 361
Nominal, 196
non-capturing group, 18
non-ﬁnite postmodiﬁer, 203
non-greedy, 15
non-logical vocabulary, 301
non-terminal symbols, 196,
197
normal form, 213, 213
normalization
dates, 435
temporal, 346
word, 23
normalization of
probabilities, 40
normalized, 328
normalizing, 139
noun, 152
abstract, 152, 202
common, 152
count, 152
days of the week coded
as, 153
mass, 152, 202
proper, 152

noun phrase, 194
constituents, 196
NP, 196, 197
NP attachment, 246
null hypothesis, 77
numerals
as closed class, 153

object, syntactic
frequency of pronouns
as, 245
observation bias, 171
observation likelihood
role in forward, 469
role in Viterbi, 162, 472
OCR, 492
old information, and word
order, 246
on-line sentence-processing
experiments, 267
one sense per collocation,

511

one-hot vector, 146
one-of, 75
ontology, 316
OntoNotes, 514
OOV (out of vocabulary)
words, 48
OOV rate, 48
open class, 152
open information
extraction, 342
open vocabulary system
unknown words in, 48
operation list, 30
operator precedence, 14, 14
optical character
recognition, 492
optionality
of determiners, 202
use of ? in regular
expressions for, 12
ordinal number, 203
overﬁtting, 94

parallel distributed
processing, 150
parent annotation, 249
parse tree, 196, 199
parsed corpus, 266
parsing
ambiguity, 223
chunking, 231
CKY, 226, 243
CYK, see CKY
evaluation, 263
history, 235
partial, 231
probabilistic CKY, 243
relation to grammars,
200
shallow, 231
syntactic, 223
well-formed substring
table, 235
part-of-speech
adjective, 152
adverb, 152

Subject Index

555

as used in CFG, 196
closed class, 152, 153
greeting, 154
interjection, 154
negative, 154
noun, 152
open class, 152
particle, 153
subtle distinction
between verb and
noun, 152
usefulness of, 151
verb, 152
part-of-speech tagger

PART S, 174

TAGGIT, 174
Part-of-speech tagging, 156
part-of-speech tagging
ambiguity and, 156
amount of ambiguity in
Brown corpus, 156
and morphological
analysis, 172
capitalization, 167
feature templates, 169
for phrases, 155
history of, 174
Hungarian, 172
Stanford tagger, 172
state of the art, 157
Turkish, 172
unknown words, 167
part-whole, 496
partial parsing, 231
particle, 153
PART S tagger, 174
parts-of-speech, 151
passage retrieval, 405
passages, 405
path-length based
similarity, 500
pattern, regular expression,
11
PCFG, 238
for disambiguation, 239
lack of lexical sensitivity,
246
lexicalized, 267
parse probability, 239
poor independence
assumption, 245
rule probabilities, 238
use in language
modeling, 241
PDP, 150
Penn Treebank, 208
for statistical parsing,
243
POS tags for phrases,
155
tagging accuracy, 157
tagset, 154, 154
Penn Treebank
tokenization, 24
per-word entropy, 58
perceptron, 134
perplexity, 44, 59

556

Subject Index

as weighted average
branching factor, 44
deﬁned via
cross-entropy, 59
personal pronoun, 153
personality, 392
personalized page rank, 510
phones
in spell checking, 491
phrasal verb, 153
phrase-structure grammar,
195, 220
pipe, 14
The Pirates of Penzance,
327
planning
and speech acts, 460
shared plans, 459
plural, 202
Pointwise mutual
information, 116
politeness marker, 154
polysemy, 494
Porter stemmer, 26
POS, 151
possessive NP, 222
possessive pronoun, 153
postdeterminer, 203
postmodiﬁer, 203
postposed constructions,

195

Potts diagram, 388
PP, 197
PPMI, 116
pre-sequence, 451
precedence, 14
precedence, operator, 14
Precision, 74
precision, 234
in NER, 333
predeterminer, 204
predicate, 206
predicate-argument
relations, 206
preference semantics, 513
preposed constructions, 195
prepositional phrase, 203
attachment, 246
constituency, 197
preposing, 195
prepositions, 153
as closed class, 153
pretraining, 146
primitive decomposition,
372
principle of contrast, 103
prior probability, 66, 482
probabilistic CKY
algorithm, 242, 243
probabilistic parsing, 242
by humans, 264
productions, 195
progressive prompting, 455
projection layer, 147
Prolog, 311
prompt, 439
prompts, 438
pronoun, 153

and old information, 246
as closed class, 153
personal, 153
possessive, 153
wh-, 153
PropBank, 360
proper noun, 152
propositional meaning, 103
prosody, 453

PROTO -AGEN T, 360
PROTO - PAT I ENT, 360

pseudoword, 372
PTAG, 267
PTRANS, 373
punctuation
for numbers
cross-linguistically,
24
for sentence
segmentation, 29
part-of-speech tags, 154
stripping before
part-of-speech
tagging, 156
tokenization, 24
treated as words, 19
treated as words in LM,
47

QuAC, 419
qualia structure, 514
quantiﬁer
as part of speech, 203
semantics, 306
query
reformulation in QA, 404
question
classiﬁcation, 404
factoid, 402
question answering
evaluation, 418
factoid questions, 402
query reformulation in,

404

range, regular expression,

12

rapid reprompting, 455
RDF, 336
RDF triple, 336
RE
regular expression, 11
reading comprehension,

reading time, 264
real-word spelling errors,

409

480

Recall, 74
recall, 234
in NER, 333
recipe
meaning of, 295
reference point, 314
reformulation, 448
register in RE, 18
regression
lasso, 94
ridge, 95

regular expression, 11, 34
substitutions, 17
regularization, 94
rejection
conversation act, 455
relatedness, 104
relation extraction, 327
relative
temporal expression, 345
relative entropy, 371
relative frequency, 41
relative pronoun, 204
relexicalize, 457
ReLU, 133
reporting events, 349
representation learning, 101
Resnik similarity, 501
resolution for inference,

resolve, 156
response generation, 428
restrictive grammar, 438
restrictive relative clause,

311

204

ReVerb, 342
reversives, 103
rewrite, 196
Riau Indonesian, 152
ridge regression, 95
role-ﬁller extraction, 352
row vector, 109
rules
context-free, 195
context-free, expansion,
196, 200
context-free, sample, 197

S as start symbol in CFG,

196

SAE, 21
sampling
used in clustering, 512
saturated, 134
Sch ¨onkﬁnkelization, 308
“Schoolhouse Rock”, 151
SCISOR, 354
sclite package, 35
script
Schankian, 362
scripts, 351
second-order
co-occurrence, 124
seed pattern in IE, 339
seed tuples, 339
segmentation
Chinese word, 25
maximum matching, 25
sentence, 29
word, 23
selectional association, 371
selectional preference
strength, 370
selectional preferences
pseudowords for
evaluation, 372
selectional restriction, 368
representing with events,
369

violations in WSD, 370
semantic analysis, 296
semantic concordance, 505
semantic drift in IE, 340
semantic feature, 128
semantic ﬁeld, 104
semantic frame, 104
semantic grammars, 434
semantic network
for word sense
disambiguation, 513
semantic networks
origins, 322
semantic processing, 295
semantic relations in IE,

334

360

table, 335
semantic role, 357, 357,

Semantic role labeling, 364
semantics, 295
lexical, 102
sense
accuracy in WSD, 506
word, 493

SEN SEVA L

and WSD evaluation, 506
SEN SEVA L corpus, 505
sentence
segmentation, 29
sentence realization, 456
sentence segmentation, 11
sentence selection, 410
sentential complements,

205

sentiment, 105
origin of term, 397
sentiment analysis, 63
sentiment lexicons, 71
SentiWordNet, 387
sequence model, 157
SGNS, 119
Shakespeare
n-gram approximations
to, 46
shallow parse, 231
shared plans, 459
shift-reduce parsing, 275
SHRDLU, 322
side sequence, 451
sigmoid, 84, 132
similarity, 103
Simple Recurrent
Networks, 178
Simpliﬁed Lesk, 507
skip-gram, 119
slot ﬁlling, 354, 434
slots, 431
smoothing, 49, 49
absolute discounting, 53
add-one, 49
discounting, 49
for HMM POS tagging,
165
interpolation, 51
Katz backoff, 52
Kneser-Ney discounting,

53

Laplace, 49
linear interpolation, 52
snippets, 407
softmax, 95, 139
Soundex, 491
spam detection, 63, 71
span, 407
speech acts, 447
spell checking
pronunciation, 491
spelling correction
use of n-grams in, 37

S PEL L ING CORREC T ION
A LGOR I THM, 483,

491
spelling errors
context-dependent, 480
correction, EM, 484
detection, real words,
486
noisy channel model for
correction, 483
non-word, 480
real word, 480
split, 248
split and merge, 250
SQuAD, 409, 419
SRILM, 61
SRL, 364
Stacked RNNs, 186
Stanford tagger, 172
start symbol, 196
state
semantic representation
of, 311
stationary stochastic
process, 58
statistical parsing, 242
stative expressions, 315
stem, 26
Stemming, 11
stemming, 26
stop words, 68
strong equivalence of
grammars, 213
structural ambiguity, 223
stupid backoff, 56
subcategorization
and probabilistic
grammars, 237
tagsets for, 206
subcategorization frame,

206

examples, 206
subcategorize for, 206
subdialog, 451
subject, syntactic
frequency of pronouns
as, 245
in wh-questions, 201
subjectivity, 378, 397
substitutability, 220
substitution in TAG, 221
substitution operator
(regular
expressions), 17
subsumption, 317, 319
superordinate, 104, 496

Supertagging, 257
supertagging, 267
supervised machine
learning, 64
Switchboard, 155
Switchboard Corpus, 19
synonyms, 103, 496
synset, 497
syntactic categories, 151
syntactic disambiguation,

225

syntactic movement, 208
syntax, 194
origin of term, 151
system-initiative, 432

TAG, 221, 267
TAGGIT, 174
tagset
difference between Penn
Treebank and
Brown, 155
history of Penn
Treebank, 155
Penn Treebank, 154, 154
table of Penn Treebank
tags, 154
tanh, 133
target embedding, 122
Tay, 443
TBox, 316
technai, 151
telic eventualities, 316
template ﬁlling, 328, 351
template recognition, 351
template, in IE, 351
template-based generation,

438

temporal adverb, 153
temporal anchor, 348
temporal expression
absolute, 345
metaphor for, 315
recognition, 328
relative, 345
temporal expressions, 328
temporal logic, 313
temporal normalization,

346

temporal reasoning, 323
tense logic, 313
term
clustering, 513, 514
in FOL, 304
term frequency, 113
term-document matrix, 108
term-term matrix, 110
terminal symbol, 196
terminology
in description logics, 316
test set, 43
development, 44
how to choose, 44
text categorization, 63
bag of words assumption,
65
naive Bayes approach, 65
unknown words, 68

text normalization, 10
part-of-speech tagging,
155
tf-idf, 114
thematic grid, 359
thematic role, 357
and diathesis alternation,

examples of, 358
problems, 359
theme, 357
theme, as thematic role, 357
there, existential in English,

359

154

thesaurus, 513
time, representation of, 312
TimeBank, 349
tokenization, 10
sentence, 29
word, 23
tokens, word, 20
topic (information
structure), 246
topic models, 104
trace, 201, 208
training oracle, 280
training set, 43
cross-validation, 76
how to choose, 44
Transformations and
Discourse Analysis
Project (TDAP),
174
transition probability
role in forward, 469
role in Viterbi, 162, 472
transitive verbs, 206
TREC, 421
Tree Adjoining Grammar
(TAG), 221
adjunction in, 221
probabilistic, 267
substitution in, 221
treebank, 208, 243
trigram, 42
truth-conditional semantics,

303

Turkish
part-of-speech tagging,
172
turn correction ratio, 441
turns, 423
type raising, 216
typed dependency structure,

270

types
word, 20

ungrammatical sentences,

198

unique beginner, 498
unit production, 226
unit vector, 112
universal, 433
Universal Dependencies,

272

Unix, 11

<UNK>, 48

Subject Index

557

unknown words, 27
in n-grams, 48
in part-of-speech
tagging, 167
in text categorization, 68
user-centered design, 442
utterance, 19

V (vocabulary), 482
vagueness, 298
tests distinguishing from
ambiguity, 298
variable, 305
existentially quantiﬁed,
307
universally quantiﬁed,
307
variables in FOL, 305
vector, 108, 132
vector length, 111
vector semantics, 101, 106
vector space, 108
vector space model, 108
verb
copula, 154
modal, 154
phrasal, 153
verb alternations, 359
verb phrase, 196, 205
Verbs, 152
veriﬁability, 297
Viterbi algorithm, 31, 161,

471

backtrace in, 473
decoding in MEMM, 171
history of, 479

V I TERB I A LGOR I THM,

161, 472
voice user interface, 442
VoiceXML, 438
VP attachment, 246

weak equivalence of
grammars, 213
Web Ontology Language,

WebQuestions, 419
well-formed substring
table, 235
WFST, 235
wh-non-subject-question,

321

201

wh-phrase, 201, 201
wh-pronoun, 153
wh-subject-questions, 201
wh-word, 201
WikiQA, 419
wildcard, regular
expression, 13
Wizard-of-Oz system, 442
word
boundary, regular
expression notation,
14
closed class, 152
deﬁnition of, 19
fragment, 20
function, 152, 173

558

Subject Index

open class, 152
punctuation as, 19
tokens, 20
types, 20
word error rate, 26
word normalization, 23
word segmentation, 23
word sense, 493
word sense disambiguation,
504, see WSD
word sense induction, 511
word shape, 169, 331
word tokenization, 23

word-word matrix, 110
word2vec, 119
wordform, 20
and lemma, 102
versus lemma, 20
WordNet, 497, 497
world knowledge, 295
WSD, 504
AI-oriented efforts, 513
all-words task, 505
bootstrapping, 510, 514
decision tree approach,
514

evaluation of, 506
history, 513
history of, 514
lexical sample task, 504
neural network
approaches, 513
robust approach, 513
supervised machine
learning, 514
unsupervised machine
learning, 511
WSI, 511
WSJ, 155

X-bar schemata, 220

Yarowsky algorithm, 510
yes-no questions, 200, 452
yield, 240
Yonkers Racetrack, 57

zero-width, 19
zeros, 48
zeugma, 495

